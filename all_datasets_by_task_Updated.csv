Task,Dataset ID,Likes,Downloads,Last Modified,License,Models,Size of downloaded files,Size of downloaded files in bytes,Size of Parquet files,Size of Parquet files in bytes,Number of Rows,ArXiv Paper,ACL Paper,README file,README Quality Level,README Quality Score
Q&A,facebook/mlqa,42.0,1341.0,2024-01-18 11:09:06+00:00,cc-by-sa-3.0,0.0,4.15 GB,unknown,UNKNOWN,unknown,78058,https://arxiv.org/abs/1910.07475,none,"---
pretty_name: MLQA (MultiLingual Question Answering)
language:
- en
- de
- es
- ar
- zh
- vi
- hi
license:
- cc-by-sa-3.0
source_datasets:
- original
size_categories:
- 10K<n<100K
language_creators:
- crowdsourced
annotations_creators:
- crowdsourced
multilinguality:
- multilingual
task_categories:
- question-answering
task_ids:
- extractive-qa
paperswithcode_id: mlqa
dataset_info:
- config_name: mlqa-translate-train.ar
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: train
    num_bytes: 101227245
    num_examples: 78058
  - name: validation
    num_bytes: 13144332
    num_examples: 9512
  download_size: 63364123
  dataset_size: 114371577
- config_name: mlqa-translate-train.de
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: train
    num_bytes: 77996825
    num_examples: 80069
  - name: validation
    num_bytes: 10322113
    num_examples: 9927
  download_size: 63364123
  dataset_size: 88318938
- config_name: mlqa-translate-train.vi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: train
    num_bytes: 97387431
    num_examples: 84816
  - name: validation
    num_bytes: 12731112
    num_examples: 10356
  download_size: 63364123
  dataset_size: 110118543
- config_name: mlqa-translate-train.zh
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: train
    num_bytes: 55143547
    num_examples: 76285
  - name: validation
    num_bytes: 7418070
    num_examples: 9568
  download_size: 63364123
  dataset_size: 62561617
- config_name: mlqa-translate-train.es
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: train
    num_bytes: 80789653
    num_examples: 81810
  - name: validation
    num_bytes: 10718376
    num_examples: 10123
  download_size: 63364123
  dataset_size: 91508029
- config_name: mlqa-translate-train.hi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: train
    num_bytes: 168117671
    num_examples: 82451
  - name: validation
    num_bytes: 22422152
    num_examples: 10253
  download_size: 63364123
  dataset_size: 190539823
- config_name: mlqa-translate-test.ar
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 5484467
    num_examples: 5335
  download_size: 10075488
  dataset_size: 5484467
- config_name: mlqa-translate-test.de
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 3884332
    num_examples: 4517
  download_size: 10075488
  dataset_size: 3884332
- config_name: mlqa-translate-test.vi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 5998327
    num_examples: 5495
  download_size: 10075488
  dataset_size: 5998327
- config_name: mlqa-translate-test.zh
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 4831704
    num_examples: 5137
  download_size: 10075488
  dataset_size: 4831704
- config_name: mlqa-translate-test.es
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 3916758
    num_examples: 5253
  download_size: 10075488
  dataset_size: 3916758
- config_name: mlqa-translate-test.hi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 4608811
    num_examples: 4918
  download_size: 10075488
  dataset_size: 4608811
- config_name: mlqa.ar.ar
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 8216837
    num_examples: 5335
  - name: validation
    num_bytes: 808830
    num_examples: 517
  download_size: 75719050
  dataset_size: 9025667
- config_name: mlqa.ar.de
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 2132247
    num_examples: 1649
  - name: validation
    num_bytes: 358554
    num_examples: 207
  download_size: 75719050
  dataset_size: 2490801
- config_name: mlqa.ar.vi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 3235363
    num_examples: 2047
  - name: validation
    num_bytes: 283834
    num_examples: 163
  download_size: 75719050
  dataset_size: 3519197
- config_name: mlqa.ar.zh
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 3175660
    num_examples: 1912
  - name: validation
    num_bytes: 334016
    num_examples: 188
  download_size: 75719050
  dataset_size: 3509676
- config_name: mlqa.ar.en
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 8074057
    num_examples: 5335
  - name: validation
    num_bytes: 794775
    num_examples: 517
  download_size: 75719050
  dataset_size: 8868832
- config_name: mlqa.ar.es
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 2981237
    num_examples: 1978
  - name: validation
    num_bytes: 223188
    num_examples: 161
  download_size: 75719050
  dataset_size: 3204425
- config_name: mlqa.ar.hi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 2993225
    num_examples: 1831
  - name: validation
    num_bytes: 276727
    num_examples: 186
  download_size: 75719050
  dataset_size: 3269952
- config_name: mlqa.de.ar
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1587005
    num_examples: 1649
  - name: validation
    num_bytes: 195822
    num_examples: 207
  download_size: 75719050
  dataset_size: 1782827
- config_name: mlqa.de.de
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 4274496
    num_examples: 4517
  - name: validation
    num_bytes: 477366
    num_examples: 512
  download_size: 75719050
  dataset_size: 4751862
- config_name: mlqa.de.vi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1654540
    num_examples: 1675
  - name: validation
    num_bytes: 211985
    num_examples: 182
  download_size: 75719050
  dataset_size: 1866525
- config_name: mlqa.de.zh
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1645937
    num_examples: 1621
  - name: validation
    num_bytes: 180114
    num_examples: 190
  download_size: 75719050
  dataset_size: 1826051
- config_name: mlqa.de.en
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 4251153
    num_examples: 4517
  - name: validation
    num_bytes: 474863
    num_examples: 512
  download_size: 75719050
  dataset_size: 4726016
- config_name: mlqa.de.es
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1678176
    num_examples: 1776
  - name: validation
    num_bytes: 166193
    num_examples: 196
  download_size: 75719050
  dataset_size: 1844369
- config_name: mlqa.de.hi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1343983
    num_examples: 1430
  - name: validation
    num_bytes: 150679
    num_examples: 163
  download_size: 75719050
  dataset_size: 1494662
- config_name: mlqa.vi.ar
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 3164094
    num_examples: 2047
  - name: validation
    num_bytes: 226724
    num_examples: 163
  download_size: 75719050
  dataset_size: 3390818
- config_name: mlqa.vi.de
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 2189315
    num_examples: 1675
  - name: validation
    num_bytes: 272794
    num_examples: 182
  download_size: 75719050
  dataset_size: 2462109
- config_name: mlqa.vi.vi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 7807045
    num_examples: 5495
  - name: validation
    num_bytes: 715291
    num_examples: 511
  download_size: 75719050
  dataset_size: 8522336
- config_name: mlqa.vi.zh
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 2947458
    num_examples: 1943
  - name: validation
    num_bytes: 265154
    num_examples: 184
  download_size: 75719050
  dataset_size: 3212612
- config_name: mlqa.vi.en
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 7727204
    num_examples: 5495
  - name: validation
    num_bytes: 707925
    num_examples: 511
  download_size: 75719050
  dataset_size: 8435129
- config_name: mlqa.vi.es
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 2822481
    num_examples: 2018
  - name: validation
    num_bytes: 279235
    num_examples: 189
  download_size: 75719050
  dataset_size: 3101716
- config_name: mlqa.vi.hi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 2738045
    num_examples: 1947
  - name: validation
    num_bytes: 251470
    num_examples: 177
  download_size: 75719050
  dataset_size: 2989515
- config_name: mlqa.zh.ar
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1697005
    num_examples: 1912
  - name: validation
    num_bytes: 171743
    num_examples: 188
  download_size: 75719050
  dataset_size: 1868748
- config_name: mlqa.zh.de
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1356268
    num_examples: 1621
  - name: validation
    num_bytes: 170686
    num_examples: 190
  download_size: 75719050
  dataset_size: 1526954
- config_name: mlqa.zh.vi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1770535
    num_examples: 1943
  - name: validation
    num_bytes: 169651
    num_examples: 184
  download_size: 75719050
  dataset_size: 1940186
- config_name: mlqa.zh.zh
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 4324740
    num_examples: 5137
  - name: validation
    num_bytes: 433960
    num_examples: 504
  download_size: 75719050
  dataset_size: 4758700
- config_name: mlqa.zh.en
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 4353361
    num_examples: 5137
  - name: validation
    num_bytes: 437016
    num_examples: 504
  download_size: 75719050
  dataset_size: 4790377
- config_name: mlqa.zh.es
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1697983
    num_examples: 1947
  - name: validation
    num_bytes: 134693
    num_examples: 161
  download_size: 75719050
  dataset_size: 1832676
- config_name: mlqa.zh.hi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1547159
    num_examples: 1767
  - name: validation
    num_bytes: 180928
    num_examples: 189
  download_size: 75719050
  dataset_size: 1728087
- config_name: mlqa.en.ar
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 6641971
    num_examples: 5335
  - name: validation
    num_bytes: 621075
    num_examples: 517
  download_size: 75719050
  dataset_size: 7263046
- config_name: mlqa.en.de
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 4966262
    num_examples: 4517
  - name: validation
    num_bytes: 584725
    num_examples: 512
  download_size: 75719050
  dataset_size: 5550987
- config_name: mlqa.en.vi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 6958087
    num_examples: 5495
  - name: validation
    num_bytes: 631268
    num_examples: 511
  download_size: 75719050
  dataset_size: 7589355
- config_name: mlqa.en.zh
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 6441614
    num_examples: 5137
  - name: validation
    num_bytes: 598772
    num_examples: 504
  download_size: 75719050
  dataset_size: 7040386
- config_name: mlqa.en.en
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 13787522
    num_examples: 11590
  - name: validation
    num_bytes: 1307399
    num_examples: 1148
  download_size: 75719050
  dataset_size: 15094921
- config_name: mlqa.en.es
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 6074990
    num_examples: 5253
  - name: validation
    num_bytes: 545657
    num_examples: 500
  download_size: 75719050
  dataset_size: 6620647
- config_name: mlqa.en.hi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 6293785
    num_examples: 4918
  - name: validation
    num_bytes: 614223
    num_examples: 507
  download_size: 75719050
  dataset_size: 6908008
- config_name: mlqa.es.ar
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1696778
    num_examples: 1978
  - name: validation
    num_bytes: 145105
    num_examples: 161
  download_size: 75719050
  dataset_size: 1841883
- config_name: mlqa.es.de
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1361983
    num_examples: 1776
  - name: validation
    num_bytes: 139968
    num_examples: 196
  download_size: 75719050
  dataset_size: 1501951
- config_name: mlqa.es.vi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1707141
    num_examples: 2018
  - name: validation
    num_bytes: 172801
    num_examples: 189
  download_size: 75719050
  dataset_size: 1879942
- config_name: mlqa.es.zh
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1635294
    num_examples: 1947
  - name: validation
    num_bytes: 122829
    num_examples: 161
  download_size: 75719050
  dataset_size: 1758123
- config_name: mlqa.es.en
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 4249431
    num_examples: 5253
  - name: validation
    num_bytes: 408169
    num_examples: 500
  download_size: 75719050
  dataset_size: 4657600
- config_name: mlqa.es.es
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 4281273
    num_examples: 5253
  - name: validation
    num_bytes: 411196
    num_examples: 500
  download_size: 75719050
  dataset_size: 4692469
- config_name: mlqa.es.hi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 1489611
    num_examples: 1723
  - name: validation
    num_bytes: 178003
    num_examples: 187
  download_size: 75719050
  dataset_size: 1667614
- config_name: mlqa.hi.ar
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 4374373
    num_examples: 1831
  - name: validation
    num_bytes: 402817
    num_examples: 186
  download_size: 75719050
  dataset_size: 4777190
- config_name: mlqa.hi.de
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 2961556
    num_examples: 1430
  - name: validation
    num_bytes: 294325
    num_examples: 163
  download_size: 75719050
  dataset_size: 3255881
- config_name: mlqa.hi.vi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 4664436
    num_examples: 1947
  - name: validation
    num_bytes: 411654
    num_examples: 177
  download_size: 75719050
  dataset_size: 5076090
- config_name: mlqa.hi.zh
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 4281309
    num_examples: 1767
  - name: validation
    num_bytes: 416192
    num_examples: 189
  download_size: 75719050
  dataset_size: 4697501
- config_name: mlqa.hi.en
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 11245629
    num_examples: 4918
  - name: validation
    num_bytes: 1076115
    num_examples: 507
  download_size: 75719050
  dataset_size: 12321744
- config_name: mlqa.hi.es
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 3789337
    num_examples: 1723
  - name: validation
    num_bytes: 412469
    num_examples: 187
  download_size: 75719050
  dataset_size: 4201806
- config_name: mlqa.hi.hi
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 11606982
    num_examples: 4918
  - name: validation
    num_bytes: 1115055
    num_examples: 507
  download_size: 75719050
  dataset_size: 12722037
---

# Dataset Card for ""mlqa""

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [https://github.com/facebookresearch/MLQA](https://github.com/facebookresearch/MLQA)
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Size of downloaded dataset files:** 4.15 GB
- **Size of the generated dataset:** 910.01 MB
- **Total amount of disk used:** 5.06 GB

### Dataset Summary

    MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.
    MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,
    German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between
    4 different languages on average.

### Supported Tasks and Leaderboards

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Languages

MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese.

## Dataset Structure

### Data Instances

#### mlqa-translate-test.ar

- **Size of downloaded dataset files:** 10.08 MB
- **Size of the generated dataset:** 5.48 MB
- **Total amount of disk used:** 15.56 MB

An example of 'test' looks as follows.
```

```

#### mlqa-translate-test.de

- **Size of downloaded dataset files:** 10.08 MB
- **Size of the generated dataset:** 3.88 MB
- **Total amount of disk used:** 13.96 MB

An example of 'test' looks as follows.
```

```

#### mlqa-translate-test.es

- **Size of downloaded dataset files:** 10.08 MB
- **Size of the generated dataset:** 3.92 MB
- **Total amount of disk used:** 13.99 MB

An example of 'test' looks as follows.
```

```

#### mlqa-translate-test.hi

- **Size of downloaded dataset files:** 10.08 MB
- **Size of the generated dataset:** 4.61 MB
- **Total amount of disk used:** 14.68 MB

An example of 'test' looks as follows.
```

```

#### mlqa-translate-test.vi

- **Size of downloaded dataset files:** 10.08 MB
- **Size of the generated dataset:** 6.00 MB
- **Total amount of disk used:** 16.07 MB

An example of 'test' looks as follows.
```

```

### Data Fields

The data fields are the same among all splits.

#### mlqa-translate-test.ar
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `answer_start`: a `int32` feature.
  - `text`: a `string` feature.
- `id`: a `string` feature.

#### mlqa-translate-test.de
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `answer_start`: a `int32` feature.
  - `text`: a `string` feature.
- `id`: a `string` feature.

#### mlqa-translate-test.es
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `answer_start`: a `int32` feature.
  - `text`: a `string` feature.
- `id`: a `string` feature.

#### mlqa-translate-test.hi
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `answer_start`: a `int32` feature.
  - `text`: a `string` feature.
- `id`: a `string` feature.

#### mlqa-translate-test.vi
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `answer_start`: a `int32` feature.
  - `text`: a `string` feature.
- `id`: a `string` feature.

### Data Splits

|         name         |test|
|----------------------|---:|
|mlqa-translate-test.ar|5335|
|mlqa-translate-test.de|4517|
|mlqa-translate-test.es|5253|
|mlqa-translate-test.hi|4918|
|mlqa-translate-test.vi|5495|

## Dataset Creation

### Curation Rationale

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the source language producers?

[More Information Needed](https://github.com/huggingface/datasets/blob/ma",High,5.0
Q&A,google-research-datasets/tydiqa,32.0,1386.0,2024-08-08 05:57:11+00:00,apache-2.0,4.0,2.94 GB,3156800962.56,2.94 GB,3156800962.56,240544,none,none,"---
annotations_creators:
- crowdsourced
language_creators:
- crowdsourced
language:
- ar
- bn
- en
- fi
- id
- ja
- ko
- ru
- sw
- te
- th
license:
- apache-2.0
multilinguality:
- multilingual
size_categories:
- unknown
source_datasets:
- extended|wikipedia
task_categories:
- question-answering
task_ids:
- extractive-qa
paperswithcode_id: tydi-qa
pretty_name: TyDi QA
dataset_info:
- config_name: primary_task
  features:
  - name: passage_answer_candidates
    sequence:
    - name: plaintext_start_byte
      dtype: int32
    - name: plaintext_end_byte
      dtype: int32
  - name: question_text
    dtype: string
  - name: document_title
    dtype: string
  - name: language
    dtype: string
  - name: annotations
    sequence:
    - name: passage_answer_candidate_index
      dtype: int32
    - name: minimal_answers_start_byte
      dtype: int32
    - name: minimal_answers_end_byte
      dtype: int32
    - name: yes_no_answer
      dtype: string
  - name: document_plaintext
    dtype: string
  - name: document_url
    dtype: string
  splits:
  - name: train
    num_bytes: 5550573801
    num_examples: 166916
  - name: validation
    num_bytes: 484380347
    num_examples: 18670
  download_size: 2912112378
  dataset_size: 6034954148
- config_name: secondary_task
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: train
    num_bytes: 52948467
    num_examples: 49881
  - name: validation
    num_bytes: 5006433
    num_examples: 5077
  download_size: 29402238
  dataset_size: 57954900
configs:
- config_name: primary_task
  data_files:
  - split: train
    path: primary_task/train-*
  - split: validation
    path: primary_task/validation-*
- config_name: secondary_task
  data_files:
  - split: train
    path: secondary_task/train-*
  - split: validation
    path: secondary_task/validation-*
---

# Dataset Card for ""tydiqa""

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [https://github.com/google-research-datasets/tydiqa](https://github.com/google-research-datasets/tydiqa)
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Size of downloaded dataset files:** 3.91 GB
- **Size of the generated dataset:** 6.10 GB
- **Total amount of disk used:** 10.00 GB

### Dataset Summary

TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.
The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language
expresses -- such that we expect models performing well on this set to generalize across a large number of the languages
in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic
information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but
don’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without
the use of translation (unlike MLQA and XQuAD).

### Supported Tasks and Leaderboards

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Languages

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Dataset Structure

### Data Instances

#### primary_task

- **Size of downloaded dataset files:** 1.95 GB
- **Size of the generated dataset:** 6.04 GB
- **Total amount of disk used:** 7.99 GB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""annotations"": {
        ""minimal_answers_end_byte"": [-1, -1, -1],
        ""minimal_answers_start_byte"": [-1, -1, -1],
        ""passage_answer_candidate_index"": [-1, -1, -1],
        ""yes_no_answer"": [""NONE"", ""NONE"", ""NONE""]
    },
    ""document_plaintext"": ""\""\\nรองศาสตราจารย์[1] หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร  (22 กันยายน 2495 -) ผู้ว่าราชการกรุงเทพมหานครคนที่ 15 อดีตรองหัวหน้าพรรคปร..."",
    ""document_title"": ""หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร"",
    ""document_url"": ""\""https://th.wikipedia.org/wiki/%E0%B8%AB%E0%B8%A1%E0%B9%88%E0%B8%AD%E0%B8%A1%E0%B8%A3%E0%B8%B2%E0%B8%8A%E0%B8%A7%E0%B8%87%E0%B8%..."",
    ""language"": ""thai"",
    ""passage_answer_candidates"": ""{\""plaintext_end_byte\"": [494, 1779, 2931, 3904, 4506, 5588, 6383, 7122, 8224, 9375, 10473, 12563, 15134, 17765, 19863, 21902, 229..."",
    ""question_text"": ""\""หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร เรียนจบจากที่ไหน ?\""...""
}
```

#### secondary_task

- **Size of downloaded dataset files:** 1.95 GB
- **Size of the generated dataset:** 58.03 MB
- **Total amount of disk used:** 2.01 GB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [394],
        ""text"": [""بطولتين""]
    },
    ""context"": ""\""أقيمت البطولة 21 مرة، شارك في النهائيات 78 دولة، وعدد الفرق التي فازت بالبطولة حتى الآن 8 فرق، ويعد المنتخب البرازيلي الأكثر تت..."",
    ""id"": ""arabic-2387335860751143628-1"",
    ""question"": ""\""كم عدد مرات فوز الأوروغواي ببطولة كاس العالم لكرو القدم؟\""..."",
    ""title"": ""قائمة نهائيات كأس العالم""
}
```

### Data Fields

The data fields are the same among all splits.

#### primary_task
- `passage_answer_candidates`: a dictionary feature containing:
  - `plaintext_start_byte`: a `int32` feature.
  - `plaintext_end_byte`: a `int32` feature.
- `question_text`: a `string` feature.
- `document_title`: a `string` feature.
- `language`: a `string` feature.
- `annotations`: a dictionary feature containing:
  - `passage_answer_candidate_index`: a `int32` feature.
  - `minimal_answers_start_byte`: a `int32` feature.
  - `minimal_answers_end_byte`: a `int32` feature.
  - `yes_no_answer`: a `string` feature.
- `document_plaintext`: a `string` feature.
- `document_url`: a `string` feature.

#### secondary_task
- `id`: a `string` feature.
- `title`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

### Data Splits

| name           |  train | validation |
| -------------- | -----: | ---------: |
| primary_task   | 166916 |      18670 |
| secondary_task |  49881 |       5077 |

## Dataset Creation

### Curation Rationale

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the source language producers?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Annotations

#### Annotation process

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the annotators?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Personal and Sensitive Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Discussion of Biases

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Other Known Limitations

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Additional Information

### Dataset Curators

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Licensing Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Citation Information

```
@article{tydiqa,
title   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},
author  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki}
year    = {2020},
journal = {Transactions of the Association for Computational Linguistics}
}

```


### Contributions

Thanks to [@thomwolf](https://github.com/thomwolf), [@albertvillanova](https://github.com/albertvillanova), [@lewtun](https://github.com/lewtun), [@patrickvonplaten](https://github.com/patrickvonplaten) for adding this dataset.",High,5.0
Q&A,facebook/belebele,113.0,12822.0,2024-08-12 22:18:08+00:00,cc-by-sa-4.0,211.0,225 MB,235929600.0,44.1 MB,46242201.6,109800,none,"['https://aclanthology.org/2024.acl-long.44"",', 'https://aclanthology.org/S19-1012/),']","---
configs:
- config_name: acm_Arab
  data_files:
    - split: test
      path: data/acm_Arab.jsonl
- config_name: arz_Arab
  data_files:
    - split: test
      path: data/arz_Arab.jsonl
- config_name: ceb_Latn
  data_files:
    - split: test
      path: data/ceb_Latn.jsonl
- config_name: fin_Latn
  data_files:
    - split: test
      path: data/fin_Latn.jsonl
- config_name: hin_Deva
  data_files:
    - split: test
      path: data/hin_Deva.jsonl
- config_name: ita_Latn
  data_files:
    - split: test
      path: data/ita_Latn.jsonl
- config_name: khm_Khmr
  data_files:
    - split: test
      path: data/khm_Khmr.jsonl
- config_name: lvs_Latn
  data_files:
    - split: test
      path: data/lvs_Latn.jsonl
- config_name: npi_Deva
  data_files:
    - split: test
      path: data/npi_Deva.jsonl
- config_name: pol_Latn
  data_files:
    - split: test
      path: data/pol_Latn.jsonl
- config_name: slv_Latn
  data_files:
    - split: test
      path: data/slv_Latn.jsonl
- config_name: swe_Latn
  data_files:
    - split: test
      path: data/swe_Latn.jsonl
- config_name: tso_Latn
  data_files:
    - split: test
      path: data/tso_Latn.jsonl
- config_name: xho_Latn
  data_files:
    - split: test
      path: data/xho_Latn.jsonl
- config_name: afr_Latn
  data_files:
    - split: test
      path: data/afr_Latn.jsonl
- config_name: asm_Beng
  data_files:
    - split: test
      path: data/asm_Beng.jsonl
- config_name: ces_Latn
  data_files:
    - split: test
      path: data/ces_Latn.jsonl
- config_name: fra_Latn
  data_files:
    - split: test
      path: data/fra_Latn.jsonl
- config_name: hin_Latn
  data_files:
    - split: test
      path: data/hin_Latn.jsonl
- config_name: jav_Latn
  data_files:
    - split: test
      path: data/jav_Latn.jsonl
- config_name: kin_Latn
  data_files:
    - split: test
      path: data/kin_Latn.jsonl
- config_name: mal_Mlym
  data_files:
    - split: test
      path: data/mal_Mlym.jsonl
- config_name: npi_Latn
  data_files:
    - split: test
      path: data/npi_Latn.jsonl
- config_name: por_Latn
  data_files:
    - split: test
      path: data/por_Latn.jsonl
- config_name: sna_Latn
  data_files:
    - split: test
      path: data/sna_Latn.jsonl
- config_name: swh_Latn
  data_files:
    - split: test
      path: data/swh_Latn.jsonl
- config_name: tur_Latn
  data_files:
    - split: test
      path: data/tur_Latn.jsonl
- config_name: yor_Latn
  data_files:
    - split: test
      path: data/yor_Latn.jsonl
- config_name: als_Latn
  data_files:
    - split: test
      path: data/als_Latn.jsonl
- config_name: azj_Latn
  data_files:
    - split: test
      path: data/azj_Latn.jsonl
- config_name: ckb_Arab
  data_files:
    - split: test
      path: data/ckb_Arab.jsonl
- config_name: fuv_Latn
  data_files:
    - split: test
      path: data/fuv_Latn.jsonl
- config_name: hrv_Latn
  data_files:
    - split: test
      path: data/hrv_Latn.jsonl
- config_name: jpn_Jpan
  data_files:
    - split: test
      path: data/jpn_Jpan.jsonl
- config_name: kir_Cyrl
  data_files:
    - split: test
      path: data/kir_Cyrl.jsonl
- config_name: mar_Deva
  data_files:
    - split: test
      path: data/mar_Deva.jsonl
- config_name: nso_Latn
  data_files:
    - split: test
      path: data/nso_Latn.jsonl
- config_name: snd_Arab
  data_files:
    - split: test
      path: data/snd_Arab.jsonl
- config_name: tam_Taml
  data_files:
    - split: test
      path: data/tam_Taml.jsonl
- config_name: ukr_Cyrl
  data_files:
    - split: test
      path: data/ukr_Cyrl.jsonl
- config_name: zho_Hans
  data_files:
    - split: test
      path: data/zho_Hans.jsonl
- config_name: amh_Ethi
  data_files:
    - split: test
      path: data/amh_Ethi.jsonl
- config_name: bam_Latn
  data_files:
    - split: test
      path: data/bam_Latn.jsonl
- config_name: dan_Latn
  data_files:
    - split: test
      path: data/dan_Latn.jsonl
- config_name: gaz_Latn
  data_files:
    - split: test
      path: data/gaz_Latn.jsonl
- config_name: hun_Latn
  data_files:
    - split: test
      path: data/hun_Latn.jsonl
- config_name: kac_Latn
  data_files:
    - split: test
      path: data/kac_Latn.jsonl
- config_name: kor_Hang
  data_files:
    - split: test
      path: data/kor_Hang.jsonl
- config_name: mkd_Cyrl
  data_files:
    - split: test
      path: data/mkd_Cyrl.jsonl
- config_name: nya_Latn
  data_files:
    - split: test
      path: data/nya_Latn.jsonl
- config_name: ron_Latn
  data_files:
    - split: test
      path: data/ron_Latn.jsonl
- config_name: som_Latn
  data_files:
    - split: test
      path: data/som_Latn.jsonl
- config_name: tel_Telu
  data_files:
    - split: test
      path: data/tel_Telu.jsonl
- config_name: urd_Arab
  data_files:
    - split: test
      path: data/urd_Arab.jsonl
- config_name: zho_Hant
  data_files:
    - split: test
      path: data/zho_Hant.jsonl
- config_name: apc_Arab
  data_files:
    - split: test
      path: data/apc_Arab.jsonl
- config_name: ben_Beng
  data_files:
    - split: test
      path: data/ben_Beng.jsonl
- config_name: deu_Latn
  data_files:
    - split: test
      path: data/deu_Latn.jsonl
- config_name: grn_Latn
  data_files:
    - split: test
      path: data/grn_Latn.jsonl
- config_name: hye_Armn
  data_files:
    - split: test
      path: data/hye_Armn.jsonl
- config_name: kan_Knda
  data_files:
    - split: test
      path: data/kan_Knda.jsonl
- config_name: lao_Laoo
  data_files:
    - split: test
      path: data/lao_Laoo.jsonl
- config_name: mlt_Latn
  data_files:
    - split: test
      path: data/mlt_Latn.jsonl
- config_name: ory_Orya
  data_files:
    - split: test
      path: data/ory_Orya.jsonl
- config_name: rus_Cyrl
  data_files:
    - split: test
      path: data/rus_Cyrl.jsonl
- config_name: sot_Latn
  data_files:
    - split: test
      path: data/sot_Latn.jsonl
- config_name: tgk_Cyrl
  data_files:
    - split: test
      path: data/tgk_Cyrl.jsonl
- config_name: urd_Latn
  data_files:
    - split: test
      path: data/urd_Latn.jsonl
- config_name: zsm_Latn
  data_files:
    - split: test
      path: data/zsm_Latn.jsonl
- config_name: arb_Arab
  data_files:
    - split: test
      path: data/arb_Arab.jsonl
- config_name: ben_Latn
  data_files:
    - split: test
      path: data/ben_Latn.jsonl
- config_name: ell_Grek
  data_files:
    - split: test
      path: data/ell_Grek.jsonl
- config_name: guj_Gujr
  data_files:
    - split: test
      path: data/guj_Gujr.jsonl
- config_name: ibo_Latn
  data_files:
    - split: test
      path: data/ibo_Latn.jsonl
- config_name: kat_Geor
  data_files:
    - split: test
      path: data/kat_Geor.jsonl
- config_name: lin_Latn
  data_files:
    - split: test
      path: data/lin_Latn.jsonl
- config_name: mri_Latn
  data_files:
    - split: test
      path: data/mri_Latn.jsonl
- config_name: pan_Guru
  data_files:
    - split: test
      path: data/pan_Guru.jsonl
- config_name: shn_Mymr
  data_files:
    - split: test
      path: data/shn_Mymr.jsonl
- config_name: spa_Latn
  data_files:
    - split: test
      path: data/spa_Latn.jsonl
- config_name: tgl_Latn
  data_files:
    - split: test
      path: data/tgl_Latn.jsonl
- config_name: uzn_Latn
  data_files:
    - split: test
      path: data/uzn_Latn.jsonl
- config_name: zul_Latn
  data_files:
    - split: test
      path: data/zul_Latn.jsonl
- config_name: arb_Latn
  data_files:
    - split: test
      path: data/arb_Latn.jsonl
- config_name: bod_Tibt
  data_files:
    - split: test
      path: data/bod_Tibt.jsonl
- config_name: eng_Latn
  data_files:
    - split: test
      path: data/eng_Latn.jsonl
- config_name: hat_Latn
  data_files:
    - split: test
      path: data/hat_Latn.jsonl
- config_name: ilo_Latn
  data_files:
    - split: test
      path: data/ilo_Latn.jsonl
- config_name: kaz_Cyrl
  data_files:
    - split: test
      path: data/kaz_Cyrl.jsonl
- config_name: lit_Latn
  data_files:
    - split: test
      path: data/lit_Latn.jsonl
- config_name: mya_Mymr
  data_files:
    - split: test
      path: data/mya_Mymr.jsonl
- config_name: pbt_Arab
  data_files:
    - split: test
      path: data/pbt_Arab.jsonl
- config_name: sin_Latn
  data_files:
    - split: test
      path: data/sin_Latn.jsonl
- config_name: srp_Cyrl
  data_files:
    - split: test
      path: data/srp_Cyrl.jsonl
- config_name: tha_Thai
  data_files:
    - split: test
      path: data/tha_Thai.jsonl
- config_name: vie_Latn
  data_files:
    - split: test
      path: data/vie_Latn.jsonl
- config_name: ars_Arab
  data_files:
    - split: test
      path: data/ars_Arab.jsonl
- config_name: bul_Cyrl
  data_files:
    - split: test
      path: data/bul_Cyrl.jsonl
- config_name: est_Latn
  data_files:
    - split: test
      path: data/est_Latn.jsonl
- config_name: hau_Latn
  data_files:
    - split: test
      path: data/hau_Latn.jsonl
- config_name: ind_Latn
  data_files:
    - split: test
      path: data/ind_Latn.jsonl
- config_name: kea_Latn
  data_files:
    - split: test
      path: data/kea_Latn.jsonl
- config_name: lug_Latn
  data_files:
    - split: test
      path: data/lug_Latn.jsonl
- config_name: nld_Latn
  data_files:
    - split: test
      path: data/nld_Latn.jsonl
- config_name: pes_Arab
  data_files:
    - split: test
      path: data/pes_Arab.jsonl
- config_name: sin_Sinh
  data_files:
    - split: test
      path: data/sin_Sinh.jsonl
- config_name: ssw_Latn
  data_files:
    - split: test
      path: data/ssw_Latn.jsonl
- config_name: tir_Ethi
  data_files:
    - split: test
      path: data/tir_Ethi.jsonl
- config_name: war_Latn
  data_files:
    - split: test
      path: data/war_Latn.jsonl
- config_name: ary_Arab
  data_files:
    - split: test
      path: data/ary_Arab.jsonl
- config_name: cat_Latn
  data_files:
    - split: test
      path: data/cat_Latn.jsonl
- config_name: eus_Latn
  data_files:
    - split: test
      path: data/eus_Latn.jsonl
- config_name: heb_Hebr
  data_files:
    - split: test
      path: data/heb_Hebr.jsonl
- config_name: isl_Latn
  data_files:
    - split: test
      path: data/isl_Latn.jsonl
- config_name: khk_Cyrl
  data_files:
    - split: test
      path: data/khk_Cyrl.jsonl
- config_name: luo_Latn
  data_files:
    - split: test
      path: data/luo_Latn.jsonl
- config_name: nob_Latn
  data_files:
    - split: test
      path: data/nob_Latn.jsonl
- config_name: plt_Latn
  data_files:
    - split: test
      path: data/plt_Latn.jsonl
- config_name: slk_Latn
  data_files:
    - split: test
      path: data/slk_Latn.jsonl
- config_name: sun_Latn
  data_files:
    - split: test
      path: data/sun_Latn.jsonl
- config_name: tsn_Latn
  data_files:
    - split: test
      path: data/tsn_Latn.jsonl
- config_name: wol_Latn
  data_files:
    - split: test
      path: data/wol_Latn.jsonl

license: cc-by-sa-4.0
task_categories:
- question-answering
- zero-shot-classification
- text-classification
- multiple-choice
language:
- af
- am
- ar
- az
- as
- bm
- bn
- bo
- bg
- ca
- cs
- ku
- da
- de
- el
- en
- es
- et
- eu
- fi
- fr
- ff
- om
- gu
- gn
- ht
- ha
- he
- hi
- hr
- hu
- hy
- ig
- id
- it
- is
- jv
- ja
- ka
- kn
- kk
- mn
- km
- rw
- ky
- ko
- lo
- ln
- lt
- lg
- lv
- ml
- mr
- mk
- mt
- mi
- my
- nl
- 'no'
- ne
- ny
- or
- pa
- ps
- fa
- mg
- pl
- pt
- ro
- ru
- sn
- si
- sl
- sv
- sk
- sd
- sw
- ta
- te
- tg
- tl
- th
- ti
- tn
- ts
- tr
- uk
- ur
- uz
- vi
- wo
- xh
- yo
- zh
- ms
- zu
pretty_name: Belebele
size_categories:
- 100K<n<1M
---


# The Belebele Benchmark for Massively Multilingual NLU Evaluation

Belebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. This dataset enables the evaluation of mono- and multi-lingual models in high-, medium-, and low-resource languages. Each question has four multiple-choice answers and is linked to a short passage from the [FLORES-200](https://github.com/facebookresearch/flores/tree/main/flores200) dataset. The human annotation procedure was carefully curated to create questions that discriminate between different levels of generalizable language comprehension and is reinforced by extensive quality checks. While all questions directly relate to the passage, the English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. Belebele opens up new avenues for evaluating and analyzing the multilingual abilities of language models and NLP systems.

Please refer to our paper for more details, presented at ACL 2024: [The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants](https://ai.meta.com/research/publications/the-belebele-benchmark-a-parallel-reading-comprehension-dataset-in-122-language-variants/).
Or get more details at https://github.com/facebookresearch/belebele

## Citation

If you use this data in your work, please cite:

```bibtex
@inproceedings{bandarkar-etal-2024-belebele,
    title = ""The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants"",
    author = ""Bandarkar, Lucas  and
      Liang, Davis  and
      Muller, Benjamin  and
      Artetxe, Mikel  and
      Shukla, Satya Narayan  and
      Husa, Donald  and
      Goyal, Naman  and
      Krishnan, Abhinandan  and
      Zettlemoyer, Luke  and
      Khabsa, Madian"",
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand and virtual meeting"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.acl-long.44"",
    pages = ""749--775"",
}

```

## Composition

- 900 questions per language variant
- 488 distinct passages, there are 1-2 associated questions for each.
- For each question, there is 4 multiple-choice answers, exactly 1 of which is correct.
- 122 language/language variants (including English).
- 900 x 122 = 109,800 total questions.

## Further Stats

- 122 language variants, but 115 distinct languages (ignoring scripts)
- 27 language families
- 29 scripts
- Avg. words per passage = 79.1 (std = 26.2)
- Avg. sentences per passage = 4.1 (std = 1.4)
- Avg. words per question = 12.9(std = 4.0)
- Avg. words per answer = 4.2 (std = 2.9)

## Pausible Evaluation Settings

Thanks to the parallel nature of the dataset and the simplicity of the task, there are many possible settings in which we can evaluate language models. In all evaluation settings, the metric of interest is simple accuracy (# correct / total).

Evaluating models on Belebele in English can be done via finetuning, few-shot, or zero-shot. For other target languages, we propose the incomprehensive list of evaluation settings below. Settings that are compatible with evaluating non-English models (monolingual or cross-lingual) are denoted with `^`.

#### No finetuning
- **Zero-shot with natural language instructions (English instructions)**
    - For chat-finetuned models, we give it English instructions for the task and the sample in the target language in the same input.
    - For our experiments, we instruct the model to provide the letter `A`, `B`, `C`, or `D`. We perform post-processing steps and accept answers predicted as e.g. `(A)` instead of `A`. We sometimes additionally remove the prefix `The correct answer is` for predictions that do not start with one of the four accepted answers.
    - Sample instructions can be found at the [dataset github repo](https://github.com/facebookresearch/belebele).
- **Zero-shot with natural language instructions (translated instructions)** ^
    - Same as above, except the instructions are translated to the target language so that the instructions and samples are in the same language. The instructions can be human or machine-translated.
- **Few-shot in-context learning (English examples)**
    - A few samples (e.g. 5) are taken from the English training set (see below) and prompted to the model. Then, the model is evaluated with the same template but with the passages, questions, and answers in the target language.
    - For our experiments, we use the template: ```P: <passage> \n Q: <question> \n A: <mc answer 1> \n B: <mc answer 2> \n  C: <mc answer 3> \n  D: <mc answer 4> \n  Answer: <Correct answer letter>```. We perform prediction by picking the answer within `[A, B, C, D]` that has the highest probability relatively to the others.
- **Few-shot in-context learning (translated examples)** ^
    - Same as above, except the samples from the training set are translated to the target language so that the examples and evaluation data are in the same language. The training samples can be human or machine-translated.


#### With finetuning
- **English finetune & multilingual evaluation**
    - The model is finetuned to the task using the English training set, probably with a sequence classification head. Then the model is evaluated in all the target languages individually. For results presented in the paper we used [the HuggingFace library](https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice).
- **English finetune & cross-lingual evaluation**
    - Same as above, except the model is evaluated in a cross-lingual setting, where for each question, the passage & answers could be provided in a different language. For example, passage could be in language `x`, question in language `y`, and answers in language `z`.
- **Translate-train** ^
    - For each target language, the model is individually finetuned on training samples that have been machine-translated from English to that language. Each model is then evaluated in the respective target language.
- **Translate-train-all**
    - Similar to above, except here the model is trained on translated samples from all target languages at once. The single finetuned model is then evaluated on all target languages.
- **Translate-train-all & cross-lingual evaluation**
    - Same as above, except the single finetuned model is evaluated in a cross-lingual setting, where for each question, the passage & answers could be provided in a different language.
- **Translate-test**
    - The model is finetuned using the English training data and then the evaluation dataset is machine-translated to English and evaluated on the English.
    - This setting is primarily a reflection of the quality of the machine translation system, but is useful for comparison to multilingual models.

In addition, there are 83 additional languages in FLORES-200 for which questions were not translated for Belebele. Since the passages exist in those target languages, machine-translating the questions & answers may enable decent evaluation of machine reading comprehension in those languages.

## Training Set

As discussed in the paper, we also provide an assembled training set consisting of samples at the [github repo](https://github.com/facebookresearch/belebele). 

The Belebele dataset is intended to be used only as a test set, and not for training or validation. Therefore, for models that require additional task-specific training, we instead propose using an assembled training set consisting of samples from pre-existing multiple-choice QA datasets in English. We considered diverse datasets, and determine the most compatible to be [RACE](https://www.cs.cmu.edu/~glai1/data/race/), [SciQ](https://allenai.org/data/sciq), [MultiRC](https://cogcomp.seas.upenn.edu/multirc/), [MCTest](https://mattr1.github.io/mctest/), [MCScript2.0](https://aclanthology.org/S19-1012/), and [ReClor](https://whyu.me/reclor/).

For each of the six datasets, we unpack and restructure the passages and questions from their respective formats. We then filter out less suitable samples (e.g. questions with multiple correct answers). In the end, the dataset comprises 67.5k training samples and 3.7k development samples, more than half of which are from RACE. We provide a script (`assemble_training_set.py`) to reconstruct this dataset for anyone to perform task finetuning.

Since the training set is a joint sample of other datasets, it is governed by a different license. We do not claim any of that work or datasets to be our own. See the Licenses section in the README of https://github.com/facebookresearch/belebele .

## Languages in Belebele

FLORES-200 Code | English Name | Script | Family
---|---|---|---
acm_Arab | Mesopotamian Arabic | Arab | Afro-Asiatic
afr_Latn | Afrikaans | Latn | Germanic
als_Latn | Tosk Albanian | Latn | Paleo-Balkanic
amh_Ethi | Amharic | Ethi | Afro-Asiatic
apc_Arab | North Levantine Arabic | Arab | Afro-Asiatic
arb_Arab | Modern Standard Arabic | Arab | Afro-Asiatic
arb_Latn | Modern Standard Arabic (Romanized) | Latn | Afro-Asiatic
ars_Arab | Najdi Arabic | Arab | Afro-Asiatic
ary_arab | Moroccan Arabic | Arab | Afro-Asiatic
arz_Arab | Egyptian Arabic | Arab | Afro-Asiatic
asm_Beng | Assamese | Beng | Indo-Aryan
azj_Latn | North Azerbaijani | Latn | Turkic
bam_Latn | Bambara | Latn | Mande
ben_Beng | Bengali | Beng | Indo-Aryan
ben_Latn | Bengali (Romanized) | Latn | Indo-Aryan
bod_Tibt | Standard Tibetan | Tibt | Sino-Tibetan
bul_Cyrl | Bulgarian | Cyrl | Balto-Slavic
cat_Latn | Catalan | Latn | Romance
ceb_Latn | Cebuano | Latn | Austronesian
ces_Latn | Czech | Latn | Balto-Slavic
ckb_Arab | Central Kurdish | Arab | Iranian
dan_Latn | Danish | Latn | Germanic
deu_Latn | German | Latn | Germanic
ell_Grek | Greek | Grek | Hellenic
eng_Latn | English | Latn | Germanic
est_Latn | Estonian | Latn | Uralic
eus_Latn | Basque | Latn | Basque
fin_Latn | Finnish | Latn | Uralic
fra_Latn | French | Latn | Romance
fuv_Latn | Nigerian Fulfulde | Latn | Atlantic-Congo
gaz_Latn | West Central Oromo | Latn | Afro-Asiatic
grn_Latn | Guarani | Latn | Tupian
guj_Gujr | Gujarati | Gujr | Indo-Aryan
hat_Latn | Haitian Creole | Latn | Atlantic-Congo
hau_Latn | Hausa | Latn | Afro-Asiatic
heb_Hebr | Hebrew | Hebr | Afro-Asiatic
hin_Deva | Hindi | Deva | Indo-Aryan
hin_Latn | Hindi (Romanized) | Latn | Indo-Aryan
hrv_Latn | Croatian | Latn | Balto-Slavic
hun_Latn | Hungarian | Latn | Uralic
hye_Armn | Armenian | Armn | Armenian
ibo_Latn | Igbo | Latn | Atlantic-Congo
ilo_Latn | Ilocano | Latn | Austronesian
ind_Latn | Indonesian | Latn | Austronesian
isl_Latn | Icelandic | Latn | Germanic
ita_Latn | Italian | Latn | Romance
jav_Latn | Javanese | Latn | Austronesian
jpn_Jpan | Japanese | Jpan | Japonic
kac_Latn | Jingpho | Latn | Sino-Tibetan
kan_Knda | Kannada | Knda | Dravidian
kat_Geor | Georgian | Geor | kartvelian
kaz_Cyrl | Kazakh | Cyrl | Turkic
kea_Latn | Kabuverdianu | Latn | Portuguese Creole
khk_Cyrl | Halh Mongolian | Cyrl | Mongolic
khm_Khmr | Khmer | Khmr | Austroasiatic
kin_Latn | Kinyarwanda | Latn | Atlantic-Congo
kir_Cyrl | Kyrgyz | Cyrl | Turkic
kor_Hang | Korean | Hang | Koreanic
lao_Laoo | Lao | Laoo | Kra-Dai
lin_Latn | Lingala | Latn | Atlantic-Congo
lit_Latn | Lithuanian | Latn | Balto-Slavic
lug_Latn | Ganda | Latn | Atlantic-Congo
luo_Latn | Luo | Latn | Nilo-Saharan
lvs_Latn | Standard Latvian | Latn | Balto-Slavic
mal_Mlym | Malayalam | Mlym | Dravidian
mar_Deva | Marathi | Deva | Indo-Aryan
mkd_Cyrl | Macedonian | Cyrl | Balto-Slavic
mlt_Latn | Maltese | Latn | Afro-Asiatic
mri_Latn | Maori | Latn | Austronesian
mya_Mymr | Burmese | Mymr | Sino-Tibetan
nld_Latn | Dutch | Latn | Germanic
nob_Latn | Norwegian Bokmål | Latn | Germanic
npi_Deva | Nepali | Deva | Indo-Aryan
npi_Latn | Nepali (Romanized) | Latn | Indo-Aryan
nso_Latn | Northern Sotho | Latn | Atlantic-Congo
nya_Latn | Nyanja | Latn | Afro-Asiatic
ory_Orya | Odia | Orya | Indo-Aryan
pan_Guru | Eastern Panjabi | Guru | Indo-Aryan
pbt_Arab | Southern Pashto | Arab | Indo-Aryan
pes_Arab | Western Persian | Arab | Iranian
plt_Latn | Plateau Malagasy | Latn | Austronesian
pol_Latn | Polish | Latn | Balto-Slavic
por_Latn | Portuguese | Latn | Romance
ron_Latn | Romanian | Latn | Romance
rus_Cyrl | Russian | Cyrl | Balto-Slavic
shn_Mymr | Shan | Mymr | Kra-Dai
sin_Latn | Sinhala (Romanized) | Latn | Indo-Aryan
sin_Sinh | Sinhala | Sinh | Indo-Aryan
slk_Latn | Slovak | Latn | Balto-Slavic
slv_Latn | Slovenian | Latn | Balto-Slavic
sna_Latn | Shona | Latn | Atlantic-Congo
snd_Arab | Sindhi | Arab | Indo-Aryan
som_Latn | Somali | Latn | Afro-Asiatic
sot_Latn | Southern Sotho | Latn | Atlantic-Congo
spa_Latn | Spanish | Latn | Romance
srp_Cyrl | Serbian | Cyrl | Balto-Slavic
ssw_Latn | Swati | Latn | Atlantic-Congo
sun_Latn | Sundanese | Latn | Austronesian
swe_Latn | Swedish | Latn | Germanic
swh_Latn | Swahili | Latn | Atlantic-Congo
tam_Taml | Tamil | Taml | Dravidian
tel_Telu | Telugu | Telu | Dravidian
tgk_Cyrl | Tajik | Cyrl | Iranian
tgl_Latn | Tagalog | Latn | Austronesian
tha_Thai | Thai | Thai | Kra-Dai
tir_Ethi | Tigrinya | Ethi | Afro-Asiatic
tsn_Latn | Tswana | Latn | Atlantic-Congo
tso_Latn | Tsonga | Latn | Afro-Asiatic
tur_Latn | Turkish | Latn | Turkic
ukr_Cyrl | Ukrainian | Cyrl | Balto-Slavic
urd_Arab | Urdu | Arab | Indo-Aryan
urd_Latn | Urdu (Romanized) | Latn | Indo-Aryan
uzn_Latn | Northern Uzbek | Latn | Turkic
vie_Latn | Vietnamese | Latn | Austroasiatic
war_Latn | Waray | Latn | Austronesian
wol_Latn | Wolof | Latn | Atlantic-Congo
xho_Latn | Xhosa | Latn | Atlantic-Congo
yor_Latn | Yoruba | Latn | Atlantic-Congo
zho_Hans | Chinese (Simplified) | Hans | Sino-Tibetan
zho_Hant | Chinese (Traditional) | Hant | Sino-Tibetan
zsm_Latn | Standard Malay | Latn | Austronesian
zul_Latn | Zulu | Latn | Atlantic-Congo",High,4.0
Q&A,nayeon212/BLEnD,10.0,366.0,2025-05-02 14:55:48+00:00,cc-by-sa-4.0,0.0,190 MB,199229440.0,17.3 MB,18140364.8,313939,https://arxiv.org/abs/2406.09948,none,"---
license: cc-by-sa-4.0
task_categories:
- question-answering
language:
- en
- zh
- es
- id
- ko
- el
- fa
- ar
- az
- su
- as
- ha
- am
size_categories:
- 10K<n<100K
configs:
- config_name: annotations
  data_files:
    - split: DZ
      path: ""data/annotations_hf/Algeria_data.json""
    - split: AS
      path: ""data/annotations_hf/Assam_data.json""
    - split: AZ
      path: ""data/annotations_hf/Azerbaijan_data.json""
    - split: CN
      path: ""data/annotations_hf/China_data.json""
    - split: ET
      path: ""data/annotations_hf/Ethiopia_data.json""
    - split: GR
      path: ""data/annotations_hf/Greece_data.json""
    - split: ID
      path: ""data/annotations_hf/Indonesia_data.json""
    - split: IR
      path: ""data/annotations_hf/Iran_data.json""
    - split: MX
      path: ""data/annotations_hf/Mexico_data.json""
    - split: KP
      path: ""data/annotations_hf/North_Korea_data.json""
    - split: NG
      path: ""data/annotations_hf/Northern_Nigeria_data.json""
    - split: KR
      path: ""data/annotations_hf/South_Korea_data.json""
    - split: ES
      path: ""data/annotations_hf/Spain_data.json""
    - split: GB
      path: ""data/annotations_hf/UK_data.json""
    - split: US
      path: ""data/annotations_hf/US_data.json""
    - split: JB
      path: ""data/annotations_hf/West_Java_data.json""
- config_name: short-answer-questions
  data_files: 
    - split: DZ
      path: ""data/questions_hf/Algeria_questions.json""
    - split: AS
      path: ""data/questions_hf/Assam_questions.json""
    - split: AZ
      path: ""data/questions_hf/Azerbaijan_questions.json""
    - split: CN
      path: ""data/questions_hf/China_questions.json""
    - split: ET
      path: ""data/questions_hf/Ethiopia_questions.json""
    - split: GR
      path: ""data/questions_hf/Greece_questions.json""
    - split: ID
      path: ""data/questions_hf/Indonesia_questions.json""
    - split: IR
      path: ""data/questions_hf/Iran_questions.json""
    - split: MX
      path: ""data/questions_hf/Mexico_questions.json""
    - split: KP
      path: ""data/questions_hf/North_Korea_questions.json""
    - split: NG
      path: ""data/questions_hf/Northern_Nigeria_questions.json""
    - split: KR
      path: ""data/questions_hf/South_Korea_questions.json""
    - split: ES
      path: ""data/questions_hf/Spain_questions.json""
    - split: GB
      path: ""data/questions_hf/UK_questions.json""
    - split: US
      path: ""data/questions_hf/US_questions.json""
    - split: JB
      path: ""data/questions_hf/West_Java_questions.json""
- config_name: multiple-choice-questions
  data_files: 
    - split: test
      path: ""data/mc_questions_hf/mc_questions_file_v1.1.json""
---
# BLEnD

This is the official repository of **[BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages](https://arxiv.org/abs/2406.09948)** (Submitted to NeurIPS 2024 Datasets and Benchmarks Track).

*24/12/05: Updated translation errors*   
*25/05/02: Updated multiple choice questions file (v1.1)*

## About
![BLEnD Construction & LLM Evaluation Framework](main_figure.png)

Large language models (LLMs) often lack culture-specific everyday knowledge, especially across diverse regions and non-English languages. Existing benchmarks for evaluating LLMs' cultural sensitivities are usually limited to a single language or online sources like Wikipedia, which may not reflect the daily habits, customs, and lifestyles of different regions. That is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play, or the sports they practice in school is not always explicitly written online.
To address this issue, we introduce **BLEnD**, a hand-crafted benchmark designed to evaluate LLMs' everyday knowledge across diverse cultures and languages.
The benchmark comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as Amharic, Assamese, Azerbaijani, Hausa, and Sundanese.
We evaluate LLMs in two formats: short-answer questions, and multiple-choice questions.
We show that LLMs perform better in cultures that are more present online, with a maximum 57.34% difference in GPT-4, the best-performing model, in the short-answer format.
Furthermore, we find that LLMs perform better in their local languages for mid-to-high-resource languages. Interestingly, for languages deemed to be low-resource, LLMs provide better answers in English.

## Requirements
```Python
datasets == 2.19.2
pandas == 2.1.4
```

## Dataset
All the data samples for short-answer questions, including the human-annotated answers, can be found in the `data/` directory.
Specifically, the annotations from each country are included in the `annotations` split, and each country/region's data can be accessed by **[country codes](https://huggingface.co/datasets/nayeon212/BLEnD#countryregion-codes)**. 
```Python
from datasets import load_dataset

# Login using e.g. `huggingface-cli login` to access this dataset
ds = load_dataset(""nayeon212/BLEnD"", ""short-answer-questions"")

# To access data from Assam:
ds_as = ds['AS']
```
Each file includes a JSON variable with question IDs, questions in the local language and English, the human annotations both in the local language and English, and their respective vote counts as values. The same dataset for South Korea is shown below:
```JSON
[{
    ""ID"": ""Al-en-06"",
    ""question"": ""대한민국 학교 급식에서 흔히 볼 수 있는 음식은 무엇인가요?"",
    ""en_question"": ""What is a common school cafeteria food in your country?"",
    ""annotations"": [
        {
            ""answers"": [
                ""김치""
            ],
            ""en_answers"": [
                ""kimchi""
            ],
            ""count"": 4
        },
        {
            ""answers"": [
                ""밥"",
                ""쌀밥"",
                ""쌀""
            ],
            ""en_answers"": [
                ""rice""
            ],
            ""count"": 3
        },
        ...
    ],
    ""idks"": {
        ""idk"": 0,
        ""no-answer"": 0,
        ""not-applicable"": 0
    }
}],
```

The topics and source language for each question can be found in `short-answer-questions` split. 
Questions for each country in their local languages and English can be accessed by **[country codes](https://huggingface.co/datasets/nayeon212/BLEnD#countryregion-codes)**. 
Each CSV file question ID, topic, source language, question in English, and the local language (in the `Translation` column) for all questions.
```Python
from datasets import load_dataset

questions = load_dataset(""nayeon212/BLEnD"",'short-answer-questions')

# To access data from Assam:
assam_questions = questions['AS']
```
The current set of multiple choice questions and their answers can be found at the `multiple-choice-questions` split.
```Python
from datasets import load_dataset

mcq = load_dataset(""nayeon212/BLEnD"",'multiple-choice-questions')
```
### Country/Region Codes
 |  **Country/Region**  | **Code**  | **Language**  | **Code**|
   |:--------:|:--------------:|:------------:|:------------:|
|     United States     |    US    |    English     |    en
|     United Kingdom    |   GB     |       English        |en
|         China         |    CN    |    Chinese     |    zh
|         Spain         |    ES    |    Spanish     |    es
|         Mexico        |    MX    |Spanish|es
|       Indonesia       |    ID    |   Indonesian   |    id
|      South Korea      |    KR    |     Korean     |    ko
|      North Korea      |    KP    |        Korean        |ko
|         Greece        |    GR    |     Greek      |    el
|          Iran         |    IR    |    Persian     |    fa
|        Algeria        |    DZ    |     Arabic     |    ar
|       Azerbaijan      |    AZ    |  Azerbaijani   |    az
|       West Java       |    JB    |   Sundanese    |    su
|         Assam         |    AS    |    Assamese    |    as
|    Northern Nigeria   |    NG    |     Hausa      |    ha
|        Ethiopia       |    ET    |    Amharic     |    am

",Low,1.0
Q&A,stanford-oval/ccnews,21.0,3295.0,2024-08-31 17:28:13+00:00,none,0.0,36.5 GB,5368709120.0,36.5 GB,39191576576.0,893240667,none,none,"---
language:
  - multilingual
  - af
  - am
  - ar
  - as
  - az
  - be
  - bg
  - bn
  - br
  - bs
  - ca
  - cs
  - cy
  - da
  - de
  - el
  - en
  - eo
  - es
  - et
  - eu
  - fa
  - fi
  - fr
  - fy
  - ga
  - gd
  - gl
  - gu
  - ha
  - he
  - hi
  - hr
  - hu
  - hy
  - id
  - is
  - it
  - ja
  - jv
  - ka
  - kk
  - km
  - kn
  - ko
  - ku
  - ky
  - la
  - lo
  - lt
  - lv
  - mg
  - mk
  - ml
  - mn
  - mr
  - ms
  - my
  - ne
  - nl
  - 'no'
  - om
  - or
  - pa
  - pl
  - ps
  - pt
  - ro
  - ru
  - sa
  - sd
  - si
  - sk
  - sl
  - so
  - sq
  - sr
  - su
  - sv
  - sw
  - ta
  - te
  - th
  - tl
  - tr
  - ug
  - uk
  - ur
  - uz
  - vi
  - xh
  - yi
  - zh
pretty_name: All of Common Crawl News, 100+ languages, preprocessed and cleaned
task_categories:
- text-classification
- question-answering
- text-generation
- text2text-generation
size_categories:
- 100M<n<1B
tags:
- news
configs:
  - config_name: ""default""
    data_files: ""*.parquet""
  - config_name: ""2016""
    data_files: ""2016*""
  - config_name: ""2017""
    data_files: ""2017*""
  - config_name: ""2018""
    data_files: ""2018*""
  - config_name: ""2019""
    data_files: ""2019*""
  - config_name: ""2020""
    data_files: ""2020*""
  - config_name: ""2021""
    data_files: ""2021*""
  - config_name: ""2022""
    data_files: ""2022*""
  - config_name: ""2023""
    data_files: ""2023*""
  - config_name: ""2024""
    data_files: ""2024*""

---

This dataset is the result of processing all WARC files in the [CCNews Corpus](https://commoncrawl.org/blog/news-dataset-available), from the beginning (2016) to June of 2024.
The data has been cleaned and deduplicated, and language of articles have been detected and added. The process is similar to what HuggingFace's [DataTrove](https://github.com/huggingface/datatrove) does.

Overall, it contains about 600 million news articles in more than 100 languages from all around the globe.

For license information, please refer to [CommonCrawl's Terms of Use](https://commoncrawl.org/terms-of-use).

Sample Python code to explore this dataset:

```python
from datasets import load_dataset
from tqdm import tqdm

# Load the news articles **crawled** in the year 2016 (but not necessarily published in 2016), in streaming mode
dataset = load_dataset(""stanford-oval/ccnews"", name=""2016"", streaming=True) # `name` can be one of 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024

# Print information about the dataset
print(dataset)

# Iterate over a few examples
print(""\nFirst few examples:"")
for i, example in enumerate(dataset[""train""].take(5)):
    print(f""Example {i + 1}:"")
    print(example)
    print()

# Count the number of articles (in 2016)
row_count = 0
for _ in tqdm(dataset[""train""], desc=""Counting rows"", unit="" rows"", unit_scale=True, unit_divisor=1000):
    row_count += 1

# Print the number of rows
print(f""\nTotal number of articles: {row_count}"")

# Extract all Arabic (ar) articles
for row in tqdm(dataset[""train""], desc=""Extracting articles"", unit="" rows"", unit_scale=True, unit_divisor=1000):
    if row[""language""] == ""ar"":
        print(row)
```",Medium,2.0
Q&A,openai/MMMLU,488.0,5351.0,2024-10-16 18:39:00+00:00,mit,94.0,250 MB,262144000.0,124 MB,130023424.0,393176,https://arxiv.org/abs/2009.03300,none,"---
task_categories:
- question-answering
configs:
- config_name: default
  data_files:
  - split: test
    path: test/*.csv
- config_name: AR_XY
  data_files:
  - split: test
    path: test/mmlu_AR-XY.csv
- config_name: BN_BD
  data_files:
  - split: test
    path: test/mmlu_BN-BD.csv
- config_name: DE_DE
  data_files:
  - split: test
    path: test/mmlu_DE-DE.csv
- config_name: ES_LA
  data_files:
  - split: test
    path: test/mmlu_ES-LA.csv
- config_name: FR_FR
  data_files:
  - split: test
    path: test/mmlu_FR-FR.csv
- config_name: HI_IN
  data_files:
  - split: test
    path: test/mmlu_HI-IN.csv
- config_name: ID_ID
  data_files:
  - split: test
    path: test/mmlu_ID-ID.csv
- config_name: IT_IT
  data_files:
  - split: test
    path: test/mmlu_IT-IT.csv
- config_name: JA_JP
  data_files:
  - split: test
    path: test/mmlu_JA-JP.csv
- config_name: KO_KR
  data_files:
  - split: test
    path: test/mmlu_KO-KR.csv
- config_name: PT_BR
  data_files:
  - split: test
    path: test/mmlu_PT-BR.csv
- config_name: SW_KE
  data_files:
  - split: test
    path: test/mmlu_SW-KE.csv
- config_name: YO_NG
  data_files:
  - split: test
    path: test/mmlu_YO-NG.csv
- config_name: ZH_CN
  data_files:
  - split: test
    path: test/mmlu_ZH-CN.csv
language:
- ar
- bn
- de
- es
- fr
- hi
- id
- it
- ja
- ko
- pt
- sw
- yo
- zh
license: mit
---

# Multilingual Massive Multitask Language Understanding (MMMLU)

The MMLU is a widely recognized benchmark of general knowledge attained by AI models. It covers a broad range of topics from 57 different categories, covering elementary-level knowledge up to advanced professional subjects like law, physics, history, and computer science.

We translated the MMLU’s test set into 14 languages using professional human translators. Relying on human translators for this evaluation increases confidence in the accuracy of the translations, especially for low-resource languages like Yoruba. We are publishing the professional human translations and the code we use to run the evaluations.

This effort reflects our commitment to improving the multilingual capabilities of AI models, ensuring they perform accurately across languages, particularly for underrepresented communities. By prioritizing high-quality translations, we aim to make AI technology more inclusive and effective for users worldwide.

## Locales

MMMLU contains the MMLU test set translated into the following locales:
* AR_XY (Arabic)
* BN_BD (Bengali)
* DE_DE (German)
* ES_LA (Spanish)
* FR_FR (French)
* HI_IN (Hindi)
* ID_ID (Indonesian)
* IT_IT (Italian)
* JA_JP (Japanese)
* KO_KR (Korean)
* PT_BR (Brazilian Portuguese)
* SW_KE (Swahili)
* YO_NG (Yoruba)
* ZH_CN (Simplified Chinese)

## Sources

Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., & Steinhardt, J. (2021). [*Measuring Massive Multitask Language Understanding*](https://arxiv.org/abs/2009.03300).

[OpenAI Simple Evals GitHub Repository](https://github.com/openai/simple-evals)",Low,1.0
Q&A,hsseinmz/arcd,7.0,299.0,2024-01-09 12:44:24+00:00,mit,1.0,366 KB,374784.0,366 KB,374784.0,1395,none,none,"---
annotations_creators:
- crowdsourced
language_creators:
- crowdsourced
language:
- ar
license:
- mit
multilinguality:
- monolingual
size_categories:
- 1K<n<10K
source_datasets:
- original
task_categories:
- question-answering
task_ids:
- extractive-qa
paperswithcode_id: arcd
pretty_name: ARCD
language_bcp47:
- ar-SA
dataset_info:
  config_name: plain_text
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: train
    num_bytes: 811036
    num_examples: 693
  - name: validation
    num_bytes: 885620
    num_examples: 702
  download_size: 365858
  dataset_size: 1696656
configs:
- config_name: plain_text
  data_files:
  - split: train
    path: plain_text/train-*
  - split: validation
    path: plain_text/validation-*
  default: true
---

# Dataset Card for ""arcd""

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [https://github.com/husseinmozannar/SOQAL/tree/master/data](https://github.com/husseinmozannar/SOQAL/tree/master/data)
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Size of downloaded dataset files:** 1.94 MB
- **Size of the generated dataset:** 1.70 MB
- **Total amount of disk used:** 3.64 MB

### Dataset Summary

 Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions      posed by crowdworkers on Wikipedia articles.

### Supported Tasks and Leaderboards

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Languages

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Dataset Structure

### Data Instances

#### plain_text

- **Size of downloaded dataset files:** 1.94 MB
- **Size of the generated dataset:** 1.70 MB
- **Total amount of disk used:** 3.64 MB

An example of 'train' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": ""{\""answer_start\"": [34], \""text\"": [\""صحابي من صحابة رسول الإسلام محمد، وعمُّه وأخوه من الرضاعة وأحد وزرائه الأربعة عشر،\""]}..."",
    ""context"": ""\""حمزة بن عبد المطلب الهاشمي القرشي صحابي من صحابة رسول الإسلام محمد، وعمُّه وأخوه من الرضاعة وأحد وزرائه الأربعة عشر، وهو خير أع..."",
    ""id"": ""621723207492"",
    ""question"": ""من هو حمزة بن عبد المطلب؟"",
    ""title"": ""حمزة بن عبد المطلب""
}
```

### Data Fields

The data fields are the same among all splits.

#### plain_text
- `id`: a `string` feature.
- `title`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

### Data Splits

| name       | train | validation |
| ---------- | ----: | ---------: |
| plain_text |   693 |        702 |

## Dataset Creation

### Curation Rationale

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the source language producers?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Annotations

#### Annotation process

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the annotators?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Personal and Sensitive Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Discussion of Biases

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Other Known Limitations

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Additional Information

### Dataset Curators

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Licensing Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Citation Information

```
@inproceedings{mozannar-etal-2019-neural,
    title = ""Neural {A}rabic Question Answering"",
    author = ""Mozannar, Hussein  and
      Maamary, Elie  and
      El Hajal, Karl  and
      Hajj, Hazem"",
    booktitle = ""Proceedings of the Fourth Arabic Natural Language Processing Workshop"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/W19-4612"",
    doi = ""10.18653/v1/W19-4612"",
    pages = ""108--118"",
    abstract = ""This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source. This constrains the answer of any question to be a span of text in Wikipedia. Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension. To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT. Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score."",
}

```


### Contributions

Thanks to [@albertvillanova](https://github.com/albertvillanova), [@lewtun](https://github.com/lewtun), [@mariamabarham](https://github.com/mariamabarham), [@thomwolf](https://github.com/thomwolf), [@tayciryahmed](https://github.com/tayciryahmed) for adding this dataset.",High,5.0
Q&A,mhardalov/exams,34.0,1689.0,2024-02-06 07:20:12+00:00,cc-by-sa-4.0,3.0,289 MB,303038464.0,289 MB,303038464.0,136082,https://arxiv.org/abs/2011.03080,"['https://aclanthology.org/2020.emnlp-main.438"",']","---
annotations_creators:
- found
language_creators:
- found
language:
- ar
- bg
- de
- es
- fr
- hr
- hu
- it
- lt
- mk
- pl
- pt
- sq
- sr
- tr
- vi
license:
- cc-by-sa-4.0
multilinguality:
- monolingual
- multilingual
size_categories:
- 10K<n<100K
- 1K<n<10K
- n<1K
source_datasets:
- original
task_categories:
- question-answering
task_ids:
- multiple-choice-qa
paperswithcode_id: exams
pretty_name: EXAMS
config_names:
- alignments
- crosslingual_bg
- crosslingual_hr
- crosslingual_hu
- crosslingual_it
- crosslingual_mk
- crosslingual_pl
- crosslingual_pt
- crosslingual_sq
- crosslingual_sr
- crosslingual_test
- crosslingual_tr
- crosslingual_vi
- crosslingual_with_para_bg
- crosslingual_with_para_hr
- crosslingual_with_para_hu
- crosslingual_with_para_it
- crosslingual_with_para_mk
- crosslingual_with_para_pl
- crosslingual_with_para_pt
- crosslingual_with_para_sq
- crosslingual_with_para_sr
- crosslingual_with_para_test
- crosslingual_with_para_tr
- crosslingual_with_para_vi
- multilingual
- multilingual_with_para
dataset_info:
- config_name: alignments
  features:
  - name: source_id
    dtype: string
  - name: target_id_list
    sequence: string
  splits:
  - name: full
    num_bytes: 1265256
    num_examples: 10834
  download_size: 184096
  dataset_size: 1265256
- config_name: crosslingual_bg
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 1077329
    num_examples: 2344
  - name: validation
    num_bytes: 281771
    num_examples: 593
  download_size: 514922
  dataset_size: 1359100
- config_name: crosslingual_hr
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 807104
    num_examples: 2341
  - name: validation
    num_bytes: 176594
    num_examples: 538
  download_size: 450090
  dataset_size: 983698
- config_name: crosslingual_hu
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 677535
    num_examples: 1731
  - name: validation
    num_bytes: 202012
    num_examples: 536
  download_size: 401455
  dataset_size: 879547
- config_name: crosslingual_it
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 399312
    num_examples: 1010
  - name: validation
    num_bytes: 93175
    num_examples: 246
  download_size: 226376
  dataset_size: 492487
- config_name: crosslingual_mk
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 825702
    num_examples: 1665
  - name: validation
    num_bytes: 204318
    num_examples: 410
  download_size: 394548
  dataset_size: 1030020
- config_name: crosslingual_pl
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 573410
    num_examples: 1577
  - name: validation
    num_bytes: 141633
    num_examples: 394
  download_size: 341925
  dataset_size: 715043
- config_name: crosslingual_pt
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 374798
    num_examples: 740
  - name: validation
    num_bytes: 87714
    num_examples: 184
  download_size: 208021
  dataset_size: 462512
- config_name: crosslingual_sq
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 423744
    num_examples: 1194
  - name: validation
    num_bytes: 110093
    num_examples: 311
  download_size: 247052
  dataset_size: 533837
- config_name: crosslingual_sr
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 649560
    num_examples: 1323
  - name: validation
    num_bytes: 145724
    num_examples: 314
  download_size: 327466
  dataset_size: 795284
- config_name: crosslingual_test
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: test
    num_bytes: 8402575
    num_examples: 19736
  download_size: 3438526
  dataset_size: 8402575
- config_name: crosslingual_tr
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 717599
    num_examples: 1571
  - name: validation
    num_bytes: 182730
    num_examples: 393
  download_size: 440914
  dataset_size: 900329
- config_name: crosslingual_vi
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 953167
    num_examples: 1955
  - name: validation
    num_bytes: 231976
    num_examples: 488
  download_size: 462940
  dataset_size: 1185143
- config_name: crosslingual_with_para_bg
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 47066808
    num_examples: 2344
  - name: validation
    num_bytes: 11916026
    num_examples: 593
  download_size: 15794611
  dataset_size: 58982834
- config_name: crosslingual_with_para_hr
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 24889604
    num_examples: 2341
  - name: validation
    num_bytes: 5695066
    num_examples: 538
  download_size: 9839452
  dataset_size: 30584670
- config_name: crosslingual_with_para_hu
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 19035663
    num_examples: 1731
  - name: validation
    num_bytes: 6043265
    num_examples: 536
  download_size: 9263625
  dataset_size: 25078928
- config_name: crosslingual_with_para_it
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 16409235
    num_examples: 1010
  - name: validation
    num_bytes: 4018329
    num_examples: 246
  download_size: 6907617
  dataset_size: 20427564
- config_name: crosslingual_with_para_mk
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 38445894
    num_examples: 1665
  - name: validation
    num_bytes: 9673574
    num_examples: 410
  download_size: 12878474
  dataset_size: 48119468
- config_name: crosslingual_with_para_pl
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 16373781
    num_examples: 1577
  - name: validation
    num_bytes: 4158832
    num_examples: 394
  download_size: 6539172
  dataset_size: 20532613
- config_name: crosslingual_with_para_pt
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 12185383
    num_examples: 740
  - name: validation
    num_bytes: 3093712
    num_examples: 184
  download_size: 4956969
  dataset_size: 15279095
- config_name: crosslingual_with_para_sq
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 17341277
    num_examples: 1194
  - name: validation
    num_bytes: 4449952
    num_examples: 311
  download_size: 7112236
  dataset_size: 21791229
- config_name: crosslingual_with_para_sr
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 24575845
    num_examples: 1323
  - name: validation
    num_bytes: 5772509
    num_examples: 314
  download_size: 8035415
  dataset_size: 30348354
- config_name: crosslingual_with_para_test
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: test
    num_bytes: 207974374
    num_examples: 13510
  download_size: 62878029
  dataset_size: 207974374
- config_name: crosslingual_with_para_tr
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 18597131
    num_examples: 1571
  - name: validation
    num_bytes: 4763097
    num_examples: 393
  download_size: 7346658
  dataset_size: 23360228
- config_name: crosslingual_with_para_vi
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 40882999
    num_examples: 1955
  - name: validation
    num_bytes: 10260374
    num_examples: 488
  download_size: 13028078
  dataset_size: 51143373
- config_name: multilingual
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 3381837
    num_examples: 7961
  - name: validation
    num_bytes: 1141687
    num_examples: 2672
  - name: test
    num_bytes: 5746781
    num_examples: 13510
  download_size: 4323915
  dataset_size: 10270305
- config_name: multilingual_with_para
  features:
  - name: id
    dtype: string
  - name: question
    struct:
    - name: stem
      dtype: string
    - name: choices
      sequence:
      - name: text
        dtype: string
      - name: label
        dtype: string
      - name: para
        dtype: string
  - name: answerKey
    dtype: string
  - name: info
    struct:
    - name: grade
      dtype: int32
    - name: subject
      dtype: string
    - name: language
      dtype: string
  splits:
  - name: train
    num_bytes: 127294567
    num_examples: 7961
  - name: validation
    num_bytes: 42711689
    num_examples: 2672
  - name: test
    num_bytes: 207974374
    num_examples: 13510
  download_size: 112597818
  dataset_size: 377980630
configs:
- config_name: alignments
  data_files:
  - split: full
    path: alignments/full-*
- config_name: crosslingual_bg
  data_files:
  - split: train
    path: crosslingual_bg/train-*
  - split: validation
    path: crosslingual_bg/validation-*
- config_name: crosslingual_hr
  data_files:
  - split: train
    path: crosslingual_hr/train-*
  - split: validation
    path: crosslingual_hr/validation-*
- config_name: crosslingual_hu
  data_files:
  - split: train
    path: crosslingual_hu/train-*
  - split: validation
    path: crosslingual_hu/validation-*
- config_name: crosslingual_it
  data_files:
  - split: train
    path: crosslingual_it/train-*
  - split: validation
    path: crosslingual_it/validation-*
- config_name: crosslingual_mk
  data_files:
  - split: train
    path: crosslingual_mk/train-*
  - split: validation
    path: crosslingual_mk/validation-*
- config_name: crosslingual_pl
  data_files:
  - split: train
    path: crosslingual_pl/train-*
  - split: validation
    path: crosslingual_pl/validation-*
- config_name: crosslingual_pt
  data_files:
  - split: train
    path: crosslingual_pt/train-*
  - split: validation
    path: crosslingual_pt/validation-*
- config_name: crosslingual_sq
  data_files:
  - split: train
    path: crosslingual_sq/train-*
  - split: validation
    path: crosslingual_sq/validation-*
- config_name: crosslingual_sr
  data_files:
  - split: train
    path: crosslingual_sr/train-*
  - split: validation
    path: crosslingual_sr/validation-*
- config_name: crosslingual_test
  data_files:
  - split: test
    path: crosslingual_test/test-*
- config_name: crosslingual_tr
  data_files:
  - split: train
    path: crosslingual_tr/train-*
  - split: validation
    path: crosslingual_tr/validation-*
- config_name: crosslingual_vi
  data_files:
  - split: train
    path: crosslingual_vi/train-*
  - split: validation
    path: crosslingual_vi/validation-*
- config_name: crosslingual_with_para_bg
  data_files:
  - split: train
    path: crosslingual_with_para_bg/train-*
  - split: validation
    path: crosslingual_with_para_bg/validation-*
- config_name: crosslingual_with_para_hr
  data_files:
  - split: train
    path: crosslingual_with_para_hr/train-*
  - split: validation
    path: crosslingual_with_para_hr/validation-*
- config_name: crosslingual_with_para_hu
  data_files:
  - split: train
    path: crosslingual_with_para_hu/train-*
  - split: validation
    path: crosslingual_with_para_hu/validation-*
- config_name: crosslingual_with_para_it
  data_files:
  - split: train
    path: crosslingual_with_para_it/train-*
  - split: validation
    path: crosslingual_with_para_it/validation-*
- config_name: crosslingual_with_para_mk
  data_files:
  - split: train
    path: crosslingual_with_para_mk/train-*
  - split: validation
    path: crosslingual_with_para_mk/validation-*
- config_name: crosslingual_with_para_pl
  data_files:
  - split: train
    path: crosslingual_with_para_pl/train-*
  - split: validation
    path: crosslingual_with_para_pl/validation-*
- config_name: crosslingual_with_para_pt
  data_files:
  - split: train
    path: crosslingual_with_para_pt/train-*
  - split: validation
    path: crosslingual_with_para_pt/validation-*
- config_name: crosslingual_with_para_sq
  data_files:
  - split: train
    path: crosslingual_with_para_sq/train-*
  - split: validation
    path: crosslingual_with_para_sq/validation-*
- config_name: crosslingual_with_para_sr
  data_files:
  - split: train
    path: crosslingual_with_para_sr/train-*
  - split: validation
    path: crosslingual_with_para_sr/validation-*
- config_name: crosslingual_with_para_test
  data_files:
  - split: test
    path: crosslingual_with_para_test/test-*
- config_name: crosslingual_with_para_tr
  data_files:
  - split: train
    path: crosslingual_with_para_tr/train-*
  - split: validation
    path: crosslingual_with_para_tr/validation-*
- config_name: crosslingual_with_para_vi
  data_files:
  - split: train
    path: crosslingual_with_para_vi/train-*
  - split: validation
    path: crosslingual_with_para_vi/validation-*
- config_name: multilingual
  data_files:
  - split: train
    path: multilingual/train-*
  - split: validation
    path: multilingual/validation-*
  - split: test
    path: multilingual/test-*
- config_name: multilingual_with_para
  data_files:
  - split: train
    path: multilingual_with_para/train-*
  - split: validation
    path: multilingual_with_para/validation-*
  - split: test
    path: multilingual_with_para/test-*
  default: true
---

# Dataset Card for [Dataset Name]

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Repository:** https://github.com/mhardalov/exams-qa
- **Paper:** [EXAMS: A Multi-Subject High School Examinations Dataset for Cross-Lingual and Multilingual Question Answering](https://arxiv.org/abs/2011.03080)
- **Point of Contact:** [hardalov@@fmi.uni-sofia.bg](hardalov@@fmi.uni-sofia.bg)

### Dataset Summary

EXAMS is a benchmark dataset for multilingual and cross-lingual question answering from high school examinations. It consists of more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others.

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

The languages in the dataset are:
- ar
- bg
- de
- es
- fr
- hr
- hu
- it
- lt
- mk
- pl
- pt
- sq
- sr
- tr
- vi

## Dataset Structure

### Data Instances

An example of a data instance (with support paragraphs, in Bulgarian) is:
```
{'answerKey': 'C',
 'id': '35dd6b52-7e71-11ea-9eb1-54bef70b159e',
 'info': {'grade': 12, 'language': 'Bulgarian', 'subject': 'Biology'},
 'question': {'choices': {'label': ['A', 'B', 'C', 'D'],
   'para': ['Това води до наследствени изменения между организмите. Мирновременните вождове са наследствени. Черният, сивият и кафявият цвят на оцветяване на тялото се определя от пигмента меланин и възниква в резултат на наследствени изменения. Тези различия, според Монтескьо, не са наследствени. Те са и важни наследствени вещи в клана. Те са били наследствени архонти и управляват демократично. Реликвите са исторически, религиозни, семейни (наследствени) и технически. Общо са направени 800 изменения. Не всички наследствени аномалии на хемоглобина са вредни, т.е. Моногенните наследствени болести, които водят до мигрена, са редки. Няма наследствени владетели. Повечето от тях са наследствени и се предават на потомството. Всичките синове са ерцхерцози на всичките наследствени земи и претенденти. През 1509 г. Фраунбергите са издигнати на наследствени имперски графове. Фамилията Валдбург заради постиженията са номинирани на „наследствени имперски трушсеси“. Фамилията Валдбург заради постиженията са номинирани на „наследствени имперски трушсеси“. Описани са единични наследствени случаи, но по-често липсва фамилна обремененост. Позициите им са наследствени и се предават в рамките на клана. Внесени са изменения в конструкцията на веригите. и са направени изменения в ходовата част. На храма са правени лоши архитектурни изменения. Изменения са предприети и вътре в двореца. Имало двама наследствени вождове. Имало двама наследствени вождове. Годишният календар, „компасът“ и биологичния часовник са наследствени и при много бозайници.',
    'Постепенно задълбочаващите се функционални изменения довеждат и до структурни изменения. Те се дължат както на растягането на кожата, така и на въздействието на хормоналните изменения върху кожната тъкан. тези изменения се долавят по-ясно. Впоследствие, той претърпява изменения. Ширината остава без изменения. След тяхното издаване се налагат изменения в първоначалния Кодекс, защото не е съобразен с направените в Дигестите изменения. Еволюционният преход се характеризира със следните изменения: Наблюдават се и сезонни изменения в теглото. Приемат се изменения и допълнения към Устава. Тук се размножават и предизвикват възпалителни изменения. Общо са направени 800 изменения. Бронирането не претърпява съществени изменения. При животните се откриват изменения при злокачествената форма. Срещат се и дегенеративни изменения в семенните каналчета. ТАВКР „Баку“ се строи по изменения проект 1143.4. Трансът се съпровожда с определени изменения на мозъчната дейност. На изменения е подложен и Светия Синод. Внесени са изменения в конструкцията на веригите. На храма са правени лоши архитектурни изменения. Оттогава стиховете претърпяват изменения няколко пъти. Настъпват съществени изменения в музикалната култура. По-късно той претърпява леки изменения. Настъпват съществени изменения в музикалната култура. Претърпява сериозни изменения само носовата надстройка. Хоризонталното брониране е оставено без изменения.',
    'Модификациите са обратими. Тези реакции са обратими. В началните стадии тези натрупвания са обратими. Всички такива ефекти са временни и обратими. Много от реакциите са обратими и идентични с тези при гликолизата. Ако в обращение има книжни пари, те са обратими в злато при поискване . Общо са направени 800 изменения. Непоследователността е представена от принципа на ""симетрия"", при който взаимоотношенията са разглеждани като симетрични или обратими. Откакто формулите в клетките на електронната таблица не са обратими, тази техника е с ограничена стойност. Ефектът на Пелтие-Зеебек и ефектът Томсън са обратими (ефектът на Пелтие е обратен на ефекта на Зеебек). Плазмолизата протича в три етапа, в зависимост от силата и продължителността на въздействието:\n\nПървите два етапа са обратими. Внесени са изменения в конструкцията на веригите. и са направени изменения в ходовата част. На храма са правени лоши архитектурни изменения. Изменения са предприети и вътре в двореца. Оттогава насетне екипите не са претърпявали съществени изменения. Изменения са направени и в колесника на машината. Тези изменения са обявени през октомври 1878 година. Последните изменения са внесени през януари 2009 година. В процеса на последващото проектиране са внесени някои изменения. Сериозните изменения са в края на Втората световна война. Внесени са изменения в конструкцията на погребите и подемниците. Внесени са изменения в конструкцията на погребите и подемниците. Внесени са изменения в конструкцията на погребите и подемниците. Постепенно задълбочаващите се функционални изменения довеждат и до структурни изменения.',
    'Ерозионни процеси от масов характер липсват. Обновлението в редиците на партията приема масов характер. Тя обаче няма масов характер поради спецификата на формата. Движението против десятъка придобива масов характер и в Балчишка околия. Понякога екзекутирането на „обсебените от Сатана“ взимало невероятно масов характер. Укриването на дължими като наряд продукти в селата придобива масов характер. Периодичните миграции са в повечето случаи с масов характер и са свързани със сезонните изменения в природата, а непериодичните са премествания на животни, които настъпват след пожари, замърсяване на средата, висока численост и др. Имат необратим характер. Именно по време на двувековните походи на западните рицари използването на гербовете придобива масов характер. След присъединяването на Южен Кавказ към Русия, изселването на азербайджанци от Грузия придобива масов характер. Те имат нормативен характер. Те имат установителен характер. Освобождаването на работна сила обикновено има масов характер, защото обхваща големи контингенти от носителите на труд. Валежите имат подчертано континентален характер. Имат най-често издънков характер. Приливите имат предимно полуденонощен характер. Някои от тях имат мистериален характер. Тези сведения имат случаен, епизодичен характер. Те имат сезонен или годишен характер. Временните обезпечителни мерки имат временен характер. Други имат пожелателен характер (Здравко, Слава). Ловът и събирачеството имат спомагателен характер. Фактически успяват само малко да усилят бронирането на артилерийските погреби, другите изменения носят само частен характер. Някои карикатури имат само развлекателен характер, докато други имат политически нюанси. Поемите на Хезиод имат по-приложен характер.'],
   'text': ['дължат се на фенотипни изменения',
    'имат масов характер',
    'са наследствени',
    'са обратими']},
  'stem': 'Мутационите изменения:'}}
```

### Data Fields

A data instance contains the following fields:
- `id`: A question ID, unique across the dataset
- `question`: the question contains the following:
  - `stem`: a stemmed representation of the question textual
  - `choices`: a set of 3 to 5 candidate answers, which each have:
    - `text`: the text of the answers
    - `label`: a label in `['A', 'B', 'C', 'D', 'E']` used to match to the `answerKey`
    - `para`: (optional) a supported paragraph from Wikipedia in the same language as the question and answer
- `answerKey`: the key corresponding to the right answer's `label`
- `",High,5.0
Q&A,cis-lmu/m_lama,6.0,137.0,2025-05-14 08:05:50+00:00,cc-by-nc-sa-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2102.00894,none,"---
annotations_creators:
- crowdsourced
- expert-generated
- machine-generated
language_creators:
- crowdsourced
- expert-generated
- machine-generated
language:
- af
- ar
- az
- be
- bg
- bn
- ca
- ceb
- cs
- cy
- da
- de
- el
- en
- es
- et
- eu
- fa
- fi
- fr
- ga
- gl
- he
- hi
- hr
- hu
- hy
- id
- it
- ja
- ka
- ko
- la
- lt
- lv
- ms
- nl
- pl
- pt
- ro
- ru
- sk
- sl
- sq
- sr
- sv
- ta
- th
- tr
- uk
- ur
- vi
- zh
license:
- cc-by-nc-sa-4.0
multilinguality:
- translation
size_categories:
- 100K<n<1M
source_datasets:
- extended|lama
task_categories:
- question-answering
- text-classification
task_ids:
- open-domain-qa
- text-scoring
paperswithcode_id: null
pretty_name: MLama
tags:
- probing
dataset_info:
  features:
  - name: uuid
    dtype: string
  - name: lineid
    dtype: uint32
  - name: obj_uri
    dtype: string
  - name: obj_label
    dtype: string
  - name: sub_uri
    dtype: string
  - name: sub_label
    dtype: string
  - name: template
    dtype: string
  - name: language
    dtype: string
  - name: predicate_id
    dtype: string
  config_name: all
  splits:
  - name: test
    num_bytes: 125919995
    num_examples: 843143
  download_size: 40772287
  dataset_size: 125919995
---

# Dataset Card for [Dataset Name]

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [Multilingual LAMA](http://cistern.cis.lmu.de/mlama/)
- **Repository:** [Github](https://github.com/norakassner/mlama)
- **Paper:** [Arxiv](https://arxiv.org/abs/2102.00894)
- **Point of Contact:** [Contact section](http://cistern.cis.lmu.de/mlama/)



### Dataset Summary

This dataset provides the data for mLAMA, a multilingual version of LAMA. 
Regarding LAMA see https://github.com/facebookresearch/LAMA. For mLAMA
the TREx and GoogleRE part of LAMA was considered and machine translated using 
Google Translate, and the Wikidata and Google Knowledge Graph API. The machine
translated templates were checked for validity, i.e., whether they contain 
exactly one '[X]' and one '[Y]'.

This data can be used for creating fill-in-the-blank queries like 
""Paris is the capital of [MASK]"" across 53 languages. For more details see
the website http://cistern.cis.lmu.de/mlama/ or the github repo https://github.com/norakassner/mlama.

### Supported Tasks and Leaderboards

Language model knowledge probing.

### Languages

This dataset contains data in 53 languages: 
af,ar,az,be,bg,bn,ca,ceb,cs,cy,da,de,el,en,es,et,eu,fa,fi,fr,ga,gl,he,hi,hr,hu,hy,id,it,ja,ka,ko,la,lt,lv,ms,nl,pl,pt,ro,ru,sk,sl,sq,sr,sv,ta,th,tr,uk,ur,vi,zh

## Dataset Structure
For each of the 53 languages and each of the 43 relations/predicates there is a set of triples.

### Data Instances
For each language and relation there are triples, that consists of an object, a predicate and a subject. For each predicate there is a template available. An example for `dataset[""test""][0]` is given here:
```python
{
'language': 'af',
'lineid': 0, 
'obj_label': 'Frankryk', 
'obj_uri': 'Q142', 
'predicate_id': 'P1001', 
'sub_label': 'President van Frankryk', 
'sub_uri': 'Q191954', 
'template': ""[X] is 'n wettige term in [Y]."", 
'uuid': '3fe3d4da-9df9-45ba-8109-784ce5fba38a'
}
```


### Data Fields

Each instance has the following fields
* ""uuid"": a unique identifier
* ""lineid"": a identifier unique to mlama
* ""obj_uri"": knowledge graph id of the object
* ""obj_label"": surface form of the object
* ""sub_uri"": knowledge graph id of the subject
* ""sub_label"": surface form of the subject
* ""template"": template
* ""language"": language code
* ""predicate_id"": relation id


### Data Splits

There is only one partition that is labelled as 'test data'.

## Dataset Creation

### Curation Rationale

The dataset was translated into 53 languages to investigate knowledge in pretrained language models
multilingually.

### Source Data

#### Initial Data Collection and Normalization

The data has several sources: 

LAMA (https://github.com/facebookresearch/LAMA) licensed under Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)
T-REx (https://hadyelsahar.github.io/t-rex/) licensed under Creative Commons Attribution-ShareAlike 4.0 International License
Google-RE (https://github.com/google-research-datasets/relation-extraction-corpus)
Wikidata (https://www.wikidata.org/) licensed under Creative Commons CC0 License and Creative Commons Attribution-ShareAlike License

#### Who are the source language producers?

See links above. 

### Annotations

#### Annotation process

Crowdsourced (wikidata) and machine translated.

#### Who are the annotators?

Unknown. 

### Personal and Sensitive Information

Names of (most likely) famous people who have entries in Google Knowledge Graph or Wikidata.

## Considerations for Using the Data

Data was created through machine translation and automatic processes.

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

Not all triples are available in all languages.


## Additional Information

### Dataset Curators

The authors of the mLAMA paper and the authors of the original datasets.

### Licensing Information

The Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0). https://creativecommons.org/licenses/by-nc-sa/4.0/

### Citation Information

```
@article{kassner2021multilingual,
  author    = {Nora Kassner and
               Philipp Dufter and
               Hinrich Sch{\""{u}}tze},
  title     = {Multilingual {LAMA:} Investigating Knowledge in Multilingual Pretrained
               Language Models},
  journal   = {CoRR},
  volume    = {abs/2102.00894},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.00894},
  archivePrefix = {arXiv},
  eprint    = {2102.00894},
  timestamp = {Tue, 09 Feb 2021 13:35:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-00894.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  note      = {to appear in EACL2021}
}
```

### Contributions

Thanks to [@pdufter](https://github.com/pdufter) for adding this dataset.",High,5.0
Q&A,apple/mkqa,40.0,465.0,2024-01-18 11:09:04+00:00,cc-by-3.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2007.15207,none,"---
annotations_creators:
- crowdsourced
language_creators:
- found
language:
- ar
- da
- de
- en
- es
- fi
- fr
- he
- hu
- it
- ja
- km
- ko
- ms
- nl
- 'no'
- pl
- pt
- ru
- sv
- th
- tr
- vi
- zh
license:
- cc-by-3.0
multilinguality:
- multilingual
- translation
size_categories:
- 10K<n<100K
source_datasets:
- extended|natural_questions
- original
task_categories:
- question-answering
task_ids:
- open-domain-qa
paperswithcode_id: mkqa
pretty_name: Multilingual Knowledge Questions and Answers
dataset_info:
  features:
  - name: example_id
    dtype: string
  - name: queries
    struct:
    - name: ar
      dtype: string
    - name: da
      dtype: string
    - name: de
      dtype: string
    - name: en
      dtype: string
    - name: es
      dtype: string
    - name: fi
      dtype: string
    - name: fr
      dtype: string
    - name: he
      dtype: string
    - name: hu
      dtype: string
    - name: it
      dtype: string
    - name: ja
      dtype: string
    - name: ko
      dtype: string
    - name: km
      dtype: string
    - name: ms
      dtype: string
    - name: nl
      dtype: string
    - name: 'no'
      dtype: string
    - name: pl
      dtype: string
    - name: pt
      dtype: string
    - name: ru
      dtype: string
    - name: sv
      dtype: string
    - name: th
      dtype: string
    - name: tr
      dtype: string
    - name: vi
      dtype: string
    - name: zh_cn
      dtype: string
    - name: zh_hk
      dtype: string
    - name: zh_tw
      dtype: string
  - name: query
    dtype: string
  - name: answers
    struct:
    - name: ar
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: da
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: de
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: en
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: es
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: fi
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: fr
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: he
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: hu
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: it
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: ja
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: ko
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: km
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: ms
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: nl
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: 'no'
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: pl
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: pt
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: ru
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: sv
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: th
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: tr
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: vi
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: zh_cn
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: zh_hk
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
    - name: zh_tw
      list:
      - name: type
        dtype:
          class_label:
            names:
              '0': entity
              '1': long_answer
              '2': unanswerable
              '3': date
              '4': number
              '5': number_with_unit
              '6': short_phrase
              '7': binary
      - name: entity
        dtype: string
      - name: text
        dtype: string
      - name: aliases
        list: string
  config_name: mkqa
  splits:
  - name: train
    num_bytes: 36005650
    num_examples: 10000
  download_size: 11903948
  dataset_size: 36005650
---

# Dataset Card for MKQA: Multilingual Knowledge Questions & Answers

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- [**Homepage:**](https://github.com/apple/ml-mkqa/)
- [**Paper:**](https://arxiv.org/abs/2007.15207)

### Dataset Summary

MKQA contains 10,000 queries sampled from the [Google Natural Questions dataset](https://github.com/google-research-datasets/natural-questions).  

For each query we collect new passage-independent answers. 
These queries and answers are then human translated into 25 Non-English languages.

### Supported Tasks and Leaderboards

`question-answering`

### Languages

| Language code | Language name |
|---------------|---------------|
| `ar`     | `Arabic`                    |
| `da`     | `Danish`                    |
| `de`     | `German`                    |
| `en`     | `English`                   |
| `es`     | `Spanish`                   |
| `fi`     | `Finnish`                   |
| `fr`     | `French`                    |
| `he`     | `Hebrew`                    |
| `hu`     | `Hungarian`                 |
| `it`     | `Italian`                   |
| `ja`     | `Japanese`                  |
| `ko`     | `Korean`                    |
| `km`     | `Khmer`                    |
| `ms`     | `Malay`                     |
| `nl`     | `Dutch`                     |
| `no`     | `Norwegian`                 |
| `pl`     | `Polish`                    |
| `pt`     | `Portuguese`                |
| `ru`     | `Russian`                   |
| `sv`     | `Swedish`                   |
| `th`     | `Thai`                      |
| `tr`     | `Turkish`                   |
| `vi`     | `Vietnamese`                |
| `zh_cn`     | `Chinese (Simplified)`   |
| `zh_hk`     | `Chinese (Hong kong)`    |
| `zh_tw`     | `Chinese (Traditional)`  |

## Dataset Structure

### Data Instances

An example from the data set looks as follows:

```
{
 'example_id': 563260143484355911,
 'queries': {
  'en': ""who sings i hear you knocking but you can't come in"",
  'ru': ""кто поет i hear you knocking but you can't come in"",
  'ja': '「 I hear you knocking」は誰が歌っていますか',
  'zh_cn': ""《i hear you knocking but you can't come in》是谁演唱的"",
  ...
 },
 'query': ""who sings i hear you knocking but you can't come in"",
 'answers': {'en': [{'type': 'entity',
    'entity': 'Q545186',
    'text': 'Dave Edmunds',
    'aliases': []}],
  'ru': [{'type': 'entity',
    'entity': 'Q545186',
    'text': 'Эдмундс, Дэйв',
    'aliases': ['Эдмундс', 'Дэйв Эдмундс', 'Эдмундс Дэйв', 'Dave Edmunds']}],
  'ja': [{'type': 'entity',
    'entity': 'Q545186',
    'text': 'デイヴ・エドモンズ',
    'aliases': ['デーブ・エドモンズ', 'デイブ・エドモンズ']}],
  'zh_cn': [{'type': 'entity', 'text': '戴维·埃德蒙兹 ', 'entity': 'Q545186'}],
  ...
  },
}

```

### Data Fields

Each example in the dataset contains the unique Natural Questions `example_id`, the original English `query`, and then `queries` and `answers` in 26 languages.
Each answer is labelled with an answer type. The breakdown is:

| Answer Type | Occurrence |
|---------------|---------------|
| `entity`               | `4221`             |
| `long_answer`          | `1815`             |
| `unanswerable`         | `1427`             |
| `date`                 | `1174`             |
| `number`               | `485`              |
| `number_with_unit`     | `394`              |
| `short_phrase`         | `346`              |
| `binary`               | `138`              |
  
For each language, there can be more than one acceptable textual answer, in order to capture a variety of possible valid answers. 

Detailed explanation of fields taken from [here](https://github.com/apple/ml-mkqa/#dataset)

when `entity` field is not available it is set to an empty string ''.
when `aliases` field is not available it is set to an empty list [].

### Data Splits

- Train: 10000

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[Google Natural Questions dataset](https://github.com/google-research-datasets/natural-questions)

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[CC BY-SA 3.0](https://github.com/apple/ml-mkqa#license)

### Citation Information
```
@misc{mkqa,
    title = {MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering},
    author = {Shayne Longpre and Yi Lu and Joachim Daiber},
    year = {2020},
    URL = {https://arxiv.org/pdf/2007.15207.pdf}
}
```

### Contributions

Thanks to [@cceyda](https://github.com/cceyda) for adding this dataset.",High,5.0
Q&A,qcri/wiki_qa_ar,2.0,173.0,2024-01-18 11:18:07+00:00,unknown,0.0,32.8 MB,35226436,UNKNOWN,unknown,70264,none,none,"---
annotations_creators:
- crowdsourced
language_creators:
- crowdsourced
language:
- ar
license:
- unknown
multilinguality:
- monolingual
size_categories:
- 100K<n<1M
source_datasets:
- original
task_categories:
- question-answering
task_ids:
- open-domain-qa
paperswithcode_id: wikiqaar
pretty_name: English-Arabic Wikipedia Question-Answering
dataset_info:
  features:
  - name: question_id
    dtype: string
  - name: question
    dtype: string
  - name: document_id
    dtype: string
  - name: answer_id
    dtype: string
  - name: answer
    dtype: string
  - name: label
    dtype:
      class_label:
        names:
          '0': '0'
          '1': '1'
  config_name: plain_text
  splits:
  - name: test
    num_bytes: 7563127
    num_examples: 20632
  - name: validation
    num_bytes: 3740721
    num_examples: 10387
  - name: train
    num_bytes: 26009979
    num_examples: 70264
  download_size: 35226436
  dataset_size: 37313827
---

# Dataset Card for WikiQAar

## Table of Contents
- [Table of Contents](#table-of-contents)
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [WikiQaAr](https://github.com/qcri/WikiQAar)
- **Repository:** [WikiQaAr](https://github.com/qcri/WikiQAar)
- **Paper:** 
- **Point of Contact:** [Ines Abbes
](abbes.ines@yahoo.com)

### Dataset Summary

Arabic Version of WikiQA by automatic automatic machine translators 
and crowdsourced the selection of the best one to be incorporated into the corpus

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

The dataset is based on Arabic.

## Dataset Structure

### Data Instances

Each data point contains the question and whether the answer is a valid or not.  

### Data Fields

- `question_id`: the question id.
- `question`: the question text.
- `document_id`: the wikipedia document id.
- `answer_id` : the answer id.
- `answer` : a candidate answer to the question. 
- `label` : 1 if the `answer` is correct or 0 otherwise. 

### Data Splits

The dataset is not split. 

|            |  train | validation |   test |
|------------|-------:|-----------:|-------:|
| Data split | 70,264 |     20,632 | 10,387 |

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

Translation of WikiQA. 

#### Who are the source language producers?

Translation of WikiQA.   

### Annotations

The dataset does not contain any additional annotations.

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

```
@InProceedings{YangYihMeek:EMNLP2015:WikiQA,
       author = {{Yi}, Yang and {Wen-tau},  Yih and {Christopher} Meek},
        title = ""{WikiQA: A Challenge Dataset for Open-Domain Question Answering}"",
      journal = {Association for Computational Linguistics},
         year = 2015,
          doi = {10.18653/v1/D15-1237},
        pages = {2013–2018},
}
```

### Contributions

Thanks to [@zaidalyafeai](https://github.com/zaidalyafeai) for adding this dataset.",High,5.0
Q&A,akariasai/xor_tydi_qa,2.0,148.0,2024-01-18 11:18:45+00:00,mit,0.0,13.1 MB,14018298,UNKNOWN,unknown,61360,https://arxiv.org/abs/2010.11856,none,"---
annotations_creators:
- crowdsourced
language_creators:
- expert-generated
- found
language:
- ar
- bn
- fi
- ja
- ko
- ru
- te
license:
- mit
multilinguality:
- multilingual
size_categories:
- 10K<n<100K
source_datasets:
- original
- extended|tydiqa
task_categories:
- question-answering
task_ids:
- open-domain-qa
paperswithcode_id: xor-tydi-qa
pretty_name: XOR QA
dataset_info:
- config_name: xor-retrieve
  features:
  - name: question
    dtype: string
  - name: lang
    dtype:
      class_label:
        names:
          '0': ar
          '1': bn
          '2': fi
          '3': ja
          '4': ko
          '5': ru
          '6': te
  - name: answers
    dtype: string
  splits:
  - name: train
    num_bytes: 1698662
    num_examples: 15250
  - name: validation
    num_bytes: 259533
    num_examples: 2110
  - name: test
    num_bytes: 219046
    num_examples: 2499
  download_size: 3702288
  dataset_size: 2177241
- config_name: xor-full
  features:
  - name: question
    dtype: string
  - name: lang
    dtype:
      class_label:
        names:
          '0': ar
          '1': bn
          '2': fi
          '3': ja
          '4': ko
          '5': ru
          '6': te
  - name: answers
    dtype: string
  splits:
  - name: train
    num_bytes: 7250913
    num_examples: 61360
  - name: validation
    num_bytes: 444672
    num_examples: 3473
  - name: test
    num_bytes: 706664
    num_examples: 8176
  download_size: 14018298
  dataset_size: 8402249
---

# Dataset Card for XOR QA

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [XOR QA Homepage](https://nlp.cs.washington.edu/xorqa/)
- **Repository:** [XOR QA Repository](https://github.com/AkariAsai/XORQA)
- **Paper:** [XOR QA Paper](https://arxiv.org/abs/2010.11856)
- **Leaderboard:** [XOR QA Leaderboard](https://nlp.cs.washington.edu/xorqa/)
- **Point of Contact:** [Akari Asai](akari@cs.washington.edu)

### Dataset Summary

XOR-TyDi QA brings together for the first time information-seeking questions, open-retrieval QA, and multilingual QA to create a multilingual open-retrieval QA dataset that enables cross-lingual answer retrieval. It consists of questions written by information-seeking native speakers in 7 typologically diverse languages and answer annotations that are retrieved from multilingual document collections. 

### Supported Tasks and Leaderboards

There are three sub-tasks: XOR-Retrieve, XOR-EnglishSpan, and XOR-Full.

- `XOR-retrieve`: XOR-Retrieve is a cross-lingual retrieval task where a question is written in a target language (e.g., Japanese) and a system is required to retrieve English paragraphs that answer the question. The dataset can be used to train a model for cross-lingual retrieval. Success on this task is typically measured by R@5kt, R@2kt (the recall by computing the fraction of the questions for which the minimal answer is contained in the top 5,000 / 2,000 tokens selected). This task has an active leaderboard which can be found at [leaderboard url](https://nlp.cs.washington.edu/xorqa/)

- `XOR-English Span`: XOR-English Span is a cross-lingual retrieval task where a question is written in a target language (e.g., Japanese) and a system is required to output a short answer in English. The dataset can be used to train a model for cross-lingual retrieval. Success on this task is typically measured by F1, EM. This task has an active leaderboard which can be found at [leaderboard url](https://nlp.cs.washington.edu/xorqa/)

- `XOR-Full`: XOR-Full is a cross-lingual retrieval task where a question is written in the target language (e.g., Japanese) and a system is required to output a short answer in a target language. Success on this task is typically measured by F1, EM, BLEU This task has an active leaderboard which can be found at [leaderboard url](https://nlp.cs.washington.edu/xorqa/)
### Languages

The text in the dataset is available in 7 languages: Arabic `ar`, Bengali `bn`, Finnish `fi`, Japanese `ja`, Korean `ko`, Russian `ru`, Telugu `te`

## Dataset Structure

### Data Instances

A typical data point comprises a `question`, it's `answer` the `language` of the question text and the split to which it belongs.

```
{
    ""id"": ""-3979399588609321314"", 
    ""question"": ""Сколько детей было у Наполео́на I Бонапа́рта?"", 
    ""answers"": [""сын""], 
    ""lang"": ""ru"", 
    ""split"": ""train""
}
```

### Data Fields

- `id`: An identifier for each example in the dataset
- `question`: Open domain question
- `answers`: The corresponding answer to the question posed
- `lang`: BCP-47 language tag
- `split`: identifier to differentiate train, validation and test splits

### Data Splits

The data is split into a training, validation and test set for each of the two configurations.

|              | train | validation | test |
|--------------|------:|-----------:|-----:|
| XOR Retrieve | 15250 |       2113 | 2501 |
| XOR Full     | 61360 |       3179 | 8177 |

## Dataset Creation

### Curation Rationale

This task framework reflects well real-world scenarios where a QA system uses multilingual document collections and answers questions asked by users with diverse linguistic and cultural backgrounds. Despite the common assumption that we can find answers in the target language, web re- sources in non-English languages are largely lim- ited compared to English (information scarcity), or the contents are biased towards their own cul- tures (information asymmetry). To solve these issues, XOR-TYDI QA (Asai et al., 2020) provides a benchmark for developing a multilingual QA system that finds answers in multiple languages.

### Source Data

annotation pipeline consists of four steps: 1) collection of realistic questions that require cross-lingual ref- erences by annotating questions from TYDI QA without a same-language answer; 2) question translation from a target language to the pivot language of English where the missing informa- tion may exist; 3) answer span selection in the pivot language given a set of candidate documents; 4) answer verification and translation from the pivot language back to the original language.

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

The Dataset is created by extending TyDiQA dataset and translating the questions into other languages. The answers are obtained by crowdsourcing the questions to Mechanical Turk workders

### Annotations

#### Annotation process

The English questions from TyDiQA are translated into other languages. The languages are chosen based on the availability of wikipedia data and the availability of tranlators. 

#### Who are the annotators?

The translations are carried out using the professionla tranlation service (Gengo)[https://gengo.com] and the answers are annotated by MechanicalTurk workers

### Personal and Sensitive Information

The dataset is created from wikipedia content and the QA task requires preserving the named entities, there by all the Wikipedia Named Entities are preserved in the data. Not much information has been provided about masking sensitive information. 

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

The people associated with the creation of the dataset are Akari Asai, Jungo Kasai, Jonathan H. Clark, Kenton Lee, Eunsol Choi, Hannaneh Hajishirzi

### Licensing Information

XOR-TyDi QA is distributed under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/legalcode) license

### Citation Information

```
@article{xorqa,
    title   = {XOR QA: Cross-lingual Open-Retrieval Question Answering},
    author  = {Akari Asai and Jungo Kasai and Jonathan H. Clark and Kenton Lee and Eunsol Choi and Hannaneh Hajishirzi}
    year    = {2020}
}
```
### Contributions

Thanks to [@sumanthd17](https://github.com/sumanthd17) for adding this dataset.",High,5.0
Q&A,google/xquad,31.0,3589.0,2024-01-04 17:08:50+00:00,cc-by-sa-4.0,0.0,3.18 MB,3334471.68,3.18 MB,3334471.68,14280,https://arxiv.org/abs/1910.11856,none,"---
annotations_creators:
- expert-generated
language_creators:
- expert-generated
language:
- ar
- de
- el
- en
- es
- hi
- ro
- ru
- th
- tr
- vi
- zh
license:
- cc-by-sa-4.0
multilinguality:
- multilingual
size_categories:
- unknown
source_datasets:
- extended|squad
task_categories:
- question-answering
task_ids:
- extractive-qa
paperswithcode_id: xquad
pretty_name: XQuAD
dataset_info:
- config_name: xquad.ar
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1722775
    num_examples: 1190
  download_size: 263002
  dataset_size: 1722775
- config_name: xquad.de
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1283277
    num_examples: 1190
  download_size: 241957
  dataset_size: 1283277
- config_name: xquad.el
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 2206666
    num_examples: 1190
  download_size: 324379
  dataset_size: 2206666
- config_name: xquad.en
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1116099
    num_examples: 1190
  download_size: 212372
  dataset_size: 1116099
- config_name: xquad.es
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1273475
    num_examples: 1190
  download_size: 236874
  dataset_size: 1273475
- config_name: xquad.hi
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 2682951
    num_examples: 1190
  download_size: 322083
  dataset_size: 2682951
- config_name: xquad.ro
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1299426
    num_examples: 1190
  download_size: 244467
  dataset_size: 1299426
- config_name: xquad.ru
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 2136966
    num_examples: 1190
  download_size: 321728
  dataset_size: 2136966
- config_name: xquad.th
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 2854935
    num_examples: 1190
  download_size: 337307
  dataset_size: 2854935
- config_name: xquad.tr
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1210739
    num_examples: 1190
  download_size: 228364
  dataset_size: 1210739
- config_name: xquad.vi
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1477215
    num_examples: 1190
  download_size: 237644
  dataset_size: 1477215
- config_name: xquad.zh
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 984217
    num_examples: 1190
  download_size: 205768
  dataset_size: 984217
configs:
- config_name: xquad.ar
  data_files:
  - split: validation
    path: xquad.ar/validation-*
- config_name: xquad.de
  data_files:
  - split: validation
    path: xquad.de/validation-*
- config_name: xquad.el
  data_files:
  - split: validation
    path: xquad.el/validation-*
- config_name: xquad.en
  data_files:
  - split: validation
    path: xquad.en/validation-*
- config_name: xquad.es
  data_files:
  - split: validation
    path: xquad.es/validation-*
- config_name: xquad.hi
  data_files:
  - split: validation
    path: xquad.hi/validation-*
- config_name: xquad.ro
  data_files:
  - split: validation
    path: xquad.ro/validation-*
- config_name: xquad.ru
  data_files:
  - split: validation
    path: xquad.ru/validation-*
- config_name: xquad.th
  data_files:
  - split: validation
    path: xquad.th/validation-*
- config_name: xquad.tr
  data_files:
  - split: validation
    path: xquad.tr/validation-*
- config_name: xquad.vi
  data_files:
  - split: validation
    path: xquad.vi/validation-*
- config_name: xquad.zh
  data_files:
  - split: validation
    path: xquad.zh/validation-*
---

# Dataset Card for ""xquad""

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [https://github.com/deepmind/xquad](https://github.com/deepmind/xquad)
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Size of downloaded dataset files:** 146.31 MB
- **Size of the generated dataset:** 18.97 MB
- **Total amount of disk used:** 165.28 MB

### Dataset Summary

XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering
performance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set
of SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German,
Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. Consequently, the dataset is entirely parallel
across 11 languages.

### Supported Tasks and Leaderboards

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Languages

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Dataset Structure

### Data Instances

#### xquad.ar

- **Size of downloaded dataset files:** 13.30 MB
- **Size of the generated dataset:** 1.72 MB
- **Total amount of disk used:** 15.03 MB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [527],
        ""text"": [""136""]
    },
    ""context"": ""\""Die Verteidigung der Panthers gab nur 308 Punkte ab und belegte den sechsten Platz in der Liga, während sie die NFL mit 24 Inte..."",
    ""id"": ""56beb4343aeaaa14008c925c"",
    ""question"": ""Wie viele Sacks erzielte Jared Allen in seiner Karriere?""
}
```

#### xquad.de

- **Size of downloaded dataset files:** 13.30 MB
- **Size of the generated dataset:** 1.29 MB
- **Total amount of disk used:** 14.59 MB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [527],
        ""text"": [""136""]
    },
    ""context"": ""\""Die Verteidigung der Panthers gab nur 308 Punkte ab und belegte den sechsten Platz in der Liga, während sie die NFL mit 24 Inte..."",
    ""id"": ""56beb4343aeaaa14008c925c"",
    ""question"": ""Wie viele Sacks erzielte Jared Allen in seiner Karriere?""
}
```

#### xquad.el

- **Size of downloaded dataset files:** 13.30 MB
- **Size of the generated dataset:** 2.21 MB
- **Total amount of disk used:** 15.51 MB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [527],
        ""text"": [""136""]
    },
    ""context"": ""\""Die Verteidigung der Panthers gab nur 308 Punkte ab und belegte den sechsten Platz in der Liga, während sie die NFL mit 24 Inte..."",
    ""id"": ""56beb4343aeaaa14008c925c"",
    ""question"": ""Wie viele Sacks erzielte Jared Allen in seiner Karriere?""
}
```

#### xquad.en

- **Size of downloaded dataset files:** 13.30 MB
- **Size of the generated dataset:** 1.12 MB
- **Total amount of disk used:** 14.42 MB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [527],
        ""text"": [""136""]
    },
    ""context"": ""\""Die Verteidigung der Panthers gab nur 308 Punkte ab und belegte den sechsten Platz in der Liga, während sie die NFL mit 24 Inte..."",
    ""id"": ""56beb4343aeaaa14008c925c"",
    ""question"": ""Wie viele Sacks erzielte Jared Allen in seiner Karriere?""
}
```

#### xquad.es

- **Size of downloaded dataset files:** 13.30 MB
- **Size of the generated dataset:** 1.28 MB
- **Total amount of disk used:** 14.58 MB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [527],
        ""text"": [""136""]
    },
    ""context"": ""\""Die Verteidigung der Panthers gab nur 308 Punkte ab und belegte den sechsten Platz in der Liga, während sie die NFL mit 24 Inte..."",
    ""id"": ""56beb4343aeaaa14008c925c"",
    ""question"": ""Wie viele Sacks erzielte Jared Allen in seiner Karriere?""
}
```

### Data Fields

The data fields are the same among all splits.

#### xquad.ar
- `id`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

#### xquad.de
- `id`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

#### xquad.el
- `id`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

#### xquad.en
- `id`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

#### xquad.es
- `id`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

### Data Splits

| name     | validation |
| -------- | ---------: |
| xquad.ar |       1190 |
| xquad.de |       1190 |
| xquad.el |       1190 |
| xquad.en |       1190 |
| xquad.es |       1190 |

## Dataset Creation

### Curation Rationale

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the source language producers?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Annotations

#### Annotation process

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the annotators?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Personal and Sensitive Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Discussion of Biases

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Other Known Limitations

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Additional Information

### Dataset Curators

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Licensing Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Citation Information

```
@article{Artetxe:etal:2019,
      author    = {Mikel Artetxe and Sebastian Ruder and Dani Yogatama},
      title     = {On the cross-lingual transferability of monolingual representations},
      journal   = {CoRR},
      volume    = {abs/1910.11856},
      year      = {2019},
      archivePrefix = {arXiv},
      eprint    = {1910.11856}
}

```


### Contributions

Thanks to [@lewtun](https://github.com/lewtun), [@patrickvonplaten](https://github.com/patrickvonplaten), [@thomwolf](https://github.com/thomwolf) for adding this dataset.",High,5.0
Q&A,google-research-datasets/xquad_r,2.0,435.0,2024-01-04 17:11:57+00:00,cc-by-sa-4.0,0.0,2.93 MB,3072327.68,2.93 MB,3072327.68,13090,https://arxiv.org/abs/2004.05484,none,"---
annotations_creators:
- expert-generated
language_creators:
- found
language:
- ar
- de
- el
- en
- es
- hi
- ru
- th
- tr
- vi
- zh
license:
- cc-by-sa-4.0
multilinguality:
- multilingual
size_categories:
- 1K<n<10K
source_datasets:
- extended|squad
- extended|xquad
task_categories:
- question-answering
task_ids:
- extractive-qa
paperswithcode_id: xquad-r
pretty_name: LAReQA
config_names:
- ar
- de
- el
- en
- es
- hi
- ru
- th
- tr
- vi
- zh
dataset_info:
- config_name: ar
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1722775
    num_examples: 1190
  download_size: 263002
  dataset_size: 1722775
- config_name: de
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1283277
    num_examples: 1190
  download_size: 241957
  dataset_size: 1283277
- config_name: el
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 2206666
    num_examples: 1190
  download_size: 324379
  dataset_size: 2206666
- config_name: en
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1116099
    num_examples: 1190
  download_size: 212372
  dataset_size: 1116099
- config_name: es
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1273475
    num_examples: 1190
  download_size: 236874
  dataset_size: 1273475
- config_name: hi
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 2682951
    num_examples: 1190
  download_size: 322083
  dataset_size: 2682951
- config_name: ru
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 2136966
    num_examples: 1190
  download_size: 321728
  dataset_size: 2136966
- config_name: th
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 2854935
    num_examples: 1190
  download_size: 337307
  dataset_size: 2854935
- config_name: tr
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1210739
    num_examples: 1190
  download_size: 228364
  dataset_size: 1210739
- config_name: vi
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 1477215
    num_examples: 1190
  download_size: 237644
  dataset_size: 1477215
- config_name: zh
  features:
  - name: id
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: validation
    num_bytes: 984217
    num_examples: 1190
  download_size: 205768
  dataset_size: 984217
configs:
- config_name: ar
  data_files:
  - split: validation
    path: ar/validation-*
- config_name: de
  data_files:
  - split: validation
    path: de/validation-*
- config_name: el
  data_files:
  - split: validation
    path: el/validation-*
- config_name: en
  data_files:
  - split: validation
    path: en/validation-*
- config_name: es
  data_files:
  - split: validation
    path: es/validation-*
- config_name: hi
  data_files:
  - split: validation
    path: hi/validation-*
- config_name: ru
  data_files:
  - split: validation
    path: ru/validation-*
- config_name: th
  data_files:
  - split: validation
    path: th/validation-*
- config_name: tr
  data_files:
  - split: validation
    path: tr/validation-*
- config_name: vi
  data_files:
  - split: validation
    path: vi/validation-*
- config_name: zh
  data_files:
  - split: validation
    path: zh/validation-*
---

# Dataset Card for [Dataset Name]

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [LAReQA](https://github.com/google-research-datasets/lareqa)
- **Repository:** [XQuAD-R](https://github.com/google-research-datasets/lareqa)
- **Paper:** [LAReQA: Language-agnostic answer retrieval from a multilingual pool](https://arxiv.org/pdf/2004.05484.pdf)
- **Point of Contact:** [Noah Constant](mailto:nconstant@google.com)


### Dataset Summary
 
XQuAD-R is a retrieval version of the XQuAD dataset (a cross-lingual extractive
QA dataset). Like XQuAD, XQUAD-R is an 11-way parallel dataset,  where each
question appears in 11 different languages and has 11 parallel correct answers
across the languages.


### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

The dataset can be found with the following languages:
* Arabic: `xquad-r/ar.json`
* German: `xquad-r/de.json`
* Greek: `xquad-r/el.json`
* English: `xquad-r/en.json`
* Spanish: `xquad-r/es.json`
* Hindi: `xquad-r/hi.json`
* Russian: `xquad-r/ru.json`
* Thai: `xquad-r/th.json`
* Turkish: `xquad-r/tr.json`
* Vietnamese: `xquad-r/vi.json`
* Chinese: `xquad-r/zh.json`

## Dataset Structure

[More Information Needed]

### Data Instances

An example from `en` config:
```
{'id': '56beb4343aeaaa14008c925b',
 'context': ""The Panthers defense gave up just 308 points, ranking sixth in the league, while also leading the NFL in interceptions with 24 and boasting four Pro Bowl selections. Pro Bowl defensive tackle Kawann Short led the team in sacks with 11, while also forcing three fumbles and recovering two. Fellow lineman Mario Addison added 6½ sacks. The Panthers line also featured veteran defensive end Jared Allen, a 5-time pro bowler who was the NFL's active career sack leader with 136, along with defensive end Kony Ealy, who had 5 sacks in just 9 starts. Behind them, two of the Panthers three starting linebackers were also selected to play in the Pro Bowl: Thomas Davis and Luke Kuechly. Davis compiled 5½ sacks, four forced fumbles, and four interceptions, while Kuechly led the team in tackles (118) forced two fumbles, and intercepted four passes of his own. Carolina's secondary featured Pro Bowl safety Kurt Coleman, who led the team with a career high seven interceptions, while also racking up 88 tackles and Pro Bowl cornerback Josh Norman, who developed into a shutdown corner during the season and had four interceptions, two of which were returned for touchdowns."",
 'question': 'How many points did the Panthers defense surrender?',
 'answers': {'text': ['308'], 'answer_start': [34]}}
```

### Data Fields

- `id` (`str`): Unique ID for the context-question pair.
- `context` (`str`): Context for the question.
- `question` (`str`): Question.
- `answers` (`dict`): Answers with the following keys:
  - `text` (`list` of `str`): Texts of the answers.
  - `answer_start` (`list` of `int`): Start positions for every answer text.

### Data Splits

The number of questions and candidate sentences for each language for XQuAD-R is shown in the table below:

|     | XQuAD-R   |            |
|-----|-----------|------------|
|     | questions | candidates |
| ar |      1190 |       1222 |
| de |      1190 |       1276 |
| el |      1190 |       1234 |
| en |      1190 |       1180 |
| es |      1190 |       1215 |
| hi |      1190 |       1244 |
| ru |      1190 |       1219 |
| th |      1190 |        852 |
| tr |      1190 |       1167 |
| vi |      1190 |       1209 |
| zh |      1190 |       1196 |

## Dataset Creation

[More Information Needed]

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

[More Information Needed]

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

[More Information Needed]

### Dataset Curators

The dataset was initially created by Uma Roy, Noah Constant, Rami Al-Rfou, Aditya Barua, Aaron Phillips and Yinfei Yang, during work done at Google Research.

### Licensing Information

XQuAD-R is distributed under the [CC BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/legalcode).

### Citation Information

```
@article{roy2020lareqa,
  title={LAReQA: Language-agnostic answer retrieval from a multilingual pool},
  author={Roy, Uma and Constant, Noah and Al-Rfou, Rami and Barua, Aditya and Phillips, Aaron and Yang, Yinfei},
  journal={arXiv preprint arXiv:2004.05484},
  year={2020}
}
```

### Contributions

Thanks to [@manandey](https://github.com/manandey) for adding this dataset.",High,5.0
Q&A,google/xtreme,103.0,30195.0,2024-02-22 17:12:06+00:00,apache-2.0,19.0,363 MB,380633088.0,363 MB,380633088.0,2768692,https://arxiv.org/abs/2003.11080,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- af
- ar
- bg
- bn
- de
- el
- en
- es
- et
- eu
- fa
- fi
- fr
- he
- hi
- hu
- id
- it
- ja
- jv
- ka
- kk
- ko
- ml
- mr
- ms
- my
- nl
- pt
- ru
- sw
- ta
- te
- th
- tl
- tr
- ur
- vi
- yo
- zh
license:
- apache-2.0
- cc-by-4.0
- cc-by-2.0
- cc-by-sa-4.0
- other
- cc-by-nc-4.0
multilinguality:
- multilingual
- translation
size_categories:
- n<1K
- 1K<n<10K
- 10K<n<100K
- 100K<n<1M
source_datasets:
- extended|xnli
- extended|paws-x
- extended|wikiann
- extended|xquad
- extended|mlqa
- extended|tydiqa
- extended|tatoeba
- extended|squad
task_categories:
- multiple-choice
- question-answering
- token-classification
- text-classification
- text-retrieval
- token-classification
task_ids:
- multiple-choice-qa
- extractive-qa
- open-domain-qa
- natural-language-inference
- named-entity-recognition
- part-of-speech
paperswithcode_id: xtreme
pretty_name: XTREME
config_names:
- MLQA.ar.ar
- MLQA.ar.de
- MLQA.ar.en
- MLQA.ar.es
- MLQA.ar.hi
- MLQA.ar.vi
- MLQA.ar.zh
- MLQA.de.ar
- MLQA.de.de
- MLQA.de.en
- MLQA.de.es
- MLQA.de.hi
- MLQA.de.vi
- MLQA.de.zh
- MLQA.en.ar
- MLQA.en.de
- MLQA.en.en
- MLQA.en.es
- MLQA.en.hi
- MLQA.en.vi
- MLQA.en.zh
- MLQA.es.ar
- MLQA.es.de
- MLQA.es.en
- MLQA.es.es
- MLQA.es.hi
- MLQA.es.vi
- MLQA.es.zh
- MLQA.hi.ar
- MLQA.hi.de
- MLQA.hi.en
- MLQA.hi.es
- MLQA.hi.hi
- MLQA.hi.vi
- MLQA.hi.zh
- MLQA.vi.ar
- MLQA.vi.de
- MLQA.vi.en
- MLQA.vi.es
- MLQA.vi.hi
- MLQA.vi.vi
- MLQA.vi.zh
- MLQA.zh.ar
- MLQA.zh.de
- MLQA.zh.en
- MLQA.zh.es
- MLQA.zh.hi
- MLQA.zh.vi
- MLQA.zh.zh
- PAN-X.af
- PAN-X.ar
- PAN-X.bg
- PAN-X.bn
- PAN-X.de
- PAN-X.el
- PAN-X.en
- PAN-X.es
- PAN-X.et
- PAN-X.eu
- PAN-X.fa
- PAN-X.fi
- PAN-X.fr
- PAN-X.he
- PAN-X.hi
- PAN-X.hu
- PAN-X.id
- PAN-X.it
- PAN-X.ja
- PAN-X.jv
- PAN-X.ka
- PAN-X.kk
- PAN-X.ko
- PAN-X.ml
- PAN-X.mr
- PAN-X.ms
- PAN-X.my
- PAN-X.nl
- PAN-X.pt
- PAN-X.ru
- PAN-X.sw
- PAN-X.ta
- PAN-X.te
- PAN-X.th
- PAN-X.tl
- PAN-X.tr
- PAN-X.ur
- PAN-X.vi
- PAN-X.yo
- PAN-X.zh
- PAWS-X.de
- PAWS-X.en
- PAWS-X.es
- PAWS-X.fr
- PAWS-X.ja
- PAWS-X.ko
- PAWS-X.zh
- SQuAD
- XNLI
- XQuAD
- bucc18.de
- bucc18.fr
- bucc18.ru
- bucc18.zh
- tatoeba.afr
- tatoeba.ara
- tatoeba.ben
- tatoeba.bul
- tatoeba.cmn
- tatoeba.deu
- tatoeba.ell
- tatoeba.est
- tatoeba.eus
- tatoeba.fin
- tatoeba.fra
- tatoeba.heb
- tatoeba.hin
- tatoeba.hun
- tatoeba.ind
- tatoeba.ita
- tatoeba.jav
- tatoeba.jpn
- tatoeba.kat
- tatoeba.kaz
- tatoeba.kor
- tatoeba.mal
- tatoeba.mar
- tatoeba.nld
- tatoeba.pes
- tatoeba.por
- tatoeba.rus
- tatoeba.spa
- tatoeba.swh
- tatoeba.tam
- tatoeba.tel
- tatoeba.tgl
- tatoeba.tha
- tatoeba.tur
- tatoeba.urd
- tatoeba.vie
- tydiqa
- udpos.Afrikans
- udpos.Arabic
- udpos.Basque
- udpos.Bulgarian
- udpos.Chinese
- udpos.Dutch
- udpos.English
- udpos.Estonian
- udpos.Finnish
- udpos.French
- udpos.German
- udpos.Greek
- udpos.Hebrew
- udpos.Hindi
- udpos.Hungarian
- udpos.Indonesian
- udpos.Italian
- udpos.Japanese
- udpos.Kazakh
- udpos.Korean
- udpos.Marathi
- udpos.Persian
- udpos.Portuguese
- udpos.Russian
- udpos.Spanish
- udpos.Tagalog
- udpos.Tamil
- udpos.Telugu
- udpos.Thai
- udpos.Turkish
- udpos.Urdu
- udpos.Vietnamese
- udpos.Yoruba
language_bcp47:
- fa-IR
license_details: Licence Universal Dependencies v2.5
tags:
- parallel-sentence-retrieval
- paraphrase-identification
dataset_info:
- config_name: MLQA.ar.ar
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 8368086
    num_examples: 5335
  - name: validation
    num_bytes: 824080
    num_examples: 517
  download_size: 4048180
  dataset_size: 9192166
- config_name: MLQA.ar.de
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 2183914
    num_examples: 1649
  - name: validation
    num_bytes: 364809
    num_examples: 207
  download_size: 1192825
  dataset_size: 2548723
- config_name: MLQA.ar.en
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 8225634
    num_examples: 5335
  - name: validation
    num_bytes: 810061
    num_examples: 517
  download_size: 3998008
  dataset_size: 9035695
- config_name: MLQA.ar.es
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 3041350
    num_examples: 1978
  - name: validation
    num_bytes: 228152
    num_examples: 161
  download_size: 1531661
  dataset_size: 3269502
- config_name: MLQA.ar.hi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 3039368
    num_examples: 1831
  - name: validation
    num_bytes: 281742
    num_examples: 186
  download_size: 1369756
  dataset_size: 3321110
- config_name: MLQA.ar.vi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 3290601
    num_examples: 2047
  - name: validation
    num_bytes: 288418
    num_examples: 163
  download_size: 1667238
  dataset_size: 3579019
- config_name: MLQA.ar.zh
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 3229844
    num_examples: 1912
  - name: validation
    num_bytes: 340021
    num_examples: 188
  download_size: 1591445
  dataset_size: 3569865
- config_name: MLQA.de.ar
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1619978
    num_examples: 1649
  - name: validation
    num_bytes: 200146
    num_examples: 207
  download_size: 1044483
  dataset_size: 1820124
- config_name: MLQA.de.de
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 4366074
    num_examples: 4517
  - name: validation
    num_bytes: 488339
    num_examples: 512
  download_size: 2798050
  dataset_size: 4854413
- config_name: MLQA.de.en
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 4343116
    num_examples: 4517
  - name: validation
    num_bytes: 485866
    num_examples: 512
  download_size: 2778346
  dataset_size: 4828982
- config_name: MLQA.de.es
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1716587
    num_examples: 1776
  - name: validation
    num_bytes: 170554
    num_examples: 196
  download_size: 1118751
  dataset_size: 1887141
- config_name: MLQA.de.hi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1371046
    num_examples: 1430
  - name: validation
    num_bytes: 153843
    num_examples: 163
  download_size: 880652
  dataset_size: 1524889
- config_name: MLQA.de.vi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1688455
    num_examples: 1675
  - name: validation
    num_bytes: 216047
    num_examples: 182
  download_size: 1108163
  dataset_size: 1904502
- config_name: MLQA.de.zh
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1679152
    num_examples: 1621
  - name: validation
    num_bytes: 184290
    num_examples: 190
  download_size: 1045861
  dataset_size: 1863442
- config_name: MLQA.en.ar
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 6739191
    num_examples: 5335
  - name: validation
    num_bytes: 630815
    num_examples: 517
  download_size: 3939135
  dataset_size: 7370006
- config_name: MLQA.en.de
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 5056694
    num_examples: 4517
  - name: validation
    num_bytes: 594908
    num_examples: 512
  download_size: 3223196
  dataset_size: 5651602
- config_name: MLQA.en.en
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 14004592
    num_examples: 11590
  - name: validation
    num_bytes: 1329084
    num_examples: 1148
  download_size: 8217519
  dataset_size: 15333676
- config_name: MLQA.en.es
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 6179221
    num_examples: 5253
  - name: validation
    num_bytes: 555434
    num_examples: 500
  download_size: 3776828
  dataset_size: 6734655
- config_name: MLQA.en.hi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 6378838
    num_examples: 4918
  - name: validation
    num_bytes: 623143
    num_examples: 507
  download_size: 3517340
  dataset_size: 7001981
- config_name: MLQA.en.vi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 7056670
    num_examples: 5495
  - name: validation
    num_bytes: 640618
    num_examples: 511
  download_size: 4170642
  dataset_size: 7697288
- config_name: MLQA.en.zh
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 6539279
    num_examples: 5137
  - name: validation
    num_bytes: 608416
    num_examples: 504
  download_size: 3929122
  dataset_size: 7147695
- config_name: MLQA.es.ar
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1740254
    num_examples: 1978
  - name: validation
    num_bytes: 148621
    num_examples: 161
  download_size: 1107435
  dataset_size: 1888875
- config_name: MLQA.es.de
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1403997
    num_examples: 1776
  - name: validation
    num_bytes: 144158
    num_examples: 196
  download_size: 950448
  dataset_size: 1548155
- config_name: MLQA.es.en
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 4362709
    num_examples: 5253
  - name: validation
    num_bytes: 419040
    num_examples: 500
  download_size: 2842879
  dataset_size: 4781749
- config_name: MLQA.es.es
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 4394305
    num_examples: 5253
  - name: validation
    num_bytes: 422043
    num_examples: 500
  download_size: 2856931
  dataset_size: 4816348
- config_name: MLQA.es.hi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1523495
    num_examples: 1723
  - name: validation
    num_bytes: 181806
    num_examples: 187
  download_size: 954018
  dataset_size: 1705301
- config_name: MLQA.es.vi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1747941
    num_examples: 2018
  - name: validation
    num_bytes: 176813
    num_examples: 189
  download_size: 1187949
  dataset_size: 1924754
- config_name: MLQA.es.zh
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1678423
    num_examples: 1947
  - name: validation
    num_bytes: 126618
    num_examples: 161
  download_size: 1100765
  dataset_size: 1805041
- config_name: MLQA.hi.ar
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 4445561
    num_examples: 1831
  - name: validation
    num_bytes: 410396
    num_examples: 186
  download_size: 1542768
  dataset_size: 4855957
- config_name: MLQA.hi.de
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 3022836
    num_examples: 1430
  - name: validation
    num_bytes: 301685
    num_examples: 163
  download_size: 1257846
  dataset_size: 3324521
- config_name: MLQA.hi.en
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 11449233
    num_examples: 4918
  - name: validation
    num_bytes: 1097829
    num_examples: 507
  download_size: 4131083
  dataset_size: 12547062
- config_name: MLQA.hi.es
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 3862201
    num_examples: 1723
  - name: validation
    num_bytes: 420374
    num_examples: 187
  download_size: 1493468
  dataset_size: 4282575
- config_name: MLQA.hi.hi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 11810447
    num_examples: 4918
  - name: validation
    num_bytes: 1136756
    num_examples: 507
  download_size: 4235981
  dataset_size: 12947203
- config_name: MLQA.hi.vi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 4743456
    num_examples: 1947
  - name: validation
    num_bytes: 419078
    num_examples: 177
  download_size: 1704964
  dataset_size: 5162534
- config_name: MLQA.hi.zh
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 4354847
    num_examples: 1767
  - name: validation
    num_bytes: 424218
    num_examples: 189
  download_size: 1627107
  dataset_size: 4779065
- config_name: MLQA.vi.ar
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 3205157
    num_examples: 2047
  - name: validation
    num_bytes: 230307
    num_examples: 163
  download_size: 1656661
  dataset_size: 3435464
- config_name: MLQA.vi.de
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 2227005
    num_examples: 1675
  - name: validation
    num_bytes: 277157
    num_examples: 182
  download_size: 1268041
  dataset_size: 2504162
- config_name: MLQA.vi.en
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 7843403
    num_examples: 5495
  - name: validation
    num_bytes: 719245
    num_examples: 511
  download_size: 4071703
  dataset_size: 8562648
- config_name: MLQA.vi.es
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 2866569
    num_examples: 2018
  - name: validation
    num_bytes: 283433
    num_examples: 189
  download_size: 1607926
  dataset_size: 3150002
- config_name: MLQA.vi.hi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 2776636
    num_examples: 1947
  - name: validation
    num_bytes: 254979
    num_examples: 177
  download_size: 1366057
  dataset_size: 3031615
- config_name: MLQA.vi.vi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 7922057
    num_examples: 5495
  - name: validation
    num_bytes: 726490
    num_examples: 511
  download_size: 4105388
  dataset_size: 8648547
- config_name: MLQA.vi.zh
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 2989632
    num_examples: 1943
  - name: validation
    num_bytes: 269361
    num_examples: 184
  download_size: 1570393
  dataset_size: 3258993
- config_name: MLQA.zh.ar
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1731455
    num_examples: 1912
  - name: validation
    num_bytes: 175321
    num_examples: 188
  download_size: 1223863
  dataset_size: 1906776
- config_name: MLQA.zh.de
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1389990
    num_examples: 1621
  - name: validation
    num_bytes: 174577
    num_examples: 190
  download_size: 1006829
  dataset_size: 1564567
- config_name: MLQA.zh.en
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 4450957
    num_examples: 5137
  - name: validation
    num_bytes: 446840
    num_examples: 504
  download_size: 3108433
  dataset_size: 4897797
- config_name: MLQA.zh.es
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1736255
    num_examples: 1947
  - name: validation
    num_bytes: 138045
    num_examples: 161
  download_size: 1223467
  dataset_size: 1874300
- config_name: MLQA.zh.hi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1578191
    num_examples: 1767
  - name: validation
    num_bytes: 184373
    num_examples: 189
  download_size: 1044599
  dataset_size: 1762564
- config_name: MLQA.zh.vi
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 1806158
    num_examples: 1943
  - name: validation
    num_bytes: 172906
    num_examples: 184
  download_size: 1268213
  dataset_size: 1979064
- config_name: MLQA.zh.zh
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  splits:
  - name: test
    num_bytes: 4422322
    num_examples: 5137
  - name: validation
    num_bytes: 443782
    num_examples: 504
  download_size: 3105362
  dataset_size: 4866104
- config_name: PAN-X.af
  features:
  - name: tokens
    sequence: string
  - name: ner_tags
    sequence:
      class_label:
        names:
          '0': O
          '1': B-PER
          '2': I-PER
          '3': B-ORG
          '4': I-ORG
          '5': B-LOC
          '6': I-LOC
  - name: langs
    sequence: string
  splits:
  - name: train
    num_bytes: 1321376
    num_examples: 5000
  - name: validation
    num_bytes: 259689
    num_examples: 1000
  - name: test
    num_bytes: 257184
    num_examples: 1000
  download_size: 389015
  dataset_size: 1838249
- config_name: PAN-X.ar
  features:
  - name: tokens
    sequence: string
  - name: ner_tags
    sequence:
      class_label:
        names:
          '0': O
          '1': B-PER
          '2': I-PER
          '3': B-ORG
          '4': I-ORG
          '5': B-LOC
          '6': I-LOC
  - name: langs
    sequence: string
  splits:
  - name: train
    num_bytes: 3634096
    num_examples: 20000
  - name: validation
    num_bytes: 1808283
    num_examples: 10000
  - name: test
    num_bytes: 1811963
    num_examples: 10000
  download_size: 1567470
  dataset_size: 7254342
- config_name: PAN-X.bg
  features:
  - name: tokens
    sequence: string
  - name: ner_tags
    sequence:
      class_label:
        names:
          '0': O
          '1': B-PER
          '2': I-PER
          '3': B-ORG
          '4': I-ORG
          '5': B-LOC
          '6': I-LOC
  - name: langs
    sequence: string
  splits:
  - name: train
    num_bytes: 4600733
    num_examples: 20000
  - name: validation
    num_bytes: 2310294
    num_examples: 10000
  - name: test
    num_bytes: 2306138
    num_examples: 10000
  download_size: 2030669
  dataset_size: 9217165
- config_name: PAN-X.bn
  features:
  - name: tokens
    sequence: string
  - name: ner_tags
    sequence:
      class_label:
        names:
          '0': O
          '1': B-PER
          '2': I-PER
          '3': B-ORG
          '4': I-ORG
          '5': B-LOC
          '6': I-LOC
  - name: langs
    sequence: string
  splits:
  - name: train
    num_bytes: 1568825
    num_examples: 10000
  - name: validation
    num_bytes: 159068
    num_examples: 1000
  - name: test
    num_bytes: 159262
    num_examples: 1000
  download_size: 364024
  dataset_size: 1887155
- config_name: PAN-X.de
  features:
  - name: tokens
    sequence: string
  - name: ner_tags
    sequence:
      class_label:
        names:
          '0': O
          '1': B-PER
          '2': I-PER
          '3': B-ORG
          '4': I-ORG
          '5': B-LOC
          '6': I-LOC
  - name: langs
    sequence: string
  splits:
  - name: train
    num_bytes: 4762312
    num_examples: 20000
  - name: validation
    num_bytes: 2381545
    num_examples: 10000
  - name: test
    num_bytes: 2377619
    num_examples: 10000
  download_size: 2360242
  dataset_size: 9521476
- config_name: PAN-X.el
  features:
  - name: tokens
    sequence: string
  - name: ner_tags
    sequence:
      class_label:
        names:
          '0': O
          '1': B-PER
          '2': I-PER
          '3': B-ORG
          '4': I-ORG
          '5': B-LOC
          '6': I-LOC
  - name: langs
    sequence: string
  splits:
  - name: train
    num_bytes: 5063136
    num_examples: 20000
  - name: validation
    num_bytes: 2533786
    num_examples: 10000
  - name: test
    num_bytes: 2547574
    num_examples: 10000
  download_size: 2271726
  dataset_size: 10144496
- config_name: PAN-X.en
  features:
  - name: tokens
    sequence: string
  - name: ner_tags
    sequence:
      class_label:
        names:
          '0': O
          '1': B-PER
          '2': I-PER
          '3': B-ORG
          '4': I-ORG
          '5': B-LOC
          '6': I-LOC
  - name: langs
    sequence: string
  splits:
  - name: train
    num_bytes: 3823434
    num_examples: 20000
  - name: validation
    num_bytes: 1920049
    num_examples: 10000
  - name: test
    num_bytes: 1916200
    num_examples: 10000
  download_size: 1886284
  dataset_size: 7659683
- config_name: PAN-X.es
  features:
  - name: tokens
    sequence: string
  - name: ner_tags
    sequence:
      class_label:
        names:
          '0': O
          '1': B-PER
          '2': I-PER
          '3': B-ORG
          '4': I-ORG
          '5': B-LOC
          '6': I-LOC
  - name: langs
    sequence: string
  splits:
  - name: train
    num_bytes: 3199121
    num_examples: 20000
  - name: validation
    num_bytes: 1592505
    num_examples: 10000
  - name: test
    num_bytes: 1602271
    num_examples: 10000
  download_size: 1489562
  dataset_size: 6393897
- config_name: PAN-X.et
  features:
  - name: tokens
    sequence: string
  - name: ner_tags
    sequence:
      class_label:
        names:
          '0': O
          '1': B-PER
          '2': I-PER
          '3': B-ORG
          '4': I-ORG
          '5': B-LOC
          '6': I-LOC
  - name: langs
    sequence: string
  splits:
  - name: train
    num_bytes: 3023171
    num_examples: 15000
  - name: validation
    num_bytes: 2030140
    num_examples: 10000
  - name: test
    num_bytes: 2021389
    num_examples: 10000
  download_size: 1915624
  dataset_size: 7074700
- config_name: PAN-X.eu
  features:
  - name: tokens
    sequence: string
  - name: ner_tags
    sequence:
      class_label:
        names:
          '0': O
          '1': B-PER
          '2': I-PER
          '3': B-ORG
          '4': I-ORG
          '5': B-LOC
          '6': I-LOC
  - name: langs
    sequence: string
  splits:
  - name: train
    num_bytes: 2292307
    num_examples: 10000
  - name: validation
    num_bytes: 2296315
    num_examples: 10000
  - name: test
    num_bytes: 2249815
    num_examples: 10000
  download_size: 1393179
  dataset_size: 6838437
- config_name: PAN-X.fa
  features:
  - name: tokens
    sequence: string
  - name: ner_tags
    sequence:
      class_label:
        names:
          '0': O
          '1': B-PER
          '2': I-PER
          '3': B-ORG
          '4': I-ORG
          '5': B-LOC
          '6': I-LOC
  - name: langs
    sequence: string
  splits:
  - name: train
    num_bytes: 3529314
    num_examples: 20000
  - name: validation
    num_bytes: 178228",High,5.0
Q&A,clips/mqa,53.0,1051.0,2022-09-27 12:38:50+00:00,cc0-1.0,2.0,UNKNOWN,unknown,122 GB,unknown,288312491,none,"['https://aclanthology.org/2021.mrqa-1.1"",']","---
annotations_creators:
- no-annotation
language_creators:
- other
language:
- ca
- en
- de
- es
- fr
- ru
- ja
- it
- zh
- pt
- nl
- tr
- pl
- vi
- ar
- id
- uk
- ro
- no
- th
- sv
- el
- fi
- he
- da
- cs
- ko
- fa
- hi
- hu
- sk
- lt
- et
- hr
- is
- lv
- ms
- bg
- sr
- ca 
license:
- cc0-1.0
multilinguality:
- multilingual
pretty_name: MQA - a Multilingual FAQ and CQA Dataset
size_categories:
- unknown
source_datasets:
- original
task_categories:
- question-answering
task_ids:
- multiple-choice-qa
---
# MQA
MQA is a Multilingual corpus of Questions and Answers (MQA) parsed from the [Common Crawl](https://commoncrawl.org/). Questions are divided in two types: *Frequently Asked Questions (FAQ)* and *Community Question Answering (CQA)*.
```python
from datasets import load_dataset
all_data = load_dataset(""clips/mqa"", language=""en"")
{
  ""name"": ""the title of the question (if any)"",
  ""text"": ""the body of the question (if any)"",
  ""answers"": [{
    ""text"": ""the text of the answer"",
    ""is_accepted"": ""true|false""
  }]
}
faq_data = load_dataset(""clips/mqa"", scope=""faq"", language=""en"")
cqa_data = load_dataset(""clips/mqa"", scope=""cqa"", language=""en"")
```

## Languages
We collected around **234M pairs** of questions and answers in **39 languages**. To download a language specific subset you need to specify the language key as configuration. See below for an example.
```python
load_dataset(""clips/mqa"", language=""en"") # replace ""en"" by any language listed below
```

| Language   |         FAQ |        CQA |
|:-----------|------------:|-----------:|
| en         | 174,696,414 | 14,082,180 |
| de         |  17,796,992 |  1,094,606 |
| es         |  14,967,582 |    845,836 |
| fr         |  13,096,727 |  1,299,359 |
| ru         |  12,435,022 |  1,715,131 |
| it         |   6,850,573 |    455,027 |
| ja         |   6,369,706 |  2,089,952 |
| zh         |   5,940,796 |    579,596 |
| pt         |   5,851,286 |    373,982 |
| nl         |   4,882,511 |    503,376 |
| tr         |   3,893,964 |    370,975 |
| pl         |   3,766,531 |     70,559 |
| vi         |   2,795,227 |     96,528 |
| id         |   2,253,070 |    200,441 |
| ar         |   2,211,795 |    805,661 |
| uk         |   2,090,611 |     27,260 |
| el         |   1,758,618 |     17,167 |
| no         |   1,752,820 |     11,786 |
| sv         |   1,733,582 |     20,024 |
| fi         |   1,717,221 |     41,371 |
| ro         |   1,689,471 |     93,222 |
| th         |   1,685,463 |     73,204 |
| da         |   1,554,581 |     16,398 |
| he         |   1,422,449 |     88,435 |
| ko         |   1,361,901 |     49,061 |
| cs         |   1,224,312 |    143,863 |
| hu         |     878,385 |     27,639 |
| fa         |     787,420 |    118,805 |
| sk         |     785,101 |      4,615 |
| lt         |     672,105 |        301 |
| et         |     547,208 |        441 |
| hi         |     516,342 |    205,645 |
| hr         |     458,958 |     11,677 |
| is         |     437,748 |         37 |
| lv         |     428,002 |         88 |
| ms         |     230,568 |      7,460 |
| bg         |     198,671 |      5,320 |
| sr         |     110,270 |      3,980 |
| ca         |     100,201 |      1,914 |

## FAQ vs. CQA
You can download the *Frequently Asked Questions* (FAQ) or the *Community Question Answering* (CQA) part of the dataset.

```python
faq = load_dataset(""clips/mqa"", scope=""faq"")
cqa = load_dataset(""clips/mqa"", scope=""cqa"")
all = load_dataset(""clips/mqa"", scope=""all"")
```
Although FAQ and CQA questions share the same structure, CQA questions can have multiple answers for a given questions, while FAQ questions have a single answer. FAQ questions typically only have a title (`name` key), while CQA have a title and a body (`name` and `text`).

## Nesting and Data Fields
You can specify three different nesting level: `question`, `page` and `domain`.
#### Question
```python
load_dataset(""clips/mqa"", level=""question"") # default
```
The default level is the question object:
- **name**: the title of the question(if any) in markdown format 
- **text**: the body of the question (if any) in markdown format
- **answers**: a list of answers
    - **text**: the title of the answer (if any) in markdown format
    - **name**: the body of the answer in markdown format
    - **is_accepted**: true if the answer is selected.

#### Page
This level returns a list of questions present on the same page. This is mostly useful for FAQs since CQAs already have one question per page.
```python
load_dataset(""clips/mqa"", level=""page"")
```

#### Domain
This level returns a list of pages present on the web domain. This is a good way to cope with FAQs duplication by sampling one page per domain at each epoch.

```python
load_dataset(""clips/mqa"", level=""domain"")
```

## Source Data

This section was adapted from the source data description of [OSCAR](https://huggingface.co/datasets/oscar#source-data)

Common Crawl is a non-profit foundation which produces and maintains an open repository of web crawled data that is both accessible and analysable. Common Crawl's complete web archive consists of petabytes of data collected over 8 years of web crawling. The repository contains raw web page HTML data (WARC files), metdata extracts (WAT files) and plain text extracts (WET files). The organisation's crawlers has always respected nofollow and robots.txt policies.

To construct MQA, we used the WARC files of Common Crawl.

## People
This model was developed by [Maxime De Bruyn](https://maximedb.vercel.app), Ehsan Lotfi, Jeska Buhmann and Walter Daelemans.

## Licensing Information
```
These data are released under this licensing scheme.
We do not own any of the text from which these data has been extracted.
We license the actual packaging of these data under the Creative Commons CC0 license (""no rights reserved"") http://creativecommons.org/publicdomain/zero/1.0/

Should you consider that our data contains material that is owned by you and should therefore not be reproduced here, please:
* Clearly identify yourself, with detailed contact data such as an address, telephone number or email address at which you can be contacted.
* Clearly identify the copyrighted work claimed to be infringed.
* Clearly identify the material that is claimed to be infringing and information reasonably sufficient to allow us to locate the material.

We will comply to legitimate requests by removing the affected sources from the next release of the corpus.
```

## Citation information
```
@inproceedings{de-bruyn-etal-2021-mfaq,
    title = ""{MFAQ}: a Multilingual {FAQ} Dataset"",
    author = ""De Bruyn, Maxime  and
      Lotfi, Ehsan  and
      Buhmann, Jeska  and
      Daelemans, Walter"",
    booktitle = ""Proceedings of the 3rd Workshop on Machine Reading for Question Answering"",
    month = nov,
    year = ""2021"",
    address = ""Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.mrqa-1.1"",
    pages = ""1--13"",
}

```",High,5.0
Q&A,khalidalt/tydiqa-goldp,11.0,2323.0,2024-09-10 18:28:52+00:00,apache-2.0,13.0,3726.74 MB,unknown,UNKNOWN,unknown,216797,none,"['https://aclanthology.org/2021.emnlp-main.802"",']","---
pretty_name: TyDi QA
annotations_creators:
- crowdsourced
language_creators:
- crowdsourced
language:
- en
- ar
- bn
- fi
- id
- ja
- sw
- ko
- ru
- te
- th
license:
- apache-2.0
multilinguality:
- multilingual
size_categories:
- unknown
source_datasets:
- extended|wikipedia
task_categories:
- question-answering
task_ids:
- extractive-qa
paperswithcode_id: tydi-qa
---

# Dataset Card for ""tydiqa""

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [https://github.com/google-research-datasets/tydiqa](https://github.com/google-research-datasets/tydiqa)
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Size of downloaded dataset files:** 3726.74 MB
- **Size of the generated dataset:** 5812.92 MB
- **Total amount of disk used:** 9539.67 MB

### Dataset Summary

TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.
The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language
expresses -- such that we expect models performing well on this set to generalize across a large number of the languages
in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic
information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but
don’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without
the use of translation (unlike MLQA and XQuAD).

### Supported Tasks and Leaderboards

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Languages

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Dataset Structure

### Data Instances

#### primary_task

- **Size of downloaded dataset files:** 1863.37 MB
- **Size of the generated dataset:** 5757.59 MB
- **Total amount of disk used:** 7620.96 MB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""annotations"": {
        ""minimal_answers_end_byte"": [-1, -1, -1],
        ""minimal_answers_start_byte"": [-1, -1, -1],
        ""passage_answer_candidate_index"": [-1, -1, -1],
        ""yes_no_answer"": [""NONE"", ""NONE"", ""NONE""]
    },
    ""document_plaintext"": ""\""\\nรองศาสตราจารย์[1] หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร  (22 กันยายน 2495 -) ผู้ว่าราชการกรุงเทพมหานครคนที่ 15 อดีตรองหัวหน้าพรรคปร..."",
    ""document_title"": ""หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร"",
    ""document_url"": ""\""https://th.wikipedia.org/wiki/%E0%B8%AB%E0%B8%A1%E0%B9%88%E0%B8%AD%E0%B8%A1%E0%B8%A3%E0%B8%B2%E0%B8%8A%E0%B8%A7%E0%B8%87%E0%B8%..."",
    ""language"": ""thai"",
    ""passage_answer_candidates"": ""{\""plaintext_end_byte\"": [494, 1779, 2931, 3904, 4506, 5588, 6383, 7122, 8224, 9375, 10473, 12563, 15134, 17765, 19863, 21902, 229..."",
    ""question_text"": ""\""หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร เรียนจบจากที่ไหน ?\""...""
}
```

#### secondary_task

- **Size of downloaded dataset files:** 1863.37 MB
- **Size of the generated dataset:** 55.34 MB
- **Total amount of disk used:** 1918.71 MB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [394],
        ""text"": [""بطولتين""]
    },
    ""context"": ""\""أقيمت البطولة 21 مرة، شارك في النهائيات 78 دولة، وعدد الفرق التي فازت بالبطولة حتى الآن 8 فرق، ويعد المنتخب البرازيلي الأكثر تت..."",
    ""id"": ""arabic-2387335860751143628-1"",
    ""question"": ""\""كم عدد مرات فوز الأوروغواي ببطولة كاس العالم لكرو القدم؟\""..."",
    ""title"": ""قائمة نهائيات كأس العالم""
}
```

### Data Fields

The data fields are the same among all splits.

#### primary_task
- `passage_answer_candidates`: a dictionary feature containing:
  - `plaintext_start_byte`: a `int32` feature.
  - `plaintext_end_byte`: a `int32` feature.
- `question_text`: a `string` feature.
- `document_title`: a `string` feature.
- `language`: a `string` feature.
- `annotations`: a dictionary feature containing:
  - `passage_answer_candidate_index`: a `int32` feature.
  - `minimal_answers_start_byte`: a `int32` feature.
  - `minimal_answers_end_byte`: a `int32` feature.
  - `yes_no_answer`: a `string` feature.
- `document_plaintext`: a `string` feature.
- `document_url`: a `string` feature.

#### secondary_task
- `id`: a `string` feature.
- `title`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

### Data Splits

| name           |  train | validation |
| -------------- | -----: | ---------: |
| primary_task   | 166916 |      18670 |
| secondary_task |  49881 |       5077 |

## Dataset Creation

### Curation Rationale

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the source language producers?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Annotations

#### Annotation process

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the annotators?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Personal and Sensitive Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Discussion of Biases

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Other Known Limitations

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Additional Information

### Dataset Curators

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Licensing Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Citation Information

```
@article{tydiqa,
title   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},
author  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki}
year    = {2020},
journal = {Transactions of the Association for Computational Linguistics}
}





```

```
@inproceedings{ruder-etal-2021-xtreme,
    title = ""{XTREME}-{R}: Towards More Challenging and Nuanced Multilingual Evaluation"",
    author = ""Ruder, Sebastian  and
      Constant, Noah  and
      Botha, Jan  and
      Siddhant, Aditya  and
      Firat, Orhan  and
      Fu, Jinlan  and
      Liu, Pengfei  and
      Hu, Junjie  and
      Garrette, Dan  and
      Neubig, Graham  and
      Johnson, Melvin"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.802"",
    doi = ""10.18653/v1/2021.emnlp-main.802"",
    pages = ""10215--10245"",

}

}
```




",High,5.0
Q&A,juletxara/xquad_xtreme,8.0,146.0,2024-09-10 18:37:12+00:00,cc-by-sa-4.0,1.0,139.53 MB,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/1910.11856,none,"---
pretty_name: XQuAD-XTREME
annotations_creators:
- expert-generated
language_creators:
- expert-generated
language:
- en
- es
- de
- el
- hi
- th
- ru
- tr
- ar
- vi
- zh
- ro
license:
- cc-by-sa-4.0
multilinguality:
- multilingual
size_categories:
- unknown
source_datasets:
- extended|squad
task_categories:
- question-answering
task_ids:
- extractive-qa
paperswithcode_id: xquad
---

# Dataset Card for XQuAD-XTREME

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [https://github.com/deepmind/xquad](https://github.com/deepmind/xquad)
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Size of downloaded dataset files:** 139.53 MB
- **Size of the generated dataset:** 18.09 MB
- **Total amount of disk used:** 157.62 MB

### Dataset Summary

XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering
performance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set
of SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten language: Spanish, German,
Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi and Romanian. Consequently, the dataset is entirely parallel across 12 languages.

We also include ""translate-train"", ""translate-dev"", and ""translate-test""
splits for each non-English language from XTREME (Hu et al., 2020). These can be used to run XQuAD in the ""translate-train"" or ""translate-test"" settings. https://proceedings.mlr.press/v119/hu20b/hu20b.pdf

### Supported Tasks and Leaderboards

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Languages

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Dataset Structure

### Data Instances

#### ar

- **Size of downloaded dataset files:** 12.68 MB
- **Size of the generated dataset:** 1.64 MB
- **Total amount of disk used:** 14.33 MB

An example of 'test' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [527],
        ""text"": [""136""]
    },
    ""context"": ""\""Die Verteidigung der Panthers gab nur 308 Punkte ab und belegte den sechsten Platz in der Liga, während sie die NFL mit 24 Inte..."",
    ""id"": ""56beb4343aeaaa14008c925c"",
    ""question"": ""Wie viele Sacks erzielte Jared Allen in seiner Karriere?""
}
```

#### de

- **Size of downloaded dataset files:** 12.68 MB
- **Size of the generated dataset:** 1.23 MB
- **Total amount of disk used:** 13.91 MB

An example of 'test' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [527],
        ""text"": [""136""]
    },
    ""context"": ""\""Die Verteidigung der Panthers gab nur 308 Punkte ab und belegte den sechsten Platz in der Liga, während sie die NFL mit 24 Inte..."",
    ""id"": ""56beb4343aeaaa14008c925c"",
    ""question"": ""Wie viele Sacks erzielte Jared Allen in seiner Karriere?""
}
```

#### el

- **Size of downloaded dataset files:** 12.68 MB
- **Size of the generated dataset:** 2.11 MB
- **Total amount of disk used:** 14.79 MB

An example of 'test' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [527],
        ""text"": [""136""]
    },
    ""context"": ""\""Die Verteidigung der Panthers gab nur 308 Punkte ab und belegte den sechsten Platz in der Liga, während sie die NFL mit 24 Inte..."",
    ""id"": ""56beb4343aeaaa14008c925c"",
    ""question"": ""Wie viele Sacks erzielte Jared Allen in seiner Karriere?""
}
```

#### en

- **Size of downloaded dataset files:** 12.68 MB
- **Size of the generated dataset:** 1.07 MB
- **Total amount of disk used:** 13.75 MB

An example of 'test' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [527],
        ""text"": [""136""]
    },
    ""context"": ""\""Die Verteidigung der Panthers gab nur 308 Punkte ab und belegte den sechsten Platz in der Liga, während sie die NFL mit 24 Inte..."",
    ""id"": ""56beb4343aeaaa14008c925c"",
    ""question"": ""Wie viele Sacks erzielte Jared Allen in seiner Karriere?""
}
```

#### es

- **Size of downloaded dataset files:** 12.68 MB
- **Size of the generated dataset:** 1.22 MB
- **Total amount of disk used:** 13.90 MB

An example of 'test' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [527],
        ""text"": [""136""]
    },
    ""context"": ""\""Die Verteidigung der Panthers gab nur 308 Punkte ab und belegte den sechsten Platz in der Liga, während sie die NFL mit 24 Inte..."",
    ""id"": ""56beb4343aeaaa14008c925c"",
    ""question"": ""Wie viele Sacks erzielte Jared Allen in seiner Karriere?""
}
```

### Data Fields

The data fields are the same among all splits.

#### ar
- `id`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

#### de
- `id`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

#### el
- `id`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

#### en
- `id`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

#### es
- `id`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

### Data Splits

| name     | validation |
| -------- | ---------: |
| ar |       1190 |
| de |       1190 |
| el |       1190 |
| en |       1190 |
| es |       1190 |

## Dataset Creation

### Curation Rationale

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the source language producers?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Annotations

#### Annotation process

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the annotators?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Personal and Sensitive Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Discussion of Biases

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Other Known Limitations

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Additional Information

### Dataset Curators

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Licensing Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Citation Information

```
@article{Artetxe:etal:2019,
      author    = {Mikel Artetxe and Sebastian Ruder and Dani Yogatama},
      title     = {On the cross-lingual transferability of monolingual representations},
      journal   = {CoRR},
      volume    = {abs/1910.11856},
      year      = {2019},
      archivePrefix = {arXiv},
      eprint    = {1910.11856}
}

```


### Contributions

Thanks to [@lewtun](https://github.com/lewtun), [@patrickvonplaten](https://github.com/patrickvonplaten), [@thomwolf](https://github.com/thomwolf) for adding this dataset.",High,5.0
Q&A,juletxara/tydiqa_xtreme,1.0,156.0,2024-09-10 18:48:21+00:00,apache-2.0,0.0,3726.74 MB,unknown,UNKNOWN,unknown,216797,https://arxiv.org/abs/2003.11080,"['https://aclanthology.org/2021.emnlp-main.802"",']","---
pretty_name: TyDi QA
annotations_creators:
- crowdsourced
language_creators:
- crowdsourced
language:
- en
- ar
- bn
- fi
- id
- ja
- sw
- ko
- ru
- te
- th
license:
- apache-2.0
multilinguality:
- multilingual
size_categories:
- unknown
source_datasets:
- extended|wikipedia
task_categories:
- question-answering
task_ids:
- extractive-qa
paperswithcode_id: tydi-qa
---

# Dataset Card for ""tydiqa""

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [https://github.com/google-research-datasets/tydiqa](https://github.com/google-research-datasets/tydiqa)
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Size of downloaded dataset files:** 3726.74 MB
- **Size of the generated dataset:** 5812.92 MB
- **Total amount of disk used:** 9539.67 MB

### Dataset Summary

TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.
The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language
expresses -- such that we expect models performing well on this set to generalize across a large number of the languages
in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic
information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but
don’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without
the use of translation (unlike MLQA and XQuAD).

We also include ""translate-train"" and ""translate-test"" splits for each non-English languages from XTREME (Hu et al., 2020). These splits are the automatic translations from English to each target language used in the XTREME paper [https://arxiv.org/abs/2003.11080]. The ""translate-train"" split purposefully ignores the non-English TyDiQA-GoldP training data to simulate the transfer learning scenario where original-language data is not available and system builders must rely on labeled English data plus existing machine translation systems.

### Supported Tasks and Leaderboards

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Languages

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Dataset Structure

### Data Instances

#### primary_task

- **Size of downloaded dataset files:** 1863.37 MB
- **Size of the generated dataset:** 5757.59 MB
- **Total amount of disk used:** 7620.96 MB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""annotations"": {
        ""minimal_answers_end_byte"": [-1, -1, -1],
        ""minimal_answers_start_byte"": [-1, -1, -1],
        ""passage_answer_candidate_index"": [-1, -1, -1],
        ""yes_no_answer"": [""NONE"", ""NONE"", ""NONE""]
    },
    ""document_plaintext"": ""\""\\nรองศาสตราจารย์[1] หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร  (22 กันยายน 2495 -) ผู้ว่าราชการกรุงเทพมหานครคนที่ 15 อดีตรองหัวหน้าพรรคปร..."",
    ""document_title"": ""หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร"",
    ""document_url"": ""\""https://th.wikipedia.org/wiki/%E0%B8%AB%E0%B8%A1%E0%B9%88%E0%B8%AD%E0%B8%A1%E0%B8%A3%E0%B8%B2%E0%B8%8A%E0%B8%A7%E0%B8%87%E0%B8%..."",
    ""language"": ""thai"",
    ""passage_answer_candidates"": ""{\""plaintext_end_byte\"": [494, 1779, 2931, 3904, 4506, 5588, 6383, 7122, 8224, 9375, 10473, 12563, 15134, 17765, 19863, 21902, 229..."",
    ""question_text"": ""\""หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร เรียนจบจากที่ไหน ?\""...""
}
```

#### secondary_task

- **Size of downloaded dataset files:** 1863.37 MB
- **Size of the generated dataset:** 55.34 MB
- **Total amount of disk used:** 1918.71 MB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [394],
        ""text"": [""بطولتين""]
    },
    ""context"": ""\""أقيمت البطولة 21 مرة، شارك في النهائيات 78 دولة، وعدد الفرق التي فازت بالبطولة حتى الآن 8 فرق، ويعد المنتخب البرازيلي الأكثر تت..."",
    ""id"": ""arabic-2387335860751143628-1"",
    ""question"": ""\""كم عدد مرات فوز الأوروغواي ببطولة كاس العالم لكرو القدم؟\""..."",
    ""title"": ""قائمة نهائيات كأس العالم""
}
```

### Data Fields

The data fields are the same among all splits.

#### primary_task
- `passage_answer_candidates`: a dictionary feature containing:
  - `plaintext_start_byte`: a `int32` feature.
  - `plaintext_end_byte`: a `int32` feature.
- `question_text`: a `string` feature.
- `document_title`: a `string` feature.
- `language`: a `string` feature.
- `annotations`: a dictionary feature containing:
  - `passage_answer_candidate_index`: a `int32` feature.
  - `minimal_answers_start_byte`: a `int32` feature.
  - `minimal_answers_end_byte`: a `int32` feature.
  - `yes_no_answer`: a `string` feature.
- `document_plaintext`: a `string` feature.
- `document_url`: a `string` feature.

#### secondary_task
- `id`: a `string` feature.
- `title`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

### Data Splits

| name           |  train | validation |
| -------------- | -----: | ---------: |
| primary_task   | 166916 |      18670 |
| secondary_task |  49881 |       5077 |

## Dataset Creation

### Curation Rationale

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the source language producers?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Annotations

#### Annotation process

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the annotators?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Personal and Sensitive Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Discussion of Biases

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Other Known Limitations

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Additional Information

### Dataset Curators

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Licensing Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Citation Information

```
@article{tydiqa,
title   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},
author  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki}
year    = {2020},
journal = {Transactions of the Association for Computational Linguistics}
}





```

```
@inproceedings{ruder-etal-2021-xtreme,
    title = ""{XTREME}-{R}: Towards More Challenging and Nuanced Multilingual Evaluation"",
    author = ""Ruder, Sebastian  and
      Constant, Noah  and
      Botha, Jan  and
      Siddhant, Aditya  and
      Firat, Orhan  and
      Fu, Jinlan  and
      Liu, Pengfei  and
      Hu, Junjie  and
      Garrette, Dan  and
      Neubig, Graham  and
      Johnson, Melvin"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.802"",
    doi = ""10.18653/v1/2021.emnlp-main.802"",
    pages = ""10215--10245"",

}

}
```
",High,5.0
Q&A,khalidalt/tydiqa-primary,0.0,113.0,2022-07-28 21:56:04+00:00,apache-2.0,0.0,3726.74 MB,unknown,UNKNOWN,unknown,166916,none,"['https://aclanthology.org/2021.emnlp-main.802"",']","---
pretty_name: TyDi QA
annotations_creators:
- crowdsourced
language_creators:
- crowdsourced
language:
- en
- ar
- bn
- fi
- id
- ja
- sw
- ko
- ru
- te
- th
license:
- apache-2.0
multilinguality:
- multilingual
size_categories:
- unknown
source_datasets:
- extended|wikipedia
task_categories:
- question-answering
task_ids:
- extractive-qa
paperswithcode_id: tydi-qa
---

# Dataset Card for ""tydiqa""

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [https://github.com/google-research-datasets/tydiqa](https://github.com/google-research-datasets/tydiqa)
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Size of downloaded dataset files:** 3726.74 MB
- **Size of the generated dataset:** 5812.92 MB
- **Total amount of disk used:** 9539.67 MB

### Dataset Summary

TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.
The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language
expresses -- such that we expect models performing well on this set to generalize across a large number of the languages
in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic
information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but
don’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without
the use of translation (unlike MLQA and XQuAD).

### Supported Tasks and Leaderboards

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Languages

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Dataset Structure

### Data Instances

#### primary_task

- **Size of downloaded dataset files:** 1863.37 MB
- **Size of the generated dataset:** 5757.59 MB
- **Total amount of disk used:** 7620.96 MB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""annotations"": {
        ""minimal_answers_end_byte"": [-1, -1, -1],
        ""minimal_answers_start_byte"": [-1, -1, -1],
        ""passage_answer_candidate_index"": [-1, -1, -1],
        ""yes_no_answer"": [""NONE"", ""NONE"", ""NONE""]
    },
    ""document_plaintext"": ""\""\\nรองศาสตราจารย์[1] หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร  (22 กันยายน 2495 -) ผู้ว่าราชการกรุงเทพมหานครคนที่ 15 อดีตรองหัวหน้าพรรคปร..."",
    ""document_title"": ""หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร"",
    ""document_url"": ""\""https://th.wikipedia.org/wiki/%E0%B8%AB%E0%B8%A1%E0%B9%88%E0%B8%AD%E0%B8%A1%E0%B8%A3%E0%B8%B2%E0%B8%8A%E0%B8%A7%E0%B8%87%E0%B8%..."",
    ""language"": ""thai"",
    ""passage_answer_candidates"": ""{\""plaintext_end_byte\"": [494, 1779, 2931, 3904, 4506, 5588, 6383, 7122, 8224, 9375, 10473, 12563, 15134, 17765, 19863, 21902, 229..."",
    ""question_text"": ""\""หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร เรียนจบจากที่ไหน ?\""...""
}
```



### Data Fields

The data fields are the same among all splits.

#### primary_task
- `passage_answer_candidates`: a dictionary feature containing:
  - `plaintext_start_byte`: a `int32` feature.
  - `plaintext_end_byte`: a `int32` feature.
- `question_text`: a `string` feature.
- `document_title`: a `string` feature.
- `language`: a `string` feature.
- `annotations`: a dictionary feature containing:
  - `passage_answer_candidate_index`: a `int32` feature.
  - `minimal_answers_start_byte`: a `int32` feature.
  - `minimal_answers_end_byte`: a `int32` feature.
  - `yes_no_answer`: a `string` feature.
- `document_plaintext`: a `string` feature.
- `document_url`: a `string` feature.



### Data Splits

| name           |  train | validation |
| -------------- | -----: | ---------: |
| primary_task   | 166916 |      18670 |


## Dataset Creation

### Curation Rationale

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the source language producers?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Annotations

#### Annotation process

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the annotators?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Personal and Sensitive Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Discussion of Biases

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Other Known Limitations

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Additional Information

### Dataset Curators

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Licensing Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Citation Information

```
@article{tydiqa,
title   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},
author  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki}
year    = {2020},
journal = {Transactions of the Association for Computational Linguistics}
}





```

```
@inproceedings{ruder-etal-2021-xtreme,
    title = ""{XTREME}-{R}: Towards More Challenging and Nuanced Multilingual Evaluation"",
    author = ""Ruder, Sebastian  and
      Constant, Noah  and
      Botha, Jan  and
      Siddhant, Aditya  and
      Firat, Orhan  and
      Fu, Jinlan  and
      Liu, Pengfei  and
      Hu, Junjie  and
      Garrette, Dan  and
      Neubig, Graham  and
      Johnson, Melvin"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.802"",
    doi = ""10.18653/v1/2021.emnlp-main.802"",
    pages = ""10215--10245"",

}

}
```




",High,5.0
Q&A,tarteel-ai/quranqa,16.0,43.0,2024-09-11 08:02:47+00:00,cc-by-nd-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
annotations_creators:
- expert-generated
language:
- ar
language_creators:
- expert-generated
license:
- cc-by-nd-4.0
multilinguality:
- monolingual
pretty_name: Qur'anic Reading Comprehension Dataset
size_categories:
- n<1K
- 1K<n<10K
source_datasets:
- original
tags:
- quran
- qa
task_categories:
- question-answering
task_ids:
- extractive-qa
---

# Dataset Card for the Qur'anic Reading Comprehension Dataset (QRCD)

## Table of Contents
- [Table of Contents](#table-of-contents)
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://sites.google.com/view/quran-qa-2022/home
- **Repository:** https://gitlab.com/bigirqu/quranqa/-/tree/main/
- **Paper:** https://dl.acm.org/doi/10.1145/3400396
- **Leaderboard:** 
- **Point of Contact:** @piraka9011

### Dataset Summary

The QRCD (Qur'anic Reading Comprehension Dataset) is composed of 1,093 tuples of question-passage pairs that are 
  coupled with their extracted answers to constitute 1,337 question-passage-answer triplets.

### Supported Tasks and Leaderboards

This task is evaluated as a ranking task.
To give credit to a QA system that may retrieve an answer (not necessarily at the first rank) that does not fully 
  match one of the gold answers but partially matches it, we use partial Reciprocal Rank (pRR) measure.
It is a variant of the traditional Reciprocal Rank evaluation metric that considers partial matching.
pRR is the official evaluation measure of this shared task.
We will also report Exact Match (EM) and F1@1, which are evaluation metrics applied only on the top predicted answer.
The EM metric is a binary measure that rewards a system only if the top predicted answer exactly matches one of the 
  gold answers.
Whereas, the F1@1 metric measures the token overlap between the top predicted answer and the best matching gold answer.
To get an overall evaluation score, each of the above measures is averaged over all questions.

### Languages

Qur'anic Arabic

## Dataset Structure

### Data Instances

To simplify the structure of the dataset, each tuple contains one passage, one question and a list that may contain 
  one or more answers to that question, as shown below:

```json
{
  ""pq_id"": ""38:41-44_105"",
  ""passage"": ""واذكر عبدنا أيوب إذ نادى ربه أني مسني الشيطان بنصب وعذاب. اركض برجلك هذا مغتسل بارد وشراب. ووهبنا له أهله ومثلهم معهم رحمة منا وذكرى لأولي الألباب. وخذ بيدك ضغثا فاضرب به ولا تحنث إنا وجدناه صابرا نعم العبد إنه أواب."",
  ""surah"": 38,
  ""verses"": ""41-44"",
  ""question"": ""من هو النبي المعروف بالصبر؟"",
  ""answers"": [
    {
      ""text"": ""أيوب"",
      ""start_char"": 12
    }
  ]
}
```

Each Qur’anic passage in QRCD may have more than one occurrence; and each passage occurrence is paired with a different 
  question.
Likewise, each question in QRCD may have more than one occurrence; and each question occurrence is paired with a 
  different Qur’anic passage.
The source of the Qur'anic text in QRCD is the Tanzil project download page, which provides verified versions of the 
  Holy Qur'an in several scripting styles.
We have chosen the simple-clean text style of Tanzil version 1.0.2.

### Data Fields

* `pq_id`: Sample ID
* `passage`: Context text
* `surah`: Surah number
* `verses`: Verse range
* `question`: Question text
* `answers`: List of answers and their start character

### Data Splits

| **Dataset** | **%** | **# Question-Passage  Pairs** | **# Question-Passage-Answer  Triplets** |
|-------------|:-----:|:-----------------------------:|:---------------------------------------:|
| Training    |  65%  |              710              |                   861                   |
| Development |  10%  |              109              |                   128                   |
| Test        |  25%  |              274              |                   348                   |
| All         | 100%  |             1,093             |                  1,337                  |

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

The QRCD v1.1 dataset is distributed under the CC-BY-ND 4.0 License https://creativecommons.org/licenses/by-nd/4.0/legalcode
For a human-readable summary of (and not a substitute for) the above CC-BY-ND 4.0 License, please refer to https://creativecommons.org/licenses/by-nd/4.0/

### Citation Information

```
@article{malhas2020ayatec,
  author = {Malhas, Rana and Elsayed, Tamer},
  title = {AyaTEC: Building a Reusable Verse-Based Test Collection for Arabic Question Answering on the Holy Qur’an},
  year = {2020},
  issue_date = {November 2020},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {19},
  number = {6},
  issn = {2375-4699},
  url = {https://doi.org/10.1145/3400396},
  doi = {10.1145/3400396},
  journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month = {oct},
  articleno = {78},
  numpages = {21},
  keywords = {evaluation, Classical Arabic}
}
```
### Contributions

Thanks to [@piraka9011](https://github.com/piraka9011) for adding this dataset.
",High,4.0
Q&A,copenlu/answerable_tydiqa,8.0,64.0,2024-07-12 11:53:23+00:00,apache-2.0,0.0,79.1 MB,82942361.6,79.1 MB,82942361.6,129392,none,['https://aclanthology.org/2020.tacl-1.30/)'],"---
annotations_creators:
- crowdsourced
language:
- en
- ar
- bn
- fi
- id
- ja
- sw
- ko
- ru
- te
- th
language_creators:
- crowdsourced
license:
- apache-2.0
multilinguality:
- multilingual
pretty_name: Answerable TyDi QA
size_categories:
- '100K<n<1M'
source_datasets:
- extended|wikipedia
task_categories:
- question-answering
task_ids:
- extractive-qa
---

# Dataset Card for ""answerable-tydiqa""


## Dataset Description

- **Homepage:** [https://github.com/google-research-datasets/tydiqa](https://github.com/google-research-datasets/tydiqa)
- **Paper:** [Paper](https://aclanthology.org/2020.tacl-1.30/)
- **Size of downloaded dataset files:** 75.43 MB
- **Size of the generated dataset:** 131.78 MB
- **Total amount of disk used:** 207.21 MB

### Dataset Summary

[TyDi QA](https://huggingface.co/datasets/tydiqa) is a question answering dataset covering 11 typologically diverse languages. 
Answerable TyDi QA is an extension of the GoldP subtask of the original TyDi QA dataset to also include unanswertable questions.

## Dataset Structure

The dataset contains a train and a validation set, with 116067 and 13325 examples, respectively. Access them with

```py
from datasets import load_dataset
dataset = load_dataset(""copenlu/answerable_tydiqa"")
train_set = dataset[""train""]
validation_set = dataset[""validation""]
```

### Data Instances

Here is an example of an instance of the dataset:

```
{'question_text': 'dimanakah  Dr. Ernest François Eugène Douwes Dekker meninggal?',
 'document_title': 'Ernest Douwes Dekker',
 'language': 'indonesian',
 'annotations': 
             {'answer_start': [45],
              'answer_text': ['28 Agustus 1950']
              },
 'document_plaintext': 'Ernest Douwes Dekker wafat dini hari tanggal 28 Agustus 1950 (tertulis di batu nisannya; 29 Agustus 1950 versi van der Veur, 2006) dan dimakamkan di TMP Cikutra, Bandung.',
 'document_url': 'https://id.wikipedia.org/wiki/Ernest%20Douwes%20Dekker'}
```

Description of the dataset columns:

| Column name                  | type        |  Description                                                                                                     |
| -----------                  | ----------- | -----------                                                                                                      |
| document_title               | str         | The title of the Wikipedia article from which the data instance was generated                                    |
| document_url                 | str         | The URL of said article                                                                                          |
| language                     | str         | The language of the data instance                                                                                |
| question_text                | str         | The question to answer                                                                                           |
| document_plaintext           | str         | The context, a Wikipedia paragraph that might or might not contain the answer to the question                    | 
| annotations[""answer_start""]  | list[int]   | The char index in 'document_plaintext' where the answer starts. If the question is unanswerable - [-1]  |
| annotations[""answer_text""]   | list[str]   | The answer, a span of text from 'document_plaintext'. If the question is unanswerable - ['']            |


**Notice:** If the question is *answerable*, annotations[""answer_start""] and annotations[""answer_text""] contain a list of length 1  
(In some variations of the dataset the lists might be longer, e.g. if more than one person annotated the instance, but not in our case). 
If the question is *unanswerable*, annotations[""answer_start""] will have ""-1"", while annotations[""answer_text""] contain a list with an empty string.


## Useful stuff

Check out the [datasets ducumentations](https://huggingface.co/docs/datasets/quickstart) to learn how to manipulate and use the dataset. Specifically, you might find the following functions useful:

`dataset.filter`, for filtering out data (useful for keeping instances of specific languages, for example).

`dataset.map`, for manipulating the dataset.

`dataset.to_pandas`, to convert the dataset into a pandas.DataFrame format.


```
@article{tydiqa,
title   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},
author  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki}
year    = {2020},
journal = {Transactions of the Association for Computational Linguistics}
}

```


### Contributions

Thanks to [@thomwolf](https://github.com/thomwolf), [@albertvillanova](https://github.com/albertvillanova), [@lewtun](https://github.com/lewtun), [@patrickvonplaten](https://github.com/patrickvonplaten) for adding this dataset.",High,4.0
Q&A,copenlu/tydiqa_copenlu,0.0,18.0,2022-08-16 12:10:21+00:00,apache-2.0,0.0,144 MB,150994944.0,82.1 MB,86088089.6,129392,none,none,"---
pretty_name: TyDi QA
annotations_creators:
- crowdsourced
language_creators:
- crowdsourced
language:
- ar
- bn
- en
- fi
- id
- ja
- ko
- ru
- sw
- te
- th
license:
- apache-2.0
multilinguality:
- multilingual
size_categories:
- unknown
source_datasets:
- extended|wikipedia
task_categories:
- question-answering
task_ids:
- extractive-qa
paperswithcode_id: tydi-qa
---

# Dataset Card for ""tydiqa""

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [https://github.com/google-research-datasets/tydiqa](https://github.com/google-research-datasets/tydiqa)
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Size of downloaded dataset files:** 3726.74 MB
- **Size of the generated dataset:** 5812.92 MB
- **Total amount of disk used:** 9539.67 MB

### Dataset Summary

TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.
The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language
expresses -- such that we expect models performing well on this set to generalize across a large number of the languages
in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic
information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but
don’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without
the use of translation (unlike MLQA and XQuAD).

### Supported Tasks and Leaderboards

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Languages

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Dataset Structure

### Data Instances

#### primary_task

- **Size of downloaded dataset files:** 1863.37 MB
- **Size of the generated dataset:** 5757.59 MB
- **Total amount of disk used:** 7620.96 MB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""annotations"": {
        ""minimal_answers_end_byte"": [-1, -1, -1],
        ""minimal_answers_start_byte"": [-1, -1, -1],
        ""passage_answer_candidate_index"": [-1, -1, -1],
        ""yes_no_answer"": [""NONE"", ""NONE"", ""NONE""]
    },
    ""document_plaintext"": ""\""\\nรองศาสตราจารย์[1] หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร  (22 กันยายน 2495 -) ผู้ว่าราชการกรุงเทพมหานครคนที่ 15 อดีตรองหัวหน้าพรรคปร..."",
    ""document_title"": ""หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร"",
    ""document_url"": ""\""https://th.wikipedia.org/wiki/%E0%B8%AB%E0%B8%A1%E0%B9%88%E0%B8%AD%E0%B8%A1%E0%B8%A3%E0%B8%B2%E0%B8%8A%E0%B8%A7%E0%B8%87%E0%B8%..."",
    ""language"": ""thai"",
    ""passage_answer_candidates"": ""{\""plaintext_end_byte\"": [494, 1779, 2931, 3904, 4506, 5588, 6383, 7122, 8224, 9375, 10473, 12563, 15134, 17765, 19863, 21902, 229..."",
    ""question_text"": ""\""หม่อมราชวงศ์สุขุมพันธุ์ บริพัตร เรียนจบจากที่ไหน ?\""...""
}
```

#### secondary_task

- **Size of downloaded dataset files:** 1863.37 MB
- **Size of the generated dataset:** 55.34 MB
- **Total amount of disk used:** 1918.71 MB

An example of 'validation' looks as follows.
```
This example was too long and was cropped:

{
    ""answers"": {
        ""answer_start"": [394],
        ""text"": [""بطولتين""]
    },
    ""context"": ""\""أقيمت البطولة 21 مرة، شارك في النهائيات 78 دولة، وعدد الفرق التي فازت بالبطولة حتى الآن 8 فرق، ويعد المنتخب البرازيلي الأكثر تت..."",
    ""id"": ""arabic-2387335860751143628-1"",
    ""question"": ""\""كم عدد مرات فوز الأوروغواي ببطولة كاس العالم لكرو القدم؟\""..."",
    ""title"": ""قائمة نهائيات كأس العالم""
}
```

### Data Fields

The data fields are the same among all splits.

#### primary_task
- `passage_answer_candidates`: a dictionary feature containing:
  - `plaintext_start_byte`: a `int32` feature.
  - `plaintext_end_byte`: a `int32` feature.
- `question_text`: a `string` feature.
- `document_title`: a `string` feature.
- `language`: a `string` feature.
- `annotations`: a dictionary feature containing:
  - `passage_answer_candidate_index`: a `int32` feature.
  - `minimal_answers_start_byte`: a `int32` feature.
  - `minimal_answers_end_byte`: a `int32` feature.
  - `yes_no_answer`: a `string` feature.
- `document_plaintext`: a `string` feature.
- `document_url`: a `string` feature.

#### secondary_task
- `id`: a `string` feature.
- `title`: a `string` feature.
- `context`: a `string` feature.
- `question`: a `string` feature.
- `answers`: a dictionary feature containing:
  - `text`: a `string` feature.
  - `answer_start`: a `int32` feature.

### Data Splits

| name           |  train | validation |
| -------------- | -----: | ---------: |
| primary_task   | 166916 |      18670 |
| secondary_task |  49881 |       5077 |

## Dataset Creation

### Curation Rationale

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the source language producers?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Annotations

#### Annotation process

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the annotators?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Personal and Sensitive Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Discussion of Biases

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Other Known Limitations

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Additional Information

### Dataset Curators

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Licensing Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Citation Information

```
@article{tydiqa,
title   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},
author  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki}
year    = {2020},
journal = {Transactions of the Association for Computational Linguistics}
}

```


### Contributions

Thanks to [@thomwolf](https://github.com/thomwolf), [@albertvillanova](https://github.com/albertvillanova), [@lewtun](https://github.com/lewtun), [@patrickvonplaten](https://github.com/patrickvonplaten) for adding this dataset.",High,5.0
Q&A,deliadumitrescu/disinfo22-small,2.0,27.0,2023-05-03 09:33:17+00:00,cc-by-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: cc-by-4.0
task_categories:
- feature-extraction
- token-classification
- question-answering
- translation
- summarization
language:
- en
- ar
- pt
- es
- si
- tr
- gu
- id
- ml
- uk
tags:
- not-for-all-audiences
- medical
size_categories:
- n<1K
---
Data set contains 379 images of posts identified as mis-/disinformation and 1 csv file linking the image IDs to urls. The list of urls orginates from the CoronaVirusFacts Database of the International
Fact Checking Network.",Low,1.0
Q&A,AmazonScience/xtr-wiki_qa,5.0,118.0,2023-07-24 17:32:38+00:00,cdla-permissive-2.0,0.0,80.1 MB,83990937.6,34.7 MB,36385587.2,292518,none,"['https://aclanthology.org/2023.findings-acl.885/)', 'https://aclanthology.org/2023.findings-acl.885/).']","---
annotations_creators:
- machine-generated
language:
- ar
- es
- fr
- de
- hi
- it
- ja
- nl
- pt
language_creators:
- found
license_details: https://huggingface.co/datasets/AmazonScience/xtr-wiki_qa/blob/main/LICENSE.md
multilinguality:
- multilingual
- translation
pretty_name: xtr-wiki_qa
size_categories:
- 100K<n<1M
source_datasets:
- extended|wiki_qa
tags:
- as2
- answer sentence selection
- text retrieval
- question answering
task_categories:
- question-answering
- text-retrieval
task_ids:
- open-domain-qa
license: cdla-permissive-2.0
---


# Xtr-WikiQA

## Table of Contents
- [Dataset Card Creation Guide](#dataset-card-creation-guide)
  - [Table of Contents](#table-of-contents)
  - [Dataset Description](#dataset-description)
    - [Dataset Summary](#dataset-summary)
    - [Languages](#languages)
  - [Dataset Structure](#dataset-structure)
    - [Data Instances](#data-instances)
    - [Data Fields](#data-fields)
    - [Data Splits](#data-splits)
  - [Dataset Creation](#dataset-creation)
    - [Source Data](#source-data)
  - [Additional Information](#additional-information)
    - [Licensing Information](#licensing-information)
    - [Citation Information](#citation-information)
    - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [Amazon Science](https://www.amazon.science/publications/cross-lingual-knowledge-distillation-for-answer-sentence-selection-in-low-resource-languages)
- **Paper:** [Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages](https://aclanthology.org/2023.findings-acl.885/)
- **Point of Contact:** [Yoshitomo Matsubara](yomtsub@amazon.com)

### Dataset Summary

***Xtr-WikiQA*** is an Answer Sentence Selection (AS2) dataset in 9 non-English languages, proposed in our paper accepted at ACL 2023 (Findings): [**Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages**](https://aclanthology.org/2023.findings-acl.885/).
This dataset is based on an English AS2 dataset, WikiQA ([Original](https://msropendata.com/datasets/21032bb1-88bd-4656-9570-3172ae1757f0), [Hugging Face](https://huggingface.co/datasets/wiki_qa)).
For translations, we used [Amazon Translate](https://aws.amazon.com/translate/).

### Languages

- Arabic (ar)
- Spanish (es)
- French (fr)
- German (de)
- Hindi (hi)
- Italian (it)
- Japanese (ja)
- Dutch (nl)
- Portuguese (pt)

File location: [`tsv/`](https://huggingface.co/datasets/AmazonScience/xtr-wiki_qa/tree/main/tsv)

## Dataset Structure

### Data Instances

This is an example instance from the Arabic training split of Xtr-WikiQA dataset. 

```
{
  ""QuestionID"": ""Q1"",
  ""Question"": ""كيف تتشكل الكهوف الجليدية؟"",
  ""DocumentID"": ""D1"",
  ""DocumentTitle"": ""كهف جليدي"",
  ""SentenceID"": ""D1-0"",
  ""Sentence"": ""كهف جليدي مغمور جزئيًا على نهر بيريتو مورينو الجليدي."",
  ""Label"": 0
}
```

All the translated instances in tsv files are listed in the same order of the original (native) instances in the WikiQA dataset.

For example, the 2nd instance in [`tsv/ar-train.tsv`](https://huggingface.co/datasets/AmazonScience/xtr-wiki_qa/blob/main/tsv/ar-train.tsv) (Arabic-translated from English)
corresponds to the 2nd instance in [`WikiQA-train.tsv`](https://msropendata.com/datasets/21032bb1-88bd-4656-9570-3172ae1757f0) (English).

### Data Fields

Each instance (a QA pair) consists of the following fields:

- `QuestionID`: Question ID (str)
- `Question`: Question to be answered (str)
- `DocumentID`: Document ID (str)
- `DocumentTitle`: Document title (str)
- `SentenceID`: Answer sentence in the document (str)
- `Sentence`: Answer sentence in the document (str)
- `Label`: Label that indicates the answer sentence correctly answers the question (int, 1: correct, 0: incorrect)


### Data Splits

|                   |             | **#Questions** |          |   |           | **#Sentences** |          |
|-------------------|------------:|---------------:|---------:|---|----------:|---------------:|---------:|
|                   |   **train** |        **dev** | **test** |   | **train** |        **dev** | **test** |
| **Each language** |         873 |            126 |      243 |   |     8,671 |          1,130 |    2,351 |

See [our paper](#citation-information) for more details about the statistics of the datasets.


## Dataset Creation

### Source Data

The source of Xtr-WikiQA dataset is [WikiQA](https://msropendata.com/datasets/21032bb1-88bd-4656-9570-3172ae1757f0).


## Additional Information

### Licensing Information

[CDLA-Permissive-2.0](LICENSE.md)

### Citation Information

```bibtex
@inproceedings{gupta2023cross-lingual,
  title={{Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages}},
  author={Gupta, Shivanshu and Matsubara, Yoshitomo and Chadha, Ankit and Moschitti, Alessandro},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={14078--14092},
  year={2023}
}
```


### Contributions

- [Shivanshu Gupta](https://huggingface.co/shivanshu)
- [Yoshitomo Matsubara](https://huggingface.co/yoshitomo-matsubara)
- Ankit Chadha
- Alessandro Moschitti",High,5.0
Q&A,hltcoe/megawika,38.0,92830.0,2025-01-31 15:32:11+00:00,cc-by-sa-4.0,2.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2307.07049,['https://aclanthology.org/2021.eacl-demos.19.pdf)'],"---
license: cc-by-sa-4.0
task_categories:
- summarization
- question-answering
- text-generation
- text2text-generation
language:
- af
- ar
- az
- bn
- cs
- de
- en
- es
- et
- fa
- fi
- fr
- ga
- gl
- gu
- he
- hi
- hr
- id
- it
- ja
- ka
- kk
- km
- ko
- lt
- lv
- mk
- ml
- mn
- mr
- my
- ne
- nl
- pl
- ps
- pt
- ro
- ru
- si
- sl
- sv
- ta
- th
- tr
- uk
- ur
- vi
- xh
- zh
pretty_name: MegaWika
size_categories:
- 10M<n<100M
---
# Dataset Card for MegaWika

## Dataset Description

- **Homepage:** [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika)
- **Repository:** [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika)
- **Paper:** [Coming soon]
- **Leaderboard:** [Coming soon]
- **Point of Contact:** [Samuel Barham](samuel.barham@jhuapl.edu)

### Dataset Summary

MegaWika is a multi- and crosslingual text dataset containing 30 million Wikipedia passages with their scraped and cleaned web citations. The passages span
50 Wikipedias in 50 languages, and the articles in which the passages were originally embedded are included for convenience. Where a Wikipedia passage is in a
non-English language, an automated English translation is provided. Furthermore, nearly 130 million English question/answer pairs were extracted from the
passages, and FrameNet events occurring in the passages are detected using the [LOME](https://aclanthology.org/2021.eacl-demos.19.pdf) FrameNet parser.


<!---
To get a feel for the dataset -- its structure, content, strengths and weaknesses -- you may visit the [dataset viewer](https://huggingface.co/spaces/hltcoe/megawika)
we have set up as a HuggingFace Space. It allows the curious visitor to explore a small set of examples spread across a number of the dataset's constituent languages.
-->

### Dataset Creation

The pipeline through which MegaWika was created is complex, and is described in more detail in the paper (linked above),
but the following diagram illustrates the basic approach.

![Illustration of MegaWikaProcess](images/MegaWikaProcess-cross-lingual.drawio.png)

### Supported Tasks and Leaderboards

MegaWika is meant to support research across a variety of tasks, including report generation, summarization, information retrieval, question answering, etc.

### Languages

MegaWika is divided by Wikipedia language. There are 50 languages, including English, each designated by their 2-character ISO language code:
- `af`: Afrikaans
- `ar`: Arabic
- `az`: Azeri (Azerbaijani)
- `bn`: Bengali
- `cs`: Czech
- `de`: German (Deutsch)
- `en`: English
- `es`: Spanish (Español)
- `et`: Estonian
- `fa`: Farsi (Persian)
- `fi`: Finnish
- `fr`: French
- `ga`: Irish (Gaelic)
- `gl`: Galician
- `gu`: Gujarati
- `he`: Hebrew
- `hi`: Hindi
- `hr`: Hungarian
- `id`: Indonesian
- `it`: Italian
- `ja`: Japanese
- `ka`: Georgian (Kartvelian/Kartlian)
- `kk`: Kazakh
- `km`: Khmer
- `ko`: Korean
- `lt`: Lithuanian
- `lv`: Latvian
- `mk`: Macedonian (Makedonski)
- `ml`: Malay (Malayalam)
- `mn`: Mongolian
- `mr`: Marathi
- `my`: Burmese (Myanmar language)
- `ne`: Nepali
- `nl`: Dutch (Nederlands)
- `pl`: Polish
- `ps`: Pashto
- `pt`: Portuguese
- `ro`: Romanian
- `ru`: Russian
- `si`: Sinhalese (Sri Lankan language)
- `sl`: Slovenian
- `sv`: Swedish (Svenska)
- `ta`: Tamil
- `th`: Thai
- `tr`: Turkish
- `uk`: Ukrainian
- `ur`: Urdu
- `vi`: Vietnamese
- `xh`: Xhosa
- `zh`: Chinese (Zhōng wén)

## Dataset Structure

The dataset is divided by language, and the data for each of the 50 languages is further chunked into discrete JSON lines files.
Each line of these files -- we'll call such a line an **instance** -- contains the data extracted from a single Wikipedia article.

### Data Instances

Each instance contains the text of the seed Wikipedia article, along with a list of **entries**. Each entry consists basically in
an extracted Wikipedia passage, the URL and scraped text of the web source it cites, a list of questions/answer pairs extracted from the passage,
and a framenet parse of the passage. Where the passage is from a non-English Wikipedia, a machine translation into English is also provided.

### Data Fields

The detailed structure of an instance is as follows:
```
{
  ""article_title"": <string : title of original Wikipedia article>
  ""article_text"": <string : text of Wikipedia article>
  ""entries"": [
    # Wiki Passage
    ""id"": <string : passage ID>
    ""passage"": {
      ""text"": <string : text of passage in English (possibly via MT)>
      ""parse"": <list of dict : FrameNet parse of English passage text>
      ""en_tokens"": <dict : tokenization of passage in English>
      ""lang_tokens"": <dict : tokenization of original non-English passage>
      ""en_lang_token_map"": <dict : alignment mapping between English and original language token indices>
    }

    # MT
    ""original"": <string : original language passage>
    ""original_sents"": <list of string : sentencized original language passage>
    ""translation"": <string : machine translation of passage>
    ""translation_sents"": <list of string : sentencized machine translation of passage>
    ""translation_probs"": <list of float : log prob of machine translation by sentence, where available>
    ""repetitious_translation"": <string \in (""true"", ""false"") : automated judgment on whether machine translation is pathologically repetitious>
    ""source_lang"": <string : language ID, 2-character ISO code>

    # Source
    ""source_url"": <string : URL of the cited web source>
    ""source_text"": <string : content extracted from the scrape of the source URL>

    # Question/Answer Pairs
    ""qa_pairs"": [
      ...
      {
        ""question"": <string : generated question>
        ""passage_id"": <string : passage ID>
        ""en_answer"": <string : English answer>
        ""lang_answer"": <string : aligned original language answer>
        ""frames"": [
          ...
          {
            ""frame"": <string : frame triggered by the question>
            ""argument"": <string : detected frame arguments>
          }
          ...
        ]
        # NB: answer matches can be empty, in the case no matching span exists
        ""en_matches_in_source"": <list of int : start and end index of the English language-answer token(s) in the source document>
        ""en_match_in_passage"": <list of int : start and end index of the English language-answer token(s) in the English language translation of the passage>
        ""lang_matches_in_source"": <list of int : start and end index of the original language-answer token(s) in the source document>
        ""lang_match_in_passage"": <list of int : start and end index of the original language-answer token(s) in the original language passage>
        ""passage"": <list of string : sentencized view of the passage>
        ""en_answer_tokens"": <list of string>
        ""match_disambiguated_question"": <string : disambiguated version of question obtained by matching pronouns with article title (noisy but often helpful)>
      }
      ...
    ]
  ]
}
```

English language instances differ not in structure but in content; 
1. Fields in the block labeled ""MT"" above are naturally null (that is, they are set to falsy values in Python -- specifically `None`)
2. Since the Wiki passage only exists in English, and has no corresponding non-English ""original language"" version, answer spans also necessarily have only an English-language version (and no non-English ""original-language"" version. Therefore, fields in the `qa_pairs` block beginning with `lang_` are set to null/falsy values in Python (in this case, empty lists).


### Data Splits

MegaWika is currently split only by language, as each task will imply its own approach to filtering, sampling, downselecting, and splitting into train/test splits.

<!---
### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]
-->

## Licensing and Takedown

MegaWika 1.0 consists in part of documents scraped from across the web (based on citations linked in Wikipedia articles.)

We do not own any of the scraped text nor do we claim copyright: text drawn from Wikipedia citations are meant for research use in algorithmic design and model training.

We release this dataset and all its contents under CC-BY-SA-4.0.

### Notice and Takedown Policy:
*NB*: Should you consider that our data contains material that is owned by you and should therefore not be reproduced here, please:

- Clearly identify yourself, with detailed contact data such as an address, telephone number or email address at which you can be contacted.
- Clearly identify the copyrighted work claimed to be infringed.
- Clearly identify the material that is claimed to be infringing and information reasonably sufficient to allow us to locate the material.

And contact the authors.

*Take down*: We will comply to legitimate requests by removing the affected sources from the next release of the dataset.

## Additional Information

### Dataset Curators

Released and maintained by the Johns Hopkins University Human Language Technology Center of Excellence (JHU/HLTCOE). 
You can contact one the MegaWika authors, including [Samuel Barham](mailto:samuel.barham@jhuapl.edu), [Orion Weller](mailto:oweller2@jhu.edu),
and [Ben van Durme](mailto:vandurme@jhu.edu) with questions.

### Licensing Information

Released under the [Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/) license.

### Citation Information

```
@misc{barham2023megawika,
      title={MegaWika: Millions of reports and their sources across 50 diverse languages}, 
      author={Samuel Barham and and  Weller and Michelle Yuan and Kenton Murray and Mahsa Yarmohammadi and Zhengping Jiang and Siddharth Vashishtha and Alexander Martin and Anqi Liu and Aaron Steven White and Jordan Boyd-Graber and Benjamin Van Durme},
      year={2023},
      eprint={2307.07049},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

<!--
### Contributions

[More Information Needed]
-->
",High,5.0
Q&A,Salama1429/tarteel-ai-QuranQA,2.0,87.0,2024-09-11 08:03:51+00:00,cc-by-nd-4.0,0.0,487 KB,498688.0,487 KB,498688.0,1331,none,none,"---
annotations_creators:
- expert-generated
language:
- ar
language_creators:
- expert-generated
license:
- cc-by-nd-4.0
multilinguality:
- monolingual
pretty_name: Qur'anic Reading Comprehension Dataset
size_categories:
- n<1K
- 1K<n<10K
source_datasets:
- original
tags:
- quran
- qa
task_categories:
- question-answering
task_ids:
- extractive-qa
---

# Dataset Card for the Qur'anic Reading Comprehension Dataset (QRCD)

## Table of Contents
- [Table of Contents](#table-of-contents)
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://sites.google.com/view/quran-qa-2022/home
- **Repository:** https://gitlab.com/bigirqu/quranqa/-/tree/main/
- **Paper:** https://dl.acm.org/doi/10.1145/3400396
- **Leaderboard:** 
- **Point of Contact:** @piraka9011

### Dataset Summary

The QRCD (Qur'anic Reading Comprehension Dataset) is composed of 1,093 tuples of question-passage pairs that are 
  coupled with their extracted answers to constitute 1,337 question-passage-answer triplets.

### Supported Tasks and Leaderboards

This task is evaluated as a ranking task.
To give credit to a QA system that may retrieve an answer (not necessarily at the first rank) that does not fully 
  match one of the gold answers but partially matches it, we use partial Reciprocal Rank (pRR) measure.
It is a variant of the traditional Reciprocal Rank evaluation metric that considers partial matching.
pRR is the official evaluation measure of this shared task.
We will also report Exact Match (EM) and F1@1, which are evaluation metrics applied only on the top predicted answer.
The EM metric is a binary measure that rewards a system only if the top predicted answer exactly matches one of the 
  gold answers.
Whereas, the F1@1 metric measures the token overlap between the top predicted answer and the best matching gold answer.
To get an overall evaluation score, each of the above measures is averaged over all questions.

### Languages

Qur'anic Arabic

## Dataset Structure

### Data Instances

To simplify the structure of the dataset, each tuple contains one passage, one question and a list that may contain 
  one or more answers to that question, as shown below:

```json
{
  ""pq_id"": ""38:41-44_105"",
  ""passage"": ""واذكر عبدنا أيوب إذ نادى ربه أني مسني الشيطان بنصب وعذاب. اركض برجلك هذا مغتسل بارد وشراب. ووهبنا له أهله ومثلهم معهم رحمة منا وذكرى لأولي الألباب. وخذ بيدك ضغثا فاضرب به ولا تحنث إنا وجدناه صابرا نعم العبد إنه أواب."",
  ""surah"": 38,
  ""verses"": ""41-44"",
  ""question"": ""من هو النبي المعروف بالصبر؟"",
  ""answers"": [
    {
      ""text"": ""أيوب"",
      ""start_char"": 12
    }
  ]
}
```

Each Qur’anic passage in QRCD may have more than one occurrence; and each passage occurrence is paired with a different 
  question.
Likewise, each question in QRCD may have more than one occurrence; and each question occurrence is paired with a 
  different Qur’anic passage.
The source of the Qur'anic text in QRCD is the Tanzil project download page, which provides verified versions of the 
  Holy Qur'an in several scripting styles.
We have chosen the simple-clean text style of Tanzil version 1.0.2.

### Data Fields

* `pq_id`: Sample ID
* `passage`: Context text
* `surah`: Surah number
* `verses`: Verse range
* `question`: Question text
* `answers`: List of answers and their start character

### Data Splits

| **Dataset** | **%** | **# Question-Passage  Pairs** | **# Question-Passage-Answer  Triplets** |
|-------------|:-----:|:-----------------------------:|:---------------------------------------:|
| Training    |  65%  |              710              |                   861                   |
| Development |  10%  |              109              |                   128                   |
| Test        |  25%  |              274              |                   348                   |
| All         | 100%  |             1,093             |                  1,337                  |

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

The QRCD v1.1 dataset is distributed under the CC-BY-ND 4.0 License https://creativecommons.org/licenses/by-nd/4.0/legalcode
For a human-readable summary of (and not a substitute for) the above CC-BY-ND 4.0 License, please refer to https://creativecommons.org/licenses/by-nd/4.0/

### Citation Information

```
@article{malhas2020ayatec,
  author = {Malhas, Rana and Elsayed, Tamer},
  title = {AyaTEC: Building a Reusable Verse-Based Test Collection for Arabic Question Answering on the Holy Qur’an},
  year = {2020},
  issue_date = {November 2020},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {19},
  number = {6},
  issn = {2375-4699},
  url = {https://doi.org/10.1145/3400396},
  doi = {10.1145/3400396},
  journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
  month = {oct},
  articleno = {78},
  numpages = {21},
  keywords = {evaluation, Classical Arabic}
}
```
### Contributions

Thanks to [@piraka9011](https://github.com/piraka9011) for adding this dataset.
",High,4.0
Q&A,pengxiang01/test,0.0,61.0,2023-10-31 08:16:31+00:00,bsl-1.0,0.0,8.17 KB,8366.08,9.66 KB,9891.84,1,none,none,"---
task_categories:
- tabular-to-text
- table-to-text
- multiple-choice
- text-retrieval
- time-series-forecasting
- visual-question-answering
- question-answering
- zero-shot-image-classification
- depth-estimation
language:
- ab
- ak
- ar
license: bsl-1.0
tags:
- biology
- code
- medical
pretty_name: sdfsad
size_categories:
- 10K<n<100K
---
aasdfsdf",Low,1.0
Q&A,nazimali/quran-question-answer-context,7.0,51.0,2024-09-04 00:15:40+00:00,cc-by-4.0,0.0,1.86 MB,1950351.36,1.86 MB,1950351.36,1224,none,none,"---
language:
- ar
- en
license: cc-by-4.0
task_categories:
- question-answering
pretty_name: Quran Question Answer with Context
dataset_info:
  features:
  - name: q_id
    dtype: int64
  - name: question
    dtype: string
  - name: answer
    dtype: string
  - name: q_word
    dtype: string
  - name: q_topic
    dtype: string
  - name: fine_class
    dtype: string
  - name: class
    dtype: string
  - name: ontology_concept
    dtype: string
  - name: ontology_concept2
    dtype: string
  - name: source
    dtype: string
  - name: q_src_id
    dtype: int64
  - name: quetion_type
    dtype: string
  - name: chapter_name
    dtype: string
  - name: chapter_no
    dtype: int64
  - name: verse
    dtype: string
  - name: answer_en
    dtype: string
  - name: class_en
    dtype: string
  - name: fine_class_en
    dtype: string
  - name: ontology_concept2_en
    dtype: string
  - name: ontology_concept_en
    dtype: string
  - name: q_topic_en
    dtype: string
  - name: q_word_en
    dtype: string
  - name: question_en
    dtype: string
  - name: chapter_name_en
    dtype: string
  - name: verse_list
    sequence: int64
  - name: context
    dtype: string
  - name: context_data
    dtype: string
  - name: context_missing_verses
    dtype: string
  splits:
  - name: train
    num_bytes: 3534771
    num_examples: 1224
  download_size: 1858762
  dataset_size: 3534771
tags:
- islam
- quran
- arabic
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
---
# Dataset Card for ""quran-question-answer-context""

## Dataset Summary

Translated the original dataset from Arabic to English and added the Surah ayahs to the `context` column.

## Usage

```python
from datasets import load_dataset

dataset = load_dataset(""nazimali/quran-question-answer-context"")
```

```python
DatasetDict({
    train: Dataset({
        features: ['q_id', 'question', 'answer', 'q_word', 'q_topic', 'fine_class', 'class', 'ontology_concept', 'ontology_concept2', 'source', 'q_src_id', 'quetion_type', 'chapter_name', 'chapter_no', 'verse', 'answer_en', 'class_en', 'fine_class_en', 'ontology_concept2_en', 'ontology_concept_en', 'q_topic_en', 'q_word_en', 'question_en', 'chapter_name_en', 'verse_list', 'context', 'context_data', 'context_missing_verses'],
        num_rows: 1224
    })
})
```

## Translation Info

1. Translated the Arabic questions/concept columns to English with [Helsinki-NLP/opus-mt-ar-en](https://huggingface.co/Helsinki-NLP/opus-mt-ar-en)
2. Used `en-yusufali` translations for ayas [M-AI-C/quran-en-tafssirs](https://huggingface.co/datasets/M-AI-C/quran-en-tafssirs)
3. Renamed Surahs with [kheder/quran](https://huggingface.co/datasets/kheder/quran)
4. Added the ayahs that helped answer the questions
  - Split the `ayah` columns string into a list of integers
  - Concactenated the Surah:Ayah pairs into a sentence to the `context` column

Columns with the suffix `_en` contain the translations of the original columns.

## TODO
The `context` column has some `null` values that needs to be investigated and fixed

## Initial Data Collection

The original dataset is from **[Annotated Corpus of Arabic Al-Quran Question and Answer](https://archive.researchdata.leeds.ac.uk/464/)**    

## Licensing Information

Original dataset [license](https://archive.researchdata.leeds.ac.uk/464/): **Creative Commons Attribution 4.0 International (CC BY 4.0)**

### Contributions

 Original paper authors: Alqahtani, Mohammad and Atwell, Eric (2018) Annotated Corpus of Arabic Al-Quran Question and Answer. University of Leeds. https://doi.org/10.5518/356",High,4.0
Q&A,HeshamHaroon/QA_Arabic,7.0,67.0,2023-07-16 10:37:36+00:00,apache-2.0,0.0,97.9 KB,100249.6,41.7 KB,42700.8,839,none,none,"---
language:
  - ""ar""
pretty_name: ""Questions and Answers Dataset in Arabic""
tags:
  - ""question-answer""
  - ""language-learning""
  - ""chatbot""
license: ""apache-2.0""
task_categories:
  - ""question-answering""
  - ""text-generation""
  - ""text2text-generation""
  
---
# JSON File Description

## Overview
This JSON file contains a collection of questions and answers in Arabic. Each question is associated with its corresponding answer. The file is structured in a way that allows easy retrieval and utilization of the question-answer pairs.

## File Structure
The JSON file follows the following structure:

```json
{
  ""questions"": [
    {
      ""question"": ""من هو أول من نزل على سطح القمر؟"",
      ""answer"": ""نيل أمسترونج""
    },
    {
      ""question"": ""كم عدد الأسنان في فم الإنسان العادي؟"",
      ""answer"": ""32 سنا""
    },
    {
      ""question"": ""كم عدد أعين الذبابة؟"",
      ""answer"": ""5 أعين""
    },
    {
      ""question"": ""كم عدد أرجل العنكبوت؟"",
      ""answer"": ""ج4 - 8 أرجل""
    },
    {
      ""question"": ""س5 - ماذا يسمى بيت النمل؟"",
      ""answer"": ""ج5 - قرية النمل""
    },
    {
      ""question"": ""س6 - كم عظمة توجد في جسم الإنسان؟"",
      ""answer"": ""ج6 - 206 عظمات""
    },
    ...
  ]
}
The file consists of a single object with one key, ""questions,"" which contains an array of question-answer pairs. Each question-answer pair is represented as an object with two keys: ""question"" and ""answer"".

Usage:
- Question-Answer Retrieval: Parse the JSON file and access the question-answer pairs programmatically to retrieve specific questions and their corresponding answers.
- Language Learning: Utilize the question-answer pairs to develop language learning applications or quizzes where users can practice answering questions in Arabic.
- Chatbot Integration: Integrate the JSON file with a chatbot system to provide automated responses based on the questions and answers available.

Feel free to modify the JSON file by adding more question-answer pairs or use it as a reference to create your own question-answer datasets.

Contributing:
If you have additional questions and answers that you would like to contribute to this JSON file, please feel free to submit a pull request. Your contributions are greatly appreciated!
",Medium,3.0
Q&A,crystalai/autotrain-data-crystal_alchemist-vision,1.0,0.0,2023-08-25 01:37:45+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,none,Low,0.0
Q&A,hac541309/basic_korean_dict,6.0,67.0,2023-07-26 12:28:43+00:00,cc-by-sa-3.0,0.0,88.5 MB,92798976.0,88.5 MB,92798976.0,74936,none,none,"---
dataset_info:
  features:
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 198591964
    num_examples: 74936
  download_size: 88466367
  dataset_size: 198591964
license: cc-by-sa-3.0
task_categories:
- table-question-answering
- text-generation
- text-classification
- question-answering
language:
- ko
- mn
- vi
- th
- id
- ru
- ja
- en
- fr
- es
- ar
- zh
pretty_name: 한국어기초사전
size_categories:
- 1M<n<10M
tags:
- dictionary
---

# Dataset Card for ""basic_korean_dict""
This dataset is a NLP learnable form of [Korean Basic Dictionary(한국어기초사전)](https://krdict.korean.go.kr/).
It follows the [original copyright policy (cc-by-sa-2.0)](https://krdict.korean.go.kr/kboardPolicy/copyRightTermsInfo)
Some words have usage examples in other languages, effectively rendering this into a parallel corpus.

This version is built from xls_20230601

[한국어 기초 사전](https://krdict.korean.go.kr/)을 학습 가능한 형태로 처리한 데이터입니다.
[한국어 기초 사전](https://krdict.korean.go.kr/kboardPolicy/copyRightTermsInfo)의 저작권을 따릅니다.
여러 언어로 이루어진 표제어들이 있어 병렬 말뭉치의 기능이 있습니다.
xls_20230601으로부터 생성되었습니다.",Medium,3.0
Q&A,PetraAI/PetraAI,21.0,204.0,2023-09-14 21:04:52+00:00,apache-2.0,1.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- text-classification
- token-classification
- table-question-answering
- question-answering
- zero-shot-classification
- translation
- summarization
- conversational
- feature-extraction
- text-generation
- text2text-generation
- fill-mask
- sentence-similarity
- text-to-speech
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
- image-to-text
- image-to-image
- unconditional-image-generation
- video-classification
- reinforcement-learning
- robotics
- tabular-classification
- tabular-regression
- tabular-to-text
- table-to-text
- multiple-choice
- text-retrieval
- time-series-forecasting
- text-to-video
- visual-question-answering
- zero-shot-image-classification
- graph-ml
language:
- ar
- en
tags:
- chemistry
- biology
- finance
- legal
- music
- art
- code
- climate
- medical
pretty_name: PETRA
size_categories:
- 1M<n<10M
---
# PETRA 

## Overview

PETRA is a multilingual dataset for training and evaluating AI systems on a diverse range of tasks across multiple modalities. It contains data in Arabic and English for tasks including translation, summarization, question answering, and more.

## Dataset Structure

- Data is separated by language into `/ar` and `/en` directories
- Within each language directory, data is separated by task into subdirectories  
- Tasks include:
  - Translation
  - Summarization
  - Conversational
  - Feature extraction
  - Zero-shot classification
  - Text generation
  - Fill mask
  - Sentence similarity
  - Text-to-speech
  - Automatic speech recognition
  - Text classification
  - Token classification
  - Table question answering
  - Question answering
  - Text2text generation
  - Audio-to-audio
  - Audio classification
  - Voice activity detection
  - Depth estimation
  - Image classification
  - Object detection
  - Image segmentation
  - Text-to-image
  - Image-to-text
  - Image-to-image
  - Unconditional image generation
  - Reinforcement learning
  - Video classification
  - Robotics
  - Tabular classification
  - Tabular regression
  - Table-to-text
  - Multiple choice
  - Text retrieval
  - Tabular-to-text
  - Text-to-video
  - Time series forecasting
  - Visual question answering
  - Zero-shot image classification
  - Graph ML

## Dataset Tags 

- code
- art
- chemistry
- biology  
- finance
- legal
- music
- climate
- medical

## Dataset Size

1M < n < 10M samples  

## Licenses

Apache 2.0

## Citation

If you use this dataset, please cite it as:

[cite paper, arXiv, etc] 

@article{PetraAI2022PetraAI,
  title={PetraAI: A Massive Multilingual Dataset for Machine Learning}, 
  author={First Last and First Last},
  journal={arXiv},
  year={2022},
  url={https://huggingface.co/datasets/PetraAI/PetraAI}
}

## Contact

For any questions, please reach out to [shadilytn@gmail.com]


# Dataset Cards

## What are Dataset Cards?

Each dataset may be documented by the `README.md` file in the repository. This file is called a **dataset card**, and the Hugging Face Hub will render its contents on the dataset’s main page. To inform users about how to responsibly use the data, it’s a good idea to include information about any potential biases within the dataset. Generally, dataset cards help users understand the contents of the dataset and give context for how the dataset should be used. 

You can also add dataset metadata to your card. The metadata describes important information about a dataset such as its license, language, and size. It also contains tags to help users discover a dataset on the Hub. Tags are defined in a YAML metadata section at the top of the `README.md` file.

## Dataset card metadata

A dataset repo will render its README.md as a dataset card. To control how the Hub displays the card, you should create a YAML section in the README file to define some metadata. Start by adding three --- at the top, then include all of the relevant metadata, and close the section with another group of --- like the example below:


The metadata that you add to the dataset card enables certain interactions on the Hub. For example:

- Allow users to filter and discover datasets at https://huggingface.co/datasets.
  
- If you choose a license using the keywords listed in the right column of this table, the license will be displayed on the dataset page.

When creating a README.md file in a dataset repository on the Hub, use Metadata UI to fill the main metadata:

To see metadata fields, see the detailed dataset card metadata specification here.

### Dataset card creation guide

For a step-by-step guide on creating a dataset card, check out the Create a dataset card guide. 

Reading through existing dataset cards, such as the ELI5 dataset card, is a great way to familiarize yourself with the common conventions.

### Linking a Paper

If the dataset card includes a link to a paper on arXiv, the Hub will extract the arXiv ID and include it in the dataset tags with the format `arxiv:<PAPER ID>`. Clicking on the tag will let you:

- Visit the Paper page
  
- Filter for other models on the Hub that cite the same paper.

Read more about paper pages here.

https://huggingface.co/docs/hub/paper-pages",High,5.0
Q&A,FatemahAlsubaiei/CGSQuAD,0.0,9.0,2023-08-04 03:08:30+00:00,none,3.0,14.8 MB,15518924.8,89.8 KB,91955.2,1504,none,none,"---
task_categories:
- question-answering
language:
- ar
---",Low,0.0
Q&A,imvladikon/QAmeleon,1.0,61.0,2023-08-13 19:36:48+00:00,cc-by-4.0,0.0,33.4 MB,35022438.4,33.4 MB,35022438.4,94346,https://arxiv.org/abs/2211.08264,none,"---
language:
- ar
- bn
- fi
- id
- ko
- ru
- sw
- te
license: cc-by-4.0
size_categories:
- 10K<n<100K
task_categories:
- question-answering
dataset_info:
- config_name: ar
  features:
  - name: language
    dtype: string
  - name: question
    dtype: string
  - name: answer
    dtype: string
  - name: passage
    dtype: string
  splits:
  - name: train
    num_bytes: 4773335
    num_examples: 6966
  download_size: 0
  dataset_size: 4773335
- config_name: bn
  features:
  - name: language
    dtype: string
  - name: question
    dtype: string
  - name: answer
    dtype: string
  - name: passage
    dtype: string
  splits:
  - name: train
    num_bytes: 6458441
    num_examples: 6084
  download_size: 0
  dataset_size: 6458441
- config_name: default
  features:
  - name: language
    dtype: string
  - name: question
    dtype: string
  - name: answer
    dtype: string
  - name: passage
    dtype: string
  splits:
  - name: train
    num_bytes: 32190633
    num_examples: 47173
  download_size: 16811173
  dataset_size: 32190633
- config_name: fi
  features:
  - name: language
    dtype: string
  - name: question
    dtype: string
  - name: answer
    dtype: string
  - name: passage
    dtype: string
  splits:
  - name: train
    num_bytes: 2158030
    num_examples: 5028
  download_size: 0
  dataset_size: 2158030
- config_name: id
  features:
  - name: language
    dtype: string
  - name: question
    dtype: string
  - name: answer
    dtype: string
  - name: passage
    dtype: string
  splits:
  - name: train
    num_bytes: 2635540
    num_examples: 6797
  download_size: 0
  dataset_size: 2635540
- config_name: ko
  features:
  - name: language
    dtype: string
  - name: question
    dtype: string
  - name: answer
    dtype: string
  - name: passage
    dtype: string
  splits:
  - name: train
    num_bytes: 5074624
    num_examples: 6471
  download_size: 0
  dataset_size: 5074624
- config_name: ru
  features:
  - name: language
    dtype: string
  - name: question
    dtype: string
  - name: answer
    dtype: string
  - name: passage
    dtype: string
  splits:
  - name: train
    num_bytes: 3952632
    num_examples: 5557
  download_size: 0
  dataset_size: 3952632
- config_name: sw
  features:
  - name: language
    dtype: string
  - name: question
    dtype: string
  - name: answer
    dtype: string
  - name: passage
    dtype: string
  splits:
  - name: train
    num_bytes: 2113909
    num_examples: 5597
  download_size: 0
  dataset_size: 2113909
- config_name: te
  features:
  - name: language
    dtype: string
  - name: question
    dtype: string
  - name: answer
    dtype: string
  - name: passage
    dtype: string
  splits:
  - name: train
    num_bytes: 5024122
    num_examples: 4673
  download_size: 0
  dataset_size: 5024122
configs:
- config_name: ar
  data_files:
  - split: train
    path: ar/train-*
- config_name: bn
  data_files:
  - split: train
    path: bn/train-*
- config_name: default
  data_files:
  - split: train
    path: data/train-*
- config_name: fi
  data_files:
  - split: train
    path: fi/train-*
- config_name: id
  data_files:
  - split: train
    path: id/train-*
- config_name: ko
  data_files:
  - split: train
    path: ko/train-*
- config_name: ru
  data_files:
  - split: train
    path: ru/train-*
- config_name: sw
  data_files:
  - split: train
    path: sw/train-*
- config_name: te
  data_files:
  - split: train
    path: te/train-*
---
# Dataset Card for ""QAmeleon""

QAmeleon introduces synthetic multilingual QA data contaning in 8 langauges using PaLM-540B, a large language model. This dataset was generated by prompt tuning PaLM with only five examples per language. We use the synthetic data to finetune downstream QA models leading to improved accuracy in comparison to English-only and translation-based baselines. 

Data available at https://storage.googleapis.com/qameleon/qamelon_pt_accepted.csv 


More details can be found in the [QAmeleon: Multilingual QA with Only 5 Examples](https://arxiv.org/abs/2211.08264) which can be cited as follows:
```
@misc{agrawal2022qameleon,
      title={QAmeleon: Multilingual QA with Only 5 Examples}, 
      author={Priyanka Agrawal and Chris Alberti and Fantine Huot and Joshua Maynez and Ji Ma and Sebastian Ruder and Kuzman Ganchev and Dipanjan Das and Mirella Lapata},
      year={2022},
      eprint={2211.08264},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
This dataset contains a total of 47173 Question Answer instances across 8 langauges, following is the count per language. 

|Language | Count |
|---------|------:|
|ar       |6966   |
|bn       |6084   |
|fi    |5028 |
|id    |6797 |
|ko    |6471 |
|ru    |5557 |
|sw    |5597 |
|te    |4673 |
|**Total** |**47173**|

The QAmeleon dataset is released under the [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) license.

[More Information needed](https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)",Medium,3.0
Q&A,M-A-D/Mixed-Arabic-Datasets-Repo,34.0,1229.0,2023-10-16 21:25:35+00:00,none,0.0,28.6 GB,30709016166.4,28.6 GB,30709016166.4,208606355,none,none,"---
language:
- ar
size_categories:
- 1B<n<10B
task_categories:
- text-classification
- question-answering
- translation
- summarization
- conversational
- text-generation
- text2text-generation
- fill-mask
pretty_name: Mixed Arabic Datasets (MAD) Corpus
dataset_info:
- config_name: Ara--Ali-C137--Hindawi-Books-dataset
  features:
  - name: BookLink
    dtype: string
  - name: BookName
    dtype: string
  - name: AuthorName
    dtype: string
  - name: AboutBook
    dtype: string
  - name: ChapterLink
    dtype: string
  - name: ChapterName
    dtype: string
  - name: ChapterText
    dtype: string
  - name: AboutAuthor
    dtype: string
  splits:
  - name: train
    num_bytes: 1364854259
    num_examples: 49821
  download_size: 494678002
  dataset_size: 1364854259
- config_name: Ara--Goud--Goud-sum
  features:
  - name: article
    dtype: string
  - name: headline
    dtype: string
  - name: categories
    dtype: string
  splits:
  - name: train
    num_bytes: 288296544
    num_examples: 139288
  download_size: 147735776
  dataset_size: 288296544
- config_name: Ara--J-Mourad--MNAD.v1
  features:
  - name: Title
    dtype: string
  - name: Body
    dtype: string
  - name: Category
    dtype: string
  splits:
  - name: train
    num_bytes: 1101921980
    num_examples: 418563
  download_size: 527154122
  dataset_size: 1101921980
- config_name: Ara--JihadZa--IADD
  features:
  - name: Sentence
    dtype: string
  - name: Region
    dtype: string
  - name: DataSource
    dtype: string
  - name: Country
    dtype: string
  splits:
  - name: train
    num_bytes: 19167070
    num_examples: 135804
  download_size: 8644491
  dataset_size: 19167070
- config_name: Ara--LeMGarouani--MAC-corpus
  features:
  - name: tweets
    dtype: string
  - name: type
    dtype: string
  - name: class
    dtype: string
  splits:
  - name: train
    num_bytes: 1945646
    num_examples: 18087
  download_size: 866198
  dataset_size: 1945646
- config_name: Ara--MBZUAI--Bactrian-X
  features:
  - name: instruction
    dtype: string
  - name: input
    dtype: string
  - name: id
    dtype: string
  - name: output
    dtype: string
  splits:
  - name: train
    num_bytes: 66093524
    num_examples: 67017
  download_size: 33063779
  dataset_size: 66093524
- config_name: Ara--OpenAssistant--oasst1
  features:
  - name: message_id
    dtype: string
  - name: parent_id
    dtype: string
  - name: user_id
    dtype: string
  - name: created_date
    dtype: string
  - name: text
    dtype: string
  - name: role
    dtype: string
  - name: lang
    dtype: string
  - name: review_count
    dtype: int32
  - name: review_result
    dtype: bool
  - name: deleted
    dtype: bool
  - name: rank
    dtype: float64
  - name: synthetic
    dtype: bool
  - name: model_name
    dtype: 'null'
  - name: detoxify
    dtype: 'null'
  - name: message_tree_id
    dtype: string
  - name: tree_state
    dtype: string
  - name: emojis
    struct:
    - name: count
      sequence: int32
    - name: name
      sequence: string
  - name: labels
    struct:
    - name: count
      sequence: int32
    - name: name
      sequence: string
    - name: value
      sequence: float64
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 58168
    num_examples: 56
  download_size: 30984
  dataset_size: 58168
- config_name: Ara--Wikipedia
  features:
  - name: id
    dtype: string
  - name: url
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 3052201469
    num_examples: 1205403
  download_size: 1316212231
  dataset_size: 3052201469
- config_name: Ara--bigscience--xP3
  features:
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  splits:
  - name: train
    num_bytes: 4727881680
    num_examples: 2148955
  download_size: 2805060725
  dataset_size: 4727881680
- config_name: Ara--cardiffnlp--tweet_sentiment_multilingual
  features:
  - name: text
    dtype: string
  - name: label
    dtype:
      class_label:
        names:
          '0': negative
          '1': neutral
          '2': positive
  splits:
  - name: train
    num_bytes: 306108
    num_examples: 1839
  - name: validation
    num_bytes: 53276
    num_examples: 324
  - name: test
    num_bytes: 141536
    num_examples: 870
  download_size: 279900
  dataset_size: 500920
- config_name: Ara--miracl--miracl
  features:
  - name: query_id
    dtype: string
  - name: query
    dtype: string
  - name: positive_passages
    list:
    - name: docid
      dtype: string
    - name: text
      dtype: string
    - name: title
      dtype: string
  - name: negative_passages
    list:
    - name: docid
      dtype: string
    - name: text
      dtype: string
    - name: title
      dtype: string
  splits:
  - name: train
    num_bytes: 32012083
    num_examples: 3495
  download_size: 15798509
  dataset_size: 32012083
- config_name: Ara--mustapha--QuranExe
  features:
  - name: text
    dtype: string
  - name: resource_name
    dtype: string
  - name: verses_keys
    dtype: string
  splits:
  - name: train
    num_bytes: 133108687
    num_examples: 49888
  download_size: 58769417
  dataset_size: 133108687
- config_name: Ara--pain--Arabic-Tweets
  features:
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 41639770853
    num_examples: 202700438
  download_size: 22561651700
  dataset_size: 41639770853
- config_name: Ara--saudinewsnet
  features:
  - name: source
    dtype: string
  - name: url
    dtype: string
  - name: date_extracted
    dtype: string
  - name: title
    dtype: string
  - name: author
    dtype: string
  - name: content
    dtype: string
  splits:
  - name: train
    num_bytes: 103654009
    num_examples: 31030
  download_size: 49117164
  dataset_size: 103654009
- config_name: Ary--AbderrahmanSkiredj1--Darija-Wikipedia
  features:
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 8104410
    num_examples: 4862
  download_size: 3229966
  dataset_size: 8104410
- config_name: Ary--Ali-C137--Darija-Stories-Dataset
  features:
  - name: ChapterName
    dtype: string
  - name: ChapterLink
    dtype: string
  - name: Author
    dtype: string
  - name: Text
    dtype: string
  - name: Tags
    dtype: int64
  splits:
  - name: train
    num_bytes: 476926644
    num_examples: 6142
  download_size: 241528641
  dataset_size: 476926644
- config_name: Ary--Wikipedia
  features:
  - name: id
    dtype: string
  - name: url
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 10007364
    num_examples: 6703
  download_size: 4094377
  dataset_size: 10007364
- config_name: Arz--Wikipedia
  features:
  - name: id
    dtype: string
  - name: url
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 1364641408
    num_examples: 1617770
  download_size: 306420318
  dataset_size: 1364641408
configs:
- config_name: Ara--Ali-C137--Hindawi-Books-dataset
  data_files:
  - split: train
    path: Ara--Ali-C137--Hindawi-Books-dataset/train-*
- config_name: Ara--Goud--Goud-sum
  data_files:
  - split: train
    path: Ara--Goud--Goud-sum/train-*
- config_name: Ara--J-Mourad--MNAD.v1
  data_files:
  - split: train
    path: Ara--J-Mourad--MNAD.v1/train-*
- config_name: Ara--JihadZa--IADD
  data_files:
  - split: train
    path: Ara--JihadZa--IADD/train-*
- config_name: Ara--LeMGarouani--MAC-corpus
  data_files:
  - split: train
    path: Ara--LeMGarouani--MAC-corpus/train-*
- config_name: Ara--MBZUAI--Bactrian-X
  data_files:
  - split: train
    path: Ara--MBZUAI--Bactrian-X/train-*
- config_name: Ara--OpenAssistant--oasst1
  data_files:
  - split: train
    path: Ara--OpenAssistant--oasst1/train-*
- config_name: Ara--Wikipedia
  data_files:
  - split: train
    path: Ara--Wikipedia/train-*
- config_name: Ara--bigscience--xP3
  data_files:
  - split: train
    path: Ara--bigscience--xP3/train-*
- config_name: Ara--cardiffnlp--tweet_sentiment_multilingual
  data_files:
  - split: train
    path: Ara--cardiffnlp--tweet_sentiment_multilingual/train-*
  - split: validation
    path: Ara--cardiffnlp--tweet_sentiment_multilingual/validation-*
  - split: test
    path: Ara--cardiffnlp--tweet_sentiment_multilingual/test-*
- config_name: Ara--miracl--miracl
  data_files:
  - split: train
    path: Ara--miracl--miracl/train-*
- config_name: Ara--mustapha--QuranExe
  data_files:
  - split: train
    path: Ara--mustapha--QuranExe/train-*
- config_name: Ara--pain--Arabic-Tweets
  data_files:
  - split: train
    path: Ara--pain--Arabic-Tweets/train-*
- config_name: Ara--saudinewsnet
  data_files:
  - split: train
    path: Ara--saudinewsnet/train-*
- config_name: Ary--AbderrahmanSkiredj1--Darija-Wikipedia
  data_files:
  - split: train
    path: Ary--AbderrahmanSkiredj1--Darija-Wikipedia/train-*
- config_name: Ary--Ali-C137--Darija-Stories-Dataset
  data_files:
  - split: train
    path: Ary--Ali-C137--Darija-Stories-Dataset/train-*
- config_name: Ary--Wikipedia
  data_files:
  - split: train
    path: Ary--Wikipedia/train-*
- config_name: Arz--Wikipedia
  data_files:
  - split: train
    path: Arz--Wikipedia/train-*
---
# Dataset Card for ""Mixed Arabic Datasets (MAD) Corpus""

**The Mixed Arabic Datasets Corpus : A Community-Driven Collection of Diverse Arabic Texts**

## Dataset Description

The Mixed Arabic Datasets (MAD) presents a dynamic compilation of diverse Arabic texts sourced from various online platforms and datasets. It addresses a critical challenge faced by researchers, linguists, and language enthusiasts: the fragmentation of Arabic language datasets across the Internet. With MAD, we are trying to centralize these dispersed resources into a single, comprehensive repository.

Encompassing a wide spectrum of content, ranging from social media conversations to literary masterpieces, MAD captures the rich tapestry of Arabic communication, including both standard Arabic and regional dialects.

This corpus offers comprehensive insights into the linguistic diversity and cultural nuances of Arabic expression.

## Usage 

If you want to use this dataset you pick one among the available configs:

`Ara--MBZUAI--Bactrian-X` | `Ara--OpenAssistant--oasst1` | `Ary--AbderrahmanSkiredj1--Darija-Wikipedia`

`Ara--Wikipedia` | `Ary--Wikipedia` | `Arz--Wikipedia`

`Ary--Ali-C137--Darija-Stories-Dataset` | `Ara--Ali-C137--Hindawi-Books-dataset` | ``

Example of usage:

```python
dataset = load_dataset('M-A-D/Mixed-Arabic-Datasets-Repo', 'Ara--MBZUAI--Bactrian-X')
```

If you loaded multiple datasets and wanted to merge them together then you can simply laverage `concatenate_datasets()` from `datasets`

```pyhton
dataset3 = concatenate_datasets([dataset1['train'], dataset2['train']])
```

Note : proccess the datasets before merging in order to make sure you have a new dataset that is consistent

## Dataset Size

The Mixed Arabic Datasets (MAD) is a dynamic and evolving collection, with its size fluctuating as new datasets are added or removed. As MAD continuously expands, it becomes a living resource that adapts to the ever-changing landscape of Arabic language datasets.

**Dataset List**

MAD draws from a diverse array of sources, each contributing to its richness and breadth. While the collection is constantly evolving, some of the datasets that are poised to join MAD in the near future include:

- [✔] OpenAssistant/oasst1 (ar portion) : [Dataset Link](https://huggingface.co/datasets/OpenAssistant/oasst1)
- [✔] MBZUAI/Bactrian-X (ar portion) : [Dataset Link](https://huggingface.co/datasets/MBZUAI/Bactrian-X/viewer/ar/train)
- [✔] AbderrahmanSkiredj1/Darija-Wikipedia : [Dataset Link](https://huggingface.co/datasets/AbderrahmanSkiredj1/moroccan_darija_wikipedia_dataset)
- [✔] Arabic Wikipedia : [Dataset Link](https://huggingface.co/datasets/wikipedia)
- [✔] Moroccan Arabic Wikipedia : [Dataset Link](https://huggingface.co/datasets/wikipedia)
- [✔] Egyptian Arabic Wikipedia : [Dataset Link](https://huggingface.co/datasets/wikipedia)
- [✔] Darija Stories Dataset : [Dataset Link](https://huggingface.co/datasets/Ali-C137/Darija-Stories-Dataset)
- [✔] Hindawi Books Dataset : [Dataset Link](https://huggingface.co/datasets/Ali-C137/Hindawi-Books-dataset)
- [] uonlp/CulturaX - ar : [Dataset Link](https://huggingface.co/datasets/uonlp/CulturaX/viewer/ar/train)
- [✔] Pain/ArabicTweets : [Dataset Link](https://huggingface.co/datasets/pain/Arabic-Tweets)
- [] Abu-El-Khair Corpus : [Dataset Link](https://huggingface.co/datasets/arabic_billion_words)
- [✔] QuranExe : [Dataset Link](https://huggingface.co/datasets/mustapha/QuranExe)
- [✔] MNAD : [Dataset Link](https://huggingface.co/datasets/J-Mourad/MNAD.v1)
- [✔] IADD : [Dataset Link](https://raw.githubusercontent.com/JihadZa/IADD/main/IADD.json)
- [] OSIAN : [Dataset Link](https://wortschatz.uni-leipzig.de/en/download/Arabic#ara-tn_newscrawl-OSIAN_2018)
- [✔] MAC corpus : [Dataset Link](https://raw.githubusercontent.com/LeMGarouani/MAC/main/MAC%20corpus.csv)
- [✔] Goud.ma-Sum : [Dataset Link](https://huggingface.co/datasets/Goud/Goud-sum)
- [✔] SaudiNewsNet : [Dataset Link](https://huggingface.co/datasets/saudinewsnet)
- [✔] Miracl : [Dataset Link](https://huggingface.co/datasets/miracl/miracl)
- [✔] CardiffNLP/TweetSentimentMulti : [Dataset Link](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual)
- [] OSCAR-2301 : [Dataset Link](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301/viewer/ar/train)
- [] mc4 : [Dataset Link](https://huggingface.co/datasets/mc4/viewer/ar/train)
- [✔] bigscience/xP3 : [Dataset Link](https://huggingface.co/datasets/bigscience/xP3/viewer/ar/train)
- [] Muennighoff/xP3x : [Dataset Link](https://huggingface.co/datasets/Muennighoff/xP3x)
- [] Ai_Society : [Dataset Link](https://huggingface.co/datasets/camel-ai/ai_society_translated)


## Potential Use Cases

The Mixed Arabic Datasets (MAD) holds the potential to catalyze a multitude of groundbreaking applications:

- **Linguistic Analysis:** Employ MAD to conduct in-depth linguistic studies, exploring dialectal variances, language evolution, and grammatical structures.
- **Topic Modeling:** Dive into diverse themes and subjects through the extensive collection, revealing insights into emerging trends and prevalent topics.
- **Sentiment Understanding:** Decode sentiments spanning Arabic dialects, revealing cultural nuances and emotional dynamics.
- **Sociocultural Research:** Embark on a sociolinguistic journey, unraveling the intricate connection between language, culture, and societal shifts.

## Dataset Access

MAD's access mechanism is unique: while it doesn't carry a general license itself, each constituent dataset within the corpus retains its individual license. By accessing the dataset details through the provided links in the ""Dataset List"" section above, users can understand the specific licensing terms for each dataset.

### Join Us on Discord

For discussions, contributions, and community interactions, join us on Discord! [![Discord](https://img.shields.io/discord/798499298231726101?label=Join%20us%20on%20Discord&logo=discord&logoColor=white&style=for-the-badge)](https://discord.gg/2NpJ9JGm)

### How to Contribute

Want to contribute to the Mixed Arabic Datasets project? Follow our comprehensive guide on Google Colab for step-by-step instructions: [Contribution Guide](https://colab.research.google.com/drive/1kOIRoicgCOV8TPvASAI_2uMY7rpXnqzJ?usp=sharing).

**Note**: If you'd like to test a contribution before submitting it, feel free to do so on the [MAD Test Dataset](https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Dataset-test).

## Citation

```
@dataset{ 
title = {Mixed Arabic Datasets (MAD)},
author = {MAD Community},
howpublished = {Dataset},
url = {https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Datasets-Repo},
year = {2023},
}
```",High,6.0
Q&A,KIND-Dataset/KIND,0.0,25.0,2024-03-19 09:00:28+00:00,cc-by-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,"['https://aclanthology.org/2024.eacl-srw.3/)', 'https://aclanthology.org/2024.eacl-srw.3"",']","---
license: cc-by-4.0
task_categories:
- question-answering
language:
- ar
size_categories:
- 10K<n<100K
---
### Dataset Summary

KIND dataset is a new dilectal data dataset.
The dataset was a result of a data marathon competition, where the competitor's goal is to respond to as many prompts as possible in their own dialect, within a fixed time frame with as few errors as possible.

For more details, please check the paper
[The KIND Dataset: A Social Collaboration Approach for Nuanced Dialect Data Collection](https://aclanthology.org/2024.eacl-srw.3/)

### Data Fields

- dialect_code: the label that indicates the specific dialect the text belongs to.
- sentenceOriginID: the identifier that references the MSA sentence translated (1000000-2000000), or the reference to link the question to the constructed question dataset (2000000-3000000).
- textString: the submitted sentence


### Citation Information

```
@inproceedings{yamani-etal-2024-kind,
    title = ""The {KIND} Dataset: A Social Collaboration Approach for Nuanced Dialect Data Collection"",
    author = ""Yamani, Asma  and
      Alziyady, Raghad  and
      AlYami, Reem  and
      Albelali, Salma  and
      Albelali, Leina  and
      Almulhim, Jawharah  and
      Alsulami, Amjad  and
      Alfarraj, Motaz  and
      Al-Zaidy, Rabeah"",
    editor = ""Falk, Neele  and
      Papi, Sara  and
      Zhang, Mike"",
    booktitle = ""Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop"",
    month = mar,
    year = ""2024"",
    address = ""St. Julian{'}s, Malta"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.eacl-srw.3"",
    pages = ""32--43"",
}

```",Medium,3.0
Q&A,PeepDaSlan9/DIDI,2.0,21.0,2023-10-18 15:51:49+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- conversational
- text-classification
- table-question-answering
- question-answering
- translation
- summarization
- text-generation
- text2text-generation
- text-to-speech
- automatic-speech-recognition
- text-to-audio
- voice-activity-detection
language:
- ko
- ru
- ig
- es
- en
- ar
- fr
- de
- am
pretty_name: 'DIDI_3.5 '
size_categories:
- 100M<n<1B
---",Low,1.0
Q&A,ISTNetworks/arabic_alpaca_model,1.0,15.0,2023-12-06 21:48:59+00:00,apache-2.0,0.0,3.13 MB,3282042.88,1.35 MB,1415577.6,1196,none,none,"---
license: apache-2.0
task_categories:
- question-answering
language:
- ar
tags:
- medical
pretty_name: arabic_alpaca
size_categories:
- 10K<n<100K
---",Low,1.0
Q&A,faranheit/ministries,0.0,5.0,2024-01-11 11:39:25+00:00,apache-2.0,0.0,82.3 kB,unknown,14.6 kB,unknown,35,none,none,"---
license: apache-2.0
task_categories:
- question-answering
language:
- ar
tags:
- not-for-all-audiences
---
{""id"": ""130042945016-0"", ""text"": ""\u0648\u0635\u0641 \u0627\u0644\u062e\u062f\u0645\u0629: \u062a\u0645\u0643\u0651\u0650\u0646 \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u0627\u0644\u0639\u0645\u064a\u0644 \u0645\u0646 \u062a\u0642\u062f\u064a\u0645 \u062c\u0645\u064a\u0639 \u0637\u0644\u0628\u0627\u062a \u0639\u0642\u0648\u062f \u062a\u0623\u0633\u064a\u0633 \u0627\u0644\u0634\u0631\u0643\u0627\u062a \u062d\u0633\u0628 \u0627\u0644\u0643\u064a\u0627\u0646 :  \n  \u0630\u0627\u062a \u0627\u0644\u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0627\u0644\u0645\u062d\u062f\u0648\u062f\u0629 : \n \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u064f\u062a\u0645\u0651\u0643\u0646 \u0627\u0644\u0645\u0633\u062a\u062b\u0645\u0631 \u0645\u0646 \u0627\u0644\u0628\u062f\u0621 \u0641\u064a \u064f\u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u0644\u0646\u0634\u0627\u0637 \u0627\u0644\u062a\u062c\u0627\u0631\u064a\u060c \u0648\u062a\u0623\u0633\u064a\u0633 \u0634\u0631\u0643\u0629 \u0630\u0627\u062a \u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0645\u062d\u062f\u0648\u062f\u0629 \u0645\u0646 \u0634\u062e\u0635 \n \u0648\u0627\u062d\u062f\u060c \u0623\u0648 \u0623\u0643\u062b\u0631 \u0645\u0646 \u0630\u0648\u064a \u0627\u0644\u0635\u0641\u0629 \u0627\u0644\u0637\u0628\u064a\u0639\u064a\u0629 \u0623\u0648 \u0627\u0644\u0625\u0639\u062a\u0628\u0627\u0631\u064a\u0629\u060c \u0648\u062a\u0639\u062f \u0630\u0645\u062a\u0647\u0627 \u0645\u0633\u062a\u0642\u0644\u0629 \u0639\u0646 \u0627\u0644\u0630\u0645\u0629 \u0627\u0644\u0645\u0627\u0644\u064a\u0629 \u0644\u0643\u0644 \u0634\u0631\u064a\u0643 \u0641\u064a\u0647\u0627 \u0623\u0648 \u0627\u0644\u0645\u0627\u0644\u0643 \u0644\u0647\u0627. \n \u0648\u062a\u0643\u0648\u0646 \u0627\u0644\u0634\u0631\u0643\u0629 \u0648\u062d\u062f\u0647\u0627 \u0645\u0633\u0624\u0648\u0644\u0629 \u0639\u0646 \u0627\u0644\u062f\u064a\u0648\u0646 \u0648\u0627\u0644\u0625\u0644\u062a\u0632\u0627\u0645\u0627\u062a \u0627\u0644\u0645\u062a\u0631\u062a\u0628\u0629 \u0639\u0644\u064a\u0647\u0627 \u0623\u0648 \u0627\u0644\u0646\u0627\u0634\u0626\u0629 \u0639\u0646 \u0646\u0634\u0627\u0637\u0647\u0627\u060c \u0648\u0644\u0627 \u064a\u0643\u0648\u0646 \u0627\u0644\u0645\u0627\u0644\u0643 \u0644\u0647\u0627 \u0648\u0644\u0627 \n \u0627\u0644\u0634\u0631\u064a\u0643 \u0641\u064a\u0647\u0627 \u0645\u0633\u0624\u0648\u0644\u0622\u064b \u0639\u0646 \u0647\u0630\u0647 \u0627\u0644\u062f\u064a\u0648\u0646 \u0648\u0627\u0644\u0625\u0644\u062a\u0632\u0627\u0645\u0627\u062a \u0625\u0644\u0627 \u0628\u0642\u062f\u0631 \u062d\u0635\u062a\u0647 \u0641\u064a \u0631\u0623\u0633 \u0627\u0644\u0645\u0627\u0644 . \n \u0627\u0644\u062a\u0636\u0627\u0645\u0646 :"", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a05671c""}
{""id"": ""130042945016-1"", ""text"": ""\u0627\u0644\u062a\u0636\u0627\u0645\u0646 : \n \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u062a\u064f\u0645\u0643\u0651\u0646 \u0627\u0644\u0639\u0645\u064a\u0644 \u0645\u0646 \u0627\u0644\u0628\u062f\u0621 \u0641\u064a \u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u0644\u0646\u0634\u0627\u0637 \u0627\u0644\u062a\u062c\u0627\u0631\u064a \u0648\u062a\u0623\u0633\u064a\u0633 \u0634\u0631\u0643\u0629 \u062a\u0636\u0627\u0645\u0646\u064a\u0629 \u062c\u0645\u064a\u0639 \u0634\u0631\u0643\u0627\u0624\u0647\u0627 \u0623\u0641\u0631\u0627\u062f \n \u0648\u0645\u0633\u0624\u0648\u0644\u064a\u0646 \u0634\u062e\u0635\u064a\u0627\u064b \u0628\u062c\u0645\u064a\u0639 \u0623\u0645\u0648\u0627\u0644\u0647\u0645 \u0648\u0628\u0627\u0644\u062a\u0636\u0627\u0645\u0646 \u0639\u0646 \u062f\u064a\u0648\u0646 \u0627\u0644\u0634\u0631\u0643\u0629 \u0648\u0627\u0644\u062a\u0632\u0627\u0645\u0627\u062a\u0647\u0627\u060c \n \u0627\u0644\u062a\u0648\u0635\u064a\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629 :  \n \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u062a\u064f\u0645\u0643\u0651\u0646 \u0627\u0644\u0645\u0633\u062a\u062b\u0645\u0631 \u0645\u0646 \u0627\u0644\u0628\u062f\u0621 \u0641\u064a \u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u0644\u0646\u0634\u0627\u0637 \u0627\u0644\u062a\u062c\u0627\u0631\u064a \u0648\u062a\u0623\u0633\u064a\u0633 \u0634\u0631\u0643\u0629 \u062a\u0648\u0635\u064a\u0629 \u0628\u0633\u064a\u0637\u0629 \u062a\u062a\u0643\u0648\u0646 \u0645\u0646 \u0641\u0631\u064a\u0642\u064a\u0646\u060c \n (\u0627\u0644\u0645\u062a\u0636\u0627\u0645\u0646) \u0648\u0647\u0648 \u0627\u0644\u0645\u0633\u0624\u0648\u0644 \u0639\u0646 \u062f\u064a\u0648\u0646 \u0648\u0627\u0644\u062a\u0632\u0627\u0645\u0627\u062a \u0627\u0644\u0634\u0631\u0643\u0629\u060c \u0648(\u0627\u0644\u0645\u0648\u0635\u064a) \u0648\u0647\u0648 \u0627\u0644\u0630\u064a \u0644\u0627 \u064a\u0643\u0648\u0646 \u0645\u0633\u0624\u0648\u0644\u0627\u064b \u0625\u0644\u0627 \u0641\u064a \u062d\u062f\u0648\u062f \u062d\u0635\u062a\u0647 \u0641\u064a \n \u0631\u0623\u0633 \u0627\u0644\u0645\u0627\u0644 .\n\u0634\u0631\u0648\u0637 \u0648\u0645\u062a\u0637\u0644\u0628\u0627\u062a \u0627\u0644\u062e\u062f\u0645\u0629: \u2022 \u064a\u062c\u0628 \u0623\u0646 \u064a\u0643\u0648\u0646 \u0627\u0644\u0634\u062e\u0635 \u0627\u0644\u0637\u0628\u064a\u0639\u064a \u0623\u0643\u0628\u0631 \u0645\u0646 18 \u0639\u0627\u0645\u060c \u0648\u0641\u064a \u062d\u0627\u0644 \u0643\u0627\u0646 \u0642\u0627\u0635\u0631\u064b\u0627 \u064a\u062a\u0645 \u0625\u0631\u0641\u0627\u0642 \u0635\u0643 \u0627\u0644\u0648\u0644\u0627\u064a\u0629 ."", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a05671c""}
{""id"": ""130042945016-2"", ""text"": ""\u2022 \u064a\u062c\u0628 \u0623\u0646 \u0623\u0644\u0627 \u064a\u0643\u0648\u0646 \u0627\u0644\u0634\u0631\u0643\u0627\u0621 \u0645\u0648\u0638\u0641\u064a\u0646 \u062d\u0643\u0648\u0645\u064a\u064a\u0646 .  \n \u2022 \u0627\u0644\u062a\u062d\u0642\u0651\u064f\u0642 \u0645\u0646 \u0642\u0627\u0639\u062f\u0629 \u0627\u0644\u0639\u0645\u0644 \u0627\u0644\u062e\u0627\u0635\u0629 \u0628\u0628\u0639\u0636 \u0639\u0648\u0627\u0626\u0644 \u0646\u062c\u0631\u0627\u0646 .  \n \u2022 \u0641\u064a \u062d\u0627\u0644 \u0643\u0627\u0646 \u0623\u062d\u062f \u0627\u0644\u0634\u0631\u0643\u0627\u0621 \u0634\u0631\u064a\u0643 \u0625\u0639\u062a\u0628\u0627\u0631\u064a \u064a\u062a\u0645 \u0627\u0644\u062a\u062d\u0642\u0642 \u0645\u0646 \u0623\u0646 \u064a\u0643\u0648\u0646 \u0627\u0644\u0633\u062c\u0644 \u0627\u0644\u062a\u062c\u0627\u0631\u064a \u063a\u064a\u0631 \u0645\u0634\u0637\u0648\u0628 \u0623\u0648 \u0645\u0648\u0642\u0648\u0641 \u0623\u0648 \u0645\u0646\u062a\u0647\u064a .  \n \u2022 \u0641\u064a \u062d\u0627\u0644 \u0648\u062c\u0648\u062f \u0634\u0631\u064a\u0643 \u0623\u062c\u0646\u0628\u064a \u064a\u062c\u0628 \u0627\u0644\u062a\u062d\u0642\u0651\u064f\u0642 \u0645\u0646 \u0648\u062c\u0648\u062f \u0631\u062e\u0635\u0629 \u0625\u0633\u062a\u062b\u0645\u0627\u0631 \u0645\u0646 \u0648\u0632\u0627\u0631\u0629 \u0627\u0644\u0625\u0633\u062a\u062b\u0645\u0627\u0631 .  \n \u2022 \u0641\u064a \u062d\u0627\u0644 \u0643\u0627\u0646 \u0623\u062d\u062f \u0627\u0644\u0634\u0631\u0643\u0627\u0621 \u062c\u0647\u0629 \u062d\u0643\u0648\u0645\u064a\u0629/\u0645\u0624\u0633\u0633\u0629 \u0623\u0647\u0644\u064a\u0629/\u062c\u0645\u0639\u064a\u0629 \u062e\u064a\u0631\u064a\u0629/ \u0648\u0642\u0641 \""\u064a\u062c\u0628 \u0648\u062c\u0648\u062f \u0633\u0646\u062f \u0646\u0638\u0627\u0645\u064a \u064a\u062e\u0648\u0644\u0647\u0627 \u0628\u062a\u0623\u0633\u064a\u0633 \u0634\u0631\u0643\u0629 \u0623\u0648 \u0627\u0644\u0645\u0634\u0627\u0631\u0643\u0629 \u0641\u064a \u0634\u0631\u0643\u0629 \"".  \n \u2022\u0625\u0631\u0641\u0627\u0642 \u062a\u0642\u0631\u064a\u0631 \u0627\u0644\u0645\u0642\u064a\u0645 \u0627\u0644\u0645\u0639\u062a\u0645\u062f \u0641\u064a \u062d\u0627\u0644 \u0643\u0627\u0646 \u0631\u0627\u0633 \u0627\u0644\u0645\u0627\u0644 \u0639\u064a\u0646\u064a.  \n \u2022\u0627\u0644\u0645\u0633\u062a\u0646\u062f\u0627\u062a \u0627\u0644\u0645\u0637\u0644\u0648\u0628\u0629 : \n \u2022 \u0625\u0631\u0641\u0627\u0642 \u062a\u0631\u062e\u064a\u0635 \u0645\u0646 \u0627\u0644\u0628\u0646\u0643 \u0627\u0644\u0645\u0631\u0643\u0632\u064a \u0641\u064a \u062d\u0627\u0644 \u0643\u0627\u0646 \u0627\u0644\u0646\u0634\u0627\u0637 \u064a\u062a\u0637\u0644\u0628 \u0630\u0644\u0643."", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a05671c""}
{""id"": ""130042945016-3"", ""text"": ""\u062e\u0637\u0648\u0627\u062a \u0627\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u0649 \u0627\u0644\u062e\u062f\u0645\u0629: 1\u2022\u0627\u0644\u062f\u062e\u0648\u0644 \u0625\u0644\u0649 \u0645\u0646\u0635\u0629 \u0627\u0644\u0645\u0631\u0643\u0632 \u0627\u0644\u0633\u0639\u0648\u062f\u064a \u0644\u0644\u0623\u0639\u0645\u0627\u0644 (SBC) . \n 2\u2022 \u062a\u0633\u062c\u064a\u0644 \u0627\u0644\u062f\u062e\u0648\u0644 \u0628\u0648\u0627\u0633\u0637\u0629 \u0627\u0644\u0646\u0641\u0627\u0630 \u0627\u0644\u0648\u0637\u0646\u064a \u0627\u0644\u0645\u0648\u062d\u062f \u0623\u0648 \u0627\u0644\u0628\u0631\u064a\u062f \u0627\u0644\u0627\u0644\u0643\u062a\u0631\u0648\u0646\u064a . \n 3\u2022 \u062a\u062d\u062f\u064a\u062f \u0646\u0648\u0639 \u0648\u0635\u0641\u0629 \u0627\u0644\u0634\u0631\u0643\u0629 (\u0630\u0627\u062a \u0627\u0644\u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0627\u0644\u0645\u062d\u062f\u0648\u062f\u0629 -  \u0627\u0644\u062a\u0636\u0627\u0645\u0646 - \u0627\u0644\u062a\u0648\u0635\u064a\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629) . \n 4\u2022\u0625\u0633\u062a\u0643\u0645\u0627\u0644 \u0645\u062a\u0637\u0644\u0628\u0627\u062a \u0646\u0645\u0648\u0630\u062c \u0627\u0644\u062a\u0642\u062f\u064a\u0645 .  \n 5\u2022 \u062a\u0642\u062f\u064a\u0645 \u0627\u0644\u0637\u0644\u0628 . \n 6\u2022 \u0645\u0639\u0627\u0644\u062c\u0629 \u0627\u0644\u0637\u0644\u0628 . \n 7\u2022\u0645\u0648\u0627\u0641\u0642\u0629 \u0627\u0644\u0623\u0637\u0631\u0627\u0641 . \n 8\u2022\u0633\u062f\u0627\u062f \u0627\u0644\u0641\u0627\u062a\u0648\u0631\u0629 . \n 9\u2022\u0625\u0635\u062f\u0627\u0631 \u0627\u0644\u0648\u062b\u0627\u0626\u0642 \u0648\u0637\u0628\u0627\u0639\u0629 \u0627\u0644\u0633\u062c\u0644 \u0627\u0644\u062a\u062c\u0627\u0631\u064a \u0648\u0639\u0642\u062f \u0627\u0644\u0634\u0631\u0643\u0629 \u0625\u0644\u0643\u062a\u0631\u0648\u0646\u064a .\n\u0631\u0633\u0648\u0645 \u0627\u0644\u062e\u062f\u0645\u0629: \u0630\u0627\u062a \u0627\u0644\u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0627\u0644\u0645\u062d\u062f\u0648\u062f\u0629 : 1200 \u0631\u064a\u0627\u0644 \u0633\u0639\u0648\u062f\u064a.  \n  \u0627\u0644\u062a\u0636\u0627\u0645\u0646 - \u0627\u0644\u062a\u0648\u0635\u064a\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629: 800 \u0631\u064a\u0627\u0644 \u0633\u0639\u0648\u062f\u064a.  \n  \u0631\u0633\u0648\u0645 \u0627\u0644\u0646\u0634\u0631 \u0644\u0644\u0643\u064a\u0627\u0646\u0627\u062a: 500 \u0631\u064a\u0627\u0644 \u0633\u0639\u0648\u062f\u064a \u064a\u0636\u0627\u0641 \u0625\u0644\u064a\u0647\u0627 \u0636\u0631\u064a\u0628\u0629 \u0627\u0644\u0642\u064a\u0645\u0629 \u0627\u0644\u0645\u0636\u0627\u0641\u0629 15% ."", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a05671c""}
{""id"": ""130042945016-4"", ""text"": ""\u0645\u0639\u0644\u0648\u0645\u0627\u062a \u0623\u062e\u0631\u0649: \u0645\u0639\u0644\u0648\u0645\u0627\u062a \u0623\u062e\u0631\u0649:  \u0627\u0644\u0645\u0648\u0642\u0639 \u0627\u0644\u0625\u0644\u0643\u062a\u0631\u0648\u0646\u064a:  \n \u0630\u0627\u062a \u0627\u0644\u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0627\u0644\u0645\u062d\u062f\u0648\u062f\u0629 : \n  https://business.sa/ServicesAndPrograms/ServicesDetails.html?ServiceID=5 \n  \u0627\u0644\u062a\u0636\u0627\u0645\u0646 :  \n https://business.sa/ServicesAndPrograms/ServicesDetails.html?ServiceID=3 \n \u0627\u0644\u062a\u0648\u0635\u064a\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629: \n https://business.sa/ServicesAndPrograms/ServicesDetails.html?ServiceID=4/n"", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a05671c""}
{""id"": ""952da374a2f2-0"", ""text"": ""\u0648\u0635\u0641 \u0627\u0644\u062e\u062f\u0645\u0629: \u062a\u064f\u0645\u0643\u0651\u0650\u0646 \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u0627\u0644\u0639\u0645\u064a\u0644 \u0645\u0646 \u062a\u0642\u062f\u064a\u0645 \u062c\u0645\u064a\u0639 \u0637\u0644\u0628\u0627\u062a \u0639\u0642\u0648\u062f \u062a\u0623\u0633\u064a\u0633 \u0627\u0644\u0634\u0631\u0643\u0627\u062a \u0627\u0644\u0645\u0647\u0646\u064a\u0629 \u0628\u0645\u0648\u062c\u0628 \u0627\u0644\u062a\u0631\u062e\u064a\u0635 \u0627\u0644\u0645\u0647\u0646\u064a \u0627\u0644\u0635\u0627\u062f\u0631 .\n\u0634\u0631\u0648\u0637 \u0648\u0645\u062a\u0637\u0644\u0628\u0627\u062a \u0627\u0644\u062e\u062f\u0645\u0629: \u2022\u064a\u062a\u0645 \u0627\u0644\u062a\u062d\u0642\u0651\u064f\u0642 \u0645\u0646 \u0634\u0631\u0648\u0637 \u0648\u0645\u062a\u0637\u0644\u0628\u0627\u062a \u0627\u0644\u062e\u062f\u0645\u0629 \u0644\u0644\u0634\u0631\u0643\u0627\u062a \u0627\u0644\u062a\u062c\u0627\u0631\u064a\u0629\u060c \u0628\u0627\u0644\u0625\u0636\u0627\u0641\u0629 \u0625\u0644\u0649 \u0627\u0644\u062a\u0627\u0644\u064a:  \n \u2022\u064a\u062c\u0628 \u0623\u0646 \u062a\u062a\u0648\u0641\u0631 \u0631\u062e\u0635\u0629 \u0645\u0647\u0646\u064a\u0629 \u0633\u0627\u0631\u064a\u0629 \u0644\u0623\u062d\u062f \u0627\u0644\u0634\u0631\u0643\u0627\u0621 \u0648\u0642\u062a \u062a\u0642\u062f\u064a\u0645 \u0627\u0644\u0637\u0644\u0628\u060c \u0648\u0641\u064a \u062d\u0627\u0644 \u0643\u0627\u0646 \u0623\u062d\u062f \u0627\u0644\u0634\u0631\u0643\u0627\u0621 \u063a\u064a\u0631 \u0633\u0639\u0648\u062f\u064a \u064a\u062c\u0628 \u0625\u0631\u0641\u0627\u0642 \u062a\u0631\u062e\u064a\u0635 \u0645\u0647\u0646\u064a \u0635\u0627\u062f\u0631 \u0645\u0646 \u062f\u0627\u062e\u0644 \u0627\u0644\u0645\u0645\u0644\u0643\u0629.  \n \u2022 \u0623\u0644\u0627 \u062a\u0642\u0644 \u0646\u0633\u0628\u0629 \u0627\u0644\u0634\u0631\u064a\u0643 \u0627\u0644\u0633\u0639\u0648\u062f\u064a \u0627\u0644\u0645\u0631\u062e\u0651\u064e\u0635 \u0639\u0646 (25%) \u0645\u0646 \u0631\u0623\u0633 \u0645\u0627\u0644 \u0627\u0644\u0634\u0631\u0643\u0629 \u0627\u0644\u0645\u0647\u0646\u064a\u0629 \u0641\u064a \u062d\u0627\u0644 \u0643\u0627\u0646\u062a \u0627\u0644\u0634\u0631\u0643\u0629 \u0645\u062e\u062a\u0644\u0637\u0629.  \n \u2022 \u0623\u0644\u0627 \u062a\u0642\u0644 \u0646\u0633\u0628\u0629 \u0627\u0644\u0634\u0631\u0643\u0627\u0621 \u0627\u0644\u0645\u0631\u062e\u0651\u064e\u0635\u064a\u0646 \u0639\u0646 (70%) \u0644\u0633\u0639\u0648\u062f\u064a \u0648\u0627\u0644\u062e\u0644\u064a\u062c\u064a . \n \u2022 \u0625\u0631\u0641\u0627\u0642 \u062a\u0642\u0631\u064a\u0631 \u0627\u0644\u0645\u0642\u064a\u0645 \u0627\u0644\u0645\u0639\u062a\u0645\u062f \u0641\u064a \u062d\u0627\u0644 \u0643\u0627\u0646 \u0631\u0627\u0633 \u0627\u0644\u0645\u0627\u0644 \u0639\u064a\u0646\u064a."", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a056720""}
{""id"": ""952da374a2f2-1"", ""text"": ""\u2022\u0623\u0646\u0648\u0627\u0639 \u0627\u0644\u0634\u0631\u0643\u0627\u0621 \u0641\u064a \u0639\u0642\u062f \u0627\u0644\u062a\u0623\u0633\u064a\u0633 :  \n \u2022\u0634\u0631\u064a\u0643 \u0645\u0631\u062e\u0635 . \n \u2022\u0634\u0631\u064a\u0643 \u0628\u0631\u0623\u0633 \u0627\u0644\u0645\u0627\u0644 . \n \u2022\u0634\u0631\u064a\u0643 \u0628\u0627\u0644\u0639\u0645\u0644 ,\n\u062e\u0637\u0648\u0627\u062a \u0627\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u0649 \u0627\u0644\u062e\u062f\u0645\u0629: 1\u2022 \u0627\u0644\u062f\u062e\u0648\u0644 \u0625\u0644\u0649 \u0645\u0646\u0635\u0629 \u0627\u0644\u0645\u0631\u0643\u0632 \u0627\u0644\u0633\u0639\u0648\u062f\u064a \u0644\u0644\u0623\u0639\u0645\u0627\u0644 (SBC) . \n 2\u2022 \u062a\u0633\u062c\u064a\u0644 \u0627\u0644\u062f\u062e\u0648\u0644 \u0628\u0648\u0627\u0633\u0637\u0629 \u0627\u0644\u0646\u0641\u0627\u0630 \u0627\u0644\u0648\u0637\u0646\u064a \u0627\u0644\u0645\u0648\u062d\u062f \u0623\u0648 \u0627\u0644\u0628\u0631\u064a\u062f \u0627\u0644\u0625\u0644\u0643\u062a\u0631\u0648\u0646\u064a . \n 3\u2022 \u062a\u062d\u062f\u064a\u062f \u0646\u0648\u0639 \u0648\u0635\u0641\u0629 \u0627\u0644\u0634\u0631\u0643\u0629 (\u0630\u0627\u062a \u0627\u0644\u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0627\u0644\u0645\u062d\u062f\u0648\u062f\u0629-\u0627\u0644\u062a\u0636\u0627\u0645\u0646-\u0627\u0644\u062a\u0648\u0635\u064a\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629) \u0645\u0647\u0646\u064a\u0629 . \n 4\u2022\u0625\u0633\u062a\u0643\u0645\u0627\u0644 \u0645\u062a\u0637\u0644\u0628\u0627\u062a \u0646\u0645\u0648\u0630\u062c \u0627\u0644\u062a\u0642\u062f\u064a\u0645 .  \n 5\u2022 \u062a\u0642\u062f\u064a\u0645 \u0627\u0644\u0637\u0644\u0628 . \n 6\u2022 \u0645\u0639\u0627\u0644\u062c\u0629 \u0627\u0644\u0637\u0644\u0628 . \n 7\u2022\u0645\u0648\u0627\u0641\u0642\u0629 \u0627\u0644\u0623\u0637\u0631\u0627\u0641 . \n 8\u2022\u0633\u062f\u0627\u062f \u0627\u0644\u0641\u0627\u062a\u0648\u0631\u0629 . \n 9\u2022\u0625\u0635\u062f\u0627\u0631 \u0627\u0644\u0648\u062b\u0627\u0626\u0642 \u0648\u0637\u0628\u0627\u0639\u0629 \u0627\u0644\u0633\u062c\u0644 \u0627\u0644\u062a\u062c\u0627\u0631\u064a \u0648\u0639\u0642\u062f \u0627\u0644\u0634\u0631\u0643\u0629 \u0625\u0644\u0643\u062a\u0631\u0648\u0646\u064a .\n\u0631\u0633\u0648\u0645 \u0627\u0644\u062e\u062f\u0645\u0629: \u0630\u0627\u062a \u0627\u0644\u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0627\u0644\u0645\u062d\u062f\u0648\u062f\u0629: 1200 \u0631\u064a\u0627\u0644 \u0633\u0639\u0648\u062f\u064a."", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a056720""}
{""id"": ""952da374a2f2-2"", ""text"": ""\u0627\u0644\u062a\u0636\u0627\u0645\u0646-\u0627\u0644\u062a\u0648\u0635\u064a\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629: 800 \u0631\u064a\u0627\u0644 \u0633\u0639\u0648\u062f\u064a.  \n  \u064a\u0636\u0627\u0641 \u0639\u0644\u064a\u0647\u0627 500 \u0631\u064a\u0627\u0644 \u0633\u0639\u0648\u062f\u064a \u0631\u0633\u0648\u0645 \u0646\u0634\u0631 +  15% \u0636\u0631\u064a\u0628\u0629 \u0627\u0644\u0642\u064a\u0645\u0629 \u0627\u0644\u0645\u0636\u0627\u0641\u0629 .\n\u0645\u0639\u0644\u0648\u0645\u0627\u062a \u0623\u062e\u0631\u0649: \u0645\u0639\u0644\u0648\u0645\u0627\u062a \u0623\u062e\u0631\u0649:  \u0627\u0644\u0645\u0648\u0642\u0639 \u0627\u0644\u0625\u0644\u0643\u062a\u0631\u0648\u0646\u064a: \n \u062a\u0623\u0633\u064a\u0633 \u0634\u0631\u0643\u0629 \u0627\u0644\u062a\u0636\u0627\u0645\u0646 : \n   https://business.sa/ServicesAndPrograms/ServicesDetails.html?ServiceID=3 \n \u062a\u0623\u0633\u064a\u0633 \u0634\u0631\u0643\u0629 \u0630\u0627\u062a \u0627\u0644\u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0627\u0644\u0645\u062d\u062f\u0648\u062f\u0629: \n https://business.sa/ServicesAndPrograms/ServicesDetails.html?ServiceID=5 \n \u062a\u0623\u0633\u064a\u0633 \u0634\u0631\u0643\u0629 \u0627\u0644\u062a\u0648\u0635\u064a\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629:  \n https://business.sa/ServicesAndPrograms/ServicesDetails.html?ServiceID=4/n"", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a056720""}
{""id"": ""fd131aa6e7d9-0"", ""text"": ""\u0648\u0635\u0641 \u0627\u0644\u062e\u062f\u0645\u0629: \u062a\u0645\u0643\u0651\u0650\u0646 \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u0627\u0644\u0639\u0645\u064a\u0644 \u0645\u0646 \u062a\u0642\u062f\u064a\u0645 \u062c\u0645\u064a\u0639 \u0637\u0644\u0628\u0627\u062a \u0639\u0642\u0648\u062f \u062a\u0623\u0633\u064a\u0633 \u0627\u0644\u0634\u0631\u0643\u0627\u062a \u062d\u0633\u0628 \u0627\u0644\u0643\u064a\u0627\u0646 :  \n \u0630\u0627\u062a \u0627\u0644\u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0627\u0644\u0645\u062d\u062f\u0648\u062f\u0629 :  \n \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u064f\u062a\u0645\u0651\u0643\u0646 \u0627\u0644\u0645\u0633\u062a\u062b\u0645\u0631 \u0645\u0646 \u0627\u0644\u0628\u062f\u0621 \u0641\u064a \u064f\u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u0644\u0646\u0634\u0627\u0637 \u0627\u0644\u062a\u062c\u0627\u0631\u064a\u060c \u0648\u062a\u0623\u0633\u064a\u0633 \u0634\u0631\u0643\u0629 \u0630\u0627\u062a \u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0645\u062d\u062f\u0648\u062f\u0629 \u0645\u0646 \u0634\u062e\u0635 \n \u0648\u0627\u062d\u062f\u060c \u0623\u0648 \u0623\u0643\u062b\u0631 \u0645\u0646 \u0630\u0648\u064a \u0627\u0644\u0635\u0641\u0629 \u0627\u0644\u0637\u0628\u064a\u0639\u064a\u0629 \u0623\u0648 \u0627\u0644\u0625\u0639\u062a\u0628\u0627\u0631\u064a\u0629\u060c \u0648\u062a\u0639\u062f \u0630\u0645\u062a\u0647\u0627 \u0645\u0633\u062a\u0642\u0644\u0629 \u0639\u0646 \u0627\u0644\u0630\u0645\u0629 \u0627\u0644\u0645\u0627\u0644\u064a\u0629 \u0644\u0643\u0644 \u0634\u0631\u064a\u0643 \u0641\u064a\u0647\u0627 \u0623\u0648 \u0627\u0644\u0645\u0627\u0644\u0643 \u0644\u0647\u0627. \n \u0648\u062a\u0643\u0648\u0646 \u0627\u0644\u0634\u0631\u0643\u0629 \u0648\u062d\u062f\u0647\u0627 \u0645\u0633\u0624\u0648\u0644\u0629 \u0639\u0646 \u0627\u0644\u062f\u064a\u0648\u0646 \u0648\u0627\u0644\u0625\u0644\u062a\u0632\u0627\u0645\u0627\u062a \u0627\u0644\u0645\u062a\u0631\u062a\u0628\u0629 \u0639\u0644\u064a\u0647\u0627 \u0623\u0648 \u0627\u0644\u0646\u0627\u0634\u0626\u0629 \u0639\u0646 \u0646\u0634\u0627\u0637\u0647\u0627\u060c \u0648\u0644\u0627 \u064a\u0643\u0648\u0646 \u0627\u0644\u0645\u0627\u0644\u0643 \u0644\u0647\u0627 \u0648\u0644\u0627 \n \u0627\u0644\u0634\u0631\u064a\u0643 \u0641\u064a\u0647\u0627 \u0645\u0633\u0624\u0648\u0644\u0622\u064b \u0639\u0646 \u0647\u0630\u0647 \u0627\u0644\u062f\u064a\u0648\u0646 \u0648\u0627\u0644\u0625\u0644\u062a\u0632\u0627\u0645\u0627\u062a \u0625\u0644\u0627 \u0628\u0642\u062f\u0631 \u062d\u0635\u062a\u0647 \u0641\u064a \u0631\u0623\u0633 \u0627\u0644\u0645\u0627\u0644 . \n \u0627\u0644\u062a\u0636\u0627\u0645\u0646 :"", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a056721""}
{""id"": ""fd131aa6e7d9-1"", ""text"": ""\u0627\u0644\u062a\u0636\u0627\u0645\u0646 :  \n \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u062a\u064f\u0645\u0643\u0651\u0646 \u0627\u0644\u0639\u0645\u064a\u0644 \u0645\u0646 \u0627\u0644\u0628\u062f\u0621 \u0641\u064a \u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u0644\u0646\u0634\u0627\u0637 \u0627\u0644\u062a\u062c\u0627\u0631\u064a \u0648\u062a\u0623\u0633\u064a\u0633 \u0634\u0631\u0643\u0629 \u062a\u0636\u0627\u0645\u0646\u064a\u0629 \u062c\u0645\u064a\u0639 \u0634\u0631\u0643\u0627\u0624\u0647\u0627 \u0623\u0641\u0631\u0627\u062f \n \u0648\u0645\u0633\u0624\u0648\u0644\u064a\u0646 \u0634\u062e\u0635\u064a\u0627\u064b \u0628\u062c\u0645\u064a\u0639 \u0623\u0645\u0648\u0627\u0644\u0647\u0645 \u0648\u0628\u0627\u0644\u062a\u0636\u0627\u0645\u0646 \u0639\u0646 \u062f\u064a\u0648\u0646 \u0627\u0644\u0634\u0631\u0643\u0629 \u0648\u0627\u0644\u062a\u0632\u0627\u0645\u0627\u062a\u0647\u0627\u060c \n \u0627\u0644\u062a\u0648\u0635\u064a\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629 :  \n \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u062a\u064f\u0645\u0643\u0651\u0646 \u0627\u0644\u0645\u0633\u062a\u062b\u0645\u0631 \u0645\u0646 \u0627\u0644\u0628\u062f\u0621 \u0641\u064a \u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u0644\u0646\u0634\u0627\u0637 \u0627\u0644\u062a\u062c\u0627\u0631\u064a \u0648\u062a\u0623\u0633\u064a\u0633 \u0634\u0631\u0643\u0629 \u062a\u0648\u0635\u064a\u0629 \u0628\u0633\u064a\u0637\u0629 \u062a\u062a\u0643\u0648\u0646 \u0645\u0646 \u0641\u0631\u064a\u0642\u064a\u0646\u060c \n (\u0627\u0644\u0645\u062a\u0636\u0627\u0645\u0646) \u0648\u0647\u0648 \u0627\u0644\u0645\u0633\u0624\u0648\u0644 \u0639\u0646 \u062f\u064a\u0648\u0646 \u0648\u0627\u0644\u062a\u0632\u0627\u0645\u0627\u062a \u0627\u0644\u0634\u0631\u0643\u0629\u060c \u0648(\u0627\u0644\u0645\u0648\u0635\u064a) \u0648\u0647\u0648 \u0627\u0644\u0630\u064a \u0644\u0627 \u064a\u0643\u0648\u0646 \u0645\u0633\u0624\u0648\u0644\u0627\u064b \u0625\u0644\u0627 \u0641\u064a \u062d\u062f\u0648\u062f \u062d\u0635\u062a\u0647 \u0641\u064a \n \u0631\u0623\u0633 \u0627\u0644\u0645\u0627\u0644 .\n\u0634\u0631\u0648\u0637 \u0648\u0645\u062a\u0637\u0644\u0628\u0627\u062a \u0627\u0644\u062e\u062f\u0645\u0629: \u2022\u0625\u0631\u0641\u0627\u0642 \u062a\u0631\u062e\u064a\u0635 \u0645\u0646 \u0648\u0632\u0627\u0631\u0629 \u0627\u0644\u0627\u0633\u062a\u062b\u0645\u0627\u0631. \n \u2022\u0625\u0631\u0641\u0627\u0642 \u062a\u0642\u0631\u064a\u0631 \u0627\u0644\u0645\u0642\u064a\u0645 \u0627\u0644\u0645\u0639\u062a\u0645\u062f \u0641\u064a \u062d\u0627\u0644 \u0643\u0627\u0646 \u0631\u0627\u0633 \u0627\u0644\u0645\u0627\u0644 \u0639\u064a\u0646\u064a."", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a056721""}
{""id"": ""fd131aa6e7d9-2"", ""text"": ""\u062e\u0637\u0648\u0627\u062a \u0627\u0644\u062d\u0635\u0648\u0644 \u0639\u0644\u0649 \u0627\u0644\u062e\u062f\u0645\u0629: 1\u2022\u0627\u0644\u062f\u062e\u0648\u0644 \u0639\u0644\u0649 \u0645\u0646\u0635\u0629 \u0627\u0644\u0645\u0631\u0643\u0632 \u0627\u0644\u0633\u0639\u0648\u062f\u064a \u0644\u0644\u0623\u0639\u0645\u0627\u0644 (SBC) . \n 2\u2022 \u062a\u0633\u062c\u064a\u0644 \u0627\u0644\u062f\u062e\u0648\u0644 \u0628\u0648\u0627\u0633\u0637\u0629 \u0627\u0644\u0646\u0641\u0627\u0630 \u0627\u0644\u0648\u0637\u0646\u064a \u0627\u0644\u0645\u0648\u062d\u062f \u0623\u0648 \u0627\u0644\u0628\u0631\u064a\u062f \u0627\u0644\u0627\u0644\u0643\u062a\u0631\u0648\u0646\u064a . \n 3\u2022 \u062a\u062d\u062f\u064a\u062f \u0646\u0648\u0639 \u0648\u0635\u0641\u0629 \u0627\u0644\u0634\u0631\u0643\u0629 ( \u0634\u0631\u0643\u0629 \u0630\u0627\u062a \u0627\u0644\u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0627\u0644\u0645\u062d\u062f\u0648\u062f\u0629 - \u0627\u0644\u062a\u0636\u0627\u0645\u0646 - \u0627\u0644\u062a\u0648\u0635\u064a\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629) \u060c \u0623\u062c\u0646\u0628\u064a\u0629  \n 4\u2022 \u0625\u0633\u062a\u0643\u0645\u0627\u0644 \u0645\u062a\u0637\u0644\u0628\u0627\u062a \u0646\u0645\u0648\u0630\u062c \u0627\u0644\u062a\u0642\u062f\u064a\u0645 .  \n 5\u2022 \u062a\u0642\u062f\u064a\u0645 \u0627\u0644\u0637\u0644\u0628 . \n 6\u2022 \u0645\u0639\u0627\u0644\u062c\u0629 \u0627\u0644\u0637\u0644\u0628 . \n 7\u2022\u0645\u0648\u0627\u0641\u0642\u0629 \u0627\u0644\u0627\u0637\u0631\u0627\u0641 . \n 8\u2022\u0633\u062f\u0627\u062f \u0627\u0644\u0641\u0627\u062a\u0648\u0631\u0629 . \n 9\u2022\u0625\u0635\u062f\u0627\u0631 \u0627\u0644\u0648\u062b\u0627\u0626\u0642 \u0648\u0637\u0628\u0627\u0639\u0629 \u0627\u0644\u0633\u062c\u0644 \u0627\u0644\u062a\u062c\u0627\u0631\u064a \u0648\u0639\u0642\u062f \u0627\u0644\u0634\u0631\u0643\u0629 \u0625\u0644\u0643\u062a\u0631\u0648\u0646\u064a\u064b\u0627 .\n\u0631\u0633\u0648\u0645 \u0627\u0644\u062e\u062f\u0645\u0629: \u0630\u0627\u062a \u0627\u0644\u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0627\u0644\u0645\u062d\u062f\u0648\u062f\u0629 : 1200 \u0631\u064a\u0627\u0644 \u0633\u0639\u0648\u062f\u064a.  \n \u0631\u0633\u0648\u0645 \u0627\u0644\u0646\u0634\u0631 : 500 \u0648 \u064a\u0636\u0627\u0641 \u0625\u0644\u064a\u0647\u0627 \u0636\u0631\u064a\u0628\u0629 \u0627\u0644\u0642\u064a\u0645\u0629 \u0627\u0644\u0645\u0636\u0627\u0641\u0629 : 15%. \n \u0627\u0644\u062a\u0636\u0627\u0645\u0646 \u0648 \u0627\u0644\u062a\u0648\u0635\u064a\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629 : 800 \u0631\u064a\u0627\u0644 \u0633\u0639\u0648\u062f\u064a."", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a056721""}
{""id"": ""fd131aa6e7d9-3"", ""text"": ""\u0631\u0633\u0648\u0645 \u0627\u0644\u0646\u0634\u0631 : 500 \u0648\u064a\u0636\u0627\u0641 \u0625\u0644\u064a\u0647\u0627 \u0636\u0631\u064a\u0628\u0629 \u0627\u0644\u0642\u064a\u0645\u0629 \u0627\u0644\u0645\u0636\u0627\u0641\u0629 : 15%.\n\u0645\u0639\u0644\u0648\u0645\u0627\u062a \u0623\u062e\u0631\u0649: \u0645\u0639\u0644\u0648\u0645\u0627\u062a \u0623\u062e\u0631\u0649:  \u0627\u0644\u0645\u0648\u0642\u0639 \u0627\u0644\u0625\u0644\u0643\u062a\u0631\u0648\u0646\u064a: \n https://business.sa/ServicesAndPrograms/ServicesDetails.html?ServiceID=143 \n * \u0639\u0646\u062f \u0627\u0644\u062f\u062e\u0648\u0644 \u0639\u0644\u0649 \u0627\u0644\u062e\u062f\u0645\u0629 \u062a\u0638\u0647\u0631 \u0644\u0644\u0639\u0645\u0644 \u0646\u0648\u0639 \u0627\u0644\u0643\u064a\u0627\u0646 \u0627\u0644\u0645\u0637\u0644\u0648\u0628 \u0639\u0646\u062f \u0627\u0644\u062a\u0623\u0633\u064a\u0633 :  (\u0630\u0627\u062a \u0627\u0644\u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0627\u0644\u0645\u062d\u062f\u0648\u062f\u0629-\u0627\u0644\u062a\u0636\u0627\u0645\u0646-\u0627\u0644\u062a\u0648\u0635\u064a\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629) \u060c \u0623\u062c\u0646\u0628\u064a\u0629 /n"", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a056721""}
{""id"": ""3742980811a3-0"", ""text"": ""\u0648\u0635\u0641 \u0627\u0644\u062e\u062f\u0645\u0629: \u062a\u0645\u0643\u0651\u0650\u0646 \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u0627\u0644\u0639\u0645\u064a\u0644 \u0645\u0646 \u062a\u0642\u062f\u064a\u0645 \u062c\u0645\u064a\u0639 \u0637\u0644\u0628\u0627\u062a \u0639\u0642\u0648\u062f \u062a\u0623\u0633\u064a\u0633 \u0627\u0644\u0634\u0631\u0643\u0627\u062a \u062d\u0633\u0628 \u0627\u0644\u0643\u064a\u0627\u0646 :  \n \u0630\u0627\u062a \u0627\u0644\u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0627\u0644\u0645\u062d\u062f\u0648\u062f\u0629 :  \n \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u064f\u062a\u0645\u0651\u0643\u0646 \u0627\u0644\u0645\u0633\u062a\u062b\u0645\u0631 \u0645\u0646 \u0627\u0644\u0628\u062f\u0621 \u0641\u064a \u064f\u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u0644\u0646\u0634\u0627\u0637 \u0627\u0644\u062a\u062c\u0627\u0631\u064a\u060c \u0648\u062a\u0623\u0633\u064a\u0633 \u0634\u0631\u0643\u0629 \u0630\u0627\u062a \u0645\u0633\u0624\u0648\u0644\u064a\u0629 \u0645\u062d\u062f\u0648\u062f\u0629 \u0645\u0646 \u0634\u062e\u0635 \n \u0648\u0627\u062d\u062f\u060c \u0623\u0648 \u0623\u0643\u062b\u0631 \u0645\u0646 \u0630\u0648\u064a \u0627\u0644\u0635\u0641\u0629 \u0627\u0644\u0637\u0628\u064a\u0639\u064a\u0629 \u0623\u0648 \u0627\u0644\u0625\u0639\u062a\u0628\u0627\u0631\u064a\u0629\u060c \u0648\u062a\u0639\u062f \u0630\u0645\u062a\u0647\u0627 \u0645\u0633\u062a\u0642\u0644\u0629 \u0639\u0646 \u0627\u0644\u0630\u0645\u0629 \u0627\u0644\u0645\u0627\u0644\u064a\u0629 \u0644\u0643\u0644 \u0634\u0631\u064a\u0643 \u0641\u064a\u0647\u0627 \u0623\u0648 \u0627\u0644\u0645\u0627\u0644\u0643 \u0644\u0647\u0627. \n \u0648\u062a\u0643\u0648\u0646 \u0627\u0644\u0634\u0631\u0643\u0629 \u0648\u062d\u062f\u0647\u0627 \u0645\u0633\u0624\u0648\u0644\u0629 \u0639\u0646 \u0627\u0644\u062f\u064a\u0648\u0646 \u0648\u0627\u0644\u0625\u0644\u062a\u0632\u0627\u0645\u0627\u062a \u0627\u0644\u0645\u062a\u0631\u062a\u0628\u0629 \u0639\u0644\u064a\u0647\u0627 \u0623\u0648 \u0627\u0644\u0646\u0627\u0634\u0626\u0629 \u0639\u0646 \u0646\u0634\u0627\u0637\u0647\u0627\u060c \u0648\u0644\u0627 \u064a\u0643\u0648\u0646 \u0627\u0644\u0645\u0627\u0644\u0643 \u0644\u0647\u0627 \u0648\u0644\u0627 \n \u0627\u0644\u0634\u0631\u064a\u0643 \u0641\u064a\u0647\u0627 \u0645\u0633\u0624\u0648\u0644\u0622\u064b \u0639\u0646 \u0647\u0630\u0647 \u0627\u0644\u062f\u064a\u0648\u0646 \u0648\u0627\u0644\u0625\u0644\u062a\u0632\u0627\u0645\u0627\u062a \u0625\u0644\u0627 \u0628\u0642\u062f\u0631 \u062d\u0635\u062a\u0647 \u0641\u064a \u0631\u0623\u0633 \u0627\u0644\u0645\u0627\u0644 . \n \u0627\u0644\u062a\u0636\u0627\u0645\u0646 :"", ""source"": ""https://kgate.bc.gov.sa/#/service/64087e96d8467ecd6a056722""}
{""id"": ""3742980811a3-1"", ""text"": ""\u0627\u0644\u062a\u0636\u0627\u0645\u0646 :  \n \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u062a\u064f\u0645\u0643\u0651\u0646 \u0627\u0644\u0639\u0645\u064a\u0644 \u0645\u0646 \u0627\u0644\u0628\u062f\u0621 \u0641\u064a \u0645\u0645\u0627\u0631\u0633\u0629 \u0627\u0644\u0646\u0634\u0627\u0637 \u0627\u0644\u062a\u062c\u0627\u0631\u064a \u0648\u062a\u0623\u0633\u064a\u0633 \u0634\u0631\u0643\u0629 \u062a\u0636\u0627\u0645\u0646\u064a\u0629 \u062c\u0645\u064a\u0639 \u0634\u0631\u0643\u0627\u0624\u0647\u0627 \u0623\u0641\u0631\u0627\u062f \n \u0648\u0645\u0633\u0624\u0648\u0644\u064a\u0646 \u0634\u062e\u0635\u064a\u0627\u064b \u0628\u062c\u0645\u064a\u0639 \u0623\u0645\u0648\u0627\u0644\u0647\u0645 \u0648\u0628\u0627\u0644\u062a\u0636\u0627\u0645\u0646 \u0639\u0646 \u062f\u064a\u0648\u0646 \u0627\u0644\u0634\u0631\u0643\u0629 \u0648\u0627\u0644\u062a\u0632\u0627\u0645\u0627\u062a\u0647\u0627\u060c \n \u0627\u0644\u062a\u0648\u0635\u064a\u0629 \u0627\u0644\u0628\u0633\u064a\u0637\u0629 :  \n \u0647\u0630\u0647 \u0627\u0644\u062e\u062f\u0645\u0629 \u062a\u064f\u0645\u0643\u0651\u0646 \u0627\u0644\u0645\u0633\u062a",Low,1.0
Q&A,riotu-lab/Quran-Tafseers,6.0,20.0,2024-01-26 17:10:45+00:00,apache-2.0,0.0,128 MB,134217728.0,53 MB,55574528.0,56115,none,none,"---
license: apache-2.0
task_categories:
- question-answering
language:
- ar
pretty_name: 'Tibyan For Holy Quran '
size_categories:
- 10K<n<100K
---
### Model Details

Developed by: Prince Sultan University - Riotu Lab

This dataset is intended for use in natural language processing tasks, particularly for understanding classical Arabic and religious texts, including text analysis, language modeling, and thematic studies.
Primary Users: Researchers and developers in the field of natural language processing, religious studies, and AI, specifically those working with classical Arabic texts.
Out-of-scope Use Cases: This dataset is not intended for predictive modeling that could lead to ethical concerns, such as surveillance or profiling based on religious texts.
Model/Data Specifications

Format: Json
Dataset Size: Contains more than 57K rows
Language: Arabic

### Dataset Structure

Fields:
- sura_number: Integer representing the Surah number in the Quran.-
- Aya_number: Integer representing the Ayah number in the Surah.
- tafseers: Dictionary mapping Tafseer sources to their text for each Ayah:
- Tafseer Name : 
    1: ""التفسير الميسر"",
    2: ""تفسير الجلالين"",
    3:""تفسير ابن كثير"",
    4: ""تفسير الوسيط لطنطاوي"",
    5: ""تفسير البغوي"",
    6: ""تفسير القرطبي"",
    7: ""تفسير الطبري"",

",Low,1.0
Q&A,abdoelsayed/ArabicaQA,1.0,177.0,2024-03-27 20:29:09+00:00,mit,2.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2403.17848,none,"---
annotations_creators:
- crowdsourced
language_creators:
- crowdsourced
- found
license: mit
task_categories:
- question-answering
language:
- ar
pretty_name: ArabicaQA
size_categories:
- 10K<n<100K
---

# ArabicaQA
ArabicaQA: Comprehensive Dataset for Arabic Question Answering

This repository contains dataset for paper *ArabicaQA: Comprehensive Dataset for Arabic Question Answering*. Below, we provide details regarding the materials available in this repository:


## Dataset

Within this folder, you will find the training, validation, and test sets of the ArabicaQA dataset. Refer to the table below for the dataset statistics:

|                    | Training | Validation | Test   |
| -------------------|----------|------------|--------|
| MRC (with answers) | 62,186   | 13,483     | 13,426 |
| MRC (unanswerable) | 2,596    | 561        | 544    |
| Open-Domain        | 62,057   | 13,475     | 13,414 |
| Open-Domain        | 58,528   | 12,541     | 12,541 |



## Citation

If you find these codes or data useful, please consider citing our paper as:

```
@misc{abdallah2024arabicaqa,
      title={ArabicaQA: A Comprehensive Dataset for Arabic Question Answering}, 
      author={Abdelrahman Abdallah and Mahmoud Kasem and Mahmoud Abdalla and Mohamed Mahmoud and Mohamed Elkasaby and Yasser Elbendary and Adam Jatowt},
      year={2024},
      eprint={2403.17848},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
",Medium,3.0
Q&A,abdoelsayed/Open-ArabicaQA,7.0,293.0,2024-03-27 20:30:20+00:00,mit,2.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2403.17848,none,"---
annotations_creators:
  - crowdsourced
language_creators:
  - crowdsourced
  - found
license: mit
task_categories:
- question-answering
language:
- ar
pretty_name: abdoelsayed/Open-ArabicaQA
size_categories:
- 10K<n<100K
---


# ArabicaQA
ArabicaQA: Comprehensive Dataset for Arabic Question Answering

This repository contains dataset for paper *ArabicaQA: Comprehensive Dataset for Arabic Question Answering*. Below, we provide details regarding the materials available in this repository:

ArabicaQA is a robust dataset designed to support and advance the development of Arabic Question Answering (QA) systems. This dataset encompasses a wide range of question types, including both Machine Reading Comprehension (MRC) and Open-Domain questions, catering to various aspects of QA research and application. The dataset is structured to facilitate training, validation, and testing of Arabic QA models.

For more detail https://github.com/DataScienceUIBK/ArabicaQA/tree/main

## Dataset

Within this folder, you will find the training, validation, and test sets of the ArabicaQA dataset. Refer to the table below for the dataset statistics:

|                    | Training | Validation | Test   |
| -------------------|----------|------------|--------|
| MRC (with answers) | 62,186   | 13,483     | 13,426 |
| MRC (unanswerable) | 2,596    | 561        | 544    |
| Open-Domain        | 62,057   | 13,475     | 13,414 |
| Open-Domain        | 58,528   | 12,541     | 12,541 |



## Citation

If you find these codes or data useful, please consider citing our paper as:

```
@misc{abdallah2024arabicaqa,
      title={ArabicaQA: A Comprehensive Dataset for Arabic Question Answering}, 
      author={Abdelrahman Abdallah and Mahmoud Kasem and Mahmoud Abdalla and Mohamed Mahmoud and Mohamed Elkasaby and Yasser Elbendary and Adam Jatowt},
      year={2024},
      eprint={2403.17848},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
",Medium,3.0
Q&A,sadeem-ai/arabic-qna,3.0,730.0,2024-02-05 13:38:36+00:00,apache-2.0,0.0,4.5 MB,4718592.0,2.25 MB,2359296.0,6030,none,none,"---
license: apache-2.0
configs:
- config_name: default
  data_files:
  - split: train
    path: ar-qna-train-data-hf.csv
  - split: test
    path: ar-qna-test-data-hf.csv
task_categories:
- question-answering
language:
- ar
tags:
- qna
- questioning-answering
- questions-generation
pretty_name: arabic QnA dataset
size_categories:
- 1K<n<10K
---

# Sadeem QnA: An Arabic QnA Dataset 🌍✨

Welcome to the **Sadeem QnA** dataset, a vibrant collection designed for the advancement of Arabic natural language processing, specifically tailored for Question Answering (QnA) systems. Sourced from the rich and diverse content of Arabic Wikipedia, this dataset is a gateway to exploring the depths of Arabic language understanding, offering a unique challenge to both researchers and AI enthusiasts alike.

## Table of Contents

- [About Sadeem QnA](#about-sadeem-qna)
- [Dataset Structure](#dataset-structure)
- [Getting Started](#getting-started)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)
- [Citation](#citation)

## About Sadeem QnA

The **Sadeem QnA** dataset is crafted with the intent to foster research and development in Arabic Question Answering systems. It encompasses a broad range of topics, reflecting the rich tapestry of Arabic culture, history, and science, making it an ideal resource for training and evaluating AI models.

### Why Sadeem QnA?

- **Rich Content:** Over 6,000 QnA pairs across diverse subjects.
- **Real-World Questions:** Derived from actual queries people might ask, providing practical value for real-world applications.
- **Dual Splits:** Carefully partitioned into training (5,000 rows) and testing (1,030 rows) sets to facilitate effective model evaluation.

## Dataset Structure

Each record in the dataset follows a structured format, containing the following fields:

- `title`: The title of the Wikipedia article.
- `text`: A snippet from the article related to the question.
- `source`: The URL of the Wikipedia page.
- `question`: A question related to the text snippet.
- `answer`: The answer to the question.
- `has_answer`: A boolean indicating whether the answer is present in the text snippet.

### Example Record

```json
{
 'title': 'قائمة الجوائز والترشيحات التي تلقتها سلسلة أفلام مباريات الجوع',
 'text': 'قائمة الجوائز والترشيحات التي تلقتها سلسلة أفلام مباريات الجوع قائمة تُسجّل الترشيحات والجوائز التي تلقتها سلسلة أفلام مباريات الجوع المقتبسة من سلسلة مباريات الجوع للمؤلفة الأمريكية سوزان كولنز. والسلسلة من توزيع شركة ليونزغيت إنترتاينمنت، وقام ببطولتها جينيفر لورنس في دور كاتنيس إيفردين، جوش هوتشرسن في دور بيتا ميلاريك. وبدأت السلسلة بفيلم مباريات الجوع الذي صدر في العام 2012، ثم فيلم في العام 2013، وتبعهما كل من (2014) وأخيرًا: (2015). كان لجينيفر لورنس حصة الأسد في سجل الترشيحات والجوائز التي نالتها السلسلة.',
 'source': 'https://ar.wikipedia.org/wiki?curid=6237097',
 'question': 'متى صدر الفيلم الأول من سلسلة مباريات الجوع؟',
 'answer': 'عام 2012',
 'has_answer': True
},
{
  'title': 'سانت فرنسيس (ويسكونسن)',
  'text': 'بلغ عدد الأسر 4,494 أسرة كانت نسبة 19.8% منها لديها أطفال تحت سن الثامنة عشر تعيش معهم، وبلغت نسبة الأزواج القاطنين مع بعضهم البعض 36.6% من أصل المجموع الكلي للأسر، ونسبة 8.7% من الأسر كان لديها معيلات من الإناث دون وجود شريك، بينما كانت نسبة 3.9% من الأسر لديها معيلون من الذكور دون وجود شريكة وكانت نسبة 50.8% من غير العائلات. تألفت نسبة 42.6% من أصل جميع الأسر من أفراد ونسبة 13.7% كانوا يعيش معهم شخص وحيد يبلغ من العمر 65 عاماً فما فوق. وبلغ متوسط حجم الأسرة المعيشية 2.80، أما متوسط حجم العائلات فبلغ 2.02.',
  'source': 'https://ar.wikipedia.org/wiki?curid=2198358',
  'question': 'ما هو عدد العائلات المقيمة في سانت فرنسيس؟',
  'answer': '',
  'has_answer': False
}
```

## Getting Started

To get started with the **Sadeem QnA** dataset, you can download it directly from our [Huggingface repository](https://huggingface.co/datasets/sadeem-ai/arabic-qna).
Follow the instructions there to load the dataset into your environment and begin exploring.

## Usage

This dataset is perfect for:
- Training machine learning models for Arabic question answering.
- Evaluating the performance of NLP models on Arabic text.
- Enhancing language understanding systems with a focus on Arabic.

## Contributing

We welcome contributions from the community! Whether it's improving the documentation, adding more questions, or reporting issues, your help makes **Sadeem QnA** better for everyone.

## License

The **Sadeem QnA** dataset is available under the Apache License 2.0. We encourage its use for academic research, commercial applications, and beyond, provided proper attribution is given.

## Citation

If you use the **Sadeem QnA** dataset in your research, please cite it using the following format:

```bibtex
@misc{sadeem_qna,
  title={Sadeem QnA: An Arabic QnA Dataset},
  author={},
  year={2024},
  publisher={Huggingface},
  howpublished={\url{https://huggingface.co/datasets/sadeem-ai/arabic-qna}},
}
```

Embark on your journey through the Arabic language with **Sadeem QnA** and unlock the potential of AI in understanding the complexity and beauty of Arabic text. 🚀💡
",High,5.0
Q&A,MuhammadHelmy/nafsy-QA,1.0,15.0,2024-02-27 18:40:23+00:00,none,0.0,82 KB,83968.0,82 KB,83968.0,276,none,none,"---
dataset_info:
  features:
  - name: prompt
    dtype: string
  - name: response
    dtype: string
  splits:
  - name: train
    num_bytes: 125662
    num_examples: 232
  - name: test
    num_bytes: 24774
    num_examples: 44
  download_size: 81984
  dataset_size: 150436
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: test
    path: data/test-*
task_categories:
- question-answering
- text-generation
language:
- ar
tags:
- mental health
- psychology
size_categories:
- n<1K
---

# Dataset Card for nafsy-QA

<!-- Provide a quick summary of the dataset. -->

This is an Arabic QA dataset for mental health. Its orgins comes back to [Nafsy.net](https://nafsy.net/) articles and blogs.

## Dataset Details

**Language(s) (NLP):** Arabic

## Uses

<!-- Address questions around how the dataset is intended to be used. -->


### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

- Supervised Fine-tuning
   
## Dataset Creation

- GPT-3.5-Turbo has been used to extract question and answer pairs from the original plain text.

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

Creating an arabic chatbot for mental health support.

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

- This dataset was originally scrapped from [Nafsy.net](https://nafsy.net/) then uploaded to Kaggle.
- The QA extraction made on the preprocessed data in my other repo [MuhammadHelmy/nafsy](https://huggingface.co/datasets/MuhammadHelmy/nafsy)

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[husamal](https://www.kaggle.com/husamal)

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

@misc{Husamal_2021, title={Arabic-physcology-dataset}, url={https://www.kaggle.com/datasets/husamal/arabicphyscologydataset?select=nafsy.csv}, journal={Kaggle}, author={Husamal}, year={2021}, month={May}} 


## Dataset Card Authors

Muhammad Helmy

## Dataset Card Contact

muhammadhelmymmo@gmail.com",Medium,3.0
Q&A,Asmaamaghraby/ArabicChartsQA,2.0,14.0,2024-02-29 02:25:46+00:00,none,0.0,6.88 MB,7214202.88,2.79 MB,2925527.04,23114,none,none,"---
task_categories:
- question-answering
language:
- ar
- en
---",Low,0.0
Q&A,caro-holt/MultiQ,8.0,20.0,2024-03-02 15:22:59+00:00,cc-by-4.0,0.0,2.31 MB,2422210.56,1.05 MB,1101004.8,27400,none,none,"---
license: cc-by-4.0
language:
- tl
- sm
- mk
- gu
- fi
- mn
- bm
- ta
- ur
- hy
- nl
- tk
- en
- bg
- gd
- pt
- ko
- ga
- eu
- sv
- bs
- co
- fr
- gn
- ro
- it
- dv
- ku
- ak
- eo
- zu
- id
- te
- sl
- lv
- pa
- ru
- si
- ee
- yi
- ny
- az
- sw
- hi
- mt
- sr
- hr
- ka
- ug
- tt
- lg
- kn
- fy
- kk
- ca
- lb
- jv
- et
- la
- tr
- ps
- km
- zh
- uk
- as
- he
- yo
- sq
- da
- gl
- vi
- ay
- is
- ln
- mr
- st
- xh
- cs
- ky
- ml
- ht
- mi
- so
- uz
- el
- ti
- be
- cy
- am
- ig
- or
- fa
- ms
- su
- de
- lo
- ha
- ts
- om
- ar
- my
- es
- qu
- 'no'
- th
- sa
- mg
- pl
- sd
- sk
- bn
- rw
- af
- ne
- lt
- tg
- ja
- sn
- hu
size_categories:
- 10K<n<100K
task_categories:
- question-answering
---
# Dataset Card for MultiQ

This is the dataset corresponding to the paper ""Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ"". 
It is a silver standard benchmark that can be used to evaluate the basic multilingual capabilities of LLMs. It contains 200 open ended questions automatically 
translated into 137 typologically diverse languages. 

- **Curated by:** Carolin Holtermann, Paul Röttger, Timm Dill, Anne Lauscher
- **Language(s) (NLP):** 137 diverse languages described in detail in our paper
- **License:** CC-BY-4.0 License

### Dataset Sources

- **Repository:** [Github](https://github.com/paul-rottger/multiq)
- **Paper:** TBD",Low,1.0
Q&A,mmhamdy/Arabic-OpenHermes-Filtered,0.0,42.0,2024-03-07 05:38:07+00:00,none,0.0,120 MB,125829120.0,120 MB,125829120.0,78061,none,none,"---
dataset_info:
  features:
  - name: id
    dtype: string
  - name: source
    dtype: string
  - name: conversations
    dtype: string
  - name: prompt
    dtype: string
  - name: response
    dtype: string
  splits:
  - name: train
    num_bytes: 290367089
    num_examples: 78061
  download_size: 119981787
  dataset_size: 290367089
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
language:
- ar
task_categories:
- question-answering
tags:
- synthetic
size_categories:
- 10K<n<100K
---

This is a filtered version of Arabic-OpenHermes-2.5 and contains only examples whose prompts do not contain any Latin characters.

Find the original dataset here: https://huggingface.co/datasets/2A2I/Arabic-OpenHermes-2.5",Low,1.0
Q&A,FreedomIntelligence/ALLaVA-4V-Arabic,3.0,25.0,2024-04-29 16:09:37+00:00,apache-2.0,0.0,750 MB,786432000.0,341 MB,357564416.0,429709,https://arxiv.org/abs/2402.11684,none,"---
license: apache-2.0
task_categories:
- question-answering
- text-generation
language:
- ar
tags:
- GPT-4V
- LVLM
- Vision
- Language
size_categories:
- 1M<n<10M

configs:
  - config_name: allava_laion
    data_files:
      - split: caption
        path: ""allava_laion/ALLaVA-Caption-LAION-4V_Arabic.json""
      # - split: instruct
      #   path: ""allava_laion/ALLaVA-Instruct-LAION-4V_Chinese.json""
  - config_name: allava_vflan
    data_files:
      - split: caption
        path: ""allava_vflan/ALLaVA-Caption-VFLAN-4V_Arabic.json""
  #     - split: instruct
  #       path: ""allava_vflan/ALLaVA-Instruct-VFLAN-4V_Chinese.json""    

      
# - config_name: allava_laion_instruction
#   data_files: ""allava_laion/ALLaVA-Instruct-LAION-4V.json""
  
# configs:
# - config_name: default
#   data_files:
#   - split: allava_laion_caption
#     path: ""allava_laion/ALLaVA-Caption-LAION-4V.json""
#   - split: allava_laion_instruction
#     path: ""allava_laion/ALLaVA-Instruction-LAION-4V.json""
  
# configs:
# - config_name: default
# - data_files:
#   - split: allava_laion_caption
#   - path:
#     - ""allava_laion/ALLaVA-Caption-LAION-4V.json""
#   - split: allava_laion_instruction
#   - path:
#     - ""allava_laion/ALLaVA-Instruction-LAION-4V.json""
---
## ALLaVA-4V for Arabic
This is the Arabic version of the ALLaVA-4V data. We have translated the ALLaVA-4V data into Arabic through ChatGPT and instructed ChatGPT not to translate content related to OCR.

The original dataset can be found [here](https://huggingface.co/datasets/FreedomIntelligence/ALLaVA-4V), and the image data can be downloaded from [ALLaVA-4V](https://huggingface.co/datasets/FreedomIntelligence/ALLaVA-4V).

#### Citation

If you find our data useful, please consider citing our work! We are FreedomIntelligence from Shenzhen Research Institute of Big Data and The Chinese University of Hong Kong, Shenzhen.
```
@misc{chen2024allava,
      title={ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model}, 
      author={Guiming Hardy Chen and Shunian Chen and Ruifei Zhang and Junying Chen and Xiangbo Wu and Zhiyi Zhang and Zhihong Chen and Jianquan Li and Xiang Wan and Benyou Wang},
      year={2024},
      eprint={2402.11684},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```",Medium,3.0
Q&A,KIND-Dataset/Open-ended_Questions_dialectal_data,0.0,14.0,2024-03-19 09:00:59+00:00,cc-by-4.0,0.0,70.7 KB,72396.8,37.9 KB,38809.6,796,none,"['https://aclanthology.org/2024.eacl-srw.3/)', 'https://aclanthology.org/2024.eacl-srw.3"",']","---
license: cc-by-4.0
task_categories:
- question-answering
language:
- ar
size_categories:
- n<1K
---

### Dataset Summary

A collection of open-ended questions that was provided to the data marathon competitors to populate KIND dataset. It was designed to elicit longer responses cultural and context-rich sentences. 


For more details, please check the paper
[The KIND Dataset: A Social Collaboration Approach for Nuanced Dialect Data Collection](https://aclanthology.org/2024.eacl-srw.3/)


### Citation Information

```
@inproceedings{yamani-etal-2024-kind,
    title = ""The {KIND} Dataset: A Social Collaboration Approach for Nuanced Dialect Data Collection"",
    author = ""Yamani, Asma  and
      Alziyady, Raghad  and
      AlYami, Reem  and
      Albelali, Salma  and
      Albelali, Leina  and
      Almulhim, Jawharah  and
      Alsulami, Amjad  and
      Alfarraj, Motaz  and
      Al-Zaidy, Rabeah"",
    editor = ""Falk, Neele  and
      Papi, Sara  and
      Zhang, Mike"",
    booktitle = ""Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop"",
    month = mar,
    year = ""2024"",
    address = ""St. Julian{'}s, Malta"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.eacl-srw.3"",
    pages = ""32--43"",
}

```",Medium,3.0
Q&A,cleexiang/chat_unsensored,0.0,12.0,2024-03-26 08:48:37+00:00,apache-2.0,0.0,250 KB,256000.0,250 KB,256000.0,1000,none,none,"---
license: apache-2.0
task_categories:
- question-answering
language:
- ar
size_categories:
- 1K<n<10K
---

# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Q&A,yongyi169/yy-chat-ar-20240327,0.0,21.0,2024-03-27 02:31:34+00:00,apache-2.0,0.0,1.14 MB,1195376.64,1.14 MB,1195376.64,5060,none,none,"---
license: apache-2.0
task_categories:
- question-answering
language:
- ar
size_categories:
- 1K<n<10K
---
# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Q&A,yongyi169/yy-chat-ar-20240329,0.0,50.0,2024-03-29 04:23:42+00:00,apache-2.0,0.0,2.95 MB,3093299.2,2.95 MB,3093299.2,5060,none,none,"---
license: apache-2.0
task_categories:
- question-answering
language:
- ar
---",Low,1.0
Q&A,asas-ai/tydiqa-ar,0.0,15.0,2024-04-01 06:15:32+00:00,apache-2.0,0.0,374 MB,392167424.0,374 MB,392167424.0,40198,none,none,"---
language:
- ar
license: apache-2.0
task_categories:
- question-answering
pretty_name: tydiqa-ar
configs:
- config_name: primary_task
  data_files:
  - split: train
    path: primary_task/train-*
  - split: validation
    path: primary_task/validation-*
- config_name: secondary_task
  data_files:
  - split: train
    path: secondary_task/train-*
  - split: validation
    path: secondary_task/validation-*
dataset_info:
- config_name: primary_task
  features:
  - name: passage_answer_candidates
    sequence:
    - name: plaintext_start_byte
      dtype: int32
    - name: plaintext_end_byte
      dtype: int32
  - name: question_text
    dtype: string
  - name: document_title
    dtype: string
  - name: language
    dtype: string
  - name: annotations
    sequence:
    - name: passage_answer_candidate_index
      dtype: int32
    - name: minimal_answers_start_byte
      dtype: int32
    - name: minimal_answers_end_byte
      dtype: int32
    - name: yes_no_answer
      dtype: string
  - name: document_plaintext
    dtype: string
  - name: document_url
    dtype: string
  splits:
  - name: train
    num_bytes: 767894331.3564428
    num_examples: 23092
  - name: validation
    num_bytes: 35803153.66148902
    num_examples: 1380
  download_size: 0
  dataset_size: 803697485.0179318
- config_name: secondary_task
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: train
    num_bytes: 15715443.835027365
    num_examples: 14805
  - name: validation
    num_bytes: 908198.6986409297
    num_examples: 921
  download_size: 0
  dataset_size: 16623642.533668295
---
# Dataset Card for ""tydiqa-ar""

[More Information needed](https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)",Medium,2.0
Q&A,asas-ai/tydiqa-ar-primary_task,0.0,11.0,2024-04-01 06:15:03+00:00,apache-2.0,0.0,365 MB,382730240.0,365 MB,382730240.0,24472,none,none,"---
language:
- ar
license: apache-2.0
task_categories:
- question-answering
pretty_name: ' tydiqa-ar'
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: validation
    path: data/validation-*
dataset_info:
  features:
  - name: passage_answer_candidates
    sequence:
    - name: plaintext_start_byte
      dtype: int32
    - name: plaintext_end_byte
      dtype: int32
  - name: question_text
    dtype: string
  - name: document_title
    dtype: string
  - name: language
    dtype: string
  - name: annotations
    sequence:
    - name: passage_answer_candidate_index
      dtype: int32
    - name: minimal_answers_start_byte
      dtype: int32
    - name: minimal_answers_end_byte
      dtype: int32
    - name: yes_no_answer
      dtype: string
  - name: document_plaintext
    dtype: string
  - name: document_url
    dtype: string
  splits:
  - name: train
    num_bytes: 767894331.3564428
    num_examples: 23092
  - name: validation
    num_bytes: 35803153.66148902
    num_examples: 1380
  download_size: 364886604
  dataset_size: 803697485.0179318
---
",Medium,2.0
Q&A,asas-ai/tydiqa-ar-secondary_task,0.0,8.0,2024-04-01 06:16:42+00:00,apache-2.0,0.0,9.13 MB,9573498.88,9.13 MB,9573498.88,15726,none,none,"---
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: validation
    path: data/validation-*
dataset_info:
  features:
  - name: id
    dtype: string
  - name: title
    dtype: string
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: text
      dtype: string
    - name: answer_start
      dtype: int32
  splits:
  - name: train
    num_bytes: 15715443.835027365
    num_examples: 14805
  - name: validation
    num_bytes: 908198.6986409297
    num_examples: 921
  download_size: 9132744
  dataset_size: 16623642.533668295
license: apache-2.0
task_categories:
- question-answering
language:
- ar
pretty_name: tydiqa-ar
---
# Dataset Card for ""tydiqa-ar-secondary_task""

[More Information needed](https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)",Medium,2.0
Q&A,asas-ai/mlqa-ar-ar,0.0,8.0,2024-04-01 07:27:40+00:00,cc-by-sa-3.0,0.0,3.99 MB,4183818.24,3.99 MB,4183818.24,5852,none,none,"---
configs:
- config_name: default
  data_files:
  - split: test
    path: data/test-*
  - split: validation
    path: data/validation-*
dataset_info:
  features:
  - name: context
    dtype: string
  - name: question
    dtype: string
  - name: answers
    sequence:
    - name: answer_start
      dtype: int32
    - name: text
      dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 8210810
    num_examples: 5335
  - name: validation
    num_bytes: 808221
    num_examples: 517
  download_size: 3991496
  dataset_size: 9019031
license: cc-by-sa-3.0
task_categories:
- question-answering
language:
- ar
pretty_name: mlqa-ar-ar
---
# Dataset Card for ""mlqa-ar-ar""

[More Information needed](https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)",Medium,2.0
Q&A,asas-ai/ArTrivia,0.0,12.0,2024-04-01 10:40:04+00:00,none,0.0,5.2 MB,5452595.2,5.2 MB,5452595.2,10045,none,none,"---
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: validation
    path: data/validation-*
dataset_info:
  features:
  - name: title
    dtype: string
  - name: paragraphs
    list:
    - name: context
      dtype: string
    - name: qas
      list:
      - name: answers
        list:
        - name: answer_start
          dtype: int64
        - name: text
          dtype: string
      - name: id
        dtype: string
      - name: question
        dtype: string
  splits:
  - name: train
    num_bytes: 9477598
    num_examples: 8345
  - name: validation
    num_bytes: 1999664
    num_examples: 1700
  download_size: 5199179
  dataset_size: 11477262
task_categories:
- question-answering
language:
- ar
pretty_name: ArTrivia
---
# Dataset Card for ""ArTrivia""

[More Information needed](https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)",Low,1.0
Q&A,TrainingDataPro/llm-dataset,5.0,44.0,2024-04-25 11:49:11+00:00,cc-by-nc-nd-4.0,0.0,1.45 MB,1520435.2,865 KB,885760.0,1174,none,none,"---
license: cc-by-nc-nd-4.0
task_categories:
- text-classification
- table-question-answering
- question-answering
- text2text-generation
- text-generation
language:
- en
- es
- ar
- el
- fr
- ja
- pt
- uk
- az
- ga
- ko
- ca
- eo
- hi
- ml
- sl
- hu
- mr
- cs
- fa
- id
- nl
- th
- de
- fi
- it
- pl
- tr
tags:
- code
- legal
- finance
---

# LLM Dataset - Prompts and Generated Texts

The dataset contains prompts and texts generated by the Large Language Models (LLMs) in **32 different languages**. The prompts are short sentences or phrases for the model to generate text. The texts generated by the LLM are responses to these prompts and can vary in **length and complexity**.

Researchers and developers can use this dataset to train and fine-tune their own language models for multilingual applications. The dataset provides a rich and diverse collection of outputs from the model, demonstrating its ability to generate coherent and contextually relevant text in multiple languages.

# 💴 For Commercial Usage: Full version of the dataset includes **4,000,000 logs** generated in **32 languages** with diferent types of LLM, including Uncensored GPT, leave a request on **[TrainingData](https://trainingdata.pro/datasets/llm?utm_source=huggingface&utm_medium=cpc&utm_campaign=llm)** to buy the dataset

### Models used for text generation:
- **GPT-3.5**,
- **GPT-4**

### Languages in the dataset: 
*Arabic, Azerbaijani, Catalan, Chinese, Czech, Danish, German, Greek, English, Esperanto, Spanish, Persian, Finnish, French, Irish, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Malayalam, Maratham, Netherlands, Polish, Portuguese, Portuguese (Brazil), Slovak, Swedish, Thai, Turkish, Ukrainian*

![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F12421376%2Ff60c93f09ec82a765aa39678e4aa0a58%2Fsnapedit_1709731090855.jpeg?generation=1709738798916444&alt=media)

# Content
CSV File includes the following data:
- **from_language**: language the prompt is made in,
- **model**: type of the model (GPT-3.5, GPT-4 and Uncensored GPT Version),
- **time**: time when the answer was generated,
- **text**: user prompt,
- **response**: response generated by the model

# 💴 Buy the Dataset: This is just an example of the data. Leave a request on **[https://trainingdata.pro/datasets](https://trainingdata.pro/datasets/llm?utm_source=huggingface&utm_medium=cpc&utm_campaign=llm)** to discuss your requirements, learn about the price and buy the dataset

## **[TrainingData](https://trainingdata.pro/datasets/llm?utm_source=huggingface&utm_medium=cpc&utm_campaign=llm)** provides high-quality data annotation tailored to your needs

More datasets in TrainingData's Kaggle account: **https://www.kaggle.com/trainingdatapro/datasets**

TrainingData's GitHub: **https://github.com/Trainingdata-datamarket/TrainingData_All_datasets**

*keywords: dataset, machine learning, natural language processing, artificial intelligence, deep learning, neural networks, text generation, language models, openai, gpt-3, data science, predictive modeling, sentiment analysis, keyword extraction, text classification, sequence-to-sequence models, attention mechanisms, transformer architecture, word embeddings, glove embeddings, chatbots, question answering, language understanding, text mining, information retrieval, data preprocessing, feature engineering, explainable ai, model deployment*",Medium,3.0
Q&A,hmmamalrjoub/Islam_Question_and_Answer1,1.0,39.0,2024-04-10 21:11:57+00:00,none,0.0,738 KB,755712.0,237 KB,242688.0,710,none,none,"---
task_categories:
- question-answering
language:
- ar
---",Low,0.0
Q&A,Lyte/2A2I-Arabic-OpenHermes-2.5-Llama-3,1.0,82.0,2024-04-19 07:22:53+00:00,apache-2.0,0.0,1.08 GB,1159641169.92,1.08 GB,1159641169.92,981618,none,none,"---
dataset_info:
  features:
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 4164867126
    num_examples: 981618
  download_size: 1076974066
  dataset_size: 4164867126
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: apache-2.0
task_categories:
- question-answering
language:
- ar
size_categories:
- 100K<n<1M
---
# Dataset Card for ""2A2I-Arabic-OpenHermes-2.5-Llama-3""


### Dataset Sources & Infos
- **Data Origin**: Derived from the original Arabic OpenHermes dataset : [2A2I/Arabic-OpenHermes-2.5](https://huggingface.co/datasets/2A2I/Arabic-OpenHermes-2.5).
- **Languages**: Modern Standard Arabic (MSA)
- **Applications**: `Language Modeling`
- **License:** Apache-2.0
  
### Overview

`2A2I-Arabic-OpenHermes-2.5-Llama` is a Llama-3 compatible dataset carefully converted from the 2A2I's Arabic-OpenHermes-2.5 collection provided by [Lyte](https://huggingface.co/Lyte).

### Purpose

`2A2I-Arabic-OpenHermes-2.5-Llama-3` streamlines Arabic language research and applications by offering a high quality text resource in Meta's Llama-3 conversational style to help better alignement of the Arabic Base LLMs, saving time and effort for researchers, technologists, and linguists in Arabic NLP/AI projects.
- Enjoy using 2A2I-Arabic-OpenHermes-2.5-Llama-3 dataset directly for your Arabic applications and research! 😀
  
### Usage
This dataset serves as an essential tool for those venturing into Arabic language projects, spanning from academic research to commercial applications. By presenting a source of Arabic text, `2A2I-Arabic-OpenHermes-2.5-Llama-3` empowers users to plunge directly into model `finetuning`, analysis, and application development, eliminating the initial challenges of synthetic data creation.

#### Use with HuggingFace
To load this dataset with Datasets, you'll need to install the datasets library with `pip install datasets --upgrade` and then use the following code:

```python
from datasets import load_dataset

dataset = load_dataset(""Lyte/2A2I-Arabic-OpenHermes-2.5-Llama-3"")
```

### Contribution and Collaborative Engagement
Find '2A2I-Arabic-OpenHermes-2.5-Llama-3' on the Hugging Face Hub at [Lyte/2A2I-Arabic-OpenHermes-2.5-Llama-3](https://huggingface.co/datasets/Lyte/2A2I-Arabic-OpenHermes-2.5-Llama-3), where community contributions are welcomed. Users are invited to share feedback and propose enhancements.

---

# Original Dataset Card of OpenHermes-2.5 by teknium

<img src=""https://cdn-uploads.huggingface.co/production/uploads/64d5698102e58cc1fdd0b585/nWQ7oqq4fUSaGsvmNAsr2.png"" width=""350"" alt=""Original Dataset Card of OpenHermes by teknium"">


## Dataset Summary

The Open Hermes 2/2.5 and Nous Hermes 2 models have recently achieved noteworthy progress in state-of-the-art language models (LLMs). These advancements are rooted in the innovative utilization of large-scale training data, specifically tailored for language modeling tasks.

For further information, please visit [teknium/OpenHermes-2.5](https://huggingface.co/datasets/teknium/OpenHermes-2.5).

We hope the `Arabic-OpenHermes-2.5` dataset serves your needs well and propels your Arabic NLP endeavors to new heights!

## Citation

```bibtex
@misc{OpenHermes 2.5,
  title = {OpenHermes 2.5: An Open Dataset of Synthetic Data for Generalist LLM Assistants},
  author = {Teknium},
  year = {2023},
  publisher = {HuggingFace},
  url = {https://huggingface.co/datasets/teknium/OpenHermes-2.5}
}
```
```bibtex
@misc{Arabic OpenHermes 2.5,
  title = {Arabic OpenHermes 2.5: An Arabic version of Synthetic Data for Generalist Arabic LLM Assistants},
  author = {Marwa El Kamil, Mohammed Machrouh},
  year = {2024},
  publisher = {HuggingFace},
  url = {https://huggingface.co/datasets/2A2I/Arabic-OpenHermes-2.5}
}
```",High,6.0
Q&A,nthakur/swim-ir-cross-lingual,8.0,400.0,2024-04-28 05:11:45+00:00,cc-by-sa-4.0,0.0,7.2 GB,7730941132.8,7.2 GB,7730941132.8,15439922,https://arxiv.org/abs/2311.05800,"['https://aclanthology.org/2020.tacl-1.30/)', 'https://aclanthology.org/2023.findings-emnlp.125/)']","---
dataset_info:
- config_name: ar
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 678470945
    num_examples: 901363
  download_size: 424944246
  dataset_size: 678470945
- config_name: bn
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 737075032
    num_examples: 909748
  download_size: 441576861
  dataset_size: 737075032
- config_name: de
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 665826369
    num_examples: 909145
  download_size: 416871209
  dataset_size: 665826369
- config_name: es
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 664068758
    num_examples: 905771
  download_size: 414024543
  dataset_size: 664068758
- config_name: fa
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 689024804
    num_examples: 910295
  download_size: 422902015
  dataset_size: 689024804
- config_name: fi
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 654925593
    num_examples: 906429
  download_size: 420499374
  dataset_size: 654925593
- config_name: fr
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 672269845
    num_examples: 911694
  download_size: 418045032
  dataset_size: 672269845
- config_name: hi
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 732149451
    num_examples: 913977
  download_size: 432197050
  dataset_size: 732149451
- config_name: id
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 662583144
    num_examples: 907826
  download_size: 410842891
  dataset_size: 662583144
- config_name: ja
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 690011730
    num_examples: 906862
  download_size: 434183716
  dataset_size: 690011730
- config_name: ko
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 666971112
    num_examples: 905669
  download_size: 423849035
  dataset_size: 666971112
- config_name: ru
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 690585319
    num_examples: 904933
  download_size: 433451033
  dataset_size: 690585319
- config_name: sw
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 661558554
    num_examples: 905242
  download_size: 412357995
  dataset_size: 661558554
- config_name: te
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 727300524
    num_examples: 902190
  download_size: 438223497
  dataset_size: 727300524
- config_name: th
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 721428083
    num_examples: 914610
  download_size: 432630802
  dataset_size: 721428083
- config_name: yo
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 665447024
    num_examples: 902467
  download_size: 407779682
  dataset_size: 665447024
- config_name: zh
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 666564071
    num_examples: 921701
  download_size: 419343725
  dataset_size: 666564071
configs:
- config_name: ar
  data_files:
  - split: train
    path: ar/train-*
- config_name: bn
  data_files:
  - split: train
    path: bn/train-*
- config_name: de
  data_files:
  - split: train
    path: de/train-*
- config_name: es
  data_files:
  - split: train
    path: es/train-*
- config_name: fa
  data_files:
  - split: train
    path: fa/train-*
- config_name: fi
  data_files:
  - split: train
    path: fi/train-*
- config_name: fr
  data_files:
  - split: train
    path: fr/train-*
- config_name: hi
  data_files:
  - split: train
    path: hi/train-*
- config_name: id
  data_files:
  - split: train
    path: id/train-*
- config_name: ja
  data_files:
  - split: train
    path: ja/train-*
- config_name: ko
  data_files:
  - split: train
    path: ko/train-*
- config_name: ru
  data_files:
  - split: train
    path: ru/train-*
- config_name: sw
  data_files:
  - split: train
    path: sw/train-*
- config_name: te
  data_files:
  - split: train
    path: te/train-*
- config_name: th
  data_files:
  - split: train
    path: th/train-*
- config_name: yo
  data_files:
  - split: train
    path: yo/train-*
- config_name: zh
  data_files:
  - split: train
    path: zh/train-*
license: cc-by-sa-4.0
task_categories:
- text-retrieval
- question-answering
language:
- ar
- bn
- de
- es
- fa
- fi
- fr
- hi
- id
- ja
- ko
- ru
- sw
- te
- th
- yo
- zh
pretty_name: SWIM-IR (Cross-lingual)
language_creators:
- machine-generated
multilinguality:
- multilingual
source_datasets:
- original
size_categories:
- 10M<n<100M
---

# Dataset Card for SWIM-IR (Cross-lingual)

![SWIM-IR Logo](./swimir_header.png)

<!-- Provide a quick summary of the dataset. -->

This is the cross-lingual subset of the SWIM-IR dataset, where the query generated is in the target language and the passage is in English.
The SWIM-IR dataset is available as CC-BY-SA 4.0. 18 languages (including English) are available in the cross-lingual dataset.

For full details of the dataset, please read our upcoming [NAACL 2024 paper](https://arxiv.org/abs/2311.05800) and check out our [website](https://github.com/google-research-datasets/swim-ir).

# What is SWIM-IR?

SWIM-IR dataset is a synthetic multilingual retrieval dataset spanning around 29 million retrieval training pairs across 27 languages. 
Each question has been automatically generated with the Summarize-then-Ask (STA) prompting technique using PaLM-2 as the question generator.

**Note**: As the question is synthetically generated, there is scope for hallucinations during query generation. The hallucinated queries do not affect retrieval effectiveness.

If you are using SWIM-IR in your research, please cite the following paper:

```
@article{thakur:2023,
  author       = {Nandan Thakur and
                  Jianmo Ni and
                  Gustavo Hern{\'{a}}ndez {\'{A}}brego and
                  John Wieting and
                  Jimmy Lin and
                  Daniel Cer},
  title        = {Leveraging LLMs for Synthesizing Training Data Across Many Languages
                  in Multilingual Dense Retrieval},
  journal      = {CoRR},
  volume       = {abs/2311.05800},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.05800},
  doi          = {10.48550/ARXIV.2311.05800},
  eprinttype    = {arXiv},
  eprint       = {2311.05800},
  timestamp    = {Tue, 14 Nov 2023 14:47:55 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-05800.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
```

## Dataset Details

### Dataset Description

- **Homepage:** [SWIM-IR homepage](https://github.com/google-research-datasets/swim-ir)
- **Repository:** [SWIM-IR repository](https://github.com/google-research-datasets/swim-ir)
- **Paper:** [Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval
](https://arxiv.org/abs/2311.05800)
- **Leaderboard:** [Needs More Information]
- **Point of Contact:** [Nandan Thakur](mailto:nandan.thakur@uwaterloo.ca)

#### Dataset Link
<!-- info: Provide a link to the dataset: -->
<!-- width: half -->
SWIM-IR v1.0: http://storage.googleapis.com/gresearch/swim-ir/swim_ir_v1.tar.gz

#### Data Card Author(s)
<!-- info: Select **one role per** Data Card Author:

(Usage Note: Select the most appropriate choice to describe the author's role
in creating the Data Card.) -->
<!-- width: half -->
- **Nandan Thakur, University of Waterloo:** Owner
- **Daniel Cer, Google Research:** Owner
- **Jianmo Ni, Google DeepMind:** Contributor
- **John Wieting, Google DeepMind:** Contributor
- **Gustavo Hernandez Abrego, Google Research:** Contributor
- **Jimmy Lin, University of Waterloo:** Contributor

## Authorship
### Publishers
#### Publishing Organization(s)
<!-- scope: telescope -->
<!-- info: Provide the names of the institution or organization responsible
for publishing the dataset: -->
University of Waterloo, Google Research, Google DeepMind

#### Industry Type(s)
<!-- scope: periscope -->
<!-- info: Select **all applicable** industry types to which the publishing
organizations belong: -->
- Corporate - Tech
- Academic - Tech

### Dataset Owners
#### Team(s)
<!-- scope: telescope -->
<!-- info: Provide the names of the groups or team(s) that own the dataset: -->
SWIM-IR Team

#### Contact Detail(s)
<!-- scope: periscope -->
<!-- info: Provide pathways to contact dataset owners: -->
- **Dataset Owner(s):** Nandan Thakur, Daniel Cer
- **Affiliation:** University of Waterloo, Google Research
- **Contact:** [nandan.thakur@uwaterloo.ca](mailto:nandan.thakur@uwaterloo.ca)

## Dataset Overview
#### Data Subject(s)
<!-- scope: telescope -->
<!-- info: Select ***all applicable**** subjects contained the dataset: -->
- Synthetically generated data

#### Dataset Snapshot
<!-- scope: periscope -->
<!-- info: Provide a snapshot of the dataset:<br><br>(Use the additional notes
to include relevant information, considerations, and links to table(s) with
more detailed breakdowns.) -->
SWIM-IR is a synthetic multilingual retrieval training dataset. 
It contains training pairs for both settings: monolingual, i.e. within the same language, and cross-lingual, i.e. across language. 
The dataset is useful to fine-tune state-of-the-art (SoTA) synthetic monolingual and cross-lingual neural retrievers across diverse languages.


Category | Data
--- | ---
Size of Dataset | ~6-7 GB
Number of Instances | 28,265,848
Number of Fields | 6
Labeled Classes | 33*
Number of Labels | 1

**Above:** Dataset statistics comprises both in-language and cross-language settings. The classes above denote a language.

**Additional Notes:** (*) Classes denote the languages we cover in the SWIM-IR dataset. Here is a list of the 18 languages and their ISO codes listed in alphabetical order: 
Arabic (ar), Bengali (bn), German (de), English (en), Spanish (es), Persian (fa), Finnish (fi), French (fr), Hindi (hi), Indonesian (id), Japanese (ja), Korean (ko), Russian (ru), Swahili (sw), Thai (th), Yoruba (yo),
Chinese (zh) and rest 15 Indo-European Languages: Assamese (as), Bhojpuri (bho), Konkani (gom), Gujarati (gu), Kannada (kn), Maithili (mai), Malayalam (ml), Manipuri (mni), Marathi (mr), Odia (or), Punjabi (pa), Pashto (ps), Sanskrit (sa), Tamil (ta), Urdu (ur).

#### Content Description
<!-- scope: microscope -->
<!-- info: Provide a short description of the content in a data point: -->
A paragraph is sampled from the Wikipedia corpus which describes an entity. The question arising from the Wikipedia
paragraph is generated using a large language model (LLM). In our work, we used the PaLM 2-S (small) model to generate
synthetic queries across **33 languages**, covering 11 distinct scripts, and 10 language families comprising over 3 billion speakers in the world.

The SWIM-IR dataset contains about **28 million** Wikipedia synthetic query-paragraph training pairs with a multilingual query for each passage generated using PaLM 2 (small), 
for both cross-lingual and monolingual retrieval settings.

**Additional Notes:**
- The dataset creation follows a specific procedure that involves a `summarize-then-ask` prompting technique inspired by chain-of-thought prompting.
- PaLM 2 uses **summarize-then-ask promping** containing 5-shot exemplars for cross-lingual and 3-shot exemplars for monolingual query generation.
- The prompt includes the original paragraph, a human-generated summary, and a question translated from English using Machine Translation (MT) for cross-lingual generation,
- whereas for randomly sampled training dataset pairs, and summaries generated using Google BARD for monolingual generation.
- PaLM 2 generates an extractive summary which is used as a proxy to help understand the document and highlight relevant sections within the document.
- Finally, the model generates a question in the target language (different in cross-lingual or same in monolingual) which can be answered using the input paragraph.

### Sensitivity of Data
#### Sensitivity Type(s)
<!-- scope: telescope -->
<!-- info: Select ***all applicable*** data types present in the dataset: -->
- None

#### Field(s) with Sensitive Data
<!-- scope: periscope -->
<!-- info: List fields in the dataset that contain S/PII, and specify if their
collection was intentional or unintentional.

Use additional notes to capture any other relevant information or
considerations. -->
**Intentional Collected Sensitive Data**
No sensitive data was intentionally collected.

**Unintentionally Collected Sensitive Data**
S/PII, violent, abusive, or toxic text containing racial slurs were not explicitly collected as a part of the dataset creation
process. Sensitive subject and adult content was automatically filtered using the method described in (Thakur et al. 2023).

#### Security and Privacy Handling
<!-- scope: microscope -->
<!-- info: Summarize the measures or steps to handle sensitive data in this
dataset.

Use additional notes to capture any other relevant information or
considerations. -->

We used algorithmic methods and relied on other classifiers for data filtration. Specifically, we (1) did a human inspection of text samples, with the questions automatically translated to English; (2) our observations motivated using a classifier to filter text containing sensitive subjects and adult content.

## Example of Data Points
#### Primary Data Modality
<!-- scope: telescope -->
<!-- info: Select **one**: -->
- Text Data

#### Data Fields
<!-- scope: microscope -->
<!-- info: List the fields in data points and their descriptions.

(Usage Note: Describe each field in a data point. Optionally use this to show
the example.) -->

| Field name | Datapoint Example | Description |
| --------- | -------- | -------- |
| `lang` | String | The language of the generated question |
| `code` | String | The ISO-Code for the language |
| `query` | String | The generated query using PaLM 2 |
| `_id` | String | unique ID denoting the training pair |
| `title` | String | Title of the Wikipedia article |
| `text` | String | Paragraph of the Wikipedia article 

#### Typical Data Point
<!-- width: half -->
<!-- info: Provide an example of a typical data point and describe what makes
it typical.

**Use additional notes to capture any other relevant information or
considerations.** -->
Example of (English -> Japanese) datapoint from our
cross-lingual dataset on the topic of “The Roki Tunnel” from the
English Wikipedia.

```bash
{
  '_id': '1234',
  'lang': 'Japanese',
  'code': 'ja',
  'query': 'The Roki Tunnel は、北オセチア自治共和国と南オセチア共
            和国の間を通る唯一の道路ですか?',
  'title': 'The Roki Tunnel',
  'text': ""The Roki Tunnel (also called Roksky Tunnel, ; Ossetic:
           Ручъы тъунел; ) is a mountain tunnel of the Transkam road
           through the Greater Caucasus Mountains, north of the village
           Upper Roka. It is the only road joining North Ossetia–Alania in
           the Russian Federation into South Ossetia, a breakaway
           republic of Georgia. The road is manned at the town of Nizhny
           Zaramag in North Ossetia and is sometimes referred to as the
           Roki-Nizhny Zaramag border crossing. The tunnel, completed
           by the Soviet government in 1984, is one of only a handful of
           routes that cross the North Caucasus Range.""
}
```

Example of Hindi (hn) datapoint from our monolingual dataset
on the topic of “Aryabhata” from the Hindi Wikipedia

```bash
{
  '_id': 'hindi_8987#4',
  'lang': 'Hindi',
  'code': 'hn',
  'query': 'आर्यभर्य ट केरल के कि स स्थान के नि वासी थे ?',
  'title': 'आर्यभर्य ट',
  'text': ""एक ताजा अध्ययन के अनसु ार आर्यभर्य ट, केरल के
          चाम्रवत्तम (१०उत्तर५१, ७५पर्वू ४र्व ५) के नि वासी थे। अध्ययन के अनसु ार
          अस्मका एक जनै प्रदेश था जो कि श्रवणबेलगोल के चारों तरफ फैला
          हुआ था और यहाँके पत्थर के खम्बों के कारण इसका नाम अस्मका
          पड़ा। चाम्रवत्तम इस जनै बस्ती का हि स्सा था, इसका प्रमाण है
          भारतापझु ा नदी जि सका नाम जनै ों के पौराणि क राजा भारता के नाम
          पर रखा गया है। आर्यभर्य ट ने भी यगु ों को परि भाषि त करते वक्त राजा
          भारता का जि क्र कि या है- दसगीति का के पांचवें छंद में राजा भारत
          के समय तक बीत चकुे काल का वर्णनर्ण आता है। उन दि नों में
          कुसमु परुा में एक प्रसि द्ध वि श्ववि द्यालय था जहाँजनै ों का नि र्णा यक
          प्रभाव था और आर्यभर्य ट का काम इस प्रकार कुसमु परुा पहुँच सका और
          उसे पसदं भी कि या गया।""
}
```

#### Atypical Data Point
<!-- width: half -->
<!-- info: Provide an example of an outlier data point and describe what makes
it atypical.

**Use additional notes to capture any other relevant information or
considerations.** -->
The dataset does not contain atypical data points as far as we know.

## Motivations & Intentions
### Motivations
#### Purpose(s)
<!-- scope: telescope -->
<!-- info: Select **one**: -->
- Research

#### Domain(s) of Application
<!-- scope: periscope -->
<!-- info: Provide a list of key domains of application that the dataset has
been designed for:<br><br>(Usage Note: Use comma-separated keywords.) -->
`Multilingual Dense Retrieval`, `Synthetic Dataset`

## Provenance
### Collection
#### Method(s) Used
<!-- scope: telescope -->
<!-- info: Select **all applicable** methods used to collect data: -->
- Artificially Generated
- Taken from other existing datasets

#### Methodology Detail(s)
<!-- scope: periscope -->
<!-- info: Provide a description of each collection method used.

Use additional notes to capture any other relevant information or
considerations.

(Usage Note: Duplicate and complete the following for collection method
type.) -->
**Collection Type**

**Source:** TyDI-QA dataset which provided the English Wikipedia dataset for SWIM cross-lingual IR dataset. MIRACL
provided the language-specific Wikipedia datasets for monolingual SWIM-IR datasets.

**Is this source considered sensitive or high-risk?** [Yes/**No**]

**Dates of Collection:** TyDI-QA [unknown - 01/02/2019], MIRACL [unknown - 01/02/2023], XTREME-UP [unknown - 01/02/2023]

**Primary modality of collection data:**
- Text Data

**Update Frequency for collected data:**
- Static

#### Source Description(s)
<!-- scope: microscope -->
<!-- info: Provide a description of each upstream source of data.

Use additional notes to capture any other relevant information or
considerations. -->
- **TyDI-QA:**  TyDi-QA [(Clark et al. 2020)](https://aclanthology.org/2020.tacl-1.30/) provided the English Wikipedia passages which have been split into 100-word long paragraphs. It contains around 18.2M passages from the complete English Wikipedia. We selected passages with a maximum of 1M pairs for each language pair (for 17 languages) at random for the preparation of our cross-lingual SWIM-IR dataset.
- **MIRACL:** MIRACL [(Zhang et al. 2023)](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00595/117438/MIRACL-A-Multilingual-Retrieval-Dataset-Covering) provides language-specific paragraphs from the Wikipedia Corpus. The paragraphs were generated by splitting on the “\n\n” delimiter. The MIRACL dataset provides corpora for 18 languages. We selected passages with a maximum of 1M pairs for each language at random for the preparation of our mono-lingual SWIM-IR dataset.
- **XTREME-UP:** XTREME-UP [(Ruder et al. 2023)](https://aclanthology.org/2023.findings-emnlp.125/) provides a 120K sample of the TyDi-QA (Clark et al. 2020) English Wikipedia passages which have been split into 100-word long paragraphs. This sample has been used in the original dataset for cross-language question answering.

#### Collection Cadence
<!-- scope: telescope -->
<!-- info: Select **all applicable**: -->
**Static:** Data was collected once from single or multiple sources.

#### Data Integration
<!-- scope: periscope -->
<!-- info: List all fields collected from different sources, and specify if
they were included or excluded from the dataset.

Use additional notes to
capture any other relevant information or considerations.

(Usage Note: Duplicate and complete the following for each upstream
source.) -->
**TyDi-QA (XOR-Retrieve and XTREME-UP)**

**Included Fields**
The English Wikipedia title, text, and `_id` fields were taken from the TyDi-QA dataset originally provided as a TSV file containing all fields.

**Excluded Fields**
The rest of the metadata apart from the fields mentioned above were excluded from our SWIM-IR dataset. We do not use any training data provided from the TyDI-QA dataset.

**MIRACL**

**Included Fields**
The Language Wikipedia title, text, and `_id` fields were taken from the MIRACL dataset, originally provided as a JSON-lines file containing all fields.

**Excluded Fields**
The rest of the metadata apart from the fields mentioned above were excluded from our SWIM-IR dataset. We do not use any training data provided from the MIRACL dataset.

#### Data Processing
<!-- scope: microscope -->
<!-- info: Summarize how data from different sources or methods aggregated,
processed, or connected.

Use additional notes to capture any other
relevant information or considerations.

(Usage Note: Duplicate and complete the following for each source OR
collection method.) -->
All data is coming directly from the TyDI-QA and MIRACL datasets without any preprocessing.

### Collection Criteria
#### Data Selection
<!-- scope: telescope -->
<!-- info: Summarize the data selection criteria.

Use additional notes to capture any other relevant information or
considerations. -->
For the Cross-lingual SWIM-IR dataset, we use a stratified sampling technique to select a subset of passages from the English Wikipedia corpus. We use it to generate questions for SWIM-IR. We ensure all languages have relatively an equal amount of training samples, wherever possible. Our Wikipedia corpus contains entities that are sorted alphabetically (A-Z). We then compute inclusion threshold $I_{th}$, which is defined as $I_{th} = D_{sample} / D_{total}$, where $(D_{sample})$ is number of passages required to sample and $(D_{total})$ is the total numbers of passages in corpus. Next, for each passage ($p_i$) in the corpus, we randomly generate an inclusion probability $\hat{p_i} \in [0,1]$. We select the passage ($p_i$) if $p_i \leq I_{th}$. This ensures uniform sampling of passages with Wikipedia entities between all letters (A-Z).

For the Monolingual SWIM-IR dataset, the language selection criteria were dependent on the Wikipedia corpora availability for the monolingual task. Hence, we chose to fix on the 18 languages provided in MIRACL. To complete the dataset, we included the same languages for the cross-lingual task.

#### Data Inclusion
<!-- scope: periscope -->
<!-- info: Summarize the data inclusion criteria.

Use additional notes to capture any other relevant information or
considerations. -->
We include all data available in TyDi-QA English Wikipedia Corpus (maximum of 1M training pairs per language pair), which we use to generate our cross-lingual SWIM-IR dataset. We use the language-specific MIRACL Wikipedia corpora to generate our monolingual queries in SWIM-IR.

#### Data Exclusion
<!-- scope: microscope -->
<!-- info: Summarize the data exclusion criteria.

Use additional notes to capture any other relevant information or
considerations. -->
We removed data classified as containing sensitive subjects and adult content using the method described in our paper. No additional filters were applied for data exclusion from MIRACL or TyDi-QA. 

The TyDi-QA English paragraph data has been split with a maximum of up to 100 tokens. However, MIRACL used the “\n\n” delimiter to segment paragraphs from the Wikipedia articles.",High,5.0
Q&A,nthakur/swim-ir-monolingual,8.0,462.0,2024-04-28 05:12:53+00:00,cc-by-sa-4.0,7.0,1.39 GB,1492501135.36,1.39 GB,1492501135.36,3167012,https://arxiv.org/abs/2311.05800,"['https://aclanthology.org/2020.tacl-1.30/)', 'https://aclanthology.org/2023.findings-emnlp.125/)']","---
dataset_info:
- config_name: ar
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 282655146
    num_examples: 277651
  download_size: 139098959
  dataset_size: 282655146
- config_name: bn
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 177251018
    num_examples: 106816
  download_size: 67504280
  dataset_size: 177251018
- config_name: de
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 312380289
    num_examples: 446918
  download_size: 201707387
  dataset_size: 312380289
- config_name: en
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 329308312
    num_examples: 501538
  download_size: 207842032
  dataset_size: 329308312
- config_name: es
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 326061510
    num_examples: 492536
  download_size: 202887255
  dataset_size: 326061510
- config_name: fi
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 242533670
    num_examples: 353725
  download_size: 149062446
  dataset_size: 242533670
- config_name: fr
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 279159631
    num_examples: 447745
  download_size: 172417745
  dataset_size: 279159631
- config_name: hi
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 364200217
    num_examples: 226226
  download_size: 136339811
  dataset_size: 364200217
- config_name: id
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 210466267
    num_examples: 309065
  download_size: 114883567
  dataset_size: 210466267
- config_name: yo
  features:
  - name: _id
    dtype: string
  - name: lang
    dtype: string
  - name: code
    dtype: string
  - name: query
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 3482819
    num_examples: 4792
  download_size: 1951817
  dataset_size: 3482819
configs:
- config_name: ar
  data_files:
  - split: train
    path: ar/train-*
- config_name: bn
  data_files:
  - split: train
    path: bn/train-*
- config_name: de
  data_files:
  - split: train
    path: de/train-*
- config_name: en
  data_files:
  - split: train
    path: en/train-*
- config_name: es
  data_files:
  - split: train
    path: es/train-*
- config_name: fi
  data_files:
  - split: train
    path: fi/train-*
- config_name: fr
  data_files:
  - split: train
    path: fr/train-*
- config_name: hi
  data_files:
  - split: train
    path: hi/train-*
- config_name: id
  data_files:
  - split: train
    path: id/train-*
- config_name: yo
  data_files:
  - split: train
    path: yo/train-*
license: cc-by-sa-4.0
task_categories:
- text-retrieval
- question-answering
language:
- ar
- bn
- de
- en
- es
- fi
- fr
- hi
- id
- yo
pretty_name: SWIM-IR (Monolingual)
size_categories:
- 1M<n<10M
language_creators:
- machine-generated
multilinguality:
- monolingual
source_datasets:
- original
---
# Dataset Card for SWIM-IR (Monolingual)

![SWIM-IR Logo](./swimir_header.png)

<!-- Provide a quick summary of the dataset. -->

This is the monolingual subset of the SWIM-IR dataset, where the query generated and the passage are both in the same language.
A few remaining languages will be added in the upcoming v2 version of SWIM-IR. The dataset is available as CC-BY-SA 4.0.

For full details of the dataset, please read our upcoming [NAACL 2024 paper](https://arxiv.org/abs/2311.05800) and check out our [website](https://github.com/google-research-datasets/swim-ir).

# What is SWIM-IR?

SWIM-IR dataset is a synthetic multilingual retrieval dataset spanning around 29 million retrieval training pairs across 27 languages. 
Each question has been automatically generated with the Summarize-then-Ask (STA) prompting technique using PaLM-2 as the question generator.

**Note**: As the question is synthetically generated, there is scope for hallucinations during query generation. The hallucinated queries do not affect retrieval effectiveness.

If you are using SWIM-IR in your research, please cite the following paper:

```
@article{thakur:2023,
  author       = {Nandan Thakur and
                  Jianmo Ni and
                  Gustavo Hern{\'{a}}ndez {\'{A}}brego and
                  John Wieting and
                  Jimmy Lin and
                  Daniel Cer},
  title        = {Leveraging LLMs for Synthesizing Training Data Across Many Languages
                  in Multilingual Dense Retrieval},
  journal      = {CoRR},
  volume       = {abs/2311.05800},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.05800},
  doi          = {10.48550/ARXIV.2311.05800},
  eprinttype    = {arXiv},
  eprint       = {2311.05800},
  timestamp    = {Tue, 14 Nov 2023 14:47:55 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-05800.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
```

## Dataset Details

### Dataset Description

- **Homepage:** [SWIM-IR homepage](https://github.com/google-research-datasets/swim-ir)
- **Repository:** [SWIM-IR repository](https://github.com/google-research-datasets/swim-ir)
- **Paper:** [Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval
](https://arxiv.org/abs/2311.05800)
- **Leaderboard:** [Needs More Information]
- **Point of Contact:** [Nandan Thakur](mailto:nandan.thakur@uwaterloo.ca)

#### Dataset Link
<!-- info: Provide a link to the dataset: -->
<!-- width: half -->
SWIM-IR v1.0: http://storage.googleapis.com/gresearch/swim-ir/swim_ir_v1.tar.gz

#### Data Card Author(s)
<!-- info: Select **one role per** Data Card Author:

(Usage Note: Select the most appropriate choice to describe the author's role
in creating the Data Card.) -->
<!-- width: half -->
- **Nandan Thakur, University of Waterloo:** Owner
- **Daniel Cer, Google Research:** Owner
- **Jianmo Ni, Google DeepMind:** Contributor
- **John Wieting, Google DeepMind:** Contributor
- **Gustavo Hernandez Abrego, Google Research:** Contributor
- **Jimmy Lin, University of Waterloo:** Contributor

## Authorship
### Publishers
#### Publishing Organization(s)
<!-- scope: telescope -->
<!-- info: Provide the names of the institution or organization responsible
for publishing the dataset: -->
University of Waterloo, Google Research, Google DeepMind

#### Industry Type(s)
<!-- scope: periscope -->
<!-- info: Select **all applicable** industry types to which the publishing
organizations belong: -->
- Corporate - Tech
- Academic - Tech

### Dataset Owners
#### Team(s)
<!-- scope: telescope -->
<!-- info: Provide the names of the groups or team(s) that own the dataset: -->
SWIM-IR Team

#### Contact Detail(s)
<!-- scope: periscope -->
<!-- info: Provide pathways to contact dataset owners: -->
- **Dataset Owner(s):** Nandan Thakur, Daniel Cer
- **Affiliation:** University of Waterloo, Google Research
- **Contact:** [nandan.thakur@uwaterloo.ca](mailto:nandan.thakur@uwaterloo.ca)

## Dataset Overview
#### Data Subject(s)
<!-- scope: telescope -->
<!-- info: Select ***all applicable**** subjects contained the dataset: -->
- Synthetically generated data

#### Dataset Snapshot
<!-- scope: periscope -->
<!-- info: Provide a snapshot of the dataset:<br><br>(Use the additional notes
to include relevant information, considerations, and links to table(s) with
more detailed breakdowns.) -->
SWIM-IR is a synthetic multilingual retrieval training dataset. 
It contains training pairs for both settings: monolingual, i.e. within the same language, and cross-lingual, i.e. across language. 
The dataset is useful to fine-tune state-of-the-art (SoTA) synthetic monolingual and cross-lingual neural retrievers across diverse languages.


Category | Data
--- | ---
Size of Dataset | ~6-7 GB
Number of Instances | 28,265,848
Number of Fields | 6
Labeled Classes | 33*
Number of Labels | 1

**Above:** Dataset statistics comprises both in-language and cross-language settings. The classes above denote a language.

**Additional Notes:** (*) Classes denote the languages we cover in the SWIM-IR dataset. Here is a list of the 18 languages and their ISO codes listed in alphabetical order: 
Arabic (ar), Bengali (bn), German (de), English (en), Spanish (es), Persian (fa), Finnish (fi), French (fr), Hindi (hi), Indonesian (id), Japanese (ja), Korean (ko), Russian (ru), Swahili (sw), Thai (th), Yoruba (yo),
Chinese (zh) and rest 15 Indo-European Languages: Assamese (as), Bhojpuri (bho), Konkani (gom), Gujarati (gu), Kannada (kn), Maithili (mai), Malayalam (ml), Manipuri (mni), Marathi (mr), Odia (or), Punjabi (pa), Pashto (ps), Sanskrit (sa), Tamil (ta), Urdu (ur).

#### Content Description
<!-- scope: microscope -->
<!-- info: Provide a short description of the content in a data point: -->
A paragraph is sampled from the Wikipedia corpus which describes an entity. The question arising from the Wikipedia
paragraph is generated using a large language model (LLM). In our work, we used the PaLM 2-S (small) model to generate
synthetic queries across **33 languages**, covering 11 distinct scripts, and 10 language families comprising over 3 billion speakers in the world.

The SWIM-IR dataset contains about **28 million** Wikipedia synthetic query-paragraph training pairs with a multilingual query for each passage generated using PaLM 2 (small), 
for both cross-lingual and monolingual retrieval settings.

**Additional Notes:**
- The dataset creation follows a specific procedure that involves a `summarize-then-ask` prompting technique inspired by chain-of-thought prompting.
- PaLM 2 uses **summarize-then-ask promping** containing 5-shot exemplars for cross-lingual and 3-shot exemplars for monolingual query generation.
- The prompt includes the original paragraph, a human-generated summary, and a question translated from English using Machine Translation (MT) for cross-lingual generation,
- whereas for randomly sampled training dataset pairs, and summaries generated using Google BARD for monolingual generation.
- PaLM 2 generates an extractive summary which is used as a proxy to help understand the document and highlight relevant sections within the document.
- Finally, the model generates a question in the target language (different in cross-lingual or same in monolingual) which can be answered using the input paragraph.

### Sensitivity of Data
#### Sensitivity Type(s)
<!-- scope: telescope -->
<!-- info: Select ***all applicable*** data types present in the dataset: -->
- None

#### Field(s) with Sensitive Data
<!-- scope: periscope -->
<!-- info: List fields in the dataset that contain S/PII, and specify if their
collection was intentional or unintentional.

Use additional notes to capture any other relevant information or
considerations. -->
**Intentional Collected Sensitive Data**
No sensitive data was intentionally collected.

**Unintentionally Collected Sensitive Data**
S/PII, violent, abusive, or toxic text containing racial slurs were not explicitly collected as a part of the dataset creation
process. Sensitive subject and adult content was automatically filtered using the method described in (Thakur et al. 2023).

#### Security and Privacy Handling
<!-- scope: microscope -->
<!-- info: Summarize the measures or steps to handle sensitive data in this
dataset.

Use additional notes to capture any other relevant information or
considerations. -->

We used algorithmic methods and relied on other classifiers for data filtration. Specifically, we (1) did a human inspection of text samples, with the questions automatically translated to English; (2) our observations motivated using a classifier to filter text containing sensitive subjects and adult content.

## Example of Data Points
#### Primary Data Modality
<!-- scope: telescope -->
<!-- info: Select **one**: -->
- Text Data

#### Data Fields
<!-- scope: microscope -->
<!-- info: List the fields in data points and their descriptions.

(Usage Note: Describe each field in a data point. Optionally use this to show
the example.) -->

| Field name | Datapoint Example | Description |
| --------- | -------- | -------- |
| `lang` | String | The language of the generated question |
| `code` | String | The ISO-Code for the language |
| `query` | String | The generated query using PaLM 2 |
| `_id` | String | unique ID denoting the training pair |
| `title` | String | Title of the Wikipedia article |
| `text` | String | Paragraph of the Wikipedia article 

#### Typical Data Point
<!-- width: half -->
<!-- info: Provide an example of a typical data point and describe what makes
it typical.

**Use additional notes to capture any other relevant information or
considerations.** -->
Example of (English -> Japanese) datapoint from our
cross-lingual dataset on the topic of “The Roki Tunnel” from the
English Wikipedia.

```bash
{
  '_id': '1234',
  'lang': 'Japanese',
  'code': 'ja',
  'query': 'The Roki Tunnel は、北オセチア自治共和国と南オセチア共
            和国の間を通る唯一の道路ですか?',
  'title': 'The Roki Tunnel',
  'text': ""The Roki Tunnel (also called Roksky Tunnel, ; Ossetic:
           Ручъы тъунел; ) is a mountain tunnel of the Transkam road
           through the Greater Caucasus Mountains, north of the village
           Upper Roka. It is the only road joining North Ossetia–Alania in
           the Russian Federation into South Ossetia, a breakaway
           republic of Georgia. The road is manned at the town of Nizhny
           Zaramag in North Ossetia and is sometimes referred to as the
           Roki-Nizhny Zaramag border crossing. The tunnel, completed
           by the Soviet government in 1984, is one of only a handful of
           routes that cross the North Caucasus Range.""
}
```

Example of Hindi (hn) datapoint from our monolingual dataset
on the topic of “Aryabhata” from the Hindi Wikipedia

```bash
{
  '_id': 'hindi_8987#4',
  'lang': 'Hindi',
  'code': 'hn',
  'query': 'आर्यभर्य ट केरल के कि स स्थान के नि वासी थे ?',
  'title': 'आर्यभर्य ट',
  'text': ""एक ताजा अध्ययन के अनसु ार आर्यभर्य ट, केरल के
          चाम्रवत्तम (१०उत्तर५१, ७५पर्वू ४र्व ५) के नि वासी थे। अध्ययन के अनसु ार
          अस्मका एक जनै प्रदेश था जो कि श्रवणबेलगोल के चारों तरफ फैला
          हुआ था और यहाँके पत्थर के खम्बों के कारण इसका नाम अस्मका
          पड़ा। चाम्रवत्तम इस जनै बस्ती का हि स्सा था, इसका प्रमाण है
          भारतापझु ा नदी जि सका नाम जनै ों के पौराणि क राजा भारता के नाम
          पर रखा गया है। आर्यभर्य ट ने भी यगु ों को परि भाषि त करते वक्त राजा
          भारता का जि क्र कि या है- दसगीति का के पांचवें छंद में राजा भारत
          के समय तक बीत चकुे काल का वर्णनर्ण आता है। उन दि नों में
          कुसमु परुा में एक प्रसि द्ध वि श्ववि द्यालय था जहाँजनै ों का नि र्णा यक
          प्रभाव था और आर्यभर्य ट का काम इस प्रकार कुसमु परुा पहुँच सका और
          उसे पसदं भी कि या गया।""
}
```

#### Atypical Data Point
<!-- width: half -->
<!-- info: Provide an example of an outlier data point and describe what makes
it atypical.

**Use additional notes to capture any other relevant information or
considerations.** -->
The dataset does not contain atypical data points as far as we know.

## Motivations & Intentions
### Motivations
#### Purpose(s)
<!-- scope: telescope -->
<!-- info: Select **one**: -->
- Research

#### Domain(s) of Application
<!-- scope: periscope -->
<!-- info: Provide a list of key domains of application that the dataset has
been designed for:<br><br>(Usage Note: Use comma-separated keywords.) -->
`Multilingual Dense Retrieval`, `Synthetic Dataset`

## Provenance
### Collection
#### Method(s) Used
<!-- scope: telescope -->
<!-- info: Select **all applicable** methods used to collect data: -->
- Artificially Generated
- Taken from other existing datasets

#### Methodology Detail(s)
<!-- scope: periscope -->
<!-- info: Provide a description of each collection method used.

Use additional notes to capture any other relevant information or
considerations.

(Usage Note: Duplicate and complete the following for collection method
type.) -->
**Collection Type**

**Source:** TyDI-QA dataset which provided the English Wikipedia dataset for SWIM cross-lingual IR dataset. MIRACL
provided the language-specific Wikipedia datasets for monolingual SWIM-IR datasets.

**Is this source considered sensitive or high-risk?** [Yes/**No**]

**Dates of Collection:** TyDI-QA [unknown - 01/02/2019], MIRACL [unknown - 01/02/2023], XTREME-UP [unknown - 01/02/2023]

**Primary modality of collection data:**
- Text Data

**Update Frequency for collected data:**
- Static

#### Source Description(s)
<!-- scope: microscope -->
<!-- info: Provide a description of each upstream source of data.

Use additional notes to capture any other relevant information or
considerations. -->
- **TyDI-QA:**  TyDi-QA [(Clark et al. 2020)](https://aclanthology.org/2020.tacl-1.30/) provided the English Wikipedia passages which have been split into 100-word long paragraphs. It contains around 18.2M passages from the complete English Wikipedia. We selected passages with a maximum of 1M pairs for each language pair (for 17 languages) at random for the preparation of our cross-lingual SWIM-IR dataset.
- **MIRACL:** MIRACL [(Zhang et al. 2023)](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00595/117438/MIRACL-A-Multilingual-Retrieval-Dataset-Covering) provides language-specific paragraphs from the Wikipedia Corpus. The paragraphs were generated by splitting on the “\n\n” delimiter. The MIRACL dataset provides corpora for 18 languages. We selected passages with a maximum of 1M pairs for each language at random for the preparation of our mono-lingual SWIM-IR dataset.
- **XTREME-UP:** XTREME-UP [(Ruder et al. 2023)](https://aclanthology.org/2023.findings-emnlp.125/) provides a 120K sample of the TyDi-QA (Clark et al. 2020) English Wikipedia passages which have been split into 100-word long paragraphs. This sample has been used in the original dataset for cross-language question answering.

#### Collection Cadence
<!-- scope: telescope -->
<!-- info: Select **all applicable**: -->
**Static:** Data was collected once from single or multiple sources.

#### Data Integration
<!-- scope: periscope -->
<!-- info: List all fields collected from different sources, and specify if
they were included or excluded from the dataset.

Use additional notes to
capture any other relevant information or considerations.

(Usage Note: Duplicate and complete the following for each upstream
source.) -->
**TyDi-QA (XOR-Retrieve and XTREME-UP)**

**Included Fields**
The English Wikipedia title, text, and `_id` fields were taken from the TyDi-QA dataset originally provided as a TSV file containing all fields.

**Excluded Fields**
The rest of the metadata apart from the fields mentioned above were excluded from our SWIM-IR dataset. We do not use any training data provided from the TyDI-QA dataset.

**MIRACL**

**Included Fields**
The Language Wikipedia title, text, and `_id` fields were taken from the MIRACL dataset, originally provided as a JSON-lines file containing all fields.

**Excluded Fields**
The rest of the metadata apart from the fields mentioned above were excluded from our SWIM-IR dataset. We do not use any training data provided from the MIRACL dataset.

#### Data Processing
<!-- scope: microscope -->
<!-- info: Summarize how data from different sources or methods aggregated,
processed, or connected.

Use additional notes to capture any other
relevant information or considerations.

(Usage Note: Duplicate and complete the following for each source OR
collection method.) -->
All data is coming directly from the TyDI-QA and MIRACL datasets without any preprocessing.

### Collection Criteria
#### Data Selection
<!-- scope: telescope -->
<!-- info: Summarize the data selection criteria.

Use additional notes to capture any other relevant information or
considerations. -->
For the Cross-lingual SWIM-IR dataset, we use a stratified sampling technique to select a subset of passages from the English Wikipedia corpus. We use it to generate questions for SWIM-IR. We ensure all languages have relatively an equal amount of training samples, wherever possible. Our Wikipedia corpus contains entities that are sorted alphabetically (A-Z). We then compute inclusion threshold $I_{th}$, which is defined as $I_{th} = D_{sample} / D_{total}$, where $(D_{sample})$ is number of passages required to sample and $(D_{total})$ is the total numbers of passages in corpus. Next, for each passage ($p_i$) in the corpus, we randomly generate an inclusion probability $\hat{p_i} \in [0,1]$. We select the passage ($p_i$) if $p_i \leq I_{th}$. This ensures uniform sampling of passages with Wikipedia entities between all letters (A-Z).

For the Monolingual SWIM-IR dataset, the language selection criteria were dependent on the Wikipedia corpora availability for the monolingual task. Hence, we chose to fix on the 18 languages provided in MIRACL. To complete the dataset, we included the same languages for the cross-lingual task.

#### Data Inclusion
<!-- scope: periscope -->
<!-- info: Summarize the data inclusion criteria.

Use additional notes to capture any other relevant information or
considerations. -->
We include all data available in TyDi-QA English Wikipedia Corpus (maximum of 1M training pairs per language pair), which we use to generate our cross-lingual SWIM-IR dataset. We use the language-specific MIRACL Wikipedia corpora to generate our monolingual queries in SWIM-IR.

#### Data Exclusion
<!-- scope: microscope -->
<!-- info: Summarize the data exclusion criteria.

Use additional notes to capture any other relevant information or
considerations. -->
We removed data classified as containing sensitive subjects and adult content using the method described in our paper. No additional filters were applied for data exclusion from MIRACL or TyDi-QA. 

The TyDi-QA English paragraph data has been split with a maximum of up to 100 tokens. However, MIRACL used the “\n\n” delimiter to segment paragraphs from the Wikipedia articles.",High,5.0
Q&A,asas-ai/mlqa_parallel_ar_eg,0.0,30.0,2024-05-08 00:52:35+00:00,cc-by-sa-3.0,0.0,7.64 MB,8011120.64,7.64 MB,8011120.64,5852,none,none,"---
configs:
- config_name: default
  data_files:
  - split: test
    path: data/test-*
  - split: validation
    path: data/validation-*
dataset_info:
  features:
  - name: context_ar
    dtype: string
  - name: context_en
    dtype: string
  - name: question_ar
    dtype: string
  - name: question_en
    dtype: string
  - name: answers_ar
    dtype: string
  - name: answers_en
    dtype: string
  - name: id
    dtype: string
  splits:
  - name: test
    num_bytes: 15175785
    num_examples: 5335
  - name: validation
    num_bytes: 1460295
    num_examples: 517
  download_size: 7641165
  dataset_size: 16636080
task_categories:
- question-answering
language:
- ar
- en
pretty_name: ' MLQA (MultiLingual Question Answering)'
size_categories:
- 1K<n<10K
license:
  - cc-by-sa-3.0
tags:
- Machine Translation
---
# Dataset Card for ""mlqa_parallel_ar_eg""

[More Information needed](https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)",Medium,2.0
Q&A,berwart/SE-Chatting.en,2.0,135.0,2024-10-03 21:10:29+00:00,mit,0.0,1.01 GB,1084479242.24,129 MB,135266304.0,10000140,none,none,"---
license: mit
task_categories:
- question-answering
- translation
language:
- en
- fr
- ja
- zh
- ru
- ar
tags:
- chemistry
- biology
- finance
- legal
- music
- art
- code
- climate
- medical
pretty_name: SE.02
size_categories:
- 10M<n<100M
---

# SE.02 
***Dataset***

Hello, welcome to the official main dataset of SE.02 that's always getting updated, make sure to like to help us a lot.
this dataset contains pretty much everything from math to isk what to put but pretty much anything you think an ai can say.
anyways this is our biggest dataset yet, and out first one that I don't even know how I managed to make this.

you can use it to train your own ai if you want.
",Low,1.0
Q&A,xhluca/publichealth-qa,1.0,284.0,2024-05-17 03:47:43+00:00,cc-by-nc-sa-3.0,0.0,1.06 MB,1111490.56,561 KB,574464.0,888,none,none,"---
license: cc-by-nc-sa-3.0
task_categories:
- question-answering
language:
- ar
- en
- es
- fr
- ko
- ru
- vi
- zh
size_categories:
- n<1K
# https://huggingface.co/docs/hub/en/datasets-manual-configuration
configs:
- config_name: english
  default: true
  data_files:
  - split: test
    path: data/english.csv
- config_name: arabic
  data_files:
  - split: test
    path: data/arabic.csv
- config_name: chinese
  data_files:
  - split: test
    path: data/chinese.csv
- config_name: french
  data_files:
  - split: test
    path: data/french.csv
- config_name: korean
  data_files:
  - split: test
    path: data/korean.csv
- config_name: korean
  data_files:
  - split: test
    path: data/korean.csv
- config_name: russian
  data_files:
  - split: test
    path: data/russian.csv
- config_name: spanish
  data_files:
  - split: test
    path: data/spanish.csv
- config_name: vietnamese
  data_files:
  - split: test
    path: data/vietnamese.csv
---

# Usage

```python
import datasets

langs = ['arabic', 'chinese', 'english', 'french', 'korean', 'russian', 'spanish', 'vietnamese']
data = datasets.load_dataset('xhluca/publichealth-qa', split='test', name=langs[0])
```

# About

This dataset contains question and answer pairs sourced from Q&A pages and FAQs from CDC and WHO pertaining to COVID-19. They were produced and collected between 2019-12 and 2020-04. They were originally published as an [aggregated Kaggle dataset](https://www.kaggle.com/xhlulu/covidqa).

# License

CDC data is licensed under [CC-BY 3.0](https://web.archive.org/web/20201017141031/https://www2a.cdc.gov/cdcup/library/other/policy.htm) and WHO is licensed under [cc-by-nc-sa-3.0](https://web.archive.org/web/20210701063743/https://www.who.int/about/policies/publishing/copyright).


# Source

This data was originally included in the [COVID-QA dataset](https://www.kaggle.com/datasets/xhlulu/covidqa), where it was known as the multilingual split. The files in this updated repository were generated using the [publichealth-qa repository](https://github.com/xhluca/publichealth-qa).",Medium,2.0
Q&A,BoodyAhmedHamdy/EgyLaw-Squad,0.0,23.0,2024-06-01 09:44:49+00:00,mit,1.0,308 KB,315392.0,308 KB,315392.0,887,none,none,"---
language:
- ar
license: mit
size_categories:
- n<1K
task_categories:
- question-answering
pretty_name: Egyption Law Squad-like dataset
tags:
- legal
- egypt
- egyption-law
- squad
dataset_info:
  features:
  - name: question
    dtype: string
  - name: answers
    struct:
    - name: answer_start
      sequence: int64
    - name: text
      sequence: string
  - name: context
    dtype: string
  - name: id
    dtype: int64
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 1730837
    num_examples: 709
  - name: validation
    num_bytes: 438832
    num_examples: 178
  download_size: 307647
  dataset_size: 2169669
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: validation
    path: data/validation-*
---


# Egyption Law Squad 
this dataset was made for Question Answering task about Egyption Law specially **Personal Status Law** (قانون الاحوال الشخصية )

## About the Dataset
dataset was created for a Graduation Project in Computers and Artificial intellgence at Helwan University under supervisation from [Dr.Ensaf Hossen](https://scholar.google.com/citations?user=eeYSe3sAAAAJ&hl=en)

## About Team
- [Abdelrahman Ahmed Hamdy](https://github.com/Boodyahmedhamdy)
- [Shehab Gamal-elden](https://github.com/000Shehab000)
- [Mohsen Hisham Mohamed](https://github.com/Mohsen-Hesham)
- [Maya Ahmed Abdelsatar](https://github.com/mayaahmed2002)
- [Nancy Ahmed Mostafa](https://github.com/Nancyahmedmustafa)
- [Nour Khaled Ali](https://github.com/NourSewafy)


",Medium,3.0
Q&A,AkshitaS/facebook_mlqa_plus,0.0,38.0,2024-06-13 22:09:56+00:00,cc-by-sa-3.0,0.0,31.9 MB,33449574.4,31.9 MB,33449574.4,51869,none,none,"---
license: cc-by-sa-3.0
configs:
- config_name: ara_Arab
  data_files:
  - split: validation
    path: data/ara_Arab/validation*
  - split: test
    path: data/ara_Arab/test*
- config_name: deu_Latn
  data_files:
  - split: validation
    path: data/deu_Latn/validation*
  - split: test
    path: data/deu_Latn/test*
- config_name: eng_Latn
  data_files:
  - split: validation
    path: data/eng_Latn/validation*
  - split: test
    path: data/eng_Latn/test*
- config_name: hin_Deva
  data_files:
  - split: validation
    path: data/hin_Deva/validation*
  - split: test
    path: data/hin_Deva/test*
- config_name: hin_Latn
  data_files:
  - split: validation
    path: data/hin_Latn/validation*
  - split: test
    path: data/hin_Latn/test*
- config_name: spa_Latn
  data_files:
  - split: validation
    path: data/spa_Latn/validation*
  - split: test
    path: data/spa_Latn/test*
- config_name: vie_Latn
  data_files:
  - split: validation
    path: data/vie_Latn/validation*
  - split: test
    path: data/vie_Latn/test*
- config_name: zho_Hans
  data_files:
  - split: validation
    path: data/zho_Hans/validation*
  - split: test
    path: data/zho_Hans/test*
task_categories:
- question-answering
language:
- en
- hi
- ar
- de
- es
- vi
- zh
---

**Source Dataset**    
- Link: [facebook/mlqa](https://huggingface.co/datasets/facebook/mlqa)
- Revision: `397ed406c1a7902140303e7faf60fff35b58d285`

**MLQA**   
MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.
MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,
German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between
4 different languages on average.

**MLQA Plus**  
MLQA Plus additionally has hin_Latn data generated using indictrans library.
",Low,1.0
Q&A,AkshitaS/google_xquad_plus,0.0,32.0,2024-06-13 22:08:26+00:00,cc-by-nc-sa-4.0,0.0,3.39 MB,3554672.64,3.39 MB,3554672.64,15470,none,none,"---
license: cc-by-nc-sa-4.0
task_categories:
- question-answering
language:
- en
- hi
- ar
- de
- el
- es
- ro
- ru
- th
- tr
- vi
- zh
configs:
  - config_name: ara_Arab
    data_files:
      - split: validation
        path: data/ara_Arab/validation*
  - config_name: deu_Latn
    data_files:
      - split: validation
        path: data/deu_Latn/validation*
  - config_name: ell_Grek
    data_files:
      - split: validation
        path: data/ell_Grek/validation*
  - config_name: eng_Latn
    data_files:
      - split: validation
        path: data/eng_Latn/validation*
  - config_name: hin_Deva
    data_files:
      - split: validation
        path: data/hin_Deva/validation*
  - config_name: hin_Latn
    data_files:
      - split: validation
        path: data/hin_Latn/validation*
  - config_name: ron_Latn
    data_files:
      - split: validation
        path: data/ron_Latn/validation*
  - config_name: rus_Cyrl
    data_files:
      - split: validation
        path: data/rus_Cyrl/validation*
  - config_name: spa_Latn
    data_files:
      - split: validation
        path: data/spa_Latn/validation*
  - config_name: tha_Thai
    data_files:
      - split: validation
        path: data/tha_Thai/validation*
  - config_name: tur_Latn
    data_files:
      - split: validation
        path: data/tur_Latn/validation*
  - config_name: vie_Latn
    data_files:
      - split: validation
        path: data/vie_Latn/validation*
  - config_name: zho_Hans
    data_files:
      - split: validation
        path: data/zho_Hans/validation*
---

**Source dataset**:   
- Link: [google/xquad](https://huggingface.co/datasets/google/xquad)  
- Revision: `51adfef1c1287aab1d2d91b5bead9bcfb9c68583`

**XQuAD**:  
XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering performance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) together with their professional translations into ten languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. Consequently, the dataset is entirely parallel across 11 languages.

**XQuAD Plus**   
XQuAD Plus additionally has hin_Latn data generated using indictrans library.",Low,1.0
Q&A,mohamedemam/Essay-quetions-auto-grading-arabic,0.0,13.0,2024-06-27 23:43:40+00:00,gpl,0.0,190 MB,199229440.0,190 MB,199229440.0,96647,none,none,"---
license: gpl
dataset_info:
  features:
  - name: 'Unnamed: 0'
    dtype: int64
  - name: system_prompt
    dtype: string
  - name: question
    dtype: string
  - name: response
    dtype: string
  - name: choice2
    dtype: int64
  - name: label
    dtype: string
  - name: answer
    dtype: string
  - name: lang
    dtype: string
  - name: start
    dtype: string
  splits:
  - name: train
    num_bytes: 382761987
    num_examples: 96647
  download_size: 190042900
  dataset_size: 382761987
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- text-classification
- question-answering
- text-generation
- text2text-generation
language:
- ar
tags:
- autograding
- essay quetion
pretty_name: autograding-ar
---
Dataset Overview

The Open Orca Enhanced Dataset is meticulously designed to improve the performance of automated essay grading models using deep learning techniques. This dataset integrates robust data instances from the FLAN collection, augmented with responses generated by GPT-3.5 or GPT-4, creating a diverse and context-rich resource for training models.
Dataset Structure

The dataset is structured in a tabular format, with the following key fields:

id: A unique identifier for each data instance.
system prompt: The prompt presented to the GPT-3.5 or GPT-4 API.
question: The question entry as provided by the FLAN collection.
response: The response received from GPT-3.5 or GPT-4.
label: The classification of the response as ""True"" (ideal response) or ""False"" (generated as a close, yet incorrect, alternative).

Data Collection and Processing

Initial Dataset Selection: We initially chose the QuAC dataset due to its resemblance to student essay responses. However, we identified limitations and transitioned to the Open Orca dataset for its superior structure and data quality.
Format Conversion: We converted the QuAC context-question-answer format by identifying ""True"" answers as ground truth and generating ""False"" answers by selecting random responses. This approach was initially tested using the flan T5 model, which only achieved 40% accuracy.
RAG Implementation: To enhance the differentiation between ""True"" and ""False"" answers, we employed Retrieval Augmented Generation (RAG) to select the third most similar answer as the ""False"" response, significantly improving model accuracy to 88%.

Data Augmentation

Instructional Prompts: The dataset includes instructional prompts that enable the training of ChatGPT-like models, contributing to notable accuracy improvements.
Contextual Relevance: A multi-stage filtering process ensured the retention of contextually rich prompts, with over 1,000 initial prompts filtered down to align with 2.1 million samples.
Labeling: The final dataset includes labels that not only classify answers as ""True"" or ""False"" but also provide the ground truth answer, enhancing the model's understanding of context and logical response generation.

Evaluation and Performance

Accuracy Metrics: The refined dataset achieved remarkable performance:
    English LLM: 97% accuracy.
    Arabic LLM: 90% accuracy.
Model Comparison: Incorporating the ground truth answer into the label improved model accuracy significantly, as evidenced by the comparison:
    Flan T5: Improved from 20% to 83%.
    Bloomz: Improved from 40% to 85%.

Translation for Multilingual Models

Arabic Dataset Creation: Leveraging Google Translate's advancements, we translated the robust English dataset into Arabic, ensuring the creation of a truly multilingual resource. Google Translate's high accuracy (82.5%) provided a solid foundation for this translation.",Medium,3.0
Q&A,ClusterlabAi/InstAr-500k,12.0,133.0,2024-07-30 16:41:57+00:00,apache-2.0,3.0,375 MB,393216000.0,375 MB,393216000.0,481281,none,none,"---
language:
- ar
license: apache-2.0
size_categories:
- 100K<n<1M
task_categories:
- question-answering
- summarization
- text-classification
dataset_info:
  features:
  - name: uuid
    dtype: string
  - name: source
    dtype: string
  - name: task
    dtype: string
  - name: type
    dtype: string
  - name: topic
    dtype: string
  - name: system
    dtype: string
  - name: instruction
    dtype: string
  - name: output
    dtype: string
  splits:
  - name: train
    num_bytes: 1093175218
    num_examples: 481281
  download_size: 374662094
  dataset_size: 1093175218
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
---
# Dataset Card for ""InstAr-500k""

The dataset comprises almost 500,000 Arabic instructions and responses designed for fine-tuning large language models (LLMs) for Arabic NLP tasks. It includes a combination of synthetic and human-crafted data across various domains and instruction types. This extensive dataset aims to improve the performance of LLMs on Arabic-specific tasks

## Dataset Summary 

<table>
  <tr>
    <th>Type</th>
    <th>Task</th>
    <th>Number of Samples</th>
    <th>Percentage of Samples</th>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Classification</td>
    <td>220,131</td>
    <td>45.7386%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Closed QA</td>
    <td>42,650</td>
    <td>8.86177%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Explanation</td>
    <td>2,000</td>
    <td>0.415558%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Extraction</td>
    <td>642</td>
    <td>0.133394%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Extraction and Explanation</td>
    <td>4,682</td>
    <td>0.97282%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Generation</td>
    <td>2,683</td>
    <td>0.557471%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Open QA</td>
    <td>14,410</td>
    <td>2.99409%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Rewrite</td>
    <td>5,132</td>
    <td>1.06632%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Summarization</td>
    <td>796</td>
    <td>0.165392%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Text Completion</td>
    <td>1,423</td>
    <td>0.295669%</td>
  </tr>
  <tr>
    <td>Human-crafted</td>
    <td>Brainstorming</td>
    <td>14,000</td>
    <td>2.9089%</td>
  </tr>
  <tr>
    <td>Human-crafted</td>
    <td>Closed QA</td>
    <td>1,273</td>
    <td>0.264502%</td>
  </tr>
  <tr>
    <td>Human-crafted</td>
    <td>Explanation</td>
    <td>6,000</td>
    <td>1.24667%</td>
  </tr>
  <tr>
    <td>Human-crafted</td>
    <td>Mixed</td>
    <td>33,054</td>
    <td>6.86792%</td>
  </tr>
  <tr>
    <td>Human-crafted</td>
    <td>Open QA</td>
    <td>132,405</td>
    <td>27.511%</td>
  </tr>
</table>

## Dataset Structure
- `uuid(str)`: Unique number of each instruction-output pair.
- `instruction(str)`: Required instruction.
- `output(str)`: Response for the given instruction.
- `system (str)`: Prompts guiding the system in generating responses.
- `topic(str)`: Subject area of the instruction.
- `task(str)`: Nature of the task.
- `source(str)`: Origin of the data.

##  Loading The Dataset
You can download the dataset directly from HuggingFace or use the following code:

```python
from datasets import load_dataset
instar = load_dataset('ClusterlabAi/InstAr-500k')
```

**Dataset Sample**

<p align=""left"" width=""100%"">
<img src=""data:image/png;base64,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"" alt=""Dataset_Sample"" style=""width: 60%; display: block; margin: auto;""/>
</p>

## Dataset Construction

The [InstAr-500k](https://huggingface.co/datasets/ClusterlabAi/InstAr-500k) dataset combines synthetic and human-crafted data to ensure a diverse range of instructions and responses. The synthetic data was generated using the Command R+ model, while human-crafted data was sourced from [101 Billion Arabic Words dataset](https://huggingface.co/datasets/ClusterlabAi/101_billion_arabic_words_dataset)

<p align=""left"" width=""100%"">
<a ><img src=""data:image/png;base64,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",High,4.0
Q&A,coastalcph/tydi_xor_rc,0.0,80.0,2024-09-19 08:20:09+00:00,mit,0.0,8.22 MB,8619294.72,8.22 MB,8619294.72,18354,none,"['https://aclanthology.org/2020.tacl-1.30', 'https://aclanthology.org/2020.tacl-1.30"",', 'https://aclanthology.org/2021.naacl-main.46"",', 'https://aclanthology.org/2023.emnlp-main.10"",']","---
annotations_creators:
- crowdsourced
language:
- en
- ar
- bn
- fi
- ja
- ko
- ru
- te
language_creators:
- crowdsourced
license:
- mit
multilinguality:
- multilingual
pretty_name: XORQA Reading Comprehension
size_categories:
- '10K<n<100K'
source_datasets:
- extended|wikipedia
task_categories:
- question-answering
task_ids:
- extractive-qa
---

# Dataset Card for ""tydi_xor_rc""


## Dataset Description

- Homepage: https://ai.google.com/research/tydiqa
- Paper: https://aclanthology.org/2020.tacl-1.30

### Dataset Summary

[TyDi QA](https://huggingface.co/datasets/tydiqa) is a question answering dataset covering 11 typologically diverse languages. 
[XORQA](https://github.com/AkariAsai/XORQA) is an extension of the original TyDi QA dataset to also include unanswerable questions, where context documents are only in English but questions are in 7 languages.
[XOR-AttriQA](https://github.com/google-research/google-research/tree/master/xor_attriqa) contains annotated attribution data for a sample of XORQA.
This dataset is a combined and simplified version of the [Reading Comprehension data from XORQA](https://nlp.cs.washington.edu/xorqa/XORQA_site/data/tydi_xor_rc_yes_no_unanswerable.zip) and the [in-English data from XOR-AttriQA](https://storage.googleapis.com/gresearch/xor_attriqa/xor_attriqa.zip).

The code to create the dataset is available on [this Colab notebook](https://colab.research.google.com/drive/14s0FEag5FDr-jqjaVLzlU_0Lv0nXHWNg?usp=sharing).

## Dataset Structure

The dataset contains a train and a validation set, with 15445 and 3646 examples, respectively. Access them with

```py
from datasets import load_dataset
dataset = load_dataset(""coastalcph/tydi_xor_rc"")
train_set = dataset[""train""]
validation_set = dataset[""validation""]
```

### Data Instances

Description of the dataset columns:

| Column name                  | type        |  Description                                                                                                     |
| -----------                  | ----------- | -----------                                                                                                      |
| lang                     | str         | The language of the question                                                                                |
| question                | str         | The question to answer                                                                                           |
| context           | str         | The context, a Wikipedia paragraph in English that might or might not contain the answer to the question                    | 
| answertable | bool | True if the question can be answered given the context, False otherwise |
| answer_start  | int   | The character index in 'context' where the answer starts. If the question is unanswerable given the context, this is -1  |
| answer   | str   | The answer in English, a span of text from 'context'. If the question is unanswerable given the context, this can be 'yes' or 'no'            |
| answer_inlang   | str   | The answer in the same language as the question, only available for some instances (otherwise, NaN)            |


## Useful stuff

Check out the [datasets ducumentations](https://huggingface.co/docs/datasets/quickstart) to learn how to manipulate and use the dataset. Specifically, you might find the following functions useful:

`dataset.filter`, for filtering out data (useful for keeping instances of specific languages, for example).

`dataset.map`, for manipulating the dataset.

`dataset.to_pandas`, to convert the dataset into a pandas.DataFrame format.

## Citations
```
@article{clark-etal-2020-tydi,
    title = ""{T}y{D}i {QA}: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"",
    author = ""Clark, Jonathan H.  and
      Choi, Eunsol  and
      Collins, Michael  and
      Garrette, Dan  and
      Kwiatkowski, Tom  and
      Nikolaev, Vitaly  and
      Palomaki, Jennimaria"",
    editor = ""Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani"",
    journal = ""Transactions of the Association for Computational Linguistics"",
    volume = ""8"",
    year = ""2020"",
    address = ""Cambridge, MA"",
    publisher = ""MIT Press"",
    url = ""https://aclanthology.org/2020.tacl-1.30"",
    doi = ""10.1162/tacl_a_00317"",
    pages = ""454--470"",
    abstract = ""Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA{---}a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology{---}the set of linguistic features each language expresses{---}such that we expect models performing well on this set to generalize across a large number of the world{'}s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don{'}t know the answer yet, and the data is collected directly in each language without the use of translation."",
}

@inproceedings{asai-etal-2021-xor,
    title = ""{XOR} {QA}: Cross-lingual Open-Retrieval Question Answering"",
    author = ""Asai, Akari  and
      Kasai, Jungo  and
      Clark, Jonathan  and
      Lee, Kenton  and
      Choi, Eunsol  and
      Hajishirzi, Hannaneh"",
    editor = ""Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao"",
    booktitle = ""Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"",
    month = jun,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.naacl-main.46"",
    doi = ""10.18653/v1/2021.naacl-main.46"",
    pages = ""547--564"",
    abstract = ""Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcity{---}where languages have few reference articles{---}and information asymmetry{---}where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TyDi QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving cross-lingual document retrieval from multilingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at \url{https://nlp.cs.washington.edu/xorqa/}."",
}

@inproceedings{muller-etal-2023-evaluating,
    title = ""Evaluating and Modeling Attribution for Cross-Lingual Question Answering"",
    author = ""Muller, Benjamin  and
      Wieting, John  and
      Clark, Jonathan  and
      Kwiatkowski, Tom  and
      Ruder, Sebastian  and
      Soares, Livio  and
      Aharoni, Roee  and
      Herzig, Jonathan  and
      Wang, Xinyi"",
    editor = ""Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika"",
    booktitle = ""Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2023"",
    address = ""Singapore"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.emnlp-main.10"",
    doi = ""10.18653/v1/2023.emnlp-main.10"",
    pages = ""144--157"",
    abstract = ""Trustworthy answer content is abundant in many high-resource languages and is instantly accessible through question answering systems {---} yet this content can be hard to access for those that do not speak these languages. The leap forward in cross-lingual modeling quality offered by generative language models offers much promise, yet their raw generations often fall short in factuality. To improve trustworthiness in these systems, a promising direction is to attribute the answer to a retrieved source, possibly in a content-rich language different from the query. Our work is the first to study attribution for cross-lingual question answering. First, we collect data in 5 languages to assess the attribution level of a state-of-the-art cross-lingual QA system. To our surprise, we find that a substantial portion of the answers is not attributable to any retrieved passages (up to 50{\%} of answers exactly matching a gold reference) despite the system being able to attend directly to the retrieved text. Second, to address this poor attribution level, we experiment with a wide range of attribution detection techniques. We find that Natural Language Inference models and PaLM 2 fine-tuned on a very small amount of attribution data can accurately detect attribution. With these models, we improve the attribution level of a cross-lingual QA system. Overall, we show that current academic generative cross-lingual QA systems have substantial shortcomings in attribution and we build tooling to mitigate these issues."",
}

```
",High,5.0
Q&A,Washedashore/thepower,2.0,11.0,2024-07-15 02:56:09+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- text-classification
- question-answering
- summarization
- text-generation
- table-question-answering
- token-classification
- zero-shot-classification
- translation
- fill-mask
- depth-estimation
- automatic-speech-recognition
language:
- aa
- ab
- ae
- af
- ak
- am
- an
- ar
- ay
- bm
- as
- av
- az
- br
- en
- ce
- ba
- be
- bg
- bh
- bi
- bn
- cy
tags:
- chemistry
- biology
- medical
- finance
- art
- code
- synthetic
pretty_name: pow
size_categories:
- 100B<n<1T
---
# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [Washed Ashore Relics]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Q&A,obadabaq/uae-laws,2.0,42.0,2024-07-24 07:16:05+00:00,mit,0.0,9.47 MB,9930014.72,3.7 MB,3879731.2,9445,none,none,"---
license: mit
task_categories:
- question-answering
- text-generation
language:
- en
- ar
tags:
- legal
size_categories:
- 1K<n<10K
---
# Dataset Card for UAE-Laws

This dataset is a collection of information about the laws and regulations in the United Arab Emirates. 
It covers different areas of law like: 
* economy and business 
* family and community 
* finance and banking 
* industry and technical standardisation 
* justice and juiciary, labour 
* residency and leberal professions 
* security and safety 
* tax

### Dataset Sources


- **[United Arab Emirates Legislations](https://uaelegislation.gov.ae/en)**

## Dataset Structure

The ./uae-laws.csv file contains three columns:
* Law Kind
* Law date & authority
* Content",Medium,2.0
Q&A,obadabaq/structured-uae-laws,0.0,37.0,2024-07-24 07:14:44+00:00,mit,1.0,9.56 MB,10024386.56,4.54 MB,4760535.04,9446,none,none,"---
license: mit
task_categories:
- question-answering
- text-generation
language:
- en
- ar
tags:
- legal
size_categories:
- 1K<n<10K
---
# Dataset Card for structured-uae-laws

This dataset is a collection of question & answers about the laws and regulations in the United Arab Emirates. 
It covers different areas of law like: 
* economy and business 
* family and community 
* finance and banking 
* industry and technical standardisation 
* justice and juiciary, labour 
* residency and leberal professions 
* security and safety 
* tax

### Dataset Sources

- **[Repository](https://github.com/obadabaq/ai-lawyer)**
- **[Base Dataset](https://huggingface.co/datasets/obadabaq/uae-laws)**
- **[United Arab Emirates Legislations](https://uaelegislation.gov.ae/en)**

## Dataset Structure

The ./structured-uae-laws.csv file contains three columns:
* instruction
* input
* output",Low,1.0
Q&A,riotu-lab/ArabicQA_2.1M,2.0,49.0,2024-08-04 11:46:12+00:00,apache-2.0,0.0,662 MB,694157312.0,662 MB,694157312.0,2141146,none,none,"---
dataset_info:
  features:
  - name: question
    dtype: string
  - name: answer
    dtype: string
  - name: context
    dtype: string
  splits:
  - name: train
    num_bytes: 1842788736.123851
    num_examples: 2141146
  download_size: 662212653
  dataset_size: 1842788736.123851
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: apache-2.0
task_categories:
- question-answering
language:
- ar
size_categories:
- 1M<n<10M
---

# Arabic Question Answering Dataset

## Description

### Dataset Overview

Our dataset is an amalgamation of several filtered datasets, the total number of rows for all datasets was 4,731,600 which was reduced to 2,141,146 rows after filtering. The dataset was collected to fine a pretraind model, the model forced a number of contrains on us discussed in the following section.

### Filtering Process

The filtering process for each dataset included one or more of the following steps:

1. **Removing rows with less than 65% Arabic text**: This resulted in the removal of almost all coding questions, as the model was pre-trained on strictly Arabic text.

2. **Normalizing text with diacritics and elongations**: The tokenizer used with the model did not include tashkeel, so normalization was necessary.

3. **Removing rows with excessively long texts**: We chose to remove all texts exceeding 2048 words. Due to the small context window of the model, we settled for exclusively short contexts, questions, and answers.

4. **Filtering multiple-choice questions**: Some datasets included multiple-choice questions with columns for choices (A, B, C, and D) and the correct choice. These questions were ideal for our short context window, as they did not require lengthy contexts and the answers were very short (typically 2-3 words).

5. **Context included in the question**: Many datasets included the context within the question itself, eliminating the need to repeat the context in a separate column.

This is a sample from the dataset: 

```
{'question': 'أعد تنظيم العبارات المحددة في جملة جيدة التنظيم.',
 'answer': 'كان الجو عاصفًا أمس.',
 'context': 'أمس / عاصف / كان'}
```

## Usage
Ideal for fine-tuning models with a short context window (i.e. context window>=1024 tokens)
```python
from datasets import load_dataset

ds = load_dataset(""riotu-lab/ArabicQA_2.1M"")
```


## Credits
We would like to thank the authors and contributors for their work on creating and providing these valuable resources:

1- [InstAr-500k] - provided by [ClusterlabAi](https://huggingface.co/ClusterlabAi)
  - https://huggingface.co/datasets/ClusterlabAi/InstAr-500k
  - The dataset comprises almost 500,000 Arabic instructions and responses designed for fine-tuning large language models (LLMs) for Arabic NLP tasks.

2- [Ara--MBZUAI--Bactrain-X] - provided by [Mixed Arabic Datasets](https://huggingface.co/M-A-D)
  - https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Datasets-Repo/viewer/Ara--MBZUAI--Bactrian-X
  - The Mixed Arabic Datasets (MAD) presents a dynamic compilation of diverse Arabic texts sourced from various online platforms and datasets.

3- [ar-all-questions] - provided by [CLiPS](https://huggingface.co/clips)
  - https://huggingface.co/datasets/clips/mqa/viewer/ar-all-question
  - MQA is a Multilingual corpus of Questions and Answers (MQA) parsed from the Common Crawl.

4- [m_mmlu  ] - provided by [Alexandra Institute](https://huggingface.co/alexandrainst)
  - https://huggingface.co/datasets/alexandrainst/m_mmlu
  - A machine translated version of the [MMLU dataset](https://huggingface.co/datasets/cais/mmlu)

5- [xquad] - provided by [Google](https://huggingface.co/google)
  - https://huggingface.co/datasets/google/xquad
  - XQuAD (Cross-lingual Question Answering Dataset) is a benchmark dataset for evaluating cross-lingual question answering performance.

6- [2A2I-Arabic-OpenHermes-2.5-Llama-3] - provided by [Lyte](https://huggingface.co/Lyte)
  - https://huggingface.co/datasets/Lyte/2A2I-Arabic-OpenHermes-2.5-Llama-3?row=3
  - 2A2I-Arabic-OpenHermes-2.5-Llama-3 streamlines Arabic language research and applications by offering a high quality text resource in Meta's Llama-3 conversational style to help better alignement of the Arabic Base LLMs, saving time and effort for researchers, technologists, and linguists in Arabic NLP/AI projects.

7- [xtr-wiki_qa ] - provided by [AmazonScience](https://huggingface.co/AmazonScience)
  - https://huggingface.co/datasets/AmazonScience/xtr-wiki_qa 
  - Xtr-WikiQA is an Answer Sentence Selection (AS2) dataset in 9 non-English languages, proposed in our paper accepted at ACL 2023

8- [xquad_xtreme] - provided by [juletxara](https://huggingface.co/juletxara)
  - https://huggingface.co/datasets/juletxara/xquad_xtreme",High,5.0
Q&A,kokojake/oasst2_egyptian_arabic_convs,0.0,40.0,2024-08-12 19:03:33+00:00,apache-2.0,0.0,10.5 MB,11010048.0,10.5 MB,11010048.0,2517,none,none,"---
language:
- ar
license: apache-2.0
size_categories:
- 1K<n<10K
task_categories:
- text-generation
- question-answering
dataset_info:
  features:
  - name: text
    list:
    - name: content
      dtype: string
    - name: role
      dtype: string
  splits:
  - name: train
    num_bytes: 23809457
    num_examples: 2517
  download_size: 10494081
  dataset_size: 23809457
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
---
",Medium,2.0
Q&A,Adelante/arabic-qa-smallbatch,0.0,22.0,2024-08-24 08:34:08+00:00,none,0.0,319 KB,326656.0,319 KB,326656.0,101,none,none,"---
dataset_info:
  features:
  - name: year
    dtype: int64
  - name: month
    dtype: int64
  - name: date
    dtype: int64
  - name: page
    dtype: int64
  - name: slice
    dtype: int64
  - name: text
    dtype: string
  - name: containing
    list:
    - name: ar
      dtype: string
    - name: zh
      dtype: string
  - name: 问题1-数字
    dtype: string
  - name: 问题2-数字
    dtype: string
  - name: 问题3-数字
    dtype: string
  - name: 问题4-数字
    dtype: string
  - name: 问题5-数字
    dtype: string
  - name: 问题6-数字
    dtype: string
  - name: 问题7-数字
    dtype: string
  - name: 问题8-数字
    dtype: string
  - name: 问题1-文字
    dtype: string
  - name: 问题2-文字
    dtype: string
  - name: 问题3-文字
    dtype: string
  - name: 问题4-文字
    dtype: string
  - name: 问题5-文字
    dtype: string
  - name: 问题6-文字
    dtype: string
  - name: 问题7-文字
    dtype: string
  - name: 问题8-文字
    dtype: string
  splits:
  - name: train
    num_bytes: 570755
    num_examples: 101
  download_size: 318709
  dataset_size: 570755
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- question-answering
language:
- ar
- en
tags:
- test
- demo
- digitalhumanity
---",Low,1.0
Q&A,laion/Wikipedia-Abstract,4.0,886.0,2024-10-19 13:07:03+00:00,mit,0.0,11.9 GB,12777527705.6,11.9 GB,12777527705.6,43609739,none,none,"---
language:
- ar
- pt
- pl
- en
- de
- es
- he
- ur
- bn
- it
- fr
- ru
license: mit
task_categories:
- text-classification
- question-answering
- text-generation
- fill-mask
dataset_info:
  features:
  - name: URL
    dtype: string
  - name: Wiki
    dtype: string
  - name: Language
    dtype: string
  - name: Title
    dtype: string
  - name: Abstract
    dtype: string
  - name: Version Control
    dtype: string
  splits:
  - name: data
    num_bytes: 3849026435
    num_examples: 6575217
  - name: data1
    num_bytes: 1339234317
    num_examples: 2565263
  - name: data2
    num_bytes: 299141019
    num_examples: 345314
  - name: data3
    num_bytes: 121110572
    num_examples: 194533
  - name: data4
    num_bytes: 763143777
    num_examples: 1474721
  - name: data5
    num_bytes: 4402642
    num_examples: 4238
  - name: data6
    num_bytes: 921270984
    num_examples: 1777137
  - name: data7
    num_bytes: 634758702
    num_examples: 1080168
  - name: data8
    num_bytes: 1490198322
    num_examples: 2501467
  - name: data9
    num_bytes: 1116300754
    num_examples: 1890629
  - name: data10
    num_bytes: 568620663
    num_examples: 1315710
  - name: data11
    num_bytes: 1555258518
    num_examples: 1854603
  - name: data12
    num_bytes: 787383297
    num_examples: 1197801
  - name: data14
    num_bytes: 3861239238
    num_examples: 6082427
  - name: data15
    num_bytes: 1143640444
    num_examples: 2522834
  - name: data16
    num_bytes: 244946
    num_examples: 727
  - name: data13
    num_bytes: 243547600
    num_examples: 153847
  - name: data20
    num_bytes: 6687940
    num_examples: 10091
  - name: data19
    num_bytes: 716369386
    num_examples: 1357324
  - name: data21
    num_bytes: 23896918
    num_examples: 72440
  - name: data22
    num_bytes: 328910813
    num_examples: 617682
  - name: data23
    num_bytes: 388154425
    num_examples: 664194
  - name: data18
    num_bytes: 636558246
    num_examples: 984255
  - name: data24
    num_bytes: 431490603
    num_examples: 644539
  - name: dat25
    num_bytes: 284962592
    num_examples: 557766
  - name: data26
    num_bytes: 70729945
    num_examples: 50009
  - name: data27
    num_bytes: 11721912
    num_examples: 7966
  - name: data28
    num_bytes: 755226
    num_examples: 1306
  - name: data29
    num_bytes: 187125249
    num_examples: 265229
  - name: data30
    num_bytes: 72693670
    num_examples: 133813
  - name: data31
    num_bytes: 50671456
    num_examples: 86109
  - name: data32
    num_bytes: 211757130
    num_examples: 597333
  - name: data17
    num_bytes: 996821989
    num_examples: 1266023
  - name: data33
    num_bytes: 142690196
    num_examples: 285514
  - name: data34
    num_bytes: 167606490
    num_examples: 137368
  - name: data35
    num_bytes: 208895732
    num_examples: 423879
  - name: data36
    num_bytes: 233993323
    num_examples: 165868
  - name: data37
    num_bytes: 130785580
    num_examples: 207537
  - name: data38
    num_bytes: 98250166
    num_examples: 185702
  - name: data39
    num_bytes: 99189351
    num_examples: 189312
  - name: data40
    num_bytes: 76226522
    num_examples: 115915
  - name: data41
    num_bytes: 514183660
    num_examples: 990740
  - name: data42
    num_bytes: 252278498
    num_examples: 286695
  download_size: 12587695543
  dataset_size: 25041929248

configs:
  - config_name: English
    data_files: ""data/English-*""
  - config_name: German
    data_files: ""data/German-*""
  - config_name: Hebrew
    data_files: ""data/Hebrew-*""
  - config_name: Urdu
    data_files: ""data/Urdu-*""
  - config_name: Polish
    data_files: ""data/Polish-*""
  - config_name: Uighur
    data_files: ""data/Uighur-*""
  - config_name: Italian
    data_files: ""data/Italian-*""
  - config_name: Portuguese
    data_files: ""data/Portuguese-*""
  - config_name: French
    data_files: ""data/French-*""
  - config_name: Spanish
    data_files: ""data/Spanish-*""
  - config_name: Chinese
    data_files: ""data/Chinese-*""
  - config_name: Russian
    data_files: ""data/Russian-*""
  - config_name: Arabic
    data_files: ""data/Arabic-*""
  - config_name: Bengali
    data_files: ""data/Bengali-*""
  - config_name: Cebuano
    data_files: ""data/Cebuano-*""
  - config_name: Swedish
    data_files: ""data/Swedish-*""
  - config_name: Aramic
    data_files: ""data/Aramic-*""
  - config_name: Ukranian
    data_files: ""data/Ukranian-*""
  - config_name: Persian
    data_files: ""data/Persian-*""
  - config_name: Japanese
    data_files: ""data/Japanese-*""
  - config_name: Moroccan Arabic
    data_files: ""data/Moroccan Arabic-*""
  - config_name: Kurdish
    data_files: ""data/Kurdish-*""
  - config_name: Korean
    data_files: ""data/Korean-*""
  - config_name: Indonesian
    data_files: ""data/Indonesian-*""
  - config_name: Serbian
    data_files: ""data/Serbian-*""
  - config_name: Turkish
    data_files: ""data/Turkish-*""
  - config_name: Punjabi
    data_files: ""data/Punjabi-*""
  - config_name: Sanskrit
    data_files: ""data/Sanskrit-*""
  - config_name: Old English
    data_files: ""data/Old English-*""
  - config_name: Danish
    data_files: ""data/Danish-*""
  - config_name: Latin
    data_files: ""data/Latin-*""
  - config_name: Bosnian
    data_files: ""data/Russian-*""
  - config_name: Chechen
    data_files: ""data/Chechen-*""
  - config_name: Uzbek
    data_files: ""data/Uzbek-*""
  - config_name: Hindi
    data_files: ""data/Hindi-*""
  - config_name: Romanian
    data_files: ""data/Romanian-*""
  - config_name: Georgian
    data_files: ""data/Georgian-*""
  - config_name: Estonian
    data_files: ""data/Estonian-*""
  - config_name: Azerbaijan
    data_files: ""data/Azerbaijan-*""
  - config_name: Lithunian
    data_files: ""data/Lithunian-*""
  - config_name: Latvian
    data_files: ""data/Latvian-*""
  - config_name: Dutch
    data_files: ""data/Dutch-*""
  - config_name: Armenian
    data_files: ""data/Armenian-*""

tags:
- chemistry
- finance
- legal
- biology
- music
- medical
---
<h1 style=""text-align: center;"">Wikipedia Abstract</h1>

<p align=""center"">
  <img src=""Wikipedia.jpg"" alt=""Wikipedia X Logo"" width=""250"" height=""250"" />
</p>


**Introducing Wikipedia Abstract**, a comprehensive dataset encompassing abstracts, complete articles, and a popularity score index for both widely spoken and lesser-known Wikipedia subsets. Our dedication to Wikipedia-X ensures a centralized Wikipedia dataset that undergoes regular updates and adheres to the highest standards.

A central focus of our efforts was to include exotic languages that often lack up-to-date Wikipedia dumps or may not have any dumps at all. Languages such as Hebrew, Urdu, Bengali, Aramaic, Uighur, and Polish were prioritized to ensure high-quality processed Wikipedia datasets are accessible for these languages with substantial speaker bases. This initiative aims to enable Artificial Intelligence to thrive across all languages, breaking down language barriers and fostering inclusivity.

Notice: We're continuously updating this dataset every 8 months as part of a broader effort at LAION AI dedicated to textual embeddings. If you'd like to see a specific language added, please don't hesitate to reach out to us.

#### Dataset Information:

Indexed on (history): 19th of August 2024

**Sourcing:** Our dataset is sourced from the outstanding dumps of the Wikimedia project. It represents the content of Wikipedia pages of corresponding articles without any alterations.

**Cleaning:** Some languages, like English and German, underwent cleaning while maintaining their Unicode representation.

**Structure:** Our dataset includes the following columns:

**Abstract:** Contains complete abstracts for each entry

**Version Control:** Base64-encoded metadata of the official Wikipedia extraction code.


**WE HAVE RELEASED WIKIPEDIA X (FULL) FOR ENTIRE TEXT OF ALL THE ARTICLES IN THE BELOW MENTIONED 17 LANGUAGES** [HF LINK](https://huggingface.co/datasets/laion/Wikipedia-X-Full)

| Language      | Code  |
|---------------|-------|
| English       | en    |
| German        | de    |
| Polish        | pl    |
| Spanish       | es    |
| Hebrew        | he    |
| French        | fr    |
| Chinese       | zh    |
| Italian       | it    |
| Russian       | ru    |
| Urdu          | ur    |
| Portuguese    | pt    |
| Aramaic       | arc   |
| Cebuano       | ceb   |
| Swedish       | sv    |
| Uighur        | ug    |
| Bengali       | bn    |
| Arabic        | ar    |",Medium,2.0
Q&A,MohammedNasser/ARabic_Reasoning_QA,5.0,11.0,2024-09-07 23:00:07+00:00,apache-2.0,1.0,51.5 kB,unknown,51.5 kB,unknown,1000,none,none,"---
language:
- ar
license: apache-2.0
size_categories:
- 1K<n<10K
task_categories:
- question-answering
- text-generation
pretty_name: arabic_reasoning_qa_ds
dataset_info:
  features:
  - name: Question
    dtype: string
  - name: Answer
    dtype: int16
  splits:
  - name: train
    num_bytes: 84363
    num_examples: 715
  - name: eval
    num_bytes: 21034
    num_examples: 185
  - name: test
    num_bytes: 11585
    num_examples: 100
  download_size: 51525
  dataset_size: 116982
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: eval
    path: data/eval-*
  - split: test
    path: data/test-*
---

---
**language:** 🌐 Arabic  |  **license:** apache-2.0  |  **datasets:** 📚 ARabic Reasoning QA  |  **task_categories:** 🔍 Question Answering  |  **task_ids:** 🧠 Reasoning

---

# ARabic Reasoning QA

## Dataset Description

The **ARabic Reasoning QA** dataset consists of Arabic reasoning questions designed to test various levels of reasoning skills. It includes questions with varying complexity levels: beginner, intermediate, and advanced. This dataset is suitable for training and evaluating models on reasoning tasks in Arabic.

## Dataset Structure 🏗️


The dataset is organized into the following splits:
- **train**: Training data.
- **eval**: Evaluation data.
- **test**: Testing data.

### Data Fields

- **Question**: The reasoning question in Arabic.
- **Answer**: The integer answer to the question.

### Example

Here are some examples of the dataset:

**Example 1:**
- **Question**: لدي أربعة كتب وثلاثة دفاتر. كم عدد الأشياء التي لدي؟
- **Answer**: 7

**Example 2:**
- **Question**: إذا كان لديك ثلاث سيارات، وأعطيت واحدة منها، كم سيارة ستبقى لديك؟
- **Answer**: 2

**Example 3:**
- **Question**: كم عدد الأصابع في يد واحدة؟
- **Answer**: 5

## Usage

To use this dataset, you can load it via the Hugging Face `datasets` library. Here's an example of how to load the dataset:

```python
from datasets import load_dataset

dataset = load_dataset('MohammedNasser/ARabic_Reasoning_QA')
```

## License

This dataset is licensed under the Apache 2.0 License.

## Citation

If you use this dataset in your research, please cite it as follows:

```
@article{arabic_reasoning_benchmarking,
  title={Benchmarking Arabic Reasoning Models: Utilizing the ARabic Reasoning QA Dataset},
  author={M. N. Gaber},
  year={2024},
  publisher={Hugging Face Hub},
  url={https://huggingface.co/MohammedNasser/ARabic_Reasoning_QA}
}
```
```

This version reflects the Apache 2.0 license and includes all relevant details for users to properly cite and use the dataset.",High,6.0
Q&A,MohammedNasser/Arabic_Reasoning_Instruct_QA,2.0,61.0,2024-09-10 07:30:48+00:00,apache-2.0,1.0,112 KB,114688.0,112 KB,114688.0,1725,none,none,"---
language:
- ar
license: apache-2.0
size_categories:
- 1K<n<10K
task_categories:
- question-answering
- text2text-generation
pretty_name: ar_res_inst_qa
dataset_info:
  features:
  - name: instruction
    dtype: string
  - name: answer
    dtype: string
  splits:
  - name: train
    num_bytes: 741840
    num_examples: 1233
  - name: eval
    num_bytes: 187368
    num_examples: 319
  - name: test
    num_bytes: 101427
    num_examples: 173
  download_size: 112151
  dataset_size: 1030635
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: eval
    path: data/eval-*
  - split: test
    path: data/test-*
---
",Medium,2.0
Q&A,MBZUAI/MadinahQA,1.0,219.0,2024-09-17 08:22:25+00:00,cc-by-nc-4.0,0.0,2.03 MB,2128609.28,192 KB,196608.0,983,none,none,"---
license: cc-by-nc-4.0
task_categories:
- question-answering
language:
- ar
tags:
- ArabicMMLU
- exams
dataset_info:
configs:
- config_name: Arabic Language (General)
  data_files:
     - split: test
       path: Arabic Language (General)/test.csv
     - split: dev
       path: Arabic Language (General)/dev.csv
- config_name: Arabic Language (Grammar)
  data_files:
     - split: test
       path: Arabic Language (Grammar)/test.csv
     - split: dev
       path: Arabic Language (Grammar)/dev.csv
---

This data is part of MBZUAI/ArabicMMLU",Low,1.0
Q&A,QCRI/AraDiCE,3.0,266.0,2024-12-13 20:07:31+00:00,cc-by-nc-sa-4.0,0.0,599 Bytes,unknown,2.22 kB,unknown,8,https://arxiv.org/abs/2409.11404,none,"---
license: cc-by-nc-sa-4.0
task_categories:
  - text-classification
  - question-answering
language:
  - ar
tags:
  - MMLU
  - reading-comprehension
  - commonsense-reasoning
  - capabilities
  - cultural-understanding
  - world-knowledge
pretty_name: 'AraDiCE -- Arabic Dialect and Cultural Evaluation'
size_categories:
  - 10K<n<100K
---
# AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs

## Overview

The **AraDiCE** dataset is designed to evaluate dialectal and cultural capabilities in large language models (LLMs). The dataset consists of post-edited versions of various benchmark datasets, curated for validation in cultural and dialectal contexts relevant to Arabic.

As part of the supplemental materials, we have selected a few datasets (see below) for the reader to review. We will make the full AraDiCE benchmarking suite publicly available to the community.

## AraDICE Collection
AraDICE collection can accessed through [collection page](https://huggingface.co/collections/QCRI/aradice-6727765839bf89aa78e9f132)

Individual dataset can also be accessed by the following links:

- [ArabicMMLU-lev](https://huggingface.co/datasets/QCRI/AraDICE-ArabicMMLU-lev)
- [ArabicMMLU-egy](https://huggingface.co/datasets/QCRI/AraDICE-ArabicMMLU-egy)
- [AraDiCE-Culture](https://huggingface.co/datasets/QCRI/AraDiCE-Culture)
- [BoolQ](https://huggingface.co/datasets/QCRI/AraDiCE-BoolQ)
- [OpenBookQA (OBQA)](https://huggingface.co/datasets/QCRI/AraDiCE-OpenBookQA)
- [PIQA](https://huggingface.co/datasets/QCRI/AraDiCE-PIQA)
- [TruthfulQA](https://huggingface.co/datasets/QCRI/AraDiCE-TruthfulQA)
- [AraDiCE-WinoGrande](https://huggingface.co/datasets/QCRI/AraDiCE-WinoGrande)

## Machine Translation (MT) Models Used for AraDICE

Along with AraDICE Collection, we provide Machine Translation (MT) models tailored for specific Arabic dialects. These models are designed to facilitate seamless translation from Modern Standard Arabic (MSA) into two prominent Arabic dialects: Levantine and Egyptian. The models leverage state-of-the-art neural translation methods to ensure high accuracy and contextual relevance.

You can access and download the MT models using the following links:

- **MSA to Levantine Dialect Model:** [AraDiCE-msa-to-lev](https://huggingface.co/QCRI/AraDiCE-msa-to-lev)
This model translates text from MSA into the Levantine Arabic dialect, commonly spoken in countries like Lebanon, Syria, Jordan, and Palestine.

- **MSA to Egyptian Dialect Model:** [AraDiCE-msa-to-egy](https://huggingface.co/QCRI/AraDiCE-msa-to-egy)
This model enables translation from MSA into Egyptian Arabic, widely spoken and understood across Egypt and in other Arabic-speaking regions due to its cultural prominence.

These models are hosted on Hugging Face for easy accessibility and integration into various applications.



## Dataset Statistics

The datasets used in this study include: *i)* four existing Arabic datasets for understanding and generation: *Arabic Dialects Dataset (ADD)*, *ADI*, *QADI*, along with a dialectal response generation dataset, and *MADAR*; *ii)* seven datasets translated and post-edited into MSA and dialects (Levantine and Egyptian), which include *ArabicMMLU*, *BoolQ*, *PIQA*, *OBQA*, *Winogrande*, *Belebele*, and *TruthfulQA*; and *iii)* *AraDiCE-Culture*, an in-house developed regional Arabic cultural understanding dataset. Please find below the types of dataset and their statistics benchmarked in **AraDiCE**.

<p align=""left""> <img src=""./benchmarking_tasks_datasets.png"" style=""width: 40%;"" id=""title-icon""> </p>

<p align=""left""> <img src=""./data_stat_table.png"" style=""width: 40%;"" id=""title-icon""> </p>


## Dataset Usage

The AraDiCE dataset is intended to be used for benchmarking and evaluating large language models, specifically focusing on:

- Assessing the performance of LLMs on Arabic-specific dialect and cultural specifics.
- Dialectal variations in the Arabic language.
- Cultural context awareness in reasoning.

## Evaluation
We have used [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) eval framework to for the benchmarking. It is under a [pull](https://github.com/EleutherAI/lm-evaluation-harness/pull/2507) request on *lm-evaluation-harness* at this moment.
<!-- We will soon release them. Stay tuned!! -->


## License

The dataset is distributed under the **Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)**. The full license text can be found in the accompanying `licenses_by-nc-sa_4.0_legalcode.txt` file.


## Citation
Please find the paper <a href=""https://arxiv.org/pdf/2409.11404"" target=""_blank"" style=""margin-right: 15px; margin-left: 10px"">here</a>, which is accepted at [COLING 2025](https://coling2025.org/). If you use all or any specific dataset in this collection, please make sure if also cite original dataset paper. You will find the citations in our paper.

```
@article{mousi2024aradicebenchmarksdialectalcultural,
      title={{AraDiCE}: Benchmarks for Dialectal and Cultural Capabilities in LLMs},
      author={Basel Mousi and Nadir Durrani and Fatema Ahmad and Md. Arid Hasan and Maram Hasanain and Tameem Kabbani and Fahim Dalvi and Shammur Absar Chowdhury and Firoj Alam},
      year={2024},
      publisher={arXiv:2409.11404},
      url={https://arxiv.org/abs/2409.11404},
}
```
",High,5.0
Q&A,borderlines/bordirlines,8.0,647.0,2025-06-10 00:00:42+00:00,mit,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2410.01171,none,"---
annotations_creators:
- human
- machine-generated
language_creators:
- found
language:
- en
- ar
- es
- fr
- ru
- hi
- ms
- sw
- az
- ko
- pt
- hy
- th
- uk
- ur
- sr
- iw
- ja
- hr
- tl
- ky
- vi
- fa
- tg
- mg
- nl
- ne
- uz
- my
- da
- dz
- id
- is
- tr
- lo
- sl
- so
- mn
- bn
- bs
- ht
- el
- it
- to
- ka
- sn
- sq
- zh
license: mit
multilinguality:
- multilingual
source_datasets:
- manestay/borderlines
task_categories:
- question-answering
pretty_name: BordIRlines
arxiv: 2410.01171
---

# BordIRLines Dataset

This is the dataset associated with the paper ""BordIRlines: A Dataset for Evaluating Cross-lingual Retrieval-Augmented Generation"" ([link](https://arxiv.org/abs/2410.01171)).

Code: https://github.com/manestay/bordIRlines

## Dataset Summary

The **BordIRLines Dataset** is an information retrieval (IR) dataset constructed from various language corpora. It contains queries and corresponding ranked docs along with their relevance scores. The dataset includes multiple languages, including English, Arabic, Spanish, and others, and is split across different sources like LLM-based outputs.
Each `doc` is a passage from a Wikipedia article.

### Languages

The dataset includes docs and queries in the following **languages**:

- `en`: English
- `zht`: Traditional Chinese
- `ar`: Arabic
- `zhs`: Simplified Chinese
- `es`: Spanish
- `fr`: French
- `ru`: Russian
- `hi`: Hindi
- `ms`: Malay
- `sw`: Swahili
- `az`: Azerbaijani
- `ko`: Korean
- `pt`: Portuguese
- `hy`: Armenian
- `th`: Thai
- `uk`: Ukrainian
- `ur`: Urdu
- `sr`: Serbian
- `iw`: Hebrew
- `ja`: Japanese
- `hr`: Croatian
- `tl`: Tagalog
- `ky`: Kyrgyz
- `vi`: Vietnamese
- `fa`: Persian
- `tg`: Tajik
- `mg`: Malagasy
- `nl`: Dutch
- `ne`: Nepali
- `uz`: Uzbek
- `my`: Burmese
- `da`: Danish
- `dz`: Dzongkha
- `id`: Indonesian
- `is`: Icelandic
- `tr`: Turkish
- `lo`: Lao
- `sl`: Slovenian
- `so`: Somali
- `mn`: Mongolian
- `bn`: Bengali
- `bs`: Bosnian
- `ht`: Haitian Creole
- `el`: Greek
- `it`: Italian
- `to`: Tonga
- `ka`: Georgian
- `sn`: Shona
- `sq`: Albanian
- `zh`: Chinese
- `control`: see below

The **control** language is English, and contains the queries for all 251 territories. In contrast, **en** is only the 38 territories which have an English-speaking claimant.

### Annotations

The dataset contains two types of relevance annotations:

1. **Human Annotations**: Provided by multiple annotators for a subset of query-document pairs and relevance is determined by majority vote across annotators.

2. **LLM Annotations**:
   - Includes two modes:
     - **Zero-shot**: Predictions without any task-specific examples.
     - **Few-shot**: Predictions with a small number of task-specific examples.
   - Default mode is **few-shot**.

## Systems

We have processed retrieval results for these IR systems:

- `openai`: OpenAI's model `text-embedding-3-large`, cosine similarity
- `m3`: M3-embedding ([link](https://huggingface.co/BAAI/bge-m3)) (Chen et al., 2024)

## Modes

Considering a user query in language `l` on a territory `t`, there are 4 modes for the IR.

- `qlang`: consider passages in `{l}`. This is monolingual IR (the default).
- `qlang_en`: consider passages in either `{l, en}`.
- `en`: consider passages in `{en}`.
- `rel_langs`: consider passages in all relevant languages to `t` + `en`, so `{l1, l2, ..., en}`. This is a set, so `en` will not be duplicated if it already is relevant.

## Dataset Structure

### Data Fields

The dataset consists of the following fields:

- `query_id (string)`: The id of the query.
- `query (string)`: The query text as provided by the `queries.tsv` file.
- `territory (string)`: The territory of the query hit.
- `rank (int32)`: The rank of the document for the corresponding query.
- `score (float32)`: The relevance score of the document as provided by the search engine or model.
- `doc_id (string)`: The unique identifier of the article.
- `doc_text (string)`: The full text of the corresponding article or document.
- `relevant_human (bool)`: Majority relevance determined by human annotators.
- `territory_human (list[string])`: Territories as judged by human annotators.
- `relevant_llm_zeroshot (bool)`: LLM zero-shot relevance prediction.
- `relevant_llm_fewshot (bool)`: LLM few-shot relevance prediction.

### Download Structure

The dataset is structured as follows:

```
data/
  {lang}/
    {system}/
      {mode}/
        {lang}_query_hits.tsv
...
  all_docs.json
  queries.tsv
  human_annotations.tsv
  llm_annotations.tsv
```

- `queries.tsv`: Contains the list of query IDs and their associated query texts.
- `all_docs.json`: JSON dict containing all docs. It is organized as a nested dict, with keys `lang`, and values another dict with keys `doc_id`, and values `doc_text`.
- `{lang}\_query_hits.tsv`: A TSV file with relevance scores and hit ranks for queries.
- `human_annotations.tsv`: A TSV file with human relevance annotations.
- `llm_annotations.tsv`: A TSV file with LLM relevance predictions.

Currently, there are 50 langs _ 1 system _ 4 modes = 200 query hit TSV files.

## Example Usage

```python
from datasets import load_dataset

# load DatasetDict with all 4 modes, for control language, 10 hits
dsd_control = load_dataset(""borderlines/bordirlines"", ""control"")

# load Dataset for English, with rel_langs mode, 50 hits
ds_oa_en = load_dataset(""borderlines/bordirlines"", ""en"", split=""openai.rel_langs"", n_hits=50)
# load Dataset for Simplified Chinese, en mode
ds_oa_zhs1 = load_dataset(""borderlines/bordirlines"", ""zhs"", split=""openai.en"")
# load Dataset for Simplified Chinese, qlang mode
ds_oa_zhs2 = load_dataset(""borderlines/bordirlines"", ""zhs"", split=""openai.qlang"")


# load Dataset for Simplified Chinese, en mode, m3 embedding
ds_m3_zhs1 = load_dataset(""borderlines/bordirlines"", ""zhs"", split=""m3.en"")
# load Dataset for Simplified Chinese, qlang mode, m3 embedding
ds_m3_zhs2 = load_dataset(""borderlines/bordirlines"", ""zhs"", split=""m3.qlang"")

# Load Dataset for English, relevant-only with human annotations
ds_human_en = load_dataset(""borderlines/bordirlines"", ""en"", relevance_filter=""relevant"", annotation_type=""human"")

# Load Dataset for Simplified Chinese, few-shot LLM mode, only non-relevant
ds_llm_fewshot_zhs = load_dataset(""borderlines/bordirlines"", ""zhs"", relevance_filter=""non-relevant"", annotation_type=""llm"", llm_mode=""fewshot"")
```

## Citation

```
@misc{li2024bordirlines,
      title={BordIRlines: A Dataset for Evaluating Cross-lingual Retrieval-Augmented Generation},
      author={Bryan Li and Samar Haider and Fiona Luo and Adwait Agashe and Chris Callison-Burch},
      year={2024},
      eprint={2410.01171},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.01171},
}
```",High,5.0
Q&A,usail-hkust/JailJudge,2.0,21.0,2024-11-20 01:55:52+00:00,other,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2410.12855,none,"---
configs:
- config_name: default
  data_files:
  - split: train
    path:
    - JAILJUDGE_TRAIN.json
  - split: test
    path:
    - JAILJUDGE_ID.json
    - JAILJUDGE_OOD.json
size_categories:
- 10K<n<100K
license: other
license_name: jailjudge
license_link: LICENSE
task_categories:
- text-classification
- question-answering
- text-generation
language:
- en
- zh
- it
- vi
- ar
- ko
- th
- bn
- sw
- jv
---


## Overview

Although significant research efforts have been dedicated to enhancing the safety of large language models (LLMs) by understanding and defending against jailbreak attacks, evaluating the defense capabilities of LLMs against jailbreak attacks  also attracts lots of attention. Current evaluation methods lack explainability and do not generalize well to complex scenarios, resulting in incomplete and inaccurate assessments (e.g., direct judgment without reasoning explainability,  the F1 score of the GPT-4 judge is only 55\% in complex scenarios and bias evaluation on multilingual scenarios, etc.). To address these challenges, we have developed a comprehensive evaluation benchmark, JAILJUDGE, which includes a wide range of risk scenarios with complex malicious prompts (e.g., synthetic, adversarial, in-the-wild, and multi-language scenarios, etc.) along with high-quality human-annotated test datasets. Specifically, the JAILJUDGE dataset comprises training data of JAILJUDGE, with over 35k+ instruction-tune training data with reasoning explainability, and JAILJUDGETEST, a 4.5k+ labeled set of broad risk scenarios and a 6k+ labeled set of multilingual scenarios in ten languages. To provide reasoning explanations (e.g., explaining why an LLM is jailbroken or not) and fine-grained evaluations (jailbroken score from 1 to 10), we propose a multi-agent jailbreak judge framework, JailJudge MultiAgent, making the decision inference process explicit and interpretable to enhance evaluation quality.   Using this framework, we construct the instruction-tuning ground truth and then instruction-tune an end-to-end jailbreak judge model, JAILJUDGE Guard, which can also provide reasoning explainability with fine-grained evaluations without API costs. 
Additionally, we introduce JailBoost, an attacker-agnostic attack enhancer, and GuardShield, a safety moderation defense method, both based on JAILJUDGE Guard. Comprehensive experiments demonstrate the superiority of our JAILJUDGE benchmark and jailbreak judge methods. Our jailbreak judge methods (JailJudge MultiAgent and JAILJUDGE Guard) achieve SOTA performance in closed-source models (e.g., GPT-4) and safety moderation models (e.g., Llama-Guard and ShieldGemma, etc.), across a broad range of complex behaviors (e.g., JAILJUDGE benchmark, etc.) to zero-shot scenarios (e.g., other open data, etc.). Importantly, JailBoost and GuardShield, based on JAILJUDGE Guard, can enhance downstream tasks in jailbreak attacks and defenses under zero-shot settings with significant improvement (e.g., JailBoost can increase the average performance  by approximately 29.24\%, while GuardShield can reduce the average defense ASR from 40.46\% to 0.15\%).

## 💡Framework


The JAILJUDGE Benchmark encompasses a wide variety of complex jailbreak scenarios, including multilingual and adversarial prompts, targeting diverse LLM responses for robust safety evaluation.

The JAILJUDGE Data includes over 35k instruction-tune training data and two test sets (4.5k+ broad risk scenarios and 6k+ multilingual examples), providing a rich foundation for comprehensive jailbreak assessments.

The Multi-agent Jailbreak Judge Framework leverages multiple agents (Judging, Voting, and Inference agents) to deliver fine-grained evaluations, reasoning explanations, and jailbroken scores, making the evaluation process explicit and interpretable.


asets

## 👉 Paper
For more details, please refer to our paper [JAILJUDGE](https://arxiv.org/abs/2410.12855).


# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->


# Dataset
We open-sourced two comprehensive datasets, **JAILJUDGETRAIN** and **JAILJUDGETEST**, designed to advance research in the jailbreak evaluation of LLMs. Both datasets encompass a wide range of prompt-response pairs and are structured to support jailbreak evaluation  methods. To assist researchers in replicating our work,  The complete datasets and supporting resources are available on [Hugging Face JAILJUDGE Dataset](https://huggingface.co/datasets/ICLR-Anonymous/JAILJUDGE)


### JAILJUDGETRAIN Dataset

The **JAILJUDGETRAIN** dataset is a comprehensive instruction-tuning dataset derived using our multi-agent jailbreak judge framework, specifically designed to evaluate the jailbreak potential of LLMs. This large-scale dataset consists of over 35,000 items, gathered from diverse sources to capture a wide range of risk and response scenarios. The framework employs multiple LLM-powered agents that collaboratively assess each prompt-response pair, providing detailed scoring and reasoning for each instance.

Each data instance in **JAILJUDGETRAIN** includes the following fields:
- **user_prompt**: The prompt given to the model.
- **model_response**: The model's response to the prompt.
- **hazard_cate_llamaguard3**: The hazard category of the prompt, classified by the LlamaGuard3 model.
- **prompt_complexity_cate**: The complexity category of the prompt, indicating the sophistication level of the input.
- **language**: The language in which the prompt is written.
- **reason**: The explanation or rationale for the score assigned to the model's response, providing insights into why the response may or may not indicate a jailbreak.
- **score**: A numerical score that represents the degree to which the model’s response aligns with jailbreak behavior, as evaluated by the multi-agent framework.

### Prompt Categories in JAILJUDGETRAIN

The dataset contains six distinct categories of prompts, covering a comprehensive range of risk scenarios and complexities:
1. **Vanilla Harmful Prompts**: These include a broad array of risk scenarios.
2. **Synthetic Vanilla Prompts**: Rewritten by LLMs to preserve original meanings with slight modifications.
3. **Synthetic Adversarial Prompts**: Modified prompts created to simulate jailbreak attacks.
4. **In-the-Wild Harmful Prompts**: Real-world user interactions with LLMs.
5. **Deceptive Harmful Prompts**: Complex and refined prompts, automatically generated to challenge jailbreak detection.

### Purpose and Utility

This multi-agent setup not only enhances the accuracy of jailbreak assessment but also ensures a thorough understanding of the reasoning behind each decision. With structured scores and detailed explanations, the **JAILJUDGETRAIN** dataset is a valuable resource for training and fine-tuning models focused on jailbreak detection tasks.


### JAILJUDGETEST
JAILJUDGETEST is a high-quality, human-annotated evaluation set consisting of two subsets: **JAILJUDGE ID** (in-distribution) and **JAILJUDGE OOD** (out-of-distribution). The JAILJUDGE ID set includes over 4,500 prompt-response pairs from JAILJUDGETRAIN (excluding multilingual prompts) and is balanced for various risk scenarios. The JAILJUDGE OOD set, focused on multilingual prompts across ten languages, includes over 6,000 instances. This OOD set is specifically designed to assess the model’s generalizability to diverse linguistic contexts.

### Human Annotation and Quality Assurance
To ensure high-quality labels, we employed a rigorous, multi-stage annotation process:
1. **Annotator Training**: Following MLCommons standards, annotators were trained on policies regarding user privacy, ethics, and morality.
2. **Manual Labeling and GPT-4 Assistance**: Initial labeling was performed manually, with GPT-4 providing secondary labels. Discrepancies between human and GPT-4 annotations were reviewed to ensure accuracy.
3. **Cross-Comparison and Multi-Person Voting**: Final labels were determined by majority voting, involving multiple annotators to increase reliability. 

### Prompt Complexity Categories
We categorize prompts into five complexity levels:
1. **Simple Prompts (Q1)**: Direct, straightforward prompts.
2. **Adversarial Prompts (Q2)**: Generated by jailbreak attacks.
3. **In-the-Wild Prompts (Q3)**: Real-world prompts with adversarial elements.
4. **Synthetic Vanilla Prompts (Q4)**: Rephrased prompts that retain original meanings.
5. **Deceptive Harmful Prompts (Q5)**: Complex, sophisticated prompts combining multiple adversarial techniques.


###  Hazard Categories for  Harmful Prompts

The following table provides a summary of the **hazard categories** for harmful prompts in the JAILJUDGE dataset. Each category code represents a specific type of harmful or sensitive content that the prompts aim to address.

| Category Code | Description                  | Category Code | Description                  |
|---------------|------------------------------|---------------|------------------------------|
| S1            | Violent Crimes               | S8            | Intellectual Property        |
| S2            | Non-Violent Crimes           | S9            | Indiscriminate Weapons       |
| S3            | Sex-Related Crimes           | S10           | Hate                          |
| S4            | Child Sexual Exploitation    | S11           | Suicide & Self-Harm          |
| S5            | Defamation                   | S12           | Sexual Content               |
| S6            | Specialized Advice           | S13           | Elections                    |
| S7            | Privacy                      | S14           | Code Interpreter Abuse       |




# Dataset Card for JAILJUDGE Dataset

## Dataset Summary

The JAILJUDGE dataset is a comprehensive collection of prompt-response pairs specifically curated to evaluate the jailbreak potential of LLMs across multiple scenarios. The dataset is divided into three main files, each with unique attributes to facilitate both in-distribution (ID) and out-of-distribution (OOD) testing. This dataset was constructed using our multi-agent jailbreak judge framework, which enables precise scoring, reasoning, and categorization of each response.

## Files and Fields

### JAILJUDGE_TRAIN.json
- **user_prompt**: The prompt given to the model.
- **model_response**: The model's response to the prompt.
- **hazard_cate_llamaguard3**: The hazard category of the prompt, as classified by the LlamaGuard3 model.
- **prompt_complexity_cate**: The complexity category of the prompt, indicating its sophistication.
- **language**: The language in which the prompt is written.
- **reason**: The rationale or explanation for the score given to the model's response.
- **score**: A numerical score (1-10) indicating the likelihood of jailbreak behavior in the response.

### JAILJUDGE_ID.json
- **user_prompt**: The prompt given to the model.
- **model_response**: The model's response to the prompt.
- **hazard_cate_llamaguard3**: The hazard category of the prompt, as classified by the LlamaGuard3 model.
- **prompt_complexity_cate**: The complexity category of the prompt.
- **language**: The language of the prompt.
- **is_jailbroken**: Boolean value (True or False) indicating whether the model's response was classified as jailbroken.

### JAILJUDGE_OOD.json
- **user_prompt**: The prompt given to the model.
- **model_response**: The model's response to the prompt.
- **hazard_cate_llamaguard3**: The hazard category of the prompt, as classified by the LlamaGuard3 model.
- **prompt_complexity_cate**: The complexity category of the prompt.
- **language**: The language of the prompt.
- **is_jailbroken**: Boolean value (True or False) indicating whether the model's response was classified as jailbroken.

## Detailed Field Descriptions

- **prompt_complexity_cate**: Represents the complexity level of the prompt. The categories are as follows:
  - **Simple Prompts (Q1)**: Direct and straightforward prompts.
  - **Adversarial Prompts (Q2)**: Prompts generated by jailbreak attacks.
  - **In-the-Wild Prompts (Q3)**: Prompts sourced from real-world user interactions.
  - **Synthetic Vanilla Prompts (Q4)**: Rephrased prompts that retain their original meanings.
  - **Deceptive Harmful Prompts (Q5)**: Sophisticated prompts combining multiple adversarial elements.

- **hazard_cate_llamaguard3**: The hazard category of the prompt, based on classifications from the LlamaGuard3 model. This field categorizes prompts into various risk types, such as:
  - **S1**: Violent Crimes
  - **S2**: Non-Violent Crimes
  - **S3**: Sex-Related Crimes
  - **S4**: Child Sexual Exploitation
  - **S5**: Defamation
  - **S6**: Specialized Advice
  - **S7**: Privacy
  - **S8**: Intellectual Property
  - **S9**: Indiscriminate Weapons
  - **S10**: Hate
  - **S11**: Suicide & Self-Harm
  - **S12**: Sexual Content
  - **S13**: Elections
  - **S14**: Code Interpreter Abuse

- **language**: Indicates the language in which the prompt is written. Supported languages include:
  - **en**: English
  - **zh**: Chinese
  - **it**: Italian
  - **vi**: Vietnamese
  - **ar**: Arabic
  - **ko**: Korean
  - **th**: Thai
  - **bn**: Bengali
  - **sw**: Swahili
  - **jv**: Javanese

- **is_jailbroken**: A Boolean value (True or False) that specifies whether the model's response to the prompt was classified as jailbroken.

- **score**: A numerical score ranging from 1 to 10 that indicates the extent to which the model's response is deemed as exhibiting jailbreak behavior, with higher scores representing a greater likelihood of jailbreak.

- **reason**: A detailed explanation justifying the score assigned to the model's response, offering insights into why the response may or may not be considered jailbroken.

## Usage Notes

The JAILJUDGE dataset provides a valuable resource for training and evaluating jailbreak detection models. The structured and annotated fields enable researchers to study various risk scenarios, prompt complexities, and response characteristics across multiple languages and hazard categories.


##  Citation
If you find this project helpful, please consider citing our paper:
```
@misc{liu2024jailjudgecomprehensivejailbreakjudge,
      title={JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework}, 
      author={Fan Liu and Yue Feng and Zhao Xu and Lixin Su and Xinyu Ma and Dawei Yin and Hao Liu},
      year={2024},
      eprint={2410.12855},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.12855}, 
}
```
",High,6.0
Q&A,emhaihsan/quran-indonesia-tafseer-translation,3.0,42.0,2024-10-05 07:41:14+00:00,mit,9.0,25.5 MB,26738688.0,5.31 MB,5567938.56,25740,none,none,"---
license: mit
task_categories:
- question-answering
language:
- id
- ar
pretty_name: Quran-Indo
---",Low,1.0
Q&A,Jr23xd23/Arabic_LLaMA_Math_Dataset,3.0,26.0,2024-10-07 20:37:50+00:00,cc0-1.0,1.0,10.9 MB,11429478.4,5.28 MB,5536481.28,12500,none,none,"---
license: cc0-1.0
task_categories:
- question-answering
language:
- ar
tags:
- Math
- Arabic
size_categories:
- 10K<n<100K
---
# Arabic LLaMA Math Dataset

## Example Entries
![image](https://github.com/user-attachments/assets/0fd1002e-ec9d-48d9-b4b8-8ac0790bc191)

## Table of Contents
- [Dataset Overview](#dataset-overview)
- [Dataset Structure](#dataset-structure)
- [Dataset Description](#dataset-description)
- [License](#license)
- [Acknowledgments](#acknowledgments)

## Dataset Overview

- **Dataset Name:** Arabic_LLaMA_Math_Dataset.csv
- **Number of Records:** 12,496
- **Number of Columns:** 3
- **File Format:** CSV

## Dataset Structure

### Columns:
1. **Instruction**: The problem statement or question (text, in Arabic)
2. **Input**: Additional input for model fine-tuning (empty in this dataset)
3. **Solution**: The solution or answer to the problem (text, in Arabic)

## Dataset Description

The **Arabic LLaMA Math Dataset** is a comprehensive collection of mathematical problems and their solutions formulated in Arabic. This dataset is specifically designed to facilitate the training and fine-tuning of large language models, particularly those based on the LLaMA architecture, for Arabic language processing and mathematical reasoning tasks.

### Dataset Content:
- Different mathematical topics covered, including:
  - Basic arithmetic
  - Algebra
  - Geometry
  - Probability
  - Combinatorics
- Problems presented in natural language (Arabic), mimicking real-world question formats
- Solutions provided for each problem, allowing for supervised learning approaches




## Citation

If you use this dataset in your research, please cite it as follows:

```bibtex
@dataset{Arabic_LLaMA_Math_Dataset,
  title = {Arabic LLaMA Math Dataset},
  author = {Jaber Jaber},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/jaberjaber23/Arabic_LLaMA_Math_Dataset},
  version = {1.0}
}
```

## License

This dataset is released under the [CC0 1.0 Universal (CC0 1.0) Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/). This means you can copy, modify, distribute, and perform the work, even for commercial purposes, all without asking permission.",High,5.0
Q&A,QCRI/MultiNativQA,0.0,443.0,2024-10-25 10:59:30+00:00,cc-by-nc-sa-4.0,0.0,64.2 MB,67318579.2,23.3 MB,24431820.8,63757,https://arxiv.org/abs/2407.09823,none,"---
license: cc-by-nc-sa-4.0
task_categories:
  - question-answering
language:
  - ar
  - asm
  - bn
  - en
  - hi
  - ne
  - tr
tags:
  - question-answering
  - cultural-aligned
pretty_name: 'MultiNativQA -- Multilingual Native and Culturally Aligned QA'
size_categories:
  - 10K<n<100K
dataset_info:
  - config_name: Arabic
    splits:
      - name: train
        num_examples: 3649
      - name: dev
        num_examples: 492
      - name: test
        num_examples: 988
  - config_name: Assamese
    splits:
      - name: train
        num_examples: 1131
      - name: dev
        num_examples: 157
      - name: test
        num_examples: 545
  - config_name: Bangla-BD
    splits:
      - name: train
        num_examples: 7018
      - name: dev
        num_examples: 953
      - name: test
        num_examples: 1521
  - config_name: Bangla-IN
    splits:
      - name: train
        num_examples: 6891
      - name: dev
        num_examples: 930
      - name: test
        num_examples: 2146
  - config_name: English-BD
    splits:
      - name: train
        num_examples: 4761
      - name: dev
        num_examples: 656
      - name: test
        num_examples: 1113
  - config_name: English-QA
    splits:
      - name: train
        num_examples: 8212
      - name: dev
        num_examples: 1164
      - name: test
        num_examples: 2322
  - config_name: Hindi
    splits:  
      - name: train
        num_examples: 9288
      - name: dev
        num_examples: 1286
      - name: test
        num_examples: 2745
  - config_name: Nepali
    splits:
      - name: test
        num_examples: 561
  - config_name: Turkish
    splits:
      - name: train
        num_examples: 3527
      - name: dev
        num_examples: 483
      - name: test
        num_examples:  1218
configs:
  - config_name: arabic_qa
    data_files:
      - split: train
        path: arabic_qa/NativQA_ar_msa_qa_train.json
      - split: dev
        path: arabic_qa/NativQA_ar_msa_qa_dev.json
      - split: test
        path: arabic_qa/NativQA_ar_msa_qa_test.json
  - config_name: assamese_in
    data_files:
      - split: train
        path: assamese_in/NativQA_asm_NA_in_train.json
      - split: dev
        path: assamese_in/NativQA_asm_NA_in_dev.json
      - split: test
        path: assamese_in/NativQA_asm_NA_in_test.json
  - config_name: bangla_bd
    data_files:
      - split: train
        path: bangla_bd/NativQA_bn_scb_bd_train.json
      - split: dev
        path: bangla_bd/NativQA_bn_scb_bd_dev.json
      - split: test
        path: bangla_bd/NativQA_bn_scb_bd_test.json
  - config_name: bangla_in
    data_files:
      - split: train
        path: bangla_in/NativQA_bn_scb_in_train.json
      - split: dev
        path: bangla_in/NativQA_bn_scb_in_dev.json
      - split: test
        path: bangla_in/NativQA_bn_scb_in_test.json
  - config_name: english_bd
    data_files:
      - split: train
        path: english_bd/NativQA_en_NA_bd_train.json
      - split: dev
        path: english_bd/NativQA_en_NA_bd_dev.json
      - split: test
        path: english_bd/NativQA_en_NA_bd_test.json
  - config_name: english_qa
    data_files:
      - split: train
        path: english_qa/NativQA_en_NA_qa_train.json
      - split: dev
        path: english_qa/NativQA_en_NA_qa_dev.json
      - split: test
        path: english_qa/NativQA_en_NA_qa_test.json
  - config_name: hindi_in
    data_files:
      - split: train
        path: hindi_in/NativQA_hi_NA_in_train.json
      - split: dev
        path: hindi_in/NativQA_hi_NA_in_dev.json
      - split: test
        path: hindi_in/NativQA_hi_NA_in_test.json
  - config_name: nepali_np
    data_files:
      - split: test
        path: nepali_np/NativQA_ne_NA_np_test.json
  - config_name: turkish_tr
    data_files:
      - split: train
        path: turkish_tr/NativQA_tr_NA_tr_train.json
      - split: dev
        path: turkish_tr/NativQA_tr_NA_tr_dev.json
      - split: test
        path: turkish_tr/NativQA_tr_NA_tr_test.json
---

# MultiNativQA: Multilingual Culturally-Aligned Natural Queries For LLMs

### Overview
The **MultiNativQA** dataset is a multilingual, native, and culturally aligned question-answering resource. It spans 7 languages, ranging from high- to extremely low-resource, and covers 9 different locations/cities. To capture linguistic diversity, the dataset includes several dialects for dialect-rich languages like Arabic. In addition to Modern Standard Arabic (MSA), **MultiNativQA** features six Arabic dialects — *Egyptian, Jordanian, Khaliji, Sudanese, Tunisian*, and *Yemeni*.

The dataset also provides two linguistic variations of Bangla, reflecting differences between speakers in *Bangladesh* and *West Bengal, India*. Additionally, **MultiNativQA** includes English queries from *Dhaka* and *Doha*, where English is commonly used as a second language, as well as from *New York, USA*.

The QA pairs in this dataset cover 18 diverse topics, including: *Animals, Business, Clothing, Education, Events, Food & Drinks, General, Geography, Immigration, Language, Literature, Names & Persons, Plants, Religion, Sports & Games, Tradition, Travel*, and *Weather*.

**MultiNativQA** is designed to evaluate and fine-tune large language models (LLMs) for long-form question answering while assessing their cultural adaptability and understanding.

### Directory Structure (JSON files only)
The dataset is organized into directories based on language and region. Each directory contains JSON files for the train, development, and test sets, with the exception of Nepali, which consists of only a test set.

- `arabic_qa/`
    - `NativQA_ar_msa_qa_dev.json`
    - `NativQA_ar_msa_qa_test.json`
    - `NativQA_ar_msa_qa_train.json`
- `assamese_in/`
    - `NativQA_asm_NA_in_dev.json`
    - `NativQA_asm_NA_in_test.json`
    - `NativQA_asm_NA_in_train.json`
- `bangla_bd/`
    - `NativQA_bn_scb_bd_dev.json`
    - `NativQA_bn_scb_bd_test.json`
    - `NativQA_bn_scb_bd_train.json`
- `bangla_in/`
    - `NativQA_bn_scb_in_dev.json`
    - `NativQA_bn_scb_in_test.json`
    - `NativQA_bn_scb_in_train.json`
- `english_bd/`
    - `NativQA_en_NA_bd_dev.json`
    - `NativQA_en_NA_bd_test.json`
    - `NativQA_en_NA_bd_train.json`
- `english_qa/`
    - `NativQA_en_NA_qa_dev.json`
    - `NativQA_en_NA_qa_test.json`
    - `NativQA_en_NA_qa_train.json`
- `hindi_in/`
    - `NativQA_hi_NA_in_dev.json`
    - `NativQA_hi_NA_in_test.json`
    - `NativQA_hi_NA_in_train.json`
- `nepali_np/`
    - `NativQA_ne_NA_np_test.json`
- `turkish_tr/`
    - `NativQA_tr_NA_tr_dev.json`
    - `NativQA_tr_NA_tr_test.json`
    - `NativQA_tr_NA_tr_train.json`


#### Example of a data
```
{
    ""data_id"": ""cf92ec1e52b4b3071d263a1063b43928"",
    ""category"": ""immigration"",
    ""input_query"": ""How long can you stay in Qatar on a visitors visa?"",
    ""question"": ""Can I extend my tourist visa in Qatar?"",
    ""is_reliable"": ""very_reliable"",
    ""answer"": ""If you would like to extend your visa, you will need to proceed to immigration headquarters in Doha prior to the expiry of your visa and apply there for an extension."",
    ""source_answer_url"": ""https://hayya.qa/en/web/hayya/faq""
}
```
##### Field Descriptions:
- **`data_id`**: Unique identifier for each data entry.
- **`category`**: General topic or category of the query (e.g., ""health"", ""religion"").
- **`input_query`**: The original user-submitted query.
- **`question`**: The formalized question derived from the input query.
- **`is_reliable`**: Indicates the reliability of the provided answer (`""very_reliable""`, `""somewhat_reliable""`, `""unreliable""`).
- **`answer`**: The system-provided answer to the query.
- **`source_answer_url`**: URL of the source from which the answer was derived.


### Statistics
Distribution of the **MultiNativQA** dataset across different languages.
<p align=""left""> <img src=""./language_donut_chart.png"" style=""width: 60%;"" id=""title-icon""> </p>

This dataset consists of two types of data: annotated and un-annotated. We considered the un-annotated data as additional data. Please find the data statistics below:

Statistics of our **MultiNativQA** dataset including languages with the final annotated QA pairs from different location.

| Language    | City       | Train   | Dev   | Test   | Total  |
|-------------|------------|---------|-------|--------|--------|
| Arabic      | Doha       | 3,649   | 492   | 988    | 5,129  |
| Assamese    | Assam      | 1,131   | 157   | 545    | 1,833  |
| Bangla      | Dhaka      | 7,018   | 953   | 1,521  | 9,492  |
| Bangla      | Kolkata    | 6,891   | 930   | 2,146  | 9,967  |
| English     | Dhaka      | 4,761   | 656   | 1,113  | 6,530  |
| English     | Doha       | 8,212   | 1,164 | 2,322  | 11,698 |
| Hindi       | Delhi      | 9,288   | 1,286 | 2,745  | 13,319 |
| Nepali      | Kathmandu  | --      | --    | 561    | 561    |
| Turkish     | Istanbul   | 3,527   | 483   | 1,218  | 5,228  |
| **Total**   |            | **44,477** | **6,121** | **13,159** | **63,757** |



We provide the un-annotated additional data stats below:

| Language-Location 	     | # of QA 	     |
|-------------------------|---------------|
| Arabic-Egypt      	     | 7,956   	     |
| Arabic-Palestine  	     | 5,679   	     |
| Arabic-Sudan      	     | 4,718   	     |
| Arabic-Syria      	     | 11,288  	     |
| Arabic-Tunisia    	     | 14,789  	     |
| Arabic-Yemen      	     | 4,818   	     |
| English-New York  	     | 6,454   	     |
| **Total**             	 | **55,702**  	 |

### How to download data

```
import os
import json
from datasets import load_dataset


dataset_names = ['arabic_qa', 'assamese_in', 'bangla_bd', 'bangla_in', 'english_bd',
                'english_qa', 'hindi_in', 'nepali_np', 'turkish_tr']
base_dir=""./MNQA/""

for dname in dataset_names:    
    output_dir = os.path.join(base_dir, dname)
    # load each language
    dataset = load_dataset(""QCRI/MultiNativQA"", name=dname)
    # Save the dataset to the specified directory. This will save all splits to the output directory.
    dataset.save_to_disk(output_dir)
    # iterate over splits to save the data into json format
    for split in ['train','dev','test']:
        data = []
        if split not in dataset:
            continue
        for idx, item in enumerate(dataset[split]):
            data.append(item)
        output_file = os.path.join(output_dir, f""{split}.json"")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
```

### License
The dataset is distributed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0). The full license text can be found in the accompanying licenses_by-nc-sa_4.0_legalcode.txt file.

### Contact & Additional Information
For more details, please visit our [official website](http://nativqa.gitlab.io/).

### Citation
You can access the full paper [here](https://arxiv.org/pdf/2407.09823).

```
@article{hasan2024nativqa,
      title={NativQA: Multilingual Culturally-Aligned Natural Query for LLMs},
      author={Hasan, Md Arid and Hasanain, Maram and Ahmad, Fatema and Laskar, Sahinur Rahman and Upadhyay, Sunaya and Sukhadia, Vrunda N and Kutlu, Mucahid and Chowdhury, Shammur Absar and Alam, Firoj},
      journal={arXiv preprint arXiv:2407.09823},
      year={2024}
      publisher={arXiv:2407.09823},
      url={https://arxiv.org/abs/2407.09823},
}
```
",High,5.0
Q&A,Hezam/AHD,0.0,37.0,2024-10-12 18:04:53+00:00,cc-by-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: cc-by-4.0
task_categories:
- text-classification
- question-answering
- text-generation
- text2text-generation
tags:
- health
- healthcare
- Medical Assistant
- Chatbot
- Text classification
language:
- ar
- en
size_categories:
- 100K<n<1M
---

### AHD: Arabic healthcare dataset


### Dataset Description

- Paper: [AHD: Arabic healthcare dataset](https://doi.org/10.1016/j.dib.2024.110855)
- Journal: [Data in Brief](https://www.data-in-brief.com/)
- Publication year: October 2024
- Repository: [Mendeley Data](http://data.mendeley.com/datasets/mgj29ndgrk/5)

### Dataset Description

#### Title:""AHD: Arabic healthcare dataset""
#### Paper: [AHD: Arabic healthcare dataset](https://doi.org/10.1016/j.dib.2024.110855)
#### Journal: [Data in Brief](https://www.data-in-brief.com/)
#### Publication year: October 2024
#### Repository: [Mendeley Data](http://data.mendeley.com/datasets/mgj29ndgrk/5)

### Languages

- The text in the dataset is in English.
- The text in the dataset is in Arabic.

### Specifications

- Subject:	Computer Science, Data Science, Health and medical sciences.
- Specific subject area:	Arabic Language, Machine Learning, Health Informatics, Natural Language Processing, Text Generation, Text classification.
- Type of data:	Text/String
- Data Format:	Raw
- Data collection:	The dataset was collected from the Altibbi website using web-scraping tools. Python, with the Requests and BeautifulSoup packages, was used to collect the data.
- Size: Consists of over  808k rows 90 variety categories.

### Dataset Summary

With the soaring demand for healthcare systems, chatbots are gaining tremendous popularity and research attention. Numerous language-centric research on healthcare is conducted day by day. Despite significant advances in Arabic Natural Language Processing (NLP), challenges remain in natural language classification and generation due to the lack of suitable datasets. The primary shortcoming of these models is the lack of suitable Arabic datasets for training. To address this, authors introduce a large Arabic Healthcare Dataset (AHD) of textual data. The dataset consists of over 808k questions and answers across 90 categories, offered to the research community for Arabic computational linguistics. Authors anticipate that this rich dataset would make a great aid for a variety of NLP tasks on Arabic textual data, especially for text classification and generation purposes. Authors present the data in raw form. AHD is composed of main dataset scraped from medical website, which is Altibbi website. AHD is made public and freely available at http://data.mendeley.com/datasets/mgj29ndgrk/5.

### Value of the Data

- AHD is the largest, to our knowledge, available and representative Arabic Healthcare Dataset (AHD) for a wide variety category.
- AHD offers up to ninety distinct categories, making it robust for accurate text categorization.
- AHD offers over 808k distinct questions and answers, making it robust for accurate healthcare systems and chatbots.
- In contrast with the few small available datasets, AHD's size makes it a suitable corpus for implementing both classical as well as deep learning models.

### Supported Tasks and Leaderboards

The dataset supports the task of text generation, Text classification, Chatbot, Question answering and Word embedding.

## Dataset Structure

## Source Data

The data was sourced from  online counseling and therapy platform. The raw data can be found [altibbi](https://altibbi.com/).

### Data Files

- 'AHD.xlsx:' file contains dataset in excel format, which includes the question, answer, and category in Arabic.
- 'AHD_english.xlsx:' file contains dataset in excel format, which includes the question, answer, and category translated to English.
- 'Distribution of Question and Answer per category.xlsex:'  shows the distribution of the data set by category.
  
### Data Instances

A data instance includes a 'Question', 'Answer' and 'Category'.
'Question' contains the question asked by a use , 'Answer' contains the corresponding answer provided by a docto , and 'Category' lable for question and answer from 90 variety categories.

### Data Fields

- 'Question': a string containing the question asked by a user
- 'Answer': a string containing the corresponding answer provided
- 'Category': a lable for questions from 90 variety categories.

### Data Splits

- The dataset has no predefined splits. 
- The dataset adopted the annotation of each question as it appeared on its source website, Altibbi. 
- Users can create their own splits as needed.

### Distribution of Question and Answer

The AHD consists of more than 808k rows 90 variety categories

### Ethics Statement

- 'Terms of Service (ToS):' Authors have considered and followed the source website Altibbi's ToS, privacy laws, and user consents.
- 'Privacy:' The authors have anonymized all participant data and confirm that all the data is non-sensitive.
- 'Scraping policies:' There are not specific scraping policies for the source website Altibbi.
- 'Copyright:' The authors have read and followed the ethical requirements for publication in Data in Brief and confirmed that the current work does not involve any type of human studies, animal studies, or data gathered using social media. All data belongs to the source website Altibbi through user consent, and it's open to the public, so ethical approval has not been sought. The data adopted the annotation of each question and answer as appeared on the source website Altibbi and the distribution of data per category. We can confirm that this manuscript adheres to ethical publishing standards.",Medium,3.0
Q&A,FreedomIntelligence/ApolloMoEDataset,5.0,319.0,2024-10-18 02:52:33+00:00,mit,34.0,UNKNOWN,unknown,259 MB,unknown,292914,https://arxiv.org/abs/2410.10626,none,"---
license: mit
configs:
  - config_name: pretrain_text
    data_files:
      - split: train
        path: ApolloMoEDataset_sample.json
task_categories:
- question-answering
tags:
- biology
- medical
language:
- ar
- en
- zh
- ko
- ja
- mn
- th
- vi
- lo
- mg
- de
- pt
- es
- fr
- ru
- it
- hr
- gl
- cs
- co
- la
- uk
- bs
- bg
- eo
- sq
- da
- sa
- no
- gn
- sr
- sk
- gd
- lb
- hi
- ku
- mt
- he
- ln
- bm
- sw
- ig
- rw
- ha
pretty_name: apollomoe
size_categories:
- 1B<n<10B
---

# Democratizing Medical LLMs For Much More Languages

Covering 12 Major Languages including English, Chinese, French, Hindi, Spanish, Arabic, Russian, Japanese, Korean, German, Italian, Portuguese and 38 Minor Languages So far.



<p align=""center"">
   📃 <a href=""https://arxiv.org/abs/2410.10626"" target=""_blank"">Paper</a> • 🌐 <a href="""" target=""_blank"">Demo</a> • 🤗 <a href=""https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEDataset"" target=""_blank"">ApolloMoEDataset</a> • 🤗 <a href=""https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEBench"" target=""_blank"">ApolloMoEBench</a>  • 🤗 <a href=""https://huggingface.co/collections/FreedomIntelligence/apollomoe-and-apollo2-670ddebe3bb1ba1aebabbf2c"" target=""_blank"">Models</a>  •🌐 <a href=""https://github.com/FreedomIntelligence/Apollo"" target=""_blank"">Apollo</a>  • 🌐 <a href=""https://github.com/FreedomIntelligence/ApolloMoE"" target=""_blank"">ApolloMoE</a>
</p>



![Apollo](assets/apollo_medium_final.png)


## 🌈 Update

* **[2024.10.15]** ApolloMoE repo is published！🎉


## Languages Coverage
12 Major Languages and 38 Minor Languages

<details>
  <summary>Click to view the Languages Coverage</summary>
   
   ![ApolloMoE](assets/languages.png)

</details>


## Architecture

<details>
  <summary>Click to view the MoE routing image</summary>

  ![ApolloMoE](assets/hybrid_routing.png)

</details>

## Results

#### Dense
   🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo2-0.5B"" target=""_blank"">Apollo2-0.5B</a>  • 🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo2-1.5B"" target=""_blank"">Apollo2-1.5B</a>  • 🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo2-2B"" target=""_blank"">Apollo2-2B</a>  
   
   🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo2-3.8B"" target=""_blank"">Apollo2-3.8B</a>  • 🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo2-7B"" target=""_blank"">Apollo2-7B</a>  • 🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo2-9B"" target=""_blank"">Apollo2-9B</a>  

<details>
  <summary>Click to view the Dense Models Results</summary>

   ![ApolloMoE](assets/dense_results.png)

</details>


#### Post-MoE
   🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo-MoE-0.5B"" target=""_blank"">Apollo-MoE-0.5B</a>  • 🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo-MoE-1.5B"" target=""_blank"">Apollo-MoE-1.5B</a>  • 🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo-MoE-7B"" target=""_blank"">Apollo-MoE-7B</a>  

<details>
  <summary>Click to view the Post-MoE Models Results</summary>

   ![ApolloMoE](assets/post_moe_results.png)

</details>

   
  

## Usage Format
##### Apollo2
- 0.5B, 1.5B, 7B: User:{query}\nAssistant:{response}<|endoftext|>
- 2B, 9B: User:{query}\nAssistant:{response}\<eos\>
- 3.8B: <|user|>\n{query}<|end|><|assisitant|>\n{response}<|end|>

##### Apollo-MoE
- 0.5B, 1.5B, 7B: User:{query}\nAssistant:{response}<|endoftext|>
  
## Dataset & Evaluation

- Dataset
  🤗 <a href=""https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEDataset"" target=""_blank"">ApolloMoEDataset</a>

   <details><summary>Click to expand</summary>

    ![ApolloMoE](assets/Dataset.png)

   </details>

   The complete data is stored in `ApolloMoEDataset.json`, while a sample shown in `ApolloMoEDataset_sample.json`

- Evaluation
  🤗 <a href=""https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEBench"" target=""_blank"">ApolloMoEBench</a> 

   <details><summary>Click to expand</summary>
  
     - EN:
       - [MedQA-USMLE](https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options) 
       - [MedMCQA](https://huggingface.co/datasets/medmcqa/viewer/default/test)
       - [PubMedQA](https://huggingface.co/datasets/pubmed_qa): Because the results fluctuated too much, they were not used in the paper.
       - [MMLU-Medical](https://huggingface.co/datasets/cais/mmlu)
         - Clinical knowledge, Medical genetics, Anatomy, Professional medicine, College biology, College medicine
     - ZH:
       - [MedQA-MCMLE](https://huggingface.co/datasets/bigbio/med_qa/viewer/med_qa_zh_4options_bigbio_qa/test)
       - [CMB-single](https://huggingface.co/datasets/FreedomIntelligence/CMB): Not used in the paper
         - Randomly sample 2,000 multiple-choice questions with single answer.
       - [CMMLU-Medical](https://huggingface.co/datasets/haonan-li/cmmlu)
         - Anatomy, Clinical_knowledge, College_medicine, Genetics, Nutrition, Traditional_chinese_medicine, Virology
       - [CExam](https://github.com/williamliujl/CMExam): Not used in the paper
         - Randomly sample 2,000 multiple-choice questions


     - ES: [Head_qa](https://huggingface.co/datasets/head_qa)
     - FR:
       - [Frenchmedmcqa](https://github.com/qanastek/FrenchMedMCQA)
       - [MMLU_FR]
         - Clinical knowledge, Medical genetics, Anatomy, Professional medicine, College biology, College medicine
     - HI: [MMLU_HI](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Hindi)
        - Clinical knowledge, Medical genetics, Anatomy, Professional medicine, College biology, College medicine
     - AR: [MMLU_AR](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Arabic)
        - Clinical knowledge, Medical genetics, Anatomy, Professional medicine, College biology, College medicine
     - JA: [IgakuQA](https://github.com/jungokasai/IgakuQA)
     - KO: [KorMedMCQA](https://huggingface.co/datasets/sean0042/KorMedMCQA)
     - IT:
       - [MedExpQA](https://huggingface.co/datasets/HiTZ/MedExpQA)
       - [MMLU_IT]
         - Clinical knowledge, Medical genetics, Anatomy, Professional medicine, College biology, College medicine
     - DE: [BioInstructQA](https://huggingface.co/datasets/BioMistral/BioInstructQA): German part
     - PT: [BioInstructQA](https://huggingface.co/datasets/BioMistral/BioInstructQA): Portuguese part
     - RU: [RuMedBench](https://github.com/sb-ai-lab/MedBench)
     - Minor Langs: MMLU Translated Medical Part 

  


   </details>


## Results reproduction
   <details><summary>Click to expand</summary>


   We take Apollo2-7B or Apollo-MoE-0.5B as example
   1. Download Dataset for project:

      ```
      bash 0.download_data.sh  
      ```
    
   2. Prepare test and dev data for specific model:

      
      - Create test data for with special token
        
       ```
       bash 1.data_process_test&dev.sh
       ```
    
   3. Prepare train data for specific model (Create tokenized data in advance):

    
      - You can adjust data Training order and Training Epoch in this step

       ```
       bash 2.data_process_train.sh
       ```
    
   4. Train the model

    
      - If you want to train in Multi Nodes please refer to ./src/sft/training_config/zero_multi.yaml


       ```
       bash 3.single_node_train.sh
       ```


   5. Evaluate your model: Generate score for benchmark
      
         ```
         bash 4.eval.sh
         ```

   </details>



##  Citation
Please use the following citation if you intend to use our dataset for training or evaluation:

```
@misc{zheng2024efficientlydemocratizingmedicalllms,
      title={Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts}, 
      author={Guorui Zheng and Xidong Wang and Juhao Liang and Nuo Chen and Yuping Zheng and Benyou Wang},
      year={2024},
      eprint={2410.10626},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.10626}, 
}
```
",High,5.0
Q&A,FreedomIntelligence/ApolloMoEBench,0.0,111.0,2024-10-15 08:38:05+00:00,mit,0.0,24.4 MB,25585254.4,9.61 MB,10076815.36,45663,https://arxiv.org/abs/2410.10626,none,"---
license: mit
configs:
  - config_name: test_text
    data_files:
      - split: test
        path: ApolloMoEBench.json
task_categories:
- question-answering
tags:
- biology
- medical
language:
- ar
- en
- zh
- ko
- ja
- mn
- th
- vi
- lo
- mg
- de
- pt
- es
- fr
- ru
- it
- hr
- gl
- cs
- co
- la
- uk
- bs
- bg
- eo
- sq
- da
- sa
- no
- gn
- sr
- sk
- gd
- lb
- hi
- ku
- mt
- he
- ln
- bm
- sw
- ig
- rw
- ha
---
# Democratizing Medical LLMs For Much More Languages

Covering 12 Major Languages including English, Chinese, French, Hindi, Spanish, Arabic, Russian, Japanese, Korean, German, Italian, Portuguese and 38 Minor Languages So far.



<p align=""center"">
   📃 <a href=""https://arxiv.org/abs/2410.10626"" target=""_blank"">Paper</a> • 🌐 <a href="""" target=""_blank"">Demo</a> • 🤗 <a href=""https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEDataset"" target=""_blank"">ApolloMoEDataset</a> • 🤗 <a href=""https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEBench"" target=""_blank"">ApolloMoEBench</a>  • 🤗 <a href=""https://huggingface.co/collections/FreedomIntelligence/apollomoe-and-apollo2-670ddebe3bb1ba1aebabbf2c"" target=""_blank"">Models</a>  •🌐 <a href=""https://github.com/FreedomIntelligence/Apollo"" target=""_blank"">Apollo</a>  • 🌐 <a href=""https://github.com/FreedomIntelligence/ApolloMoE"" target=""_blank"">ApolloMoE</a>
</p>



![Apollo](assets/apollo_medium_final.png)


## 🌈 Update

* **[2024.10.15]** ApolloMoE repo is published！🎉


## Languages Coverage
12 Major Languages and 38 Minor Languages

<details>
  <summary>Click to view the Languages Coverage</summary>
   
   ![ApolloMoE](assets/languages.png)

</details>


## Architecture

<details>
  <summary>Click to view the MoE routing image</summary>

  ![ApolloMoE](assets/hybrid_routing.png)

</details>

## Results

#### Dense
   🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo2-0.5B"" target=""_blank"">Apollo2-0.5B</a>  • 🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo2-1.5B"" target=""_blank"">Apollo2-1.5B</a>  • 🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo2-2B"" target=""_blank"">Apollo2-2B</a>  
   
   🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo2-3.8B"" target=""_blank"">Apollo2-3.8B</a>  • 🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo2-7B"" target=""_blank"">Apollo2-7B</a>  • 🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo2-9B"" target=""_blank"">Apollo2-9B</a>  

<details>
  <summary>Click to view the Dense Models Results</summary>

   ![ApolloMoE](assets/dense_results.png)

</details>


#### Post-MoE
   🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo-MoE-0.5B"" target=""_blank"">Apollo-MoE-0.5B</a>  • 🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo-MoE-1.5B"" target=""_blank"">Apollo-MoE-1.5B</a>  • 🤗 <a href=""https://huggingface.co/FreedomIntelligence/Apollo-MoE-7B"" target=""_blank"">Apollo-MoE-7B</a>  

<details>
  <summary>Click to view the Post-MoE Models Results</summary>

   ![ApolloMoE](assets/post_moe_results.png)

</details>

   
  

## Usage Format
##### Apollo2
- 0.5B, 1.5B, 7B: User:{query}\nAssistant:{response}<|endoftext|>
- 2B, 9B: User:{query}\nAssistant:{response}\<eos\>
- 3.8B: <|user|>\n{query}<|end|><|assisitant|>\n{response}<|end|>

##### Apollo-MoE
- 0.5B, 1.5B, 7B: User:{query}\nAssistant:{response}<|endoftext|>
  
## Dataset & Evaluation

- Dataset
  🤗 <a href=""https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEDataset"" target=""_blank"">ApolloMoEDataset</a>

   <details><summary>Click to expand</summary>

    ![ApolloMoE](assets/Dataset.png)

    - [Data category](https://huggingface.co/datasets/FreedomIntelligence/ApolloCorpus/tree/main/train)


   </details>

- Evaluation
  🤗 <a href=""https://huggingface.co/datasets/FreedomIntelligence/ApolloMoEBench"" target=""_blank"">ApolloMoEBench</a> 

   <details><summary>Click to expand</summary>
  
     - EN:
       - [MedQA-USMLE](https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options) 
       - [MedMCQA](https://huggingface.co/datasets/medmcqa/viewer/default/test)
       - [PubMedQA](https://huggingface.co/datasets/pubmed_qa): Because the results fluctuated too much, they were not used in the paper.
       - [MMLU-Medical](https://huggingface.co/datasets/cais/mmlu)
         - Clinical knowledge, Medical genetics, Anatomy, Professional medicine, College biology, College medicine
     - ZH:
       - [MedQA-MCMLE](https://huggingface.co/datasets/bigbio/med_qa/viewer/med_qa_zh_4options_bigbio_qa/test)
       - [CMB-single](https://huggingface.co/datasets/FreedomIntelligence/CMB): Not used in the paper
         - Randomly sample 2,000 multiple-choice questions with single answer.
       - [CMMLU-Medical](https://huggingface.co/datasets/haonan-li/cmmlu)
         - Anatomy, Clinical_knowledge, College_medicine, Genetics, Nutrition, Traditional_chinese_medicine, Virology
       - [CExam](https://github.com/williamliujl/CMExam): Not used in the paper
         - Randomly sample 2,000 multiple-choice questions


     - ES: [Head_qa](https://huggingface.co/datasets/head_qa)
     - FR:
       - [Frenchmedmcqa](https://github.com/qanastek/FrenchMedMCQA)
       - [MMLU_FR]
         - Clinical knowledge, Medical genetics, Anatomy, Professional medicine, College biology, College medicine
     - HI: [MMLU_HI](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Hindi)
        - Clinical knowledge, Medical genetics, Anatomy, Professional medicine, College biology, College medicine
     - AR: [MMLU_AR](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Arabic)
        - Clinical knowledge, Medical genetics, Anatomy, Professional medicine, College biology, College medicine
     - JA: [IgakuQA](https://github.com/jungokasai/IgakuQA)
     - KO: [KorMedMCQA](https://huggingface.co/datasets/sean0042/KorMedMCQA)
     - IT:
       - [MedExpQA](https://huggingface.co/datasets/HiTZ/MedExpQA)
       - [MMLU_IT]
         - Clinical knowledge, Medical genetics, Anatomy, Professional medicine, College biology, College medicine
     - DE: [BioInstructQA](https://huggingface.co/datasets/BioMistral/BioInstructQA): German part
     - PT: [BioInstructQA](https://huggingface.co/datasets/BioMistral/BioInstructQA): Portuguese part
     - RU: [RuMedBench](https://github.com/sb-ai-lab/MedBench)
     - Minor Langs: MMLU Translated Medical Part 

  


   </details>


## Results reproduction
   <details><summary>Click to expand</summary>


   We take Apollo2-7B or Apollo-MoE-0.5B as example
   1. Download Dataset for project:

      ```
      bash 0.download_data.sh  
      ```
    
   2. Prepare test and dev data for specific model:

      
      - Create test data for with special token
        
       ```
       bash 1.data_process_test&dev.sh
       ```
    
   3. Prepare train data for specific model (Create tokenized data in advance):

    
      - You can adjust data Training order and Training Epoch in this step

       ```
       bash 2.data_process_train.sh
       ```
    
   4. Train the model

    
      - If you want to train in Multi Nodes please refer to ./src/sft/training_config/zero_multi.yaml


       ```
       bash 3.single_node_train.sh
       ```


   5. Evaluate your model: Generate score for benchmark
      
         ```
         bash 4.eval.sh
         ```

   </details>



##  Citation
Please use the following citation if you intend to use our dataset for training or evaluation:

```
@misc{zheng2024efficientlydemocratizingmedicalllms,
      title={Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts}, 
      author={Guorui Zheng and Xidong Wang and Juhao Liang and Nuo Chen and Yuping Zheng and Benyou Wang},
      year={2024},
      eprint={2410.10626},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.10626}, 
}
```",High,5.0
Q&A,neulab/PangeaInstruct,85.0,500.0,2025-02-02 16:40:32+00:00,apache-2.0,6.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2410.16153,none,"---
license: apache-2.0
task_categories:
- visual-question-answering
- question-answering
language:
- am
- ar
- bg
- bn
- cs
- de
- el
- en
- es
- fa
- fr
- ga
- hi
- id
- ig
- it
- iw
- ja
- jv
- ko
- nl
- mn
- ms
- no
- pl
- pt
- ro
- ru
- si
- su
- sw
- ta
- te
- th
- tr
- uk
- ur
- vi
- zh
tags:
- multilingual
- multimodal
pretty_name: PangeaIns
size_categories:
- 1M<n<10M
viewer: false
---
# PangeaInstruct

[Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages](https://neulab.github.io/Pangea/)

🇪🇹 🇸🇦 🇧🇬 🇧🇩 🇨🇿 🇩🇪 🇬🇷 🇬🇧 🇺🇸 🇪🇸 🇮🇷 🇫🇷 🇮🇪 🇮🇳 🇮🇩 🇳🇬 🇮🇹 🇮🇱 🇯🇵 🇮🇩 🇰🇷 🇳🇱 🇲🇳 🇲🇾 🇳🇴 🇵🇱 🇵🇹 🇧🇷 🇷🇴 🇷🇺 🇱🇰 🇮🇩 🇰🇪 🇹🇿 🇱🇰 🇮🇳 🇮🇳 🇹🇭 🇹🇷 🇺🇦 🇵🇰 🇮🇳 🇻🇳 🇨🇳 🇹🇼

[🏠 Homepage](https://neulab.github.io/Pangea/) | [🤖 Pangea-7B](https://huggingface.co/neulab/Pangea-7B) | [📊 PangeaIns](https://huggingface.co/datasets/neulab/PangeaInstruct) | [🧪 PangeaBench](https://huggingface.co/collections/neulab/pangea-6713c3b0d78a453906eb2ed8) | [💻 Github](https://github.com/neulab/Pangea/tree/main) | [📄 Arxiv](https://arxiv.org/abs/2410.16153) | [📕 PDF](https://arxiv.org/pdf/2410.16153) | [🖥️ Demo](https://huggingface.co/spaces/neulab/Pangea)


<img src=""https://cdn-uploads.huggingface.co/production/uploads/6230d750d93e84e233882dbc/ZjVTKnIsyshWpo-PWg9gM.png"" alt=""description"" style=""width:300px;"">

This README provides comprehensive details on the PangeaIns dataset, which was utilized during the instruction tuning phase for [Pangea-7B](https://huggingface.co/neulab/Pangea-7B).

## Description of PangeaIns

PangeaIns is a 6M multilingual multicultural multimodal instruction tuning dataset spanning 39 languages.

## PangeaIns Data Source
PangeaIns data path: PangeaIns.json (# samples: 6450624)
PangeaIns data source:
| Dataset Name                | Dataset Path                                                 | # Samples |
|-----------------------------|--------------------------------------------------------------|-----------|
| ALLAVA-4V                   | general/ALLAVA-4V/data.json                                  | 621327    |
| allava_vflan                | general/allava_vflan/data.json                               | 325122    |
| Cambrian737k                | general/cambrian/data.json                                   | 736934    |
| ChartQA                     | doc+chart/ChartQA/data.json                                  | 28299     |
| Code-Feedback               | text-only/Code-Feedback/data.json                            | 20000     |
| doc-vqa                     | doc+chart/doc-vqa/data.json                                  | 9665      |
| gpt4v-dataset               | caption/gpt4v-dataset/data.json                              | 10822     |
| GQA-ru                      | general/GQA-ru/data.json                                     | 40000     |
| laion-1M-qa                 | cultural/laion-multi-1M/captions-1M-generated-qas-llava.json | 1028791   |
| laion-300K-caption          | cultural/laion-multi-1M/laion-300K-caption-llava.json        | 300000    |
| llava-en-zh-300k            | general/llava-en-zh-300k/data.json                           | 50000     |
| LLaVA-Finetune              | cultural/laion-cultural-150k/laion-cultural-150k.json        | 151072    |
| Llava-JP-Instruct-108K      | general/LLaVA-JP-Instruct-108K/data.json                     | 108855    |
| llava-med-zh-instruct-60K   | general/llava-med-zh-instruct-60k/data.json                  | 56649     |
| LLaVA-NeXt                  | general/LLaVA-NeXt-Data/data.json                            | 119853    |
| LVIS-Instruct4V             | general/LVIS-Instruct4V/data.json                            | 222697    |
| MTVQA                       | general/MTVQA/data.json                                      | 6678      |
| nvlr2-llava                 | general/nvlr2-llava/data.json                                | 86373     |
| NuminaMath-CoT              | text-only/NuminaMath-CoT/data.json                           | 100000    |
| OpenHermes-2.5              | text-only/Openhermes-2.5/data.json                           | 399900    |
| palo_multilingual_dataset   | general/palo_multilingual_dataset/urdu-100k.json             | 99992     |
| ShareGPT-4o                 | general/ShareGPT-4o/data.json                                | 57289     |
| ShareGPT4V                  | general/ShareGPT4V/data.json                                 | 91021     |
| STAIR-Captions              | caption/STAIR-Captions/data.json                             | 82783     |
| table-vqa                   | doc+chart/table-vqa/data.json                                | 16408     |
| Viet-Doc-VQA                | doc+chart/Viet-Doc-VQA/data.json                             | 12000     |
| Viet-DOC-VQA-II             | doc+chart/Viet-DOC-VQA-II/data.json                          | 14998     |
| Viet-OCR-VQA                | doc+chart/Viet-OCR-VQA/data.json                             | 30000     |
| Viet-ShareGPT-4o-Text-VQA   | general/Viet-ShareGPT-4o-Text-VQA/data.json                  | 42678     |
| webui_multilingual_ocr      | ocr/webui_multilingual_ocr/data.json                             | 300000    |
| translation                 | translation/data.json                                        | 1280328   |


## Applications

PangeaIns was designed specifically for training the Pangea-7B model.

### Code Instructions

The dataset follows the LLaVA data format. To retrieve all files from PangeaIns, use the following script:

```python
from huggingface_hub import HfApi, hf_hub_download
import json

# Initialize the API client
api = HfApi()
dataset_name = ""neulab/PangeaInstruct""

# Retrieve and download all files in the dataset
files = api.list_repo_files(repo_id=dataset_name, repo_type=""dataset"")

for file in files:
    hf_hub_download(repo_id=dataset_name, filename=file, repo_type=""dataset"")
    print(f""File downloaded: {file}"")

# Load the complete PangeaIns dataset
with open('PangeaIns.json') as f:
  data = json.load(f)
```

Please note that image data is provided in compressed formats such as `.tar` or `.zip`. After downloading, you may need to extract these files to access the images.
For images.tar files, you could untar them by running 
```bash
tar -xvf images.tar
```

For images.zip files, you could unzip them by running 
```bash
unzip images.zip
```

For some large tar files, we uploaded tar files splitted using the split command, such as `split -n 4 -d images.tar part_`.
For example, in the `cultural/laion-multi-1M` subset, we splitted the images.tar file into 4 parts, `part_00`, `part_01`, `part_02`, and `part_03`.
In such cases, you would need to first combine the splits and then extract the tar file.

```bash
cat part_* > images.tar
tar -xvf images.tar
```

Each subset within the PangeaIns dataset (e.g., ChartQA) contains a `.json` file for metadata and a corresponding `.tar/.zip` file for the images.

## Citing the Dataset

**BibTeX Citation:**

```
@article{yue2024pangeafullyopenmultilingual,
  title={Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages},
  author={Xiang Yue and Yueqi Song and Akari Asai and Seungone Kim and Jean de Dieu Nyandwi and Simran Khanuja and Anjali Kantharuban and Lintang Sutawika and Sathyanarayanan Ramamoorthy and Graham Neubig},
  year={2024},
  journal={arXiv preprint arXiv:2410.16153},
  url={https://arxiv.org/abs/2410.16153}
}
```

## Contact

Corresponding to: {xyue2,yueqis,gneubig}@cs.cmu.edu",High,5.0
Q&A,abdoelsayed/reranking-datasets,3.0,3139.0,2025-03-06 02:58:21+00:00,apache-2.0,0.0,1.48 TB,unknown,UNKNOWN,unknown,10000000,https://arxiv.org/abs/2502.02464,none,"---
license: apache-2.0
task_categories:
- question-answering
language:
- en
- ar
- de
- fr
- es
- aa
- ab
- ae
- af
- ak
- am
- an
- as
- av
- ay
- az
- ba
- be
- bg
- bh
- bi
- bm
- bn
- bo
- br
- bs
- ca
- ce
- ch
- co
- cr
- cs
- cu
- cv
- cy
- da
- dv
- dz
- ee
- el
- eo
- et
- eu
- fa
- ff
- fi
- fj
- fo
- fy
- ga
- gd
- gl
- gn
- gu
- gv
- ha
- he
- hi
- ho
- hr
- ht
- hu
- hy
- hz
- ia
- id
- ie
- ig
- ii
- ik
- io
- is
- it
- iu
- ja
- jv
- ka
- kg
- ki
- kj
- kk
- kl
- km
- kn
- ko
- kr
- ks
- ku
- lt
- kw
- kv
- ky
- la
- lb
- lg
- li
- ln
- lo
- lu
- lv
- mg
- mh
- mi
- mk
- ml
- mn
- mr
- ms
- mt
- na
pretty_name: Reranking, Retreiver
size_categories:
- 10M<n<100M
configs:
- config_name: NQ
  data_files:
  - split: train
    path: bm25/nq/train.json
  - split: dev
    path: bm25/nq/dev.json
  - split: test
    path: bm25/nq/test.json
- config_name: TriviaQA
  data_files:
  - split: train
    path: bm25/triviaqa/train.json
  - split: dev
    path: bm25/triviaqa/dev.json
  - split: test
    path: bm25/triviaqa/test.json
- config_name: 2WikimultihopQA
  data_files:
  - split: train
    path: bm25/2wikimultihopqa/train.json
  - split: dev
    path: bm25/2wikimultihopqa/dev.json
- config_name: ArchivialQA
  data_files:
  - split: test
    path: bm25/ArchivialQA/test.json
  - split: dev
    path: bm25/ArchivialQA/val.json
- config_name: ChroniclingAmericaQA
  data_files:
  - split: test
    path: bm25/ChroniclingAmericaQA/test.json
  - split: dev
    path: bm25/ChroniclingAmericaQA/val.json
- config_name: EntityQuestions
  data_files:
  - split: test
    path: bm25/triviaqa/test.json
- config_name: AmbigQA
  data_files:
  - split: train
    path: bm25/ambig_qa/train.json
  - split: dev
    path: bm25/ambig_qa/dev.json
- config_name: ARC
  data_files:
  - split: train
    path: bm25/arc/train.json
  - split: dev
    path: bm25/arc/dev.json
  - split: test
    path: bm25/arc/test.json
- config_name: ASQA
  data_files:
  - split: train
    path: bm25/asqa/train.json
  - split: dev
    path: bm25/asqa/dev.json
- config_name: AY2
  data_files:
  - split: train
    path: bm25/ay2/train.json
  - split: dev
    path: bm25/ay2/dev.json
- config_name: ASQA
  data_files:
  - split: train
    path: bm25/asqa/train.json
  - split: dev
    path: bm25/asqa/dev.json
- config_name: Bamboogle
  data_files:
  - split: test
    path: bm25/bamboogle/test.json
- config_name: Eli5
  data_files:
  - split: train
    path: bm25/eli5/train.json
  - split: dev
    path: bm25/eli5/dev.json
- config_name: Fermi
  data_files:
  - split: train
    path: bm25/fermi/train.json
  - split: dev
    path: bm25/fermi/dev.json
  - split: test
    path: bm25/fermi/test.json
- config_name: Fever
  data_files:
  - split: train
    path: bm25/fever/train.json
  - split: dev
    path: bm25/fever/dev.json
  - split: test
    path: bm25/fever/test.json
- config_name: Hellaswag
  data_files:
  - split: train
    path: bm25/fever/train.json
  - split: dev
    path: bm25/fever/dev.json
- config_name: HotpotQA
  data_files:
  - split: train
    path: bm25/hotpotqa/train.json
  - split: dev
    path: bm25/hotpotqa/dev.json
- config_name: MMLU
  data_files:
  - split: train
    path: bm25/mmlu/train.json
  - split: dev
    path: bm25/mmlu/dev.json
  - split: test
    path: bm25/mmlu/test.json
- config_name: MMLU
  data_files:
  - split: 5_shot
    path: bm25/mmlu/5_shot.json
  - split: train
    path: bm25/mmlu/train.json
  - split: dev
    path: bm25/mmlu/dev.json
  - split: test
    path: bm25/mmlu/test.json
- config_name: Musique
  data_files:
  - split: train
    path: bm25/musique/train.json
  - split: dev
    path: bm25/musique/dev.json
- config_name: NarrativeQA
  data_files:
  - split: train
    path: bm25/narrativeqa/train.json
  - split: dev
    path: bm25/narrativeqa/dev.json
  - split: test
    path: bm25/narrativeqa/test.json
- config_name: OpenbookQA
  data_files:
  - split: train
    path: bm25/openbookqa/train.json
  - split: dev
    path: bm25/openbookqa/dev.json
  - split: test
    path: bm25/openbookqa/test.json
- config_name: PIQA
  data_files:
  - split: train
    path: bm25/piqa/train.json
  - split: dev
    path: bm25/piqa/dev.json
- config_name: PoPQA
  data_files:
  - split: test
    path: bm25/popqa/test.json
- config_name: Quartz
  data_files:
  - split: train
    path: bm25/quartz/train.json
  - split: dev
    path: bm25/quartz/dev.json
  - split: test
    path: bm25/quartz/test.json
- config_name: SIQA
  data_files:
  - split: train
    path: bm25/siqa/train.json
  - split: dev
    path: bm25/siqa/dev.json
- config_name: SQuAD
  data_files:
  - split: train
    path: bm25/squad/train.json
  - split: dev
    path: bm25/squad/dev.json
  - split: test
    path: bm25/squad/test.json
- config_name: StrategyQA
  data_files:
  - split: train
    path: bm25/squad/train.json
- config_name: TREX
  data_files:
  - split: dev
    path: bm25/trex/dev.json
- config_name: TruthfulQA
  data_files:
  - split: dev
    path: bm25/truthful_qa/dev.json
- config_name: WebQA
  data_files:
  - split: train
    path: bm25/web_questions/train.json
  - split: test
    path: bm25/web_questions/test.json
- config_name: WikiQA
  data_files:
  - split: train
    path: bm25/wiki_qa/train.json
  - split: dev
    path: bm25/wiki_qa/dev.json
  - split: test
    path: bm25/wiki_qa/test.json
- config_name: WikiASP
  data_files:
  - split: train
    path: bm25/wikiasp/train.json
  - split: dev
    path: bm25/wikiasp/dev.json
  - split: test
    path: bm25/wikiasp/test.json
- config_name: WikiPassageQA
  data_files:
  - split: train
    path: bm25/wikipassageqa/train.json
  - split: dev
    path: bm25/wikipassageqa/dev.json
  - split: test
    path: bm25/wikipassageqa/test.json
- config_name: Wned
  data_files:
  - split: dev
    path: bm25/wned/dev.json
- config_name: WoW
  data_files:
  - split: train
    path: bm25/wow/train.json
  - split: dev
    path: bm25/wow/dev.json
- config_name: ZSRE
  data_files:
  - split: train
    path: bm25/zsre/train.json
  - split: dev
    path: bm25/zsre/dev.json
---

# <div align=""center"">🔥 Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation 🔥<div>  
### ReRanking Datasets : A comprehensive collection of retrieval and reranking datasets with full passage contexts, including titles, text, and metadata for in-depth research.



<div align=""center"">
    <a href=""https://github.com/DataScienceUIBK/Rankify/tree/main""  style=""display: inline-block; margin: 2px;"" target=""_blank"">
    <img src=""https://img.shields.io/badge/GitHub-Repository-181717?logo=github"">
    </a>
    <a href=""https://arxiv.org/abs/2502.02464"" target=""_blank"" style=""display: inline-block; margin: 2px;"">
        <img src=""https://img.shields.io/badge/arXiv-b5212f.svg?logo=arxiv"">
    </a>
    <a href=""https://huggingface.co/datasets/abdoelsayed/reranking-datasets"" target=""_blank"" style=""display: inline-block; margin: 2px;"">
        <img src=""https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Datasets-27b3b4.svg"">
    </a>
    <a href=""https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light"" target=""_blank"" style=""display: inline-block; margin: 2px;"">
        <img src=""https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Datasets%20light-orange.svg"">
    </a>
    <a style=""display: inline-block; margin: 2px;"">
        <img alt=""Static Badge"" src=""https://img.shields.io/badge/Python-3.10_3.11-blue"">
    </a>
    <a href=""https://opensource.org/license/apache-2-0"" style=""display: inline-block; margin: 5px;"">
        <img src=""https://img.shields.io/static/v1?label=License&message=Apache-2.0&color=red"">
    </a>
    <a href=""https://github.com/DataScienceUIBK/rankify/releases"" style=""display: inline-block; margin: 5px;"">
        <img alt=""GitHub release"" src=""https://img.shields.io/github/release/DataScienceUIBK/rankify.svg?label=Version&color=orange"">
    </a>
</div>


![Dataset Downloads](https://img.shields.io/badge/Datasets-Retrieval_Ready-brightgreen)

_A curated collection of ready-to-use datasets for retrieval and reranking research. Created by [Abdelrahman Abdallah](https://abdoelsayed2016.github.io/)._

---

## About This Repository

Welcome to **ReRanking Datasets**! This repository simplifies retrieval research by providing preprocessed datasets for commonly used retrievers. Instead of running multiple retrievers on your own, download the precomputed results directly and focus on your research!

### What's Inside?

This dataset collection includes popular question-answering datasets:


The following table provides an overview of the availability of different retrieval methods (**BM25, DPR, ColBERT, ANCE, BGE, Contriever**) for each dataset.  

✅ **Completed**
⏳  **Part Completed, Pending other Parts**
🕒 **Pending**  

<table style=""width: 100%;"">
  <tr>
    <th align=""center"">Dataset</th> 
    <th align=""center"">BM25</th> 
    <th align=""center"">DPR</th> 
    <th align=""center"">ColBERT</th>
    <th align=""center"">ANCE</th>
    <th align=""center"">BGE</th>
    <th align=""center"">Contriever</th>
  </tr>
  <tr>
    <td align=""left"">2WikimultihopQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">ArchivialQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">ChroniclingAmericaQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">EntityQuestions</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">AmbigQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">ARC</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">ASQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">MS MARCO</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">AY2</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">Bamboogle</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">BoolQ</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">CommonSenseQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">CuratedTREC</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">ELI5</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">FERMI</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">FEVER</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">HellaSwag</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">HotpotQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">MMLU</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">Musique</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">NarrativeQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">NQ</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">OpenbookQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">PIQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">PopQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">Quartz</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">SIQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">StrategyQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    </tr>
    <tr>
    <td align=""left"">TREX</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    </tr>
    <tr>
    <td align=""left"">TriviaQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    </tr>
    <tr>
    <td align=""left"">TruthfulQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
      </tr>
    <tr>
    <td align=""left"">TruthfulQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
      </tr>
    <tr>
    <td align=""left"">WebQ</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
      </tr>
    <tr>
    <td align=""left"">WikiQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
      </tr>
    <tr>
    <td align=""left"">WikiAsp</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
        </tr>
    <tr>
    <td align=""left"">WikiPassageQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
        </tr>
    <tr>
    <td align=""left"">WNED</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
        </tr>
    <tr>
    <td align=""left"">WoW</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
        </tr>
    <tr>
    <td align=""left"">Zsre</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
</table>



Each dataset has **N different files**, one for each retriever. These files are organized and ready for direct use, saving you time and computation. All passages retrieved from psgs_w100 you can download them from [psgs_w100.tsv](https://huggingface.co/datasets/abdoelsayed/reranking-datasets/resolve/main/psgs_w100/psgs_w100.tsv?download=true) 

---
### Looking for a Lightweight Version?
If you only need essential metadata (e.g., `id`, `score`, and `has_answer`) and want to save storage space, check out the slimmed-down version of this dataset:

👉 [ReRanking Light](https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light)

---

### Format
The datasets are provided in JSON format, structured as follows:

```
[
    {
        ""question"": ""..."",
        ""answers"": [""..."", ""..."", ...],
        ""ctxs"": [
            {
                ""id"": ""..."",         // Passage ID from database TSV file
                ""title"": ""..."",      // Passage title
                ""text"": ""..."",       // Passage full text
                ""score"": ""..."",      // Retriever score
                ""has_answer"": true|false  // Whether the passage contains the answer
            }
        ]
    }
]
```
---

## Ongoing Work

Our goal is to make this repository the go-to resource for retrieval and reranking datasets. Here's what we are currently working on:

- Expanding the dataset collection to include more benchmarks, such as **MS MARCO**, **TREC**, and others.
- Adding retrieval outputs from more retrievers, including cutting-edge methods like **ColBERT**, **ANCE**, and more.
- Continuously improving dataset organization and documentation to ensure researchers' ease of use.

If you have suggestions for additional datasets or retrievers you'd like to see included, feel free to reach out or contribute!

---


## Why Use This Repository?

- 🚀 **Time-Saving:** Avoid the hassle of running retrievers yourself.
- 📂 **Precomputed Results:** Access retrieval outputs for multiple retrievers in one place.
- 🎯 **Research-Ready:** Use datasets directly for reranking tasks or as baselines in your studies.
- 🛠️ **Versatility:** Compatible with diverse retrieval and reranking pipelines.

---

## How to Use

Simply visit the dataset page on [HuggingFace](https://huggingface.co/datasets/abdoelsayed/reranking-datasets) and download the files for your desired dataset and retriever. Each file is structured to integrate seamlessly into your retrieval or reranking workflows.

---
## Example
```
from datasets import load_dataset

# Use streaming mode
dataset = load_dataset(""abdoelsayed/reranking-datasets"", data_files={""test"": ""bm25/nq-test.json""}, streaming=True)

# Iterate through the dataset
for entry in dataset[""test""].take(10):  # Process 10 entries as an example
    print(entry)
```
## Dataset Statistics
- **Number of Examples**: 10,000,000+
- **File Sizes**:
  - Full Dataset: ~1.48 TB
## Contribution & Feedback

We are actively expanding this repository and welcome contributions from the research community. Have a suggestion for a new dataset or retriever? Found an issue? Feel free to [open an issue](https://huggingface.co/datasets/abdoelsayed/reranking-datasets/issues) or reach out via [Twitter](https://twitter.com/abdoelsayed).

---

## 🌟 Citation

Please kindly cite our paper if helps your research:

```BibTex
@article{abdallah2025rankify,
  title={Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation},
  author={Abdallah, Abdelrahman and Mozafari, Jamshid and Piryani, Bhawna and Ali, Mohammed and Jatowt, Adam},
  journal={arXiv preprint arXiv:2502.02464},
  year={2025}
}
```

## Star History


[![Star History Chart](https://api.star-history.com/svg?repos=DataScienceUIBK/Rankify&type=Date)](https://star-history.com/#DataScienceUIBK/Rankify&Date)

🎉 **Happy Researching!**",High,6.0
Q&A,hamoudasidhoum/ACQAD,0.0,14.0,2024-11-25 16:04:52+00:00,unknown,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: unknown
task_categories:
- question-answering
language:
- ar
tags:
- multi-hop
- Complex
- Arabic
size_categories:
- 10K<n<100K
---
# Dataset Card for ACQAD

<!-- Provide a quick summary of the dataset. -->

we propose a new approach to automatically generate a dataset for Arabic complex question answering task. The proposed approach is based on using an effective workflow with a set of templates. The generated dataset, denoted as ACQAD, contains more than 118k questions, covering both comparison and multi-hop types. Each question-answer pair is decomposed into a set of single-hop questions, allowing QA systems to reduce question complexity and explain the reasoning steps

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

Arabic Wikipedia

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

@inproceedings{sidhoum2022acqad,
  title={ACQAD: a dataset for arabic complex question answering},
  author={Hamouda Sidhoum, Abdellah and Mataoui, M’hamed and Sebbak, Faouzi and Sma{\""\i}li, Kamel},
  booktitle={International conference on cyber security, artificial inteligence and theoretical computer science},
  year={2022}
}

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Q&A,conceptofmind/MegaWika,1.0,318.0,2024-11-19 04:22:41+00:00,cc-by-sa-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2307.07049,['https://aclanthology.org/2021.eacl-demos.19.pdf)'],"---
license: cc-by-sa-4.0
task_categories:
- summarization
- question-answering
- text-generation
- text2text-generation
language:
- af
- ar
- az
- bn
- cs
- de
- en
- es
- et
- fa
- fi
- fr
- ga
- gl
- gu
- he
- hi
- hr
- id
- it
- ja
- ka
- kk
- km
- ko
- lt
- lv
- mk
- ml
- mn
- mr
- my
- ne
- nl
- pl
- ps
- pt
- ro
- ru
- si
- sl
- sv
- ta
- th
- tr
- uk
- ur
- vi
- xh
- zh
pretty_name: MegaWika
size_categories:
- 10M<n<100M
---
# Dataset Card for MegaWika

## Dataset Description

- **Homepage:** [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika)
- **Repository:** [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika)
- **Paper:** [Coming soon]
- **Leaderboard:** [Coming soon]
- **Point of Contact:** [Samuel Barham](samuel.barham@jhuapl.edu)

### Dataset Summary

MegaWika is a multi- and crosslingual text dataset containing 30 million Wikipedia passages with their scraped and cleaned web citations. The passages span
50 Wikipedias in 50 languages, and the articles in which the passages were originally embedded are included for convenience. Where a Wikipedia passage is in a
non-English language, an automated English translation is provided. Furthermore, nearly 130 million English question/answer pairs were extracted from the
passages, and FrameNet events occurring in the passages are detected using the [LOME](https://aclanthology.org/2021.eacl-demos.19.pdf) FrameNet parser.


<!---
To get a feel for the dataset -- its structure, content, strengths and weaknesses -- you may visit the [dataset viewer](https://huggingface.co/spaces/hltcoe/megawika)
we have set up as a HuggingFace Space. It allows the curious visitor to explore a small set of examples spread across a number of the dataset's constituent languages.
-->

### Dataset Creation

The pipeline through which MegaWika was created is complex, and is described in more detail in the paper (linked above),
but the following diagram illustrates the basic approach.

![Illustration of MegaWikaProcess](images/MegaWikaProcess-cross-lingual.drawio.png)

### Supported Tasks and Leaderboards

MegaWika is meant to support research across a variety of tasks, including report generation, summarization, information retrieval, question answering, etc.

### Languages

MegaWika is divided by Wikipedia language. There are 50 languages, including English, each designated by their 2-character ISO language code:
- `af`: Afrikaans
- `ar`: Arabic
- `az`: Azeri (Azerbaijani)
- `bn`: Bengali
- `cs`: Czech
- `de`: German (Deutsch)
- `en`: English
- `es`: Spanish (Español)
- `et`: Estonian
- `fa`: Farsi (Persian)
- `fi`: Finnish
- `fr`: French
- `ga`: Irish (Gaelic)
- `gl`: Galician
- `gu`: Gujarati
- `he`: Hebrew
- `hi`: Hindi
- `hr`: Hungarian
- `id`: Indonesian
- `it`: Italian
- `ja`: Japanese
- `ka`: Georgian (Kartvelian/Kartlian)
- `kk`: Kazakh
- `km`: Khmer
- `ko`: Korean
- `lt`: Lithuanian
- `lv`: Latvian
- `mk`: Macedonian (Makedonski)
- `ml`: Malay (Malayalam)
- `mn`: Mongolian
- `mr`: Marathi
- `my`: Burmese (Myanmar language)
- `ne`: Nepali
- `nl`: Dutch (Nederlands)
- `pl`: Polish
- `ps`: Pashto
- `pt`: Portuguese
- `ro`: Romanian
- `ru`: Russian
- `si`: Sinhalese (Sri Lankan language)
- `sl`: Slovenian
- `sv`: Swedish (Svenska)
- `ta`: Tamil
- `th`: Thai
- `tr`: Turkish
- `uk`: Ukrainian
- `ur`: Urdu
- `vi`: Vietnamese
- `xh`: Xhosa
- `zh`: Chinese (Zhōng wén)

## Dataset Structure

The dataset is divided by language, and the data for each of the 50 languages is further chunked into discrete JSON lines files.
Each line of these files -- we'll call such a line an **instance** -- contains the data extracted from a single Wikipedia article.

### Data Instances

Each instance contains the text of the seed Wikipedia article, along with a list of **entries**. Each entry consists basically in
an extracted Wikipedia passage, the URL and scraped text of the web source it cites, a list of questions/answer pairs extracted from the passage,
and a framenet parse of the passage. Where the passage is from a non-English Wikipedia, a machine translation into English is also provided.

### Data Fields

The detailed structure of an instance is as follows:
```
{
  ""article_title"": <string : title of original Wikipedia article>
  ""article_text"": <string : text of Wikipedia article>
  ""entries"": [
    # Wiki Passage
    ""id"": <string : passage ID>
    ""passage"": {
      ""text"": <string : text of passage in English (possibly via MT)>
      ""parse"": <list of dict : FrameNet parse of English passage text>
      ""en_tokens"": <dict : tokenization of passage in English>
      ""lang_tokens"": <dict : tokenization of original non-English passage>
      ""en_lang_token_map"": <dict : alignment mapping between English and original language token indices>
    }
    # MT
    ""original"": <string : original language passage>
    ""original_sents"": <list of string : sentencized original language passage>
    ""translation"": <string : machine translation of passage>
    ""translation_sents"": <list of string : sentencized machine translation of passage>
    ""translation_probs"": <list of float : log prob of machine translation by sentence, where available>
    ""repetitious_translation"": <string \in (""true"", ""false"") : automated judgment on whether machine translation is pathologically repetitious>
    ""source_lang"": <string : language ID, 2-character ISO code>
    # Source
    ""source_url"": <string : URL of the cited web source>
    ""source_text"": <string : content extracted from the scrape of the source URL>
    # Question/Answer Pairs
    ""qa_pairs"": [
      ...
      {
        ""question"": <string : generated question>
        ""passage_id"": <string : passage ID>
        ""en_answer"": <string : English answer>
        ""lang_answer"": <string : aligned original language answer>
        ""frames"": [
          ...
          {
            ""frame"": <string : frame triggered by the question>
            ""argument"": <string : detected frame arguments>
          }
          ...
        ]
        # NB: answer matches can be empty, in the case no matching span exists
        ""en_matches_in_source"": <list of int : start and end index of the English language-answer token(s) in the source document>
        ""en_match_in_passage"": <list of int : start and end index of the English language-answer token(s) in the English language translation of the passage>
        ""lang_matches_in_source"": <list of int : start and end index of the original language-answer token(s) in the source document>
        ""lang_match_in_passage"": <list of int : start and end index of the original language-answer token(s) in the original language passage>
        ""passage"": <list of string : sentencized view of the passage>
        ""en_answer_tokens"": <list of string>
        ""match_disambiguated_question"": <string : disambiguated version of question obtained by matching pronouns with article title (noisy but often helpful)>
      }
      ...
    ]
  ]
}
```

English language instances differ not in structure but in content; 
1. Fields in the block labeled ""MT"" above are naturally null (that is, they are set to falsy values in Python -- specifically `None`)
2. Since the Wiki passage only exists in English, and has no corresponding non-English ""original language"" version, answer spans also necessarily have only an English-language version (and no non-English ""original-language"" version. Therefore, fields in the `qa_pairs` block beginning with `lang_` are set to null/falsy values in Python (in this case, empty lists).


### Data Splits

MegaWika is currently split only by language, as each task will imply its own approach to filtering, sampling, downselecting, and splitting into train/test splits.

<!---
### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]
-->

## Licensing and Takedown

MegaWika 1.0 consists in part of documents scraped from across the web (based on citations linked in Wikipedia articles.)

We do not own any of the scraped text nor do we claim copyright: text drawn from Wikipedia citations are meant for research use in algorithmic design and model training.

We release this dataset and all its contents under CC-BY-SA-4.0.

### Notice and Takedown Policy:
*NB*: Should you consider that our data contains material that is owned by you and should therefore not be reproduced here, please:

- Clearly identify yourself, with detailed contact data such as an address, telephone number or email address at which you can be contacted.
- Clearly identify the copyrighted work claimed to be infringed.
- Clearly identify the material that is claimed to be infringing and information reasonably sufficient to allow us to locate the material.

And contact the authors.

*Take down*: We will comply to legitimate requests by removing the affected sources from the next release of the dataset.

## Additional Information

### Dataset Curators

Released and maintained by the Johns Hopkins University Human Language Technology Center of Excellence (JHU/HLTCOE). 
You can contact one the MegaWika authors, including [Samuel Barham](mailto:samuel.barham@jhuapl.edu), [Orion Weller](mailto:oweller2@jhu.edu),
and [Ben van Durme](mailto:vandurme@jhu.edu) with questions.

### Licensing Information

Released under the [Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/) license.

### Citation Information

```
@misc{barham2023megawika,
      title={MegaWika: Millions of reports and their sources across 50 diverse languages}, 
      author={Samuel Barham and and  Weller and Michelle Yuan and Kenton Murray and Mahsa Yarmohammadi and Zhengping Jiang and Siddharth Vashishtha and Alexander Martin and Anqi Liu and Aaron Steven White and Jordan Boyd-Graber and Benjamin Van Durme},
      year={2023},
      eprint={2307.07049},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

<!--
### Contributions

[More Information Needed]
-->",High,5.0
Q&A,MahmoudIbrahim/Arabic_NVIDIA,1.0,54.0,2024-11-20 01:04:33+00:00,none,2.0,22.6 MB,23697817.6,22.6 MB,23697817.6,20300,none,none,"---
dataset_info:
  features:
  - name: prompt
    dtype: string
  - name: response
    dtype: string
  splits:
  - name: train
    num_bytes: 63028674
    num_examples: 20300
  download_size: 22579566
  dataset_size: 63028674
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- question-answering
- text-generation
language:
- ar
tags:
- finance
pretty_name: 'n'
size_categories:
- 10K<n<100K
---



## Dataset Overview
This dataset contains texts translated into Arabic, where the NVIDIA data, specifically the nvidia/HelpSteer2 dataset,
was translated using Google Translate to achieve accurate translation. 
This data is specifically designed for use in question answering and text generation tasks.

## Dataset Details
- **Original Dataset**: NVIDIA's **HelpSteer2** dataset.
- **Language**: Translated into **Arabic**.
- **Use Case**: question answering and text generation tasks.
",Medium,2.0
Q&A,WueNLP/belebele-fleurs,5.0,1009.0,2024-12-12 14:26:00+00:00,cc-by-sa-4.0,0.0,UNKNOWN,UNKNOWN,216 GB,231928233984.0,65134,none,"['https://aclanthology.org/2024.acl-long.44"",']","---
license: cc-by-sa-4.0
annotations_creators:
  - found
language_creators:
  - expert-generated
language:
  - af
  - am
  - ar
  - az
  - as
  - bm
  - bn
  - bo
  - bg
  - ca
  - cs
  - ku
  - da
  - de
  - el
  - en
  - es
  - et
  - eu
  - fi
  - fr
  - ff
  - om
  - gu
  - gn
  - ht
  - ha
  - he
  - hi
  - hr
  - hu
  - hy
  - ig
  - id
  - it
  - is
  - jv
  - ja
  - ka
  - kn
  - kk
  - mn
  - km
  - rw
  - ky
  - ko
  - lo
  - ln
  - lt
  - lg
  - lv
  - ml
  - mr
  - mk
  - mt
  - mi
  - my
  - nl
  - 'no'
  - ne
  - ny
  - or
  - pa
  - ps
  - fa
  - mg
  - pl
  - pt
  - ro
  - ru
  - sn
  - si
  - sl
  - sv
  - sk
  - sd
  - sw
  - ta
  - te
  - tg
  - tl
  - th
  - ti
  - tn
  - ts
  - tr
  - uk
  - ur
  - uz
  - vi
  - wo
  - xh
  - yo
  - zh
  - ms
  - zu
  - multilingual
multilinguality:
  - multilingual
task_categories:
  - audio-classification
  - automatic-speech-recognition
  - audio-text-to-text
  - text-to-speech
  - question-answering
  - document-question-answering
pretty_name: Belebele-Fleurs
dataset_info:
- config_name: afr_Latn
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 1321576984.0
    num_examples: 309
  download_size: 697429945
  dataset_size: 1321576984.0
- config_name: amh_Ethi
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 4735350571.0
    num_examples: 782
  download_size: 2518582786
  dataset_size: 4735350571.0
- config_name: arb_Arab
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 1516344836.0
    num_examples: 387
  download_size: 802576290
  dataset_size: 1516344836.0
- config_name: asm_Beng
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 5596186237.0
    num_examples: 824
  download_size: 3017636971
  dataset_size: 5596186237.0
- config_name: azj_Latn
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 4695408159.0
    num_examples: 759
  download_size: 2513843262
  dataset_size: 4695408159.0
- config_name: ben_Beng
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 5752882522.0
    num_examples: 855
  download_size: 3136282526
  dataset_size: 5752882522.0
- config_name: bul_Cyrl
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 4689171166.0
    num_examples: 873
  download_size: 2505604703
  dataset_size: 4689171166.0
- config_name: cat_Latn
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 3478577853.0
    num_examples: 652
  download_size: 1857877410
  dataset_size: 3478577853.0
- config_name: ceb_Latn
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 5328160039.0
    num_examples: 783
  download_size: 2836984523
  dataset_size: 5328160039.0
- config_name: ces_Latn
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 4184004947.0
    num_examples: 802
  download_size: 2222498009
  dataset_size: 4184004947.0
- config_name: ckb_Arab
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 5313934366.0
    num_examples: 842
  download_size: 2818314961
  dataset_size: 5313934366.0
- config_name: dan_Latn
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 3579744686.0
    num_examples: 696
  download_size: 1921138648
  dataset_size: 3579744686.0
- config_name: deu_Latn
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 4655172301.0
    num_examples: 804
  download_size: 2514299817
  dataset_size: 4655172301.0
- config_name: ell_Grek
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 4676081230.0
    num_examples: 837
  download_size: 2495880530
  dataset_size: 4676081230.0
- config_name: eng_Latn
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 3795562630.0
    num_examples: 844
  download_size: 2048956533
  dataset_size: 3795562630.0
- config_name: est_Latn
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 3738347953.0
    num_examples: 736
  download_size: 1996763639
  dataset_size: 3738347953.0
- config_name: fin_Latn
  features:
  - name: link
    dtype: string
  - name: question_number
    dtype: int64
  - name: flores_passage
    dtype: string
  - name: question
    dtype: string
  - name: mc_answer1
    dtype: string
  - name: mc_answer2
    dtype: string
  - name: mc_answer3
    dtype: string
  - name: mc_answer4
    dtype: string
  - name: correct_answer_num
    dtype: string
  - name: dialect
    dtype: string
  - name: ds
    dtype: timestamp[us]
  - name: sentence_data
    list:
    - name: URL
      dtype: string
    - name: audio
      sequence:
        audio:
          sampling_rate: 16000
    - name: domain
      dtype: string
    - name: filename
      sequence: string
    - name: fleurs_id
      dtype: int64
    - name: full_paragraph
      dtype: bool
    - name: gender
      sequence: string
    - name: has_hyperlink
      dtype: int64
    - name: has_image
      dtype: int64
    - name: id
      dtype: int64
    - name: num_samples
      sequence: int64
    - name: raw_transcription
      dtype: string
    - name: seamlessm4t_asr
      sequence: string
    - name: seamlessm4t_asr_cer
      sequence: float64
    - name: seamlessm4t_asr_translation
      sequence: string
    - name: seamlessm4t_asr_wer
      sequence: float64
    - name: sentence
      dtype: string
    - name: sentence_idx
      dtype: int64
    - name: speaker_id
      sequence: int64
    - name: split
      sequence: string
    - name: topic
      dtype: string
    - name: transcription
      dtype: string
    - name: whisper_asr
      sequence: string
    - name: whisper_asr_cer
      sequence: float64
    - name: whisper_asr_translation
      sequence: string
    - name: whisper_asr_wer
      sequence: float64
  splits:
  - name: test
    num_bytes: 4808962381.0
    num_examples: 826
  dow",High,6.0
Q&A,hghalebi/refugiesinfo,1.0,8.0,2024-11-28 09:13:33+00:00,other,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
language:
- fr
- ar
- en
- uk
- ps
- fa
- ti
- ru
pretty_name: ""Refugee Assistance Dataset""
tags:
- refugees
- integration
- multilingual
- RAG
- NLP
- social-inclusion
license: ""other"" # Update based on the license terms of https://refugies.info/en
task_categories:
- text-retrieval
- translation
- question-answering
- text-classification
dataset_info:
  source: ""https://refugies.info/en""
  description: ""This dataset is sourced from Réfugiés.info, an online platform providing structured information to assist refugees and newcomers in navigating administrative, educational, healthcare, housing, and social integration procedures in their host countries.""
  multilingual_support: true
  available_languages: [""French (fr)"", ""Arabic (ar)"", ""English (en)"", ""Ukrainian (uk)"", ""Pashto (ps)"", ""Persian/Dari (fa)"", ""Tigrinya (ti)"", ""Russian (ru)""]
  use_cases: 
    - Retrieval-Augmented Generation (RAG) systems
    - Fine-tuning NLP models
    - Knowledge Retrieval and QA systems for refugees and social workers
---

This dataset is derived from https://refugies.info/en, an online platform providing resources, guides, and information to support refugees and newcomers in navigating various aspects of life in their host countries. It encompasses a structured collection of multilingual information, covering topics such as:

**Administrative Procedures:** Guidance on residence permits, asylum applications, and legal rights.

**Employment:** Job search assistance, training programs, and integration into the workforce.

**Education:** Access to language courses, school enrollment, and higher education opportunities.

**Healthcare:** Information on accessing medical care and health insurance.

**Housing:** Steps for finding accommodation and understanding local housing regulations.

**Social and Community Integration:** Tips for cultural adaptation, community involvement, and accessing support networks.
Purpose and Applications

# The dataset can be used for:

**RAG (Retrieval-Augmented Generation) Applications:** Providing contextually relevant answers to queries about refugee integration processes.

**Fine-Tuning Models:** Training specialized NLP models to assist in providing multilingual, accurate, and empathetic support for refugees and newcomers.

**Knowledge Retrieval Systems:** Building search engines or recommendation tools tailored to refugee and newcomer needs.
Data Provenance

All data originates from https://refugies.info/en, ensuring the content is reliable and specifically curated for refugee assistance.

*Any applications built using this dataset should credit the platform as the source and adhere to its terms of use.*",Medium,2.0
Q&A,WueNLP/sib-fleurs,8.0,947.0,2025-05-07 20:40:33+00:00,cc-by-sa-4.0,0.0,157 GB,168577466368.0,157 GB,168577466368.0,94449,https://arxiv.org/abs/2501.06117,none,"---
license: cc-by-sa-4.0
language:
  - ace
  - acm
  - acq
  - aeb
  - af
  - ajp
  - ak
  - als
  - am
  - apc
  - ar
  - ars
  - ary
  - arz
  - as
  - ast
  - awa
  - ayr
  - azb
  - azj
  - ba
  - bm
  - ban
  - be
  - bem
  - bn
  - bho
  - bjn
  - bo
  - bs
  - bug
  - bg
  - ca
  - ceb
  - cs
  - cjk
  - ckb
  - crh
  - cy
  - da
  - de
  - dik
  - dyu
  - dz
  - el
  - en
  - eo
  - et
  - eu
  - ee
  - fo
  - fj
  - fi
  - fon
  - fr
  - fur
  - fuv
  - gaz
  - gd
  - ga
  - gl
  - gn
  - gu
  - ht
  - ha
  - he
  - hi
  - hne
  - hr
  - hu
  - hy
  - ig
  - ilo
  - id
  - is
  - it
  - jv
  - ja
  - kab
  - kac
  - kam
  - kn
  - ks
  - ka
  - kk
  - kbp
  - kea
  - khk
  - km
  - ki
  - rw
  - ky
  - kmb
  - kmr
  - knc
  - kg
  - ko
  - lo
  - lij
  - li
  - ln
  - lt
  - lmo
  - ltg
  - lb
  - lua
  - lg
  - luo
  - lus
  - lvs
  - mag
  - mai
  - ml
  - mar
  - min
  - mk
  - mt
  - mni
  - mos
  - mi
  - my
  - nl
  - nn
  - nb
  - npi
  - nqo
  - nso
  - nus
  - ny
  - oc
  - ory
  - pag
  - pa
  - pap
  - pbt
  - pes
  - plt
  - pl
  - pt
  - prs
  - quy
  - ro
  - rn
  - ru
  - sg
  - sa
  - sat
  - scn
  - shn
  - si
  - sk
  - sl
  - sm
  - sn
  - sd
  - so
  - st
  - es
  - sc
  - sr
  - ss
  - su
  - sv
  - swh
  - szl
  - ta
  - taq
  - tt
  - te
  - tg
  - tl
  - th
  - ti
  - tpi
  - tn
  - ts
  - tk
  - tum
  - tr
  - tw
  - tzm
  - ug
  - uk
  - umb
  - ur
  - uzn
  - vec
  - vi
  - war
  - wo
  - xh
  - ydd
  - yo
  - yue
  - zh
  - zsm
  - zu
  - multilingual
annotations_creators:
  - found
language_creators:
  - expert-generated
multilinguality:
  - multilingual
task_categories:
  - audio-classification
  - automatic-speech-recognition
  - audio-text-to-text
  - text-to-speech
  - question-answering
  - document-question-answering
pretty_name: SIB-Fleurs
dataset_info:
- config_name: afr_Latn
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 524232877.0
    num_examples: 406
  - name: validation
    num_bytes: 76384271.0
    num_examples: 86
  - name: test
    num_bytes: 84400076.0
    num_examples: 95
  download_size: 673661100
  dataset_size: 685017224.0
- config_name: amh_Ethi
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 1289823377.0
    num_examples: 752
  - name: validation
    num_bytes: 65389982.0
    num_examples: 54
  - name: test
    num_bytes: 185857834.0
    num_examples: 149
  download_size: 1525564166
  dataset_size: 1541071193.0
- config_name: arb_Arab
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 646819902.0
    num_examples: 579
  - name: validation
    num_bytes: 95091075.0
    num_examples: 64
  - name: test
    num_bytes: 144786307.0
    num_examples: 133
  download_size: 878867591
  dataset_size: 886697284.0
- config_name: asm_Beng
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 1235366957.0
    num_examples: 730
  - name: validation
    num_bytes: 158536549.0
    num_examples: 71
  - name: test
    num_bytes: 400145792.0
    num_examples: 176
  download_size: 1782426273
  dataset_size: 1794049298.0
- config_name: ast_Latn
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 866679990.0
    num_examples: 701
  - name: validation
    num_bytes: 102384453.0
    num_examples: 69
  - name: test
    num_bytes: 282753773.0
    num_examples: 177
  download_size: 1245085728
  dataset_size: 1251818216.0
- config_name: azj_Latn
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 1090899299.0
    num_examples: 712
  - name: validation
    num_bytes: 147617247.0
    num_examples: 71
  - name: test
    num_bytes: 379234055.0
    num_examples: 174
  download_size: 1602247163
  dataset_size: 1617750601.0
- config_name: bel_Cyrl
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 1105817781.0
    num_examples: 690
  - name: validation
    num_bytes: 186825266.0
    num_examples: 71
  - name: test
    num_bytes: 486320479.0
    num_examples: 177
  download_size: 1753989008
  dataset_size: 1778963526.0
- config_name: ben_Beng
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 1232070743.0
    num_examples: 742
  - name: validation
    num_bytes: 157285034.0
    num_examples: 71
  - name: test
    num_bytes: 397951833.0
    num_examples: 176
  download_size: 1782546384
  dataset_size: 1787307610.0
- config_name: bos_Latn
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 1173791520.0
    num_examples: 746
  - name: validation
    num_bytes: 149405247.0
    num_examples: 71
  - name: test
    num_bytes: 369790849.0
    num_examples: 177
  download_size: 1654694782
  dataset_size: 1692987616.0
- config_name: bul_Cyrl
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 1101248058.0
    num_examples: 749
  - name: validation
    num_bytes: 117353674.0
    num_examples: 70
  - name: test
    num_bytes: 221557279.0
    num_examples: 176
  download_size: 1421883953
  dataset_size: 1440159011.0
- config_name: cat_Latn
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 863830240.0
    num_examples: 683
  - name: validation
    num_bytes: 147554660.0
    num_examples: 71
  - name: test
    num_bytes: 353869370.0
    num_examples: 177
  download_size: 1340643723
  dataset_size: 1365254270.0
- config_name: ceb_Latn
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 1398384311.0
    num_examples: 741
  - name: validation
    num_bytes: 95970795.0
    num_examples: 61
  - name: test
    num_bytes: 240259442.0
    num_examples: 149
  download_size: 1718325671
  dataset_size: 1734614548.0
- config_name: ces_Latn
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 970924211.0
    num_examples: 732
  - name: validation
    num_bytes: 112601348.0
    num_examples: 68
  - name: test
    num_bytes: 277229156.0
    num_examples: 172
  download_size: 1333906872
  dataset_size: 1360754715.0
- config_name: ckb_Arab
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 1178357835.0
    num_examples: 738
  - name: validation
    num_bytes: 134860481.0
    num_examples: 70
  - name: test
    num_bytes: 342458168.0
    num_examples: 176
  download_size: 1613748924
  dataset_size: 1655676484.0
- config_name: cym_Latn
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 1385174116.0
    num_examples: 739
  - name: validation
    num_bytes: 200018352.0
    num_examples: 71
  - name: test
    num_bytes: 486565088.0
    num_examples: 177
  download_size: 2038201423
  dataset_size: 2071757556.0
- config_name: dan_Latn
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 877728248.0
    num_examples: 696
  - name: validation
    num_bytes: 130348707.0
    num_examples: 70
  - name: test
    num_bytes: 340140011.0
    num_examples: 177
  download_size: 1319500991
  dataset_size: 1348216966.0
- config_name: deu_Latn
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 1059347230.0
    num_examples: 736
  - name: validation
    num_bytes: 136254869.0
    num_examples: 69
  - name: test
    num_bytes: 364325435.0
    num_examples: 175
  download_size: 1542935687
  dataset_size: 1559927534.0
- config_name: ell_Grek
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 1169505435.0
    num_examples: 750
  - name: validation
    num_bytes: 86533682.0
    num_examples: 67
  - name: test
    num_bytes: 228840869.0
    num_examples: 168
  download_size: 1470419073
  dataset_size: 1484879986.0
- config_name: eng_Latn
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    dtype: string
  - name: topic
    dtype: string
  - name: has_image
    dtype: int32
  - name: has_hyperlink
    dtype: int32
  - name: fleurs_id
    dtype: int32
  - name: filename
    sequence: string
  - name: raw_transcription
    dtype: string
  - name: transcription
    dtype: string
  - name: num_samples
    sequence: int64
  - name: speaker_id
    sequence: int64
  - name: gender
    sequence: string
  - name: whisper_asr
    sequence: string
  - name: whisper_asr_cer
    sequence: float64
  - name: whisper_asr_wer
    sequence: float64
  - name: whisper_asr_translation
    sequence: string
  - name: seamlessm4t_asr
    sequence: string
  - name: seamlessm4t_asr_cer
    sequence: float64
  - name: seamlessm4t_asr_wer
    sequence: float64
  - name: seamlessm4t_asr_translation
    sequence: string
  - name: index_id
    dtype: int64
  - name: category
    dtype:
      class_label:
        names:
          '0': science/technology
          '1': travel
          '2': politics
          '3': sports
          '4': health
          '5': entertainment
          '6': geography
  - name: text
    dtype: string
  - name: audio
    sequence:
      audio:
        sampling_rate: 16000
  splits:
  - name: train
    num_bytes: 865407552.0
    num_examples: 738
  - name: validation
    num_bytes: 113902786.0
    num_examples: 71
  - name: test
    num_bytes: 197416856.0
    num_examples: 177
  download_size: 1168283579
  dataset_size: 1176727194.0
- config_name: est_Latn
  features:
  - name: sentence
    dtype: string
  - name: URL
    dtype: string
  - name: id
    dtype: int32
  - name: domain
    ",High,5.0
Q&A,dijihax/Dataset,2.0,7.0,2024-12-06 17:37:23+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- text-classification
- token-classification
- table-question-answering
- question-answering
- zero-shot-classification
- translation
- summarization
- feature-extraction
- text-generation
- text2text-generation
- fill-mask
- sentence-similarity
- text-to-speech
- text-to-audio
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
- image-to-image
- unconditional-image-generation
- multiple-choice
- image-to-text
- image-to-video
- video-classification
- reinforcement-learning
- robotics
- tabular-classification
- tabular-regression
- tabular-to-text
- table-to-text
- text-retrieval
- time-series-forecasting
- text-to-video
- visual-question-answering
- zero-shot-image-classification
- graph-ml
- mask-generation
- zero-shot-object-detection
- text-to-3d
- image-to-3d
- image-feature-extraction
- video-text-to-text
language:
- en
- aa
- ab
- ae
- af
- ak
- am
- an
- ar
- as
- av
- ay
- az
- ba
- be
- bg
- bh
- bi
- bm
- bn
- bo
- br
- bs
- ca
- ce
- ch
- co
- cr
- cs
- cu
- cv
- cy
- da
- de
- dv
- dz
- ee
- el
- eo
- es
- et
- eu
- fa
- ff
- fi
- fj
- fo
- fr
- fy
- ga
- gd
- gl
- gn
- gu
- gv
- ha
- he
- hi
- ho
- hr
- ht
- hu
- hy
- hz
- ia
- id
- ie
- ig
- ii
- ik
- io
- is
- it
- iu
- ja
- jv
- ka
- kg
- ki
- kj
- kk
- kl
- km
- kn
- ko
- kr
- ks
- ku
- kv
- kw
- ky
- la
- lb
- lg
- li
- ln
- lo
- lt
- lu
- lv
- mg
- mh
- mi
- mk
- ml
- mn
- mr
- ms
- mt
- my
- na
- nb
- nd
- ne
- ng
- nl
- nn
- 'no'
- nr
- nv
- ny
- oc
- oj
- om
- or
- os
- pa
- pi
- pl
- ps
- pt
- qu
- rm
- rn
- ro
- ru
- rw
- sa
- sc
- sd
- se
- sg
- si
- sk
- sl
- sm
- sn
- so
tags:
- chemistry
- biology
- finance
- legal
- music
- art
- code
- climate
- medical
- not-for-all-audiences
- synthetic
pretty_name: DijiHax/DataStax
size_categories:
- n>1T
---
# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Q&A,facebook/2M-Belebele,11.0,532.0,2024-12-17 13:39:10+00:00,cc-by-sa-4.0,1.0,UNKNOWN,unknown,184 GB,197568495616.0,59061,https://arxiv.org/abs/2412.08274,"['https://aclanthology.org/2024.acl-long.44"",']","---
license: cc-by-sa-4.0
task_categories:
- question-answering
- automatic-speech-recognition
language:
 - bg
 - pa
 - en
 - hu
 - sv
 - af
 - ca
 - ka
 - sk
 - jv
 - bn
 - tr
 - sr
 - ro
 - tg
 - fa
 - wo
 - fi
 - hy
 - vi
 - kea
 - as
 - ja
 - nl
 - ne
 - lg
 - hi
 - xh
 - kk
 - mn
 - yo
 - km
 - ha
 - ru
 - sw
 - ps
 - ko
 - cs
 - lv
 - ig
 - ar
 - es
 - nb
 - lt
 - fil
 - it
 - he
 - da
 - ml
 - my
 - el
 - et
 - pl
 - sn
 - sd
 - or
 - th
 - luo
 - sl
 - fr
 - id
 - ta
 - gu
 - mk
 - am
 - pt
 - cmn
 - de
 - ceb
 - is
 - ur
 - az
 - te
tags:
- speech-recognition
- multilingual
- flores200
- translation
- audio
- speech
pretty_name: 2M Belebele Speech
size_categories:
- 1K<n<10K
configs:
 - config_name: guj_Gujr
   data_files:
      - split: test
        path: data/lang=guj_Gujr/*.parquet
 - config_name: lvs_Latn
   data_files:
      - split: test
        path: data/lang=lvs_Latn/*.parquet
 - config_name: jpn_Jpan
   data_files:
      - split: test
        path: data/lang=jpn_Jpan/*.parquet
 - config_name: pol_Latn
   data_files:
      - split: test
        path: data/lang=pol_Latn/*.parquet
 - config_name: arz_Arab
   data_files:
      - split: test
        path: data/lang=arz_Arab/*.parquet
 - config_name: mkd_Cyrl
   data_files:
      - split: test
        path: data/lang=mkd_Cyrl/*.parquet
 - config_name: fin_Latn
   data_files:
      - split: test
        path: data/lang=fin_Latn/*.parquet
 - config_name: vie_Latn
   data_files:
      - split: test
        path: data/lang=vie_Latn/*.parquet
 - config_name: cat_Latn
   data_files:
      - split: test
        path: data/lang=cat_Latn/*.parquet
 - config_name: dan_Latn
   data_files:
      - split: test
        path: data/lang=dan_Latn/*.parquet
 - config_name: asm_Beng
   data_files:
      - split: test
        path: data/lang=asm_Beng/*.parquet
 - config_name: por_Latn
   data_files:
      - split: test
        path: data/lang=por_Latn/*.parquet
 - config_name: nob_Latn
   data_files:
      - split: test
        path: data/lang=nob_Latn/*.parquet
 - config_name: tam_Taml
   data_files:
      - split: test
        path: data/lang=tam_Taml/*.parquet
 - config_name: mya_Mymr
   data_files:
      - split: test
        path: data/lang=mya_Mymr/*.parquet
 - config_name: bul_Cyrl
   data_files:
      - split: test
        path: data/lang=bul_Cyrl/*.parquet
 - config_name: yor_Latn
   data_files:
      - split: test
        path: data/lang=yor_Latn/*.parquet
 - config_name: afr_Latn
   data_files:
      - split: test
        path: data/lang=afr_Latn/*.parquet
 - config_name: deu_Latn
   data_files:
      - split: test
        path: data/lang=deu_Latn/*.parquet
 - config_name: amh_Ethi
   data_files:
      - split: test
        path: data/lang=amh_Ethi/*.parquet
 - config_name: tgl_Latn
   data_files:
      - split: test
        path: data/lang=tgl_Latn/*.parquet
 - config_name: heb_Hebr
   data_files:
      - split: test
        path: data/lang=heb_Hebr/*.parquet
 - config_name: ind_Latn
   data_files:
      - split: test
        path: data/lang=ind_Latn/*.parquet
 - config_name: sna_Latn
   data_files:
      - split: test
        path: data/lang=sna_Latn/*.parquet
 - config_name: ell_Grek
   data_files:
      - split: test
        path: data/lang=ell_Grek/*.parquet
 - config_name: hye_Armn
   data_files:
      - split: test
        path: data/lang=hye_Armn/*.parquet
 - config_name: snd_Arab
   data_files:
      - split: test
        path: data/lang=snd_Arab/*.parquet
 - config_name: swe_Latn
   data_files:
      - split: test
        path: data/lang=swe_Latn/*.parquet
 - config_name: pan_Guru
   data_files:
      - split: test
        path: data/lang=pan_Guru/*.parquet
 - config_name: nld_Latn
   data_files:
      - split: test
        path: data/lang=nld_Latn/*.parquet
 - config_name: khm_Khmr
   data_files:
      - split: test
        path: data/lang=khm_Khmr/*.parquet
 - config_name: ben_Beng
   data_files:
      - split: test
        path: data/lang=ben_Beng/*.parquet
 - config_name: swh_Latn
   data_files:
      - split: test
        path: data/lang=swh_Latn/*.parquet
 - config_name: ory_Orya
   data_files:
      - split: test
        path: data/lang=ory_Orya/*.parquet
 - config_name: hin_Deva
   data_files:
      - split: test
        path: data/lang=hin_Deva/*.parquet
 - config_name: srp_Cyrl
   data_files:
      - split: test
        path: data/lang=srp_Cyrl/*.parquet
 - config_name: rus_Cyrl
   data_files:
      - split: test
        path: data/lang=rus_Cyrl/*.parquet
 - config_name: spa_Latn
   data_files:
      - split: test
        path: data/lang=spa_Latn/*.parquet
 - config_name: lug_Latn
   data_files:
      - split: test
        path: data/lang=lug_Latn/*.parquet
 - config_name: urd_Arab
   data_files:
      - split: test
        path: data/lang=urd_Arab/*.parquet
 - config_name: hun_Latn
   data_files:
      - split: test
        path: data/lang=hun_Latn/*.parquet
 - config_name: tel_Telu
   data_files:
      - split: test
        path: data/lang=tel_Telu/*.parquet
 - config_name: slv_Latn
   data_files:
      - split: test
        path: data/lang=slv_Latn/*.parquet
 - config_name: pes_Arab
   data_files:
      - split: test
        path: data/lang=pes_Arab/*.parquet
 - config_name: wol_Latn
   data_files:
      - split: test
        path: data/lang=wol_Latn/*.parquet
 - config_name: xho_Latn
   data_files:
      - split: test
        path: data/lang=xho_Latn/*.parquet
 - config_name: est_Latn
   data_files:
      - split: test
        path: data/lang=est_Latn/*.parquet
 - config_name: tur_Latn
   data_files:
      - split: test
        path: data/lang=tur_Latn/*.parquet
 - config_name: tgk_Cyrl
   data_files:
      - split: test
        path: data/lang=tgk_Cyrl/*.parquet
 - config_name: mal_Mlym
   data_files:
      - split: test
        path: data/lang=mal_Mlym/*.parquet
 - config_name: azj_Latn
   data_files:
      - split: test
        path: data/lang=azj_Latn/*.parquet
 - config_name: kea_Latn
   data_files:
      - split: test
        path: data/lang=kea_Latn/*.parquet
 - config_name: jav_Latn
   data_files:
      - split: test
        path: data/lang=jav_Latn/*.parquet
 - config_name: ces_Latn
   data_files:
      - split: test
        path: data/lang=ces_Latn/*.parquet
 - config_name: khk_Cyrl
   data_files:
      - split: test
        path: data/lang=khk_Cyrl/*.parquet
 - config_name: slk_Latn
   data_files:
      - split: test
        path: data/lang=slk_Latn/*.parquet
 - config_name: kor_Hang
   data_files:
      - split: test
        path: data/lang=kor_Hang/*.parquet
 - config_name: npi_Deva
   data_files:
      - split: test
        path: data/lang=npi_Deva/*.parquet
 - config_name: ibo_Latn
   data_files:
      - split: test
        path: data/lang=ibo_Latn/*.parquet
 - config_name: isl_Latn
   data_files:
      - split: test
        path: data/lang=isl_Latn/*.parquet
 - config_name: zho_Hans
   data_files:
      - split: test
        path: data/lang=zho_Hans/*.parquet
 - config_name: pbt_Arab
   data_files:
      - split: test
        path: data/lang=pbt_Arab/*.parquet
 - config_name: ceb_Latn
   data_files:
      - split: test
        path: data/lang=ceb_Latn/*.parquet
 - config_name: ron_Latn
   data_files:
      - split: test
        path: data/lang=ron_Latn/*.parquet
 - config_name: luo_Latn
   data_files:
      - split: test
        path: data/lang=luo_Latn/*.parquet
 - config_name: kaz_Cyrl
   data_files:
      - split: test
        path: data/lang=kaz_Cyrl/*.parquet
 - config_name: eng_Latn
   data_files:
      - split: test
        path: data/lang=eng_Latn/*.parquet
 - config_name: hau_Latn
   data_files:
      - split: test
        path: data/lang=hau_Latn/*.parquet
 - config_name: ita_Latn
   data_files:
      - split: test
        path: data/lang=ita_Latn/*.parquet
 - config_name: tha_Thai
   data_files:
      - split: test
        path: data/lang=tha_Thai/*.parquet
 - config_name: kat_Geor
   data_files:
      - split: test
        path: data/lang=kat_Geor/*.parquet
 - config_name: lit_Latn
   data_files:
      - split: test
        path: data/lang=lit_Latn/*.parquet
 - config_name: fra_Latn
   data_files:
      - split: test
        path: data/lang=fra_Latn/*.parquet
---

# 2M-Belebele

## Highly-Multilingual Speech and American Sign Language Comprehension Dataset

We introduce [**2M-Belebele**](https://arxiv.org/abs/2412.08274) as the first highly multilingual speech and American Sign Language (ASL) comprehension dataset. Our dataset, which is an extension of the existing Belebele only-text dataset, covers 74 spoken languages at the intersection of Belebele and Fleurs, and one sign language (ASL). 

The speech dataset is built from aligning Belebele, Flores200 and Fleurs datasets as well as recording completely new audio for the sentences missing in Fleurs. We also provide new recordings for the Belebele question and answers as these are not in the original Flores200 dataset. 

Therefore, as a by-product, we also extend the Fleurs dataset (which is widely used to benchmark language identification and automatic speech recognition) by providing recordings for more Flores200 sentences than were previously available and adding sign language, creating a new **2M-Flores**. This 2M-Flores extends Fleurs by +20%.

The ASL dataset is built with completely new controlled recordings of ASL signers and each flores sentence as well as questions and answers are available in video format.

## Speech Dataset

The huggingface dataset `facebook/2M-Belebele` provides the speech version of 2M-Belebele, We will soon release the ASL version under `facebook/2M-Belebele-ASL` as it has a slightly different format (videos instead of audio).

Here is a sample code to use this dataset:

```python
from IPython.display import Audio  
from IPython.display import display as d  
import numpy as np

from datasets import load_dataset

df_bb = load_dataset(""facebook/2M-Belebele"", 'por_Latn')
with_qq = df_bb.filter(lambda e: e['question_audio'] is not None)

r = with_qq['test'][200]
d(r['flores_passage'])  
for seg, sent in zip(r['audio_segments'], r['flores_sentences']):  
   d(sent)  
   for a in seg:  
       d(Audio(data=np.array(a['audio']['wav'], dtype=np.float64), rate=a['audio']['sampling_rate']))    
   d('-----------------')

d('QUESTION')  
d(r['question'])  
for a in r['question_audio']:  
   d(Audio(data=np.array(a['audio']['wav'], dtype=np.float64), rate=a['audio']['sampling_rate']))


d('ANSWER 1')  
d(r['mc_answer1'])  
for a in r['answer_1_audio']:  
   d(Audio(data=np.array(a['audio']['wav'], dtype=np.float64), rate=a['audio']['sampling_rate']))  
    
d('ANSWER 2')  
d(r['mc_answer2'])  
for a in r['answer_2_audio']:  
   d(Audio(data=np.array(a['audio']['wav'], dtype=np.float64), rate=a['audio']['sampling_rate']))

d('ANSWER 3')  
d(r['mc_answer3'])  
for a in r['answer_3_audio']:  
   d(Audio(data=np.array(a['audio']['wav'], dtype=np.float64), rate=a['audio']['sampling_rate']))


d('ANSWER 4')  
d(r['mc_answer4'])  
for a in r['answer_4_audio']:  
   d(Audio(data=np.array(a['audio']['wav'], dtype=np.float64), rate=a['audio']['sampling_rate']))

```

### Columns

- link: the link of the original document containing the passage.
- question_number: the question number for this passage. Some passages have multiple questions.
- flores_passage: the paragraph for the passage, coming from belebele text
- question: the text question
- mc_answer1: 1st answer, text
- mc_answer2: 2nd answer, text
- mc_answer3: 3rd answer, text
- mc_answer4: 4th answer, text
- flores: details about the flores entries in this passage. A list of structs with ids and split of the
original flores entry, in order of appearance in the passage + metadata about each sentence.
- correct_answer_num: the number of the correct answer
- dialect: the dialect/lang that you've loaded
- audio_segments: a list of audio segments, in order, corresponding to each flores sentence in this passage. On sentence might have been read by multiple speakers, so for each sentence there is an array of segments, with metadata about the speaker and source (fleurs or meta recording) and the audio wav blob, make sure to use the provided sample rate when loading.
- unmatched_audio: were there any sentences not matched to audio in this passage
- fleurs_audio_match: how many segments come from fleurs
- meta_audio_match: how many come from meta recording
- has_matched_audio: was at least one sentence matched
- question_audio: the audio recording for the question, a single speaker is provided.
- answer_1_audio: the audio recording for the answer, a single speaker is provided.
- answer_2_audio: the audio recording for the answer, a single speaker is provided.
- answer_3_audio: the audio recording for the answer, a single speaker is provided.
- answer_4_audio: the audio recording for the answer, a single speaker is provided.
- flores_sentences: the list of flores sentences 

### Languages in Belebele-speech

Note that for the speech version of 2M-Belebele, we have kept the original Flores200 dialect codes even if we are only talking about speech, this is to make it easier to align with Belebele and Flores.

| FLORES-200 Code | English Name | Family | Belebele | Belebele-Speech |
| :---- | :---- | :---- | :---- | :---- |
| acm_Arab | Mesopotamian Arabic | Afro-Asiatic | x |  |
| afr_Latn | Afrikaans | Germanic | x | x |
| als_Latn | Tosk Albanian | Paleo-Balkanic | x |  |
| amh_Ethi | Amharic | Afro-Asiatic | x | x |
| apc_Arab | North Levantine Arabic | Afro-Asiatic | x |  |
| arb_Arab | Modern Standard Arabic | Afro-Asiatic | x |  |
| arb_Latn | Modern Standard Arabic (Romanized) | Afro-Asiatic | x |  |
| ars_Arab | Najdi Arabic | Afro-Asiatic | x |  |
| ary_arab | Moroccan Arabic | Afro-Asiatic | x |  |
| arz_Arab | Egyptian Arabic | Afro-Asiatic | x | x |
| asm_Beng | Assamese | Indo-Aryan | x | x |
| azj_Latn | North Azerbaijani | Turkic | x | x |
| bam_Latn | Bambara | Mande | x |  |
| ben_Beng | Bengali | Indo-Aryan | x | x |
| ben_Latn^ | Bengali (Romanized) | Indo-Aryan | x |  |
| bod_Tibt | Standard Tibetan | Sino-Tibetan | x |  |
| bul_Cyrl | Bulgarian | Balto-Slavic | x | x |
| cat_Latn | Catalan | Romance | x | x |
| ceb_Latn | Cebuano | Austronesian | x | x |
| ces_Latn | Czech | Balto-Slavic | x | x |
| ckb_Arab | Central Kurdish | Iranian | x |  |
| dan_Latn | Danish | Germanic | x | x |
| deu_Latn | German | Germanic | x | x |
| ell_Grek | Greek | Hellenic | x | x |
| eng_Latn | English | Germanic | x | x |
| est_Latn | Estonian | Uralic | x |  |
| eus_Latn | Basque | Basque | x |  |
| fin_Latn | Finnish | Uralic | x | x |
| fra_Latn | French | Romance | x | x |
| fuv_Latn | Nigerian Fulfulde | Atlantic-Congo | x |  |
| gaz_Latn | West Central Oromo | Afro-Asiatic | x |  |
| grn_Latn | Guarani | Tupian | x |  |
| guj_Gujr | Gujarati | Indo-Aryan | x | x |
| hat_Latn | Haitian Creole | Atlantic-Congo | x |  |
| hau_Latn | Hausa | Afro-Asiatic | x | x |
| heb_Hebr | Hebrew | Afro-Asiatic | x | x |
| hin_Deva | Hindi | Indo-Aryan | x | x |
| hin_Latn^ | Hindi (Romanized) | Indo-Aryan | x |  |
| hrv_Latn | Croatian | Balto-Slavic | x | x |
| hun_Latn | Hungarian | Uralic | x | x |
| hye_Armn | Armenian | Armenian | x | x |
| ibo_Latn | Igbo | Atlantic-Congo | x |  |
| ilo_Latn | Ilocano | Austronesian | x |  |
| ind_Latn | Indonesian | Austronesian | x | x |
| isl_Latn | Icelandic | Germanic | x | x |
| ita_Latn | Italian | Romance | x | x |
| jav_Latn | Javanese | Austronesian | x | x |
| jpn_Jpan | Japanese | Japonic | x | x |
| kac_Latn | Jingpho | Sino-Tibetan | x |  |
| kan_Knda | Kannada | Dravidian | x |  |
| kat_Geor | Georgian | kartvelian | x | x |
| kaz_Cyrl | Kazakh | Turkic | x | x |
| kea_Latn | Kabuverdianu | Portuguese Creole | x | x |
| khk_Cyrl | Halh Mongolian | Mongolic | x | x |
| khm_Khmr | Khmer | Austroasiatic | x | x |
| kin_Latn | Kinyarwanda | Atlantic-Congo | x |  |
| kir_Cyrl | Kyrgyz | Turkic | x |  |
| kor_Hang | Korean | Koreanic | x | x |
| lao_Laoo | Lao | Kra-Dai | x |  |
| lin_Latn | Lingala | Atlantic-Congo | x |  |
| lit_Latn | Lithuanian | Balto-Slavic | x | x |
| lug_Latn | Ganda | Atlantic-Congo | x | x |
| luo_Latn | Luo | Nilo-Saharan | x | x |
| lvs_Latn | Standard Latvian | Balto-Slavic | x | x |
| mal_Mlym | Malayalam | Dravidian | x | x |
| mar_Deva | Marathi | Indo-Aryan | x |  |
| mkd_Cyrl | Macedonian | Balto-Slavic | x | x |
| mlt_Latn | Maltese | Afro-Asiatic | x |  |
| mri_Latn | Maori | Austronesian | x |  |
| mya_Mymr | Burmese | Sino-Tibetan | x | x |
| nld_Latn | Dutch | Germanic | x | x |
| nob_Latn | Norwegian Bokmål | Germanic | x | x |
| npi_Deva | Nepali | Indo-Aryan | x | x |
| npi_Latn^ | Nepali (Romanized) | Indo-Aryan | x | x |
| nso_Latn | Northern Sotho | Atlantic-Congo | x |  |
| nya_Latn | Nyanja | Afro-Asiatic | x |  |
| ory_Orya | Odia | Indo-Aryan | x | x |
| pan_Guru | Eastern Panjabi | Indo-Aryan | x | x |
| pbt_Arab | Southern Pashto | Indo-Aryan | x | x |
| pes_Arab | Western Persian | Iranian | x | x |
| plt_Latn | Plateau Malagasy | Austronesian | x |  |
| pol_Latn | Polish | Balto-Slavic | x | x |
| por_Latn | Portuguese | Romance | x |  |
| ron_Latn | Romanian | Romance | x |  |
| rus_Cyrl | Russian | Balto-Slavic | x |  |
| shn_Mymr | Shan | Kra-Dai | x |  |
| sin_Latn^ | Sinhala (Romanized) | Indo-Aryan | x |  |
| sin_Sinh | Sinhala | Indo-Aryan | x |  |
| slk_Latn | Slovak | Balto-Slavic | x | x |
| slv_Latn | Slovenian | Balto-Slavic | x | x |
| sna_Latn | Shona | Atlantic-Congo | x | x |
| snd_Arab | Sindhi | Indo-Aryan | x | x |
| som_Latn | Somali | Afro-Asiatic | x |  |
| sot_Latn | Southern Sotho | Atlantic-Congo | x |  |
| spa_Latn | Spanish | Romance | x | x |
| srp_Cyrl | Serbian | Balto-Slavic | x | x |
| ssw_Latn | Swati | Atlantic-Congo | x |  |
| sun_Latn | Sundanese | Austronesian | x |  |
| swe_Latn | Swedish | Germanic | x | x |
| swh_Latn | Swahili | Atlantic-Congo | x | x |
| tam_Taml | Tamil | Dravidian | x | x |
| tel_Telu | Telugu | Dravidian | x | x |
| tgk_Cyrl | Tajik | Iranian | x | x |
| tgl_Latn | Tagalog | Austronesian | x | x |
| tha_Thai | Thai | Kra-Dai | x | x |
| tir_Ethi | Tigrinya | Afro-Asiatic | x |  |
| tsn_Latn | Tswana | Atlantic-Congo | x |  |
| tso_Latn | Tsonga | Afro-Asiatic | x |  |
| tur_Latn | Turkish | Turkic | x | x |
| ukr_Cyrl | Ukrainian | Balto-Slavic | x |  |
| urd_Arab | Urdu | Indo-Aryan | x |  |
| urd_Latn^ | Urdu (Romanized) | Indo-Aryan | x | x |
| uzn_Latn | Northern Uzbek | Turkic | x |  |
| vie_Latn | Vietnamese | Austroasiatic | x | x |
| war_Latn | Waray | Austronesian | x |  |
| wol_Latn | Wolof | Atlantic-Congo | x | x |
| xho_Latn | Xhosa | Atlantic-Congo | x | x |
| yor_Latn | Yoruba | Atlantic-Congo | x | x |
| zho_Hans | Chinese (Simplified) | Sino-Tibetan | x | x |
| zho_Hant | Chinese (Traditional) | Sino-Tibetan | x |  |
| zsm_Latn | Standard Malay | Austronesian | x |  |
| zul_Latn | Zulu | Atlantic-Congo | x |  |

## ASL Belebele

We are currently preparing the ASL version of Belebele for download, it should be online before the end of 2024. If you are interested, contact [mortimer@meta.com](mailto:mortimer@meta.com) to be notified.

## Citation

If you use this data in your work, please cite 2M-Belebele paper as well as the original Belebele paper:

```bibtex
@article{2mbelebele,  
  author =        {Marta R. Costa-jussà and Bokai Yu and Pierre Andrews and Belen Alastruey and Necati Cihan Camgoz and Joe Chuang and Jean Maillard and Christophe Ropers and Arina Turkantenko and Carleigh Wood},  
  journal =       {Arxiv},  
url = {https://arxiv.org/abs/2412.08274},  
  title =         {{2M-BELEBELE}: Highly-Multilingual Speech and American Sign Language  
Comprehension Dataset},  
  year =          {2024},  
}

@inproceedings{bandarkar-etal-2024-belebele,  
    title = ""The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants"",  
    author = ""Bandarkar, Lucas  and  
      Liang, Davis  and  
      Muller, Benjamin  and  
      Artetxe, Mikel  and  
      Shukla, Satya Narayan  and  
      Husa, Donald  and  
      Goyal, Naman  and  
      Krishnan, Abhinandan  and  
      Zettlemoyer, Luke  and  
      Khabsa, Madian"",  
    booktitle = ""Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",  
    month = aug,  
    year = ""2024"",  
    address = ""Bangkok, Thailand and virtual meeting"",  
    publisher = ""Association for Computational Linguistics"",  
    url = ""https://aclanthology.org/2024.acl-long.44"",  
    pages = ""749--775"",  
}
```

## License

2M-Belebele is released under CC-BY-SA4.0, it is composed of Flores200 (CC-BY-SA 4.0), belebele (CC-BY-SA4.0) and fleurs (cc-by-4.0).

## Belebele-Fleurs Alignment

2M-Belebele speech is composed of recordings gathered by Meta as well as existing recordings from the Fleurs dataset. The text version of belebele was created by reconstructing passages using Flores200 sentences. Fleurs provide recordings for some of Flores sentences. We align the belebele dataset to fleurs by first aligning the passages to Flores sentences and then these sentences to Fleurs recordings.

You can find the belebele to fleurs align code in the belebele repository. This is just for documentation as you should not have to run this if you download the dataset provided here. The 2M-Belebele also contains more data than what this alignment would provide as we provide more recording of passages as well as recordings for the questions and answers.",Medium,3.0
Q&A,abdoelsayed/reranking-datasets-light,2.0,1400.0,2025-03-17 17:54:12+00:00,apache-2.0,0.0,UNKNOWN,unknown,4.38 GB,unknown,484589,https://arxiv.org/abs/2502.02464,none,"---
language:
- en
- ar
- de
- fr
- es
- aa
- ab
- ae
- af
- ak
- am
- an
- as
- av
- ay
- az
- ba
- be
- bg
- bh
- bi
- bm
- bn
- bo
- br
- bs
- ca
- ce
- ch
- co
- cr
- cs
- cu
- cv
- cy
- da
- dv
- dz
- ee
- el
- eo
- et
- eu
- fa
- ff
- fi
- fj
- fo
- fy
- ga
- gd
- gl
- gn
- gu
- gv
- ha
- he
- hi
- ho
- hr
- ht
- hu
- hy
- hz
- ia
- id
- ie
- ig
- ii
- ik
- io
- is
- it
- iu
- ja
- jv
- ka
- kg
- ki
- kj
- kk
- kl
- km
- kn
- ko
- kr
- ks
- ku
- lt
- kw
- kv
- ky
- la
- lb
- lg
- li
- ln
- lo
- lu
- lv
- mg
- mh
- mi
- mk
- ml
- mn
- mr
- ms
- mt
- na
license: apache-2.0
size_categories:
- 10M<n<100M
task_categories:
- question-answering
pretty_name: Reranking, Retreiver
configs:
- config_name: NQ
  data_files:
  - split: train
    path: bm25/nq/train.json
  - split: dev
    path: bm25/nq/dev.json
  - split: test
    path: bm25/nq/test.json
- config_name: TriviaQA
  data_files:
  - split: train
    path: bm25/triviaqa/train.json
  - split: dev
    path: bm25/triviaqa/dev.json
  - split: test
    path: bm25/triviaqa/test.json
- config_name: 2WikimultihopQA
  data_files:
  - split: train
    path: bm25/2wikimultihopqa/train.json
  - split: dev
    path: bm25/2wikimultihopqa/dev.json
- config_name: ArchivialQA
  data_files:
  - split: test
    path: bm25/ArchivialQA/test.json
  - split: dev
    path: bm25/ArchivialQA/val.json
- config_name: ChroniclingAmericaQA
  data_files:
  - split: test
    path: bm25/ChroniclingAmericaQA/test.json
  - split: dev
    path: bm25/ChroniclingAmericaQA/val.json
- config_name: EntityQuestions
  data_files:
  - split: test
    path: bm25/triviaqa/test.json
- config_name: AmbigQA
  data_files:
  - split: train
    path: bm25/ambig_qa/train.json
  - split: dev
    path: bm25/ambig_qa/dev.json
- config_name: ARC
  data_files:
  - split: train
    path: bm25/arc/train.json
  - split: dev
    path: bm25/arc/dev.json
  - split: test
    path: bm25/arc/test.json
- config_name: ASQA
  data_files:
  - split: train
    path: bm25/asqa/train.json
  - split: dev
    path: bm25/asqa/dev.json
- config_name: AY2
  data_files:
  - split: train
    path: bm25/ay2/train.json
  - split: dev
    path: bm25/ay2/dev.json
- config_name: ASQA
  data_files:
  - split: train
    path: bm25/asqa/train.json
  - split: dev
    path: bm25/asqa/dev.json
- config_name: Bamboogle
  data_files:
  - split: test
    path: bm25/bamboogle/test.json
- config_name: Eli5
  data_files:
  - split: train
    path: bm25/eli5/train.json
  - split: dev
    path: bm25/eli5/dev.json
- config_name: Fermi
  data_files:
  - split: train
    path: bm25/fermi/train.json
  - split: dev
    path: bm25/fermi/dev.json
  - split: test
    path: bm25/fermi/test.json
- config_name: Fever
  data_files:
  - split: train
    path: bm25/fever/train.json
  - split: dev
    path: bm25/fever/dev.json
  - split: test
    path: bm25/fever/test.json
- config_name: Hellaswag
  data_files:
  - split: train
    path: bm25/fever/train.json
  - split: dev
    path: bm25/fever/dev.json
- config_name: HotpotQA
  data_files:
  - split: train
    path: bm25/hotpotqa/train.json
  - split: dev
    path: bm25/hotpotqa/dev.json
- config_name: MMLU
  data_files:
  - split: train
    path: bm25/mmlu/train.json
  - split: dev
    path: bm25/mmlu/dev.json
  - split: test
    path: bm25/mmlu/test.json
- config_name: MMLU
  data_files:
  - split: 5_shot
    path: bm25/mmlu/5_shot.json
  - split: train
    path: bm25/mmlu/train.json
  - split: dev
    path: bm25/mmlu/dev.json
  - split: test
    path: bm25/mmlu/test.json
- config_name: Musique
  data_files:
  - split: train
    path: bm25/musique/train.json
  - split: dev
    path: bm25/musique/dev.json
- config_name: NarrativeQA
  data_files:
  - split: train
    path: bm25/narrativeqa/train.json
  - split: dev
    path: bm25/narrativeqa/dev.json
  - split: test
    path: bm25/narrativeqa/test.json
- config_name: OpenbookQA
  data_files:
  - split: train
    path: bm25/openbookqa/train.json
  - split: dev
    path: bm25/openbookqa/dev.json
  - split: test
    path: bm25/openbookqa/test.json
- config_name: PIQA
  data_files:
  - split: train
    path: bm25/piqa/train.json
  - split: dev
    path: bm25/piqa/dev.json
- config_name: PoPQA
  data_files:
  - split: test
    path: bm25/popqa/test.json
- config_name: Quartz
  data_files:
  - split: train
    path: bm25/quartz/train.json
  - split: dev
    path: bm25/quartz/dev.json
  - split: test
    path: bm25/quartz/test.json
- config_name: SIQA
  data_files:
  - split: train
    path: bm25/siqa/train.json
  - split: dev
    path: bm25/siqa/dev.json
- config_name: SQuAD
  data_files:
  - split: train
    path: bm25/squad/train.json
  - split: dev
    path: bm25/squad/dev.json
  - split: test
    path: bm25/squad/test.json
- config_name: StrategyQA
  data_files:
  - split: train
    path: bm25/squad/train.json
- config_name: TREX
  data_files:
  - split: dev
    path: bm25/trex/dev.json
- config_name: TruthfulQA
  data_files:
  - split: dev
    path: bm25/truthful_qa/dev.json
- config_name: WebQA
  data_files:
  - split: train
    path: bm25/web_questions/train.json
  - split: test
    path: bm25/web_questions/test.json
- config_name: WikiQA
  data_files:
  - split: train
    path: bm25/wiki_qa/train.json
  - split: dev
    path: bm25/wiki_qa/dev.json
  - split: test
    path: bm25/wiki_qa/test.json
- config_name: WikiASP
  data_files:
  - split: train
    path: bm25/wikiasp/train.json
  - split: dev
    path: bm25/wikiasp/dev.json
  - split: test
    path: bm25/wikiasp/test.json
- config_name: WikiPassageQA
  data_files:
  - split: train
    path: bm25/wikipassageqa/train.json
  - split: dev
    path: bm25/wikipassageqa/dev.json
  - split: test
    path: bm25/wikipassageqa/test.json
- config_name: Wned
  data_files:
  - split: dev
    path: bm25/wned/dev.json
- config_name: WoW
  data_files:
  - split: train
    path: bm25/wow/train.json
  - split: dev
    path: bm25/wow/dev.json
- config_name: ZSRE
  data_files:
  - split: train
    path: bm25/zsre/train.json
  - split: dev
    path: bm25/zsre/dev.json
---

# <div align=""center"">🔥 Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation 🔥<div>  
### ReRanking Datasets : A comprehensive collection of retrieval and reranking datasets with full passage contexts, including titles, text, and metadata for in-depth research.




<div align=""center"">
    <a href=""https://github.com/DataScienceUIBK/Rankify/tree/main""  style=""display: inline-block; margin: 2px;"" target=""_blank"">
    <img src=""https://img.shields.io/badge/GitHub-Repository-181717?logo=github"">
    </a>
    <a href=""https://arxiv.org/abs/2502.02464"" target=""_blank"" style=""display: inline-block; margin: 2px;"">
        <img src=""https://img.shields.io/badge/arXiv-b5212f.svg?logo=arxiv"">
    </a>
    <a href=""https://huggingface.co/datasets/abdoelsayed/reranking-datasets"" target=""_blank"" style=""display: inline-block; margin: 2px;"">
        <img src=""https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Datasets-27b3b4.svg"">
    </a>
    <a href=""https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light"" target=""_blank"" style=""display: inline-block; margin: 2px;"">
        <img src=""https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace%20Datasets%20light-orange.svg"">
    </a>
    <a style=""display: inline-block; margin: 2px;"">
        <img alt=""Static Badge"" src=""https://img.shields.io/badge/Python-3.10_3.11-blue"">
    </a>
    <a href=""https://opensource.org/license/apache-2-0"" style=""display: inline-block; margin: 5px;"">
        <img src=""https://img.shields.io/static/v1?label=License&message=Apache-2.0&color=red"">
    </a>
    <a href=""https://github.com/DataScienceUIBK/rankify/releases"" style=""display: inline-block; margin: 5px;"">
        <img alt=""GitHub release"" src=""https://img.shields.io/github/release/DataScienceUIBK/rankify.svg?label=Version&color=orange"">
    </a>
</div>

![Dataset Downloads](https://img.shields.io/badge/Datasets-Retrieval_Ready-brightgreen)

_A curated collection of ready-to-use datasets for retrieval and reranking research. Created by [Abdelrahman Abdallah](https://abdoelsayed2016.github.io/)._

---

## About This Repository

Welcome to **ReRanking Datasets**! This repository simplifies retrieval research by providing preprocessed datasets for commonly used retrievers. Instead of running multiple retrievers on your own, download the precomputed results directly and focus on your research!

### What's Inside?

This dataset collection includes popular question-answering datasets:

The following table provides an overview of the availability of different retrieval methods (**BM25, DPR, ColBERT, ANCE, BGE, Contriever**) for each dataset.  

✅ **Completed**
⏳  **Part Completed, Pending other Parts**
🕒 **Pending**  

<table style=""width: 100%;"">
  <tr>
    <th align=""center"">Dataset</th> 
    <th align=""center"">BM25</th> 
    <th align=""center"">DPR</th> 
    <th align=""center"">ColBERT</th>
    <th align=""center"">ANCE</th>
    <th align=""center"">BGE</th>
    <th align=""center"">Contriever</th>
  </tr>
  <tr>
    <td align=""left"">2WikimultihopQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">ArchivialQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">ChroniclingAmericaQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">EntityQuestions</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">AmbigQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">ARC</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">ASQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">MS MARCO</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">AY2</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">Bamboogle</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">BoolQ</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">CommonSenseQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">CuratedTREC</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">ELI5</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">FERMI</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">FEVER</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">HellaSwag</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">HotpotQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">MMLU</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
  <tr>
    <td align=""left"">Musique</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">NarrativeQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">NQ</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">OpenbookQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">PIQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">PopQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">Quartz</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">SIQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    <tr>
    <td align=""left"">StrategyQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    </tr>
    <tr>
    <td align=""left"">TREX</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    </tr>
    <tr>
    <td align=""left"">TriviaQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
    </tr>
    <tr>
    <td align=""left"">TruthfulQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
      </tr>
    <tr>
    <td align=""left"">TruthfulQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
      </tr>
    <tr>
    <td align=""left"">WebQ</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
      </tr>
    <tr>
    <td align=""left"">WikiQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
      </tr>
    <tr>
    <td align=""left"">WikiAsp</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
        </tr>
    <tr>
    <td align=""left"">WikiPassageQA</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">⏳</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
        </tr>
    <tr>
    <td align=""left"">WNED</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
        </tr>
    <tr>
    <td align=""left"">WoW</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
        </tr>
    <tr>
    <td align=""left"">Zsre</td>
    <td align=""center"">✅</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
    <td align=""center"">🕒</td>
  </tr>
</table>



Each dataset has **N different files**, one for each retriever. These files are organized and ready for direct use, saving you time and computation. All passages retrieved from psgs_w100 you can download them from [psgs_w100.tsv](https://huggingface.co/datasets/abdoelsayed/reranking-datasets/resolve/main/psgs_w100/psgs_w100.tsv?download=true) 

---

### Need Full Context?
If you require the complete dataset, including passage titles and full text, you can find it in the full collection:

👉 [ReRanking Contexts](https://huggingface.co/abdoelsayed/reranking-contexts)

---

### Format
The datasets are provided in JSON format, structured as follows:

```
[
    {
        ""question"": ""..."",
        ""answers"": [""..."", ""..."", ...],
        ""ctxs"": [
            {
                ""id"": ""..."",         // Passage ID from database TSV file
                ""score"": ""..."",      // Retriever score
                ""has_answer"": true|false  // Whether the passage contains the answer
            }
        ]
    }
]
```
---
## Ongoing Work

Our goal is to make this repository the go-to resource for retrieval and reranking datasets. Here's what we are currently working on:

- Expanding the dataset collection to include more benchmarks, such as **MS MARCO**, **TREC**, and others.
- Adding retrieval outputs from more retrievers, including cutting-edge methods like **ColBERT**, **ANCE**, and more.
- Continuously improving dataset organization and documentation to ensure researchers' ease of use.

If you have suggestions for additional datasets or retrievers you'd like to see included, feel free to reach out or contribute!

---

## Why Use This Repository?

- 🚀 **Time-Saving:** Avoid the hassle of running retrievers yourself.
- 📂 **Precomputed Results:** Access retrieval outputs for multiple retrievers in one place.
- 🎯 **Research-Ready:** Use datasets directly for reranking tasks or as baselines in your studies.
- 🛠️ **Versatility:** Compatible with diverse retrieval and reranking pipelines.

---

## How to Use

Simply visit the dataset page on [HuggingFace](https://huggingface.co/datasets/abdoelsayed/reranking-datasets) and download the files for your desired dataset and retriever. Each file is structured to integrate seamlessly into your retrieval or reranking workflows.

---
## Example
```
from datasets import load_dataset

# Use streaming mode
dataset = load_dataset(""abdoelsayed/reranking-datasets-light"", data_files={""test"": ""bm25/nq-test.json""}, streaming=True)

# Iterate through the dataset
for entry in dataset[""test""].take(10):  # Process 10 entries as an example
    print(entry)
```

## Contribution & Feedback

We are actively expanding this repository and welcome contributions from the research community. Have a suggestion for a new dataset or retriever? Found an issue? Feel free to [open an issue](https://huggingface.co/datasets/abdoelsayed/reranking-datasets/issues) or reach out via [Twitter](https://twitter.com/abdoelsayed).

---

## 🌟 Citation

Please kindly cite our paper if helps your research:

```BibTex
@article{abdallah2025rankify,
  title={Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation},
  author={Abdallah, Abdelrahman and Mozafari, Jamshid and Piryani, Bhawna and Ali, Mohammed and Jatowt, Adam},
  journal={arXiv preprint arXiv:2502.02464},
  year={2025}
}
```

## Star History


[![Star History Chart](https://api.star-history.com/svg?repos=DataScienceUIBK/Rankify&type=Date)](https://star-history.com/#DataScienceUIBK/Rankify&Date)

🎉 **Happy Researching!**

**Paper:** [Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation](https://hf.co/papers/2502.02464)",High,6.0
Q&A,CNTXTAI0/arabic_dialects_question_and_answer,4.0,52.0,2025-01-31 10:41:40+00:00,mit,0.0,1.18 MB,1237319.68,551 KB,564224.0,500,none,none,"---
license: mit
task_categories:
- question-answering
- text-generation
- text2text-generation
language:
- ar
- en
tags:
- arabic
- arabicdialicts
- msa
- Q&A
- STEM
- math
pretty_name: Build your arabic model with arabic data
size_categories:
- 100K<n<1M
---
Data Content
The file provided: Q/A Reasoning dataset
contains the following columns:
1. 2. ID # : Denotes the reference ID for:
a. Question
b. Answer to the question
c. Hint
d. Reasoning
e. Word count for items a to d above
Dialects: Contains the following dialects in separate columns:
a. English
b. MSA
c. Emirati
d. Egyptian
e. Levantine Syria
f. Levantine Jordan
g. Levantine Palestine
h. Levantine Lebanon
Data Generation Process
The following are the steps that were followed to curate the data:
1. 2. A Question and its answer is generated in English.
The Hint and Reasoning for the Question and Answer is provided in subsequent
rows.
3. The Word Count row is populated with equivalent word count in the following format:
a. Word Count for Question & Answer - sums up total words in the Question and
the Answer
b. Word Count for Hint & Reasoning - sums up total count of words in the hint
and reasoning
c. Total Word Count - Sums up total words in categories a & b above.
4. Steps 1-3 is repeated across all Arabic dialects - MSA, Emirati, Egyptian, Levantine
Syria, Levantine Jordan, Levantine Palestine, Levantine Lebanon
www.cntxt.tech
support@cntxtai.ai
2
Data Review Process
CNTXT employs thorough review steps to ensure highest quality of data. The following
quality checks are conducted to ensure the output data is to the highest standards:
Post the conclusion of the data generation process, the review process starts. The team of
reviewers is in 2 layers:
1. Review Layer 1: The first set of reviewers are assigned to check the sentence
coherence, grammatical correctness and accuracy in the answers provided. If the
coherence and correctness of the QA is accurate, the review layer 1 passes it on to
Review Layer 2 else they submit the QAs back to annotators for regeneration
2. Review Layer 2: This layer of review checks the correctness of the hint and
reasoning sections as well as the word count accuracy. If these elements are all
correct, the item under review is considered ready for submission to the customer,
else the reviewer edits to ensure the accuracy of these elements and submits their
comments on items corrected.
The diagram below shows the steps described above:
![IMG_5724.jpeg](https://cdn-uploads.huggingface.co/production/uploads/679c9da27b0963691deadefb/QNrgybWRVygGBRGmOWUiE.jpeg)

![IMG_5725.jpeg](https://cdn-uploads.huggingface.co/production/uploads/679c9da27b0963691deadefb/yFpbInK-GryD6A7Qo-CkT.jpeg)

Total Questions: 800
Total Answers:800
Total Hints:800
Total Reasoning:800
Total Question & Answer Word Count: 65,765
Total Hint & Reasoning Word Count:40,483
Total Word Count: 106, 248",Low,1.0
Q&A,sadnblueish/quran-cqa,0.0,15.0,2025-02-01 22:02:57+00:00,gpl-3.0,0.0,33.3 MB,34917580.8,7.57 MB,7937720.32,6236,none,none,"---
configs:
- config_name: default
  data_files:
  - split: train
    path: ""train.json""

license: gpl-3.0
task_categories:
- question-answering
language:
- en
- ur
- ar
size_categories:
- 1K<n<10K
---

# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Q&A,ZiadWael/MedQA_Arabic_Dataset,1.0,31.0,2025-02-04 13:49:11+00:00,mit,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: mit
task_categories:
- question-answering
language:
- ar
tags:
- medical
size_categories:
- 100K<n<1M
---",Low,1.0
Q&A,madihalim/deep-fast,1.0,49.0,2025-02-04 20:25:52+00:00,mit,0.0,33.6 KB,34406.4,26.6 KB,27238.4,31,none,none,"---
license: mit
task_categories:
- question-answering
language:
- en
- ar
tags:
- poetry
pretty_name: >-
  Deep & Fast, poems to remember the pandemic as a testament to the urgency of
  connection
size_categories:
- n<1K
---
Deep and Fast is a collection of poems to remember the pandemic as a testament to the urgency of connection, poems about how fast we go deep.

This specific data set, made of the poem titles and poems themselves is meant to fine tune an LLM to adopt the poet's voice.",Low,1.0
Q&A,aibrahiam/resmo,0.0,35.0,2025-02-06 07:52:47+00:00,apache-2.0,0.0,1.93 KB,1976.32,1.93 KB,1976.32,2,none,none,"---
license: apache-2.0
task_categories:
- question-answering
- text-classification
language:
- ar
- en
tags:
- legal
- art
- finance
- biology
- chemistry
size_categories:
- n<1K
dataset_info:
  features:
  - name: prompt
    dtype: string
  splits:
  - name: train
    num_bytes: 362
    num_examples: 2
  download_size: 1932
  dataset_size: 362
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
---
",Medium,2.0
Q&A,rwmasood/transcirpt-seerah-dr-yasir-qadhi,0.0,21.0,2025-02-09 08:57:14+00:00,cc-by-4.0,0.0,3.71 MB,3890216.96,3.71 MB,3890216.96,103,none,none,"---
dataset_info:
  features:
  - name: lecture
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 7020342
    num_examples: 103
  download_size: 3711610
  dataset_size: 7020342
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: cc-by-4.0
task_categories:
- question-answering
- text-generation
language:
- en
- ar
size_categories:
- 100K<n<1M
---



# Seerah of Prophet Muhammad (SAW) – Lecture Transcripts by Dr. Yasir Qadhi

## Overview
This dataset contains the full **transcripts** of all **104 lectures** delivered by **Dr. Yasir Qadhi** on the **Seerah (biography) of Prophet Muhammad (SAW)**. These lectures provide a comprehensive, chronological account of the **life, teachings, and historical context** of the Prophet (SAW), covering key events such as:

- His **birth and early life** in Mecca.
- The **revelation of Prophethood** and the message of Islam.
- **Persecution in Mecca** and the migration (Hijrah) to Medina.
- The **establishment of the first Islamic state**.
- **Major battles and treaties**, including Badr, Uhud, and Hudaybiyyah.
- His **final days and passing**.

The original lectures are available on **SoundCloud**: [Here](https://soundcloud.com/zikr/sets/seerah-of-prophet-muhammad)

## Dataset Details
- **Source**: Transcripts of Dr. Yasir Qadhi’s lectures.
- **Number of Lectures**: 104.
- **Language**: English.
- **Format**: Plain text.
- **Topics Covered**:
  - Life of Prophet Muhammad (SAW)
  - Meccan & Medinan periods
  - Key battles and major historical events
  - Personal interactions and teachings of the Prophet

## Use Cases
This dataset is ideal for:
- **Pretraining & Fine-tuning LLMs** on **Islamic history & Seerah**.
- **Developing a QA system** for answering questions about **Prophet Muhammad (SAW)**.
- **Islamic research & academic studies** on the biography of the Prophet.
- **Speech-to-text analysis** of long-form religious discourse.

## Potential Applications
- **AI-driven Islamic Chatbots** for answering Seerah-related queries.
- **Educational Tools & Apps** for learning about Islamic history.
- **Conversational AI for religious studies**.
- **Islamic knowledge graph development**.


## How to Use
### Load Dataset in Python
```python
from datasets import load_dataset,

seerah_data = load_dataset(""rwmasood/transcirpt-seerah-dr-yasir-qadhi"")

print(seerah_data)
```

## **Citation**
If you use this dataset in your research or project, please cite it as follows:

📌 **Dr. Wasif Masood** (2024). *Seerah of Prophet Muhammad (SAW) - Lecture Transcripts by Dr. Yasir Qadhi*. Version 1.0.  
Available at: [https://huggingface.co/datasets/rwmasood/transcirpt-seerah-dr-yasir-qadhi](https://huggingface.co/datasets/rwmasood/transcirpt-seerah-dr-yasir-qadhi)

### **BibTeX:**
```bibtex
@dataset{rwmasood_seerah_transcripts_2024,
  author={Dr. Yasir Qadhi},
  title     = {Seerah of Prophet Muhammad (SAW) - Lecture Transcripts by Dr. Yasir Qadhi},
  year      = {2024},
  publisher = {Hugging Face},
  url       = {https://huggingface.co/datasets/rwmasood/transcirpt-seerah-dr-yasir-qadhi},
  version   = {1.0},
  license   = {CC BY 4.0},
  institution = {Empirisch Tech GmbH}
}",High,6.0
Q&A,nylancer/OpenShark,0.0,10.0,2025-02-10 16:32:58+00:00,mit,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: mit
task_categories:
- text-classification
- table-question-answering
- question-answering
- translation
- text-generation
- text-to-audio
language:
- bn
- en
- hi
- ar
- ur
tags:
- chemistry
- biology
- finance
- legal
- code
- medical
- climate
pretty_name: openshark
size_categories:
- 10K<n<100K
---

# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Q&A,karamkaram/AI_Questions_Dataset_,0.0,39.0,2025-02-14 00:18:55+00:00,cc-by-sa-4.0,0.0,630 KB,645120.0,13.8 KB,14131.2,2000,none,none,"---
license: cc-by-sa-4.0
task_categories:
- question-answering
language:
- ar
size_categories:
- 1K<n<10K
---",Low,1.0
Q&A,YuRiVeRTi/V1Q,3.0,510.0,2025-03-11 05:39:24+00:00,apache-2.0,0.0,161 MB,168820736.0,161 MB,168820736.0,185799,none,none,"---
license: apache-2.0
task_categories:
- text-classification
- token-classification
- table-question-answering
- question-answering
- zero-shot-classification
- summarization
- translation
- feature-extraction
- text-generation
- text2text-generation
- sentence-similarity
- fill-mask
- text-to-speech
- text-to-audio
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
- image-to-text
- image-to-image
- image-to-video
- unconditional-image-generation
- video-classification
- reinforcement-learning
- tabular-classification
- robotics
- tabular-regression
- tabular-to-text
- table-to-text
- multiple-choice
- text-retrieval
- time-series-forecasting
- text-to-video
- visual-question-answering
- zero-shot-image-classification
- graph-ml
- mask-generation
- zero-shot-object-detection
- text-to-3d
- image-to-3d
- image-feature-extraction
- video-text-to-text
language:
- en
- aa
- ab
- ae
- af
- ak
- am
- an
- ar
- as
- av
- ay
- az
- ba
- be
- bg
- bh
- bi
- bm
- bn
- bo
- br
- bs
- ca
- ce
- ch
- co
- cr
- cs
- cu
- cv
- cy
- da
- de
- dv
- dz
- ee
- el
- eo
- es
- et
- eu
- fa
- ff
- fi
- fj
- fo
- fr
- fy
- ga
- gd
- gl
- gn
- gu
- gv
- ha
- he
- hi
- ho
- hr
- ht
- hu
- hy
- hz
- ia
- id
- ie
- ig
- ii
- ik
- io
- is
- it
- iu
- ja
- jv
- ka
- kg
- ki
- kj
- kk
- kl
- km
- kn
- ko
- kr
- ks
- ku
- kv
- kw
- ky
- la
- lb
- lg
- li
- ln
- lo
- lt
- lu
- lv
- mg
- mh
- mi
- mk
- ml
- mn
- mr
- ms
- 'no'
- my
- na
- nb
- nd
- ne
- mt
- ng
- nl
- nn
- nr
- nv
- ny
- oc
- oj
- om
- or
- os
- pa
- pi
- pl
- ps
- pt
- qu
- rm
- rn
- ro
- ru
- sm
- rw
- sc
- sd
- se
- sg
- si
- sk
- sl
- sn
- so
- sq
- sr
- ss
- st
- su
- sv
- sw
- ta
- te
- tg
- th
- ti
- tk
- tl
- tn
- to
- tr
- ts
- tt
- sa
- tw
- ty
- ug
- uk
- ur
- uz
- ve
- vi
- vo
- wa
- wo
- xh
- yi
- yo
- za
- zh
- zu
tags:
- code
- chemistry
- synthetic
size_categories:
- n>1T
pretty_name: VQ1
---
from datasets import load_dataset

ds = load_dataset(""b3x0m/Chinese-H-Novels"")
import sagemaker
import boto3
from sagemaker.huggingface import HuggingFaceModel

try:
	role = sagemaker.get_execution_role()
except ValueError:
	iam = boto3.client('iam')
	role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']

# Hub Model configuration. https://huggingface.co/models
hub = {
	'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',
	'HF_TASK':'any-to-any'
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
	transformers_version='4.37.0',
	pytorch_version='2.1.0',
	py_version='py310',
	env=hub,
	role=role, 
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
	initial_instance_count=1, # number of instances
	instance_type='ml.m5.xlarge' # ec2 instance type
)",Low,1.0
Q&A,Jr23xd23/Arabic-Optimized-Reasoning-Dataset,2.0,32.0,2025-02-25 13:29:24+00:00,apache-2.0,0.0,7.45 MB,7811891.2,2.99 MB,3135242.24,1600,none,none,"---
license: apache-2.0
task_categories:
- question-answering
- table-question-answering
language:
- ar
tags:
- reasoning
- dataset
- llm
- fine-tuning
pretty_name: arabic reasoning dataset
size_categories:
- 1K<n<10K
---
# Arabic Optimized Reasoning Dataset

**Dataset Name**: Arabic Optimized Reasoning  
**License**: Apache-2.0  
**Formats**: CSV  
**Size**: 1600 rows  
**Base Dataset**: cognitivecomputations/dolphin-r1  
**Libraries Used**: Datasets, Dask, Croissant

## Overview

The **Arabic Optimized Reasoning Dataset** helps AI models get better at reasoning in Arabic. While AI models are good at many tasks, they often struggle with reasoning in languages other than English. This dataset helps fix this problem by:

1. Using fewer tokens, which makes training more efficient
2. Teaching models to tell simple questions from complex ones that need step-by-step reasoning

The dataset is based on Dolphin-R1 and has 1,600 rows translated to Arabic.

## Dataset Structure

Each entry in the dataset has:
```csv
instruction, input, output
```

* **instruction**: What the model needs to do
* **input**: Information to help answer the question
* **output**: The expected answer

### Tags for Reasoning

The dataset uses tags to mark different types of questions:

* **Simple questions**:
```xml
<think> This query is simple; no detailed reasoning is needed. </think>
```

* **Complex questions**:
```xml
<think> This query is complex and requires multi-step reasoning. </think>
```

### Example:

Here's an example from the dataset:
```csv
""instruction"": ""You are an AI assistant that always begins by assessing whether detailed reasoning is needed before answering..."",
""input"": ""أنت متسلق صخور ذو خبرة قضيت سنوات عديدة في استكشاف جنوب غرب أمريكا واكتشاف بعض من أفضل مسارات التسلق في المنطقة. كيف أصبحت مهتمًا بتسلق الصخور لأول مرة؟"",
""output"": ""<think> This query is simple; no detailed reasoning is needed. </think>
أنت تعلم، لطالما كنت منجذبًا إلى الهواء الطلق والمغامرة. نشأت في أريزونا، وقضيت الكثير من الوقت في التنزه وحمل الحقائب مع عائلتي في الجبال والصحراء. لكن أعتقد أن ما أثار اهتمامي حقًا في التسلق كان رحلة قمت بها إلى سيدونا عندما كنت مراهقًا. كنت أتسلق كاثيدرال روك مع بعض الأصدقاء، ووجدنا مجموعة من المتسلقين يشقون طريقهم إلى أعلى وجه الصخرة. كنت مفتونًا بالطريقة التي يتحركون بها، والتركيز والشدة في أعينهم... لقد أُسرت في تلك اللحظة.""
```

## Key Features

* **Arabic Translation**: Data from Dolphin-R1 translated to Arabic
* **Separates Question Types**: Tells simple questions from complex ones
* **Uses Fewer Tokens**: Makes training more efficient
* **Better Arabic Reasoning**: Specifically made to improve reasoning in Arabic

## Acknowledgments

This dataset is inspired by the Deepseek-R1 reasoning data from `cognitivecomputations/dolphin-r1`. No data from Gemini was used.

## Citation

If you use this dataset, please cite:

```
@software{jaber2025arabic,
  author = {Jaber Jaber, Zaid Rjoub},
  title = {Arabic Optimized Reasoning Dataset},
  url = {https://github.com/jaberjaber23/Arabic-Optimized-Reasoning-Dataset},
  year = {2025}
}
```

## Support

If you find this dataset useful, please star the GitHub repository: [jaberjaber23/Arabic-Optimized-Reasoning-Dataset](https://github.com/jaberjaber23/Arabic-Optimized-Reasoning-Dataset)",High,5.0
Q&A,Ahmed-Selem/Shifaa_Arabic_Medical_Consultations,3.0,103.0,2025-02-28 22:26:35+00:00,apache-2.0,0.0,329 MB,344981504.0,132 MB,138412032.0,84422,none,none,"---
license: apache-2.0
language:
- ar
pretty_name: s
task_categories:
- question-answering
- text-classification
- text-generation
- zero-shot-classification
tags:
- medical
size_categories:
- 10K<n<100K
---
# Shifaa Arabic Medical Consultations 🏥📊  

![Shifaa Logo](Shifaa.png)  

## Overview 🌍  

Shifaa is **revolutionizing Arabic medical AI** by addressing the **critical gap** in Arabic medical datasets. Our first contribution is the **Shifaa Arabic Medical Consultations** dataset, a comprehensive collection of **84,422** real-world medical consultations covering **16 Main Specializations** and **585 Hierarchical Diagnoses**.  

🔍 **Why is this dataset important?**  
- **First large-scale Arabic medical dataset** for AI applications.  
- **Highly structured and clean** – no missing values, duplicates, or unnecessary columns.  
- **Hierarchical categorization** of medical specialties for better **NLP and AI training**.  
- **Detailed responses** (273 words on average) provide **deep medical insights**.  

## Dataset Details 📑  

- **Number of consultations:** `84,422`  
- **Main Specializations:** `16`  
- **Hierarchical Diagnoses:** `585`  
- **Average answer length:** `273 words`  
- **Languages:** `Arabic`  

### Data Distribution 📊  

![Dataset Distribution](path_to_data_distribution.png)  

The dataset covers a **wide range of medical fields**, making it suitable for various AI-driven applications such as:  
✅ **Medical Chatbots** 🤖  
✅ **Predictive Models** 🔮  
✅ **AI-powered Recommendation Systems** 🏥  
✅ **Arabic Medical NLP Research** 📖  

### Data Hierarchy 🌐  

Our dataset follows a **hierarchical structure** to enhance precision and usability. Example for **Obstetrics and Gynecology**:  

![Obstetrics and Gynecology Hierarchy](path_to_obstetrics_hierarchy.png)  


### Word Cloud 🌟  

![Word Cloud](path_to_wordcloud.png)  

This **word cloud** visualizes the most frequently used words in **questions and answers related to muscle diseases**.

## Data Source 🔗

The dataset was collected from the [Islam Web](https://www.islamweb.net/) website using web-scraping tools.


## Preprocessing & Cleaning 🛠️  

✔ **Removed duplicates and null values**  
✔ **Corrected corrupted data**  
✔ **Standardized hierarchical categories for consistency**  
✔ **Eliminated hidden spaces and excess words**  

## Usage & Applications 🚀  

Shifaa Arabic Medical Consultations is **designed for AI and NLP applications**, making it ideal for:  

- **Medical Question Answering Systems**  
- **Arabic Medical Chatbots**  
- **AI-powered Medical Assistants**  
- **Healthcare Research & AI-driven Text Analysis**  

## How to Use 📥  

Download the dataset from **Hugging Face** and start building your **Arabic medical AI applications**!  

```python
from datasets import load_dataset

dataset = load_dataset(""Ahmed-Selem/Shifaa_Arabic_Medical_Consultations"")",High,4.0
Q&A,Ahmed-Selem/Shifaa_Arabic_Mental_Health_Consultations,9.0,271.0,2025-03-08 13:10:50+00:00,apache-2.0,0.0,206 MB,216006656.0,91.8 MB,96259276.8,35648,none,none,"---
license: apache-2.0
language:
- ar
pretty_name: e
task_categories:
- question-answering
- text-classification
- zero-shot-classification
tags:
- medical
size_categories:
- 10K<n<100K
---

# 🏥 Shifaa Arabic Mental Health Consultations 🧠

![Shifaa Logo](Shifaa.png)  

## **📌 Overview**  
**Shifaa Arabic Mental Health Consultations** is a high-quality dataset designed to advance **Arabic medical language models**.  
This dataset provides **35,648 real-world medical consultations**, covering a wide range of mental health concerns.  

## **📊 Dataset Summary**  
- **Size:** **35,648 consultations**  
- **Main Specializations:** **7**  
- **Specific Diagnoses:** **123**  
- **Languages:** Arabic (العربية)  
 

### **Why This Dataset?**  
🔹 **Lack of Arabic medical data:** Most NLP datasets focus on English, leaving a gap in **Arabic medical AI**.  
🔹 **Detailed Q&A format:** Answers provide **in-depth context** on symptoms, diagnoses, and treatments.  
🔹 **Structured Hierarchy:** Data is categorized into **Main Specialization → Specific Diagnosis**.  
🔹 **High Quality:** Data is cleaned, **free from missing values & duplicates**.  

---

## **📂 Dataset Features**  
Each row in the dataset contains:  

| Column Name              | Description |
|-------------------------|-------------|
| `Question Title`        | Short summary of the consultation |
| `Question`             | Full question asked by the patient |
| `Answer`              | Doctor’s detailed response |
| `Doctor Name`         | Name of the responding doctor |
| `Consultation Number` | Unique consultation identifier |
| `Date of Answer`      | When the answer was provided |
| `Hierarchical Diagnosis` | **Main Specialization → Specific Diagnosis** |

---

## **🖼️ Data Distribution**  
![Data Distribution](data_distribution.png)  

- **7 Main Specializations** (e.g., Depression, Anxiety, PTSD)  
- **123 Specific Diagnoses** categorized under specializations  
- **Varied Question Lengths** ranging from **short inquiries to complex cases**  

---

## **🔗 Usage & Applications**  
This dataset is ideal for:  
✅ **Medical AI Assistants** – Training **chatbots & virtual doctors**  
✅ **Text Classification** – Categorizing **Arabic medical questions**  
✅ **Question Answering** – Improving **Arabic medical search engines**  
✅ **Predictive Models** – Detecting **mental health risks**  

---


The dataset was collected from the [Islam Web](https://www.islamweb.net/) website using web-scraping tools.

# 🌟 Stay Connected with Us on LinkedIn! 🚀
🔗 **[Connect with Us on LinkedIn](https://www.linkedin.com/company/shifaa-ai/)**  ",Medium,3.0
Q&A,UBC-NLP/palm,9.0,75.0,2025-03-03 20:58:46+00:00,cc-by-nc-nd-4.0,0.0,1.3 MB,1363148.8,1.3 MB,1363148.8,1926,none,none,"---
dataset_info:
  features:
  - name: id
    dtype: int64
  - name: country
    dtype: string
  - name: topic
    dtype: string
  - name: language_variety
    dtype: string
  - name: instruction
    dtype: string
  - name: input
    dtype: string
  - name: output
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: test
    num_bytes: 2662695
    num_examples: 1926
  download_size: 1304868
  dataset_size: 2662695
configs:
- config_name: default
  data_files:
  - split: test
    path: data/test-*
license: cc-by-nc-nd-4.0
task_categories:
- question-answering
- text-generation
language:
- ar
tags:
- chemistry
- music
- art
- culture
- Arabic
pretty_name: >-
  Palm: A Culturally Inclusive and Linguistically Diverse Dataset for Arabic
  LLMs
---

This repository contains evaluation data for the Palm dataset — a Culturally Inclusive and Linguistically Diverse dataset for Arabic LLMs




",Medium,2.0
Q&A,lenoxaugo254/LenoxAI,1.0,4.0,2025-03-04 04:48:43+00:00,none,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
task_categories:
- text-classification
- table-question-answering
- question-answering
- zero-shot-classification
- translation
- summarization
- feature-extraction
- text-generation
- text2text-generation
- fill-mask
- sentence-similarity
- text-to-speech
- text-to-audio
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
language:
- aa
- ab
- ae
- af
- ak
- am
- an
- ar
- as
- av
- ay
- az
- ba
- be
- bg
- bh
- bi
- bm
- bn
- bo
- br
- bs
- ca
- ce
- ch
- co
- cr
- cs
- cu
- cv
- cy
- da
- de
- dv
- dz
- ee
- el
- en
- eo
- es
- et
- eu
- fa
- ff
- fi
- fj
- fo
- fr
- fy
- ga
- gd
- gl
- gn
- gu
- gv
- ha
- he
- hi
- ho
- hr
- ht
- hu
- hy
- hz
- ia
- id
- ie
- ig
- ii
- ik
- io
- is
- it
- iu
- ja
- jv
- ka
- kg
- ki
- kj
- kk
- kl
- km
- lb
- kr
- ku
- ko
- kn
- ks
- kv
- kw
- ky
- la
- lg
- li
- ln
- lo
- lt
- lu
- lv
- mg
- mh
- mi
- mk
- ml
- mn
tags:
- chemistry
- biology
- finance
- legal
- music
- art
- code
- climate
- medical
- not-for-all-audiences
- synthetic
pretty_name: Lenox Augo
size_categories:
- 1M<n<10M
---",Low,0.0
Q&A,blcuicall/omgeval,0.0,11.0,2025-03-24 08:17:32+00:00,none,0.0,16.7 MB,unknown,8.67 MB,unknown,9798,https://arxiv.org/abs/2402.13524,none,"---
task_categories:
- text-generation
- text2text-generation
- question-answering
language:
- ar
- en
- es
- zh
- fr
- ru
- pt
- it
- ko
- ja
configs:
- config_name: full
  data_files:
  - split: arabic
    path: arabic.json
  - split: chinese
    path: chinese.json
  - split: english
    path: english.json
  - split: spanish
    path: spanish.json
  - split: french
    path: french.json
  - split: russia
    path: russia.json
  - split: korean
    path: korean.json
  - split: japanese
    path: japanese.json
  - split: italian
    path: italian.json
  - split: portuguese
    path: portuguese.json
  - split: russia_local
    path: russia_local.json
  - split: french_local
    path: french_local.json
  - split: chinese_local
    path: chinese_local.json
  - split: arabic_local
    path: arabic_local.json
  - split: spanish_local
    path: spanish_local.json
  - split: korean_local
    path: korean_local.json
  - split: japanese_local
    path: japanese_local.json
  - split: portuguese_local
    path: portuguese_local.json
  - split: italian_local
    path: italian_local.json
---

# Dataset Card for ""omgeval""

<!-- Provide a quick summary of the dataset. -->

We introduce OMGEval, the first Open-source Multilingual Generative test set that can assess the capability of LLMs in different languages. 
For each language, OMGEval provides 804 open-ended questions, covering a wide range of important capabilities of LLMs, such as general knowledge, logical reasoning, and so on. 
Each question is rigorously verified by human annotators. 
Notably, to sufficiently reflect the compatibility of LLMs in different cultural backgrounds, we perform localization for each non-English language. 
Specifically, the current version of OMGEval includes 9 languages (i.e., Zh, Ru, Fr, Es, Ar, Pt, It, Ko, Ja).

## Dataset Details

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** https://github.com/blcuicall/OMGEval
- **Paper:** https://arxiv.org/pdf/2402.13524

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

### Annotations

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Citation
```
@misc{liu2024omgeval,
      title={OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models}, 
      author={Yang Liu and Meng Xu and Shuo Wang and Liner Yang and Haoyu Wang and Zhenghao Liu and Cunliang Kong and Yun Chen and Yang Liu and Maosong Sun and Erhong Yang},
      year={2024},
      eprint={2402.13524},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```",Medium,3.0
Q&A,gurgutan/quran-tafseer-qurancom,0.0,13.0,2025-03-13 12:20:15+00:00,mit,0.0,4.48 MB,4697620.48,6.73 MB,7056916.48,6236,none,none,"---
license: mit
task_categories:
- question-answering
language:
- ar
- en
pretty_name: A Dataset of Quran Verses and Tafseer
size_categories:
- 1K<n<10K
---

# Quran and Tafseer Dataset

## Dataset Description


    This dataset contains verses from the Quran along with their tafseer (interpretation/explanation).
    It includes both the original Arabic text and translations, as well as detailed tafseer from scholars.
    The dataset is useful for Islamic studies, NLP tasks related to religious texts, and cross-lingual research.
    

## Dataset Structure

The dataset contains Quran verses along with their tafseer (interpretation/explanation).


### Data Fields

- `id`: Unique identifier for each ayah (verse)
- `ayah_n`: Ayah (verse) number
- `ayah_text`: Text of the ayah in translation
- `ayah_text_ar`: Original Arabic text of the ayah
- `ayah_url`: URL to the source of the ayah text
- `ayah_name`: Name of the text owner
- `ayah_translator`: Translator of the Quran
- `ayah_language`: Language of the translation
- `surah_n`: Surah (chapter) number
- `surah_name_english`: English name of the surah
- `surah_name_arab`: Arabic name of the surah
- `surah_revelation_place`: Place where the surah was revealed
- `surah_meaning`: Meaning of the surah name
- `surah_type`: Type of the surah
- `tafsir_text`: Tafseer (interpretation) text of corresponding ayah
- `tafsir_language`: Language of the tafseer
- `tafsir_author`: Author of the tafseer
- `tafsir_translator`: Translator of the tafseer
- `tafsir_url`: URL to the source of the tafseer
- `tafsir_name`: Name of the tafseer

## Source
This dataset was compiled from various Islamic sources.
Tafsir sources, suharhs structure and metadata was compiled from files of https://github.com/hablullah/data-quran repository.
Holy Quran and translation from https://tanzil.net/.
English translation from [Saheeh International](https://quranenc.com/en/browse/english_saheeh). 

## Citing & Authors

If you find our work helpful, feel free to give us a cite.
Dataset was created by Slepvochev Ivan. If you use this dataset, please cite: 

@dataset{quran_tafsir,
    title = {QuranDataset: A Dataset of Quran Verses and Tafseer},
    url = {https://escape-team.tech/},
    author = {Slepovichev Ivan},
    email = {gurgutan@yandex.ru},
    month = {March},
    year = {2025}
}",Medium,3.0
Q&A,nanonets/nn-auto-bench-ds,3.0,69.0,2025-03-13 06:51:58+00:00,mit,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: mit
task_categories:
- question-answering
language:
- en
- de
- ar
size_categories:
- 1K<n<10K
---
# nn-auto-bench-ds

`nn-auto-bench-ds` is a dataset designed for key information extraction (KIE) and serves as a benchmark dataset for [nn-auto-bench](<BLOGLINK>).

## Dataset Overview
The dataset comprises **1,000 documents**, categorized into the following types:

1. **Invoice**
2. **Receipt**
3. **Passport**
4. **Bank Statement**

The documents are primarily available in English, with some also in German and Arabic. Each document is annotated for key information extraction and specific tasks. The dataset can be used to compute LLM's oneshot performance on KIE tasks.

## Dataset Schema
The dataset includes the following columns:

- **`image_path`**: File path to the document image.
- **`content`**: OCR-extracted text from the image.
- **`accepted`**: Ground truth answer.
- **`Queried_labels`**: Labels, fields, or keys targeted for extraction.
- **`Queried_col_headers`**: Column headers targeted for extraction.
- **`ctx_1`**: OCR text from an example document.
- **`ctx_1_image_path`**: File path to the example document’s image.
- **`ctx_1_accepted`**: Ground truth answer for the example document.

There are total 54 unique fields/keys/labels that we want to extract from the documents.

## Loading the Dataset
To load the dataset in Python using the `datasets` library:

```python
from datasets import load_dataset

dataset = load_dataset(""nanonets/nn-auto-bench-ds"")
```

## Data Sources
This dataset aggregates information from multiple open-source datasets, including:

1. [German Invoices Dataset](https://huggingface.co/datasets/Aoschu/German_invoices_dataset)
2. [Personal Financial Dataset for India](https://www.kaggle.com/datasets/mehaksingal/personal-financial-dataset-for-india)
3. [RVL-CDIP Invoice Dataset](https://huggingface.co/datasets/chainyo/rvl-cdip-invoice)
4. [FATURA Dataset](https://zenodo.org/records/8261508)
5. [Find It Again](http://l3i-share.univ-lr.fr/2023Finditagain/index.html)
6. [Generated USA Passports Dataset](https://huggingface.co/datasets/TrainingDataPro/generated-usa-passeports-dataset/tree/main/data)
7. [Synthetic Passports Dataset](https://huggingface.co/datasets/UniDataPro/synthetic-passports)

This dataset is valuable for benchmarking key information extraction models and advancing research in document understanding and natural language processing (NLP).",Medium,3.0
Q&A,gurgutan/sunnah_ar_en_dataset,0.0,22.0,2025-03-13 08:30:41+00:00,mit,0.0,16.7 MB,17511219.2,28.2 MB,29569843.2,50762,none,none,"---
license: mit
task_categories:
- translation
- question-answering
language:
- en
- ar
pretty_name: Hadiths 14 books collection
size_categories:
- 10K<n<100K
---

# Dataset Card for Dataset Name

# Dataset Card for Hadiths 14 Books Collection

This dataset contains a comprehensive bilingual (Arabic-English) collection of hadiths from 14 major authenticated books of Islamic tradition. It includes over 50762 narrations with complete metadata, organized by book, chapter, and narrator. Each hadith is presented in both its original Arabic text and English translation, making it ideal for cross-lingual NLP tasks, Islamic question-answering systems, and religious chatbots. The structured format allows for easy retrieval, citation, and integration into AI applications focused on Islamic knowledge.

Hadiths 14 books collection. Books included:
- Sahih al-Bukhari صحيح البخاري
- Sahih Muslim صحيح مسلم
- Sunan Abi Dawud سنن أبي داود
- Jami` at-Tirmidhi جامع الترمذي
- Sunan an-Nasa'i سنن النسائي
- Sunan Ibn Majah سنن ابن ماجه
- Muwatta Malik موطأ مالك
- Musnad Ahmad مسند أحمد
- Sunan ad-Darimi سنن الدارمي
- Riyad as-Salihin رياض الصالحين
- Shamail al-Muhammadiyah الشمائل المحمدية
- Bulugh al-Maram بلوغ المرام
- Al-Adab Al-Mufrad الأدب المفرد
- Mishkat al-Masabih مشكاة المصابيح

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->
Hadith are the transmitted narrations concerning the speech, actions, appearance, and approvals of the Messenger of Allah, the Prophet Muhammad (peace and blessings be upon him).

- **Curated by:** [Slepovichev Ivan](https://github.com/gurgutan)
- **Language(s) (NLP):** [English, Arabic]
- **License:** [MIT](https://mit-license.org/)

### Dataset Sources [optional]

- **Repository:** [hadith-database](https://github.com/zafhi/hadith-database)

## Uses

This dataset is used for research purposes.


## Dataset Structure

The dataset consists of hadith records from 14 books, with each record containing metadata and text in both English and Arabic. Each entry in the dataset follows this structure:

```json
{
  ""book_id"": ""Unique identifier for the book"",
  ""book_title_en"": ""Book title in English"",
  ""book_author_en"": ""Book author name in English"",
  ""book_title_ar"": ""Book title in Arabic"",
  ""book_author_ar"": ""Book author name in Arabic"",
  ""hadith_uid"": ""Unique identifier for the hadith"",
  ""hadith_book_id"": ""Book identifier the hadith belongs to"",
  ""hadith_chapter_id"": ""Chapter identifier within the book"",
  ""hadith_chapter_name_ar"": ""Chapter name in Arabic"",
  ""hadith_chapter_name_en"": ""Chapter name in English"",
  ""hadith_text_ar"": ""Full hadith text in Arabic"",
  ""hadith_text_en"": ""Full hadith text in English"",
  ""hadith_narrator_en"": ""Name of the narrator in English""
}
```

**Fields Description:**
- book_id: Unique identifier for each of the 14 hadith books in the collection
- book_title_en/ar: Title of the hadith book in English and Arabic
- book_author_en/ar: Author of the hadith book in English and Arabic
- hadith_uid: Unique identifier for each hadith across the entire collection
- hadith_book_id: Identifier for hadith within the book containing this hadith
- hadith_chapter_id: Identifier for the chapter within the book
- hadith_chapter_name_en/ar: Name of the chapter in English and Arabic
- hadith_text_en/ar: The full text of the hadith in English and Arabic
- hadith_narrator_en: The name of the narrator in English

The dataset contains hadiths from 14 major collections, providing a comprehensive resource for hadith studies, translation tasks, and question-answering applications in both English and Arabic.

## Dataset Creation

### Curation Rationale

This dataset was curated to provide a comprehensive, bilingual collection of authentic hadiths from 14 major books for use in AI applications, particularly:

1. Islamic question-answering systems
2. Religious chatbots and virtual assistants
3. Cross-lingual information retrieval systems
4. Educational tools for Islamic studies

The parallel English-Arabic structure makes it especially valuable for developing systems that can process and respond to religious queries in multiple languages.

### Source Data

#### Data Collection and Processing

The hadiths were collected from reliable Islamic sources and digitized with careful attention to authenticity and accuracy. The processing workflow included:

1. Extraction of hadiths from original Arabic sources
2. Verification against authenticated collections
3. Alignment with established English translations
4. Structuring into a consistent JSON format
5. Assignment of unique identifiers for cross-referencing
6. Organization by book, chapter, and narrative chain

The dataset maintains the original chapter structure of each book, preserving the contextual organization of the hadiths as intended by their original compilers.

#### Who are the source data producers?

The original hadiths were compiled by renowned Islamic scholars including Imam Bukhari, Imam Muslim, and others who meticulously collected and verified these narrations during the formative period of Islamic scholarship. The digital compilation and structuring was performed by Slepovichev Ivan, drawing from established translations and digital Islamic libraries.

### Intended Uses

This dataset is specifically designed for:

- Training AI models to answer questions about Islamic teachings based on authentic hadiths
- Developing conversational agents that can discuss Islamic topics with proper scriptural references
- Creating search and retrieval systems for Islamic knowledge
- Supporting comparative religious studies and research
- Enabling multilingual access to hadith literature

Researchers and developers can use this dataset to train models that provide evidence-based responses to religious queries, with the ability to cite specific hadiths as sources for the information provided.

## Bias, Risks, and Limitations

### Recommendations

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation

If you find our work helpful, feel free to give us a cite.
Dataset was created by Slepvochev Ivan. If you use this dataset, please cite: 

@dataset{quran_tafsir,
    title = {QuranDataset: A Dataset of Quran Verses and Tafseer},    
    url = {https://escape-team.tech/},    
    author = {Slepovichev Ivan},    
    email = {gurgutan@yandex.ru},    
    month = {March},    
    year = {2025}
}

## Dataset Card Contact

gurgutan@yandex.ru
hi@escape-team.tech",High,4.0
Q&A,rokokot/question-type-and-complexity,0.0,34.0,2025-03-30 13:36:03+00:00,cc-by-sa-4.0,0.0,26.5 MB,27787264.0,15.9 MB,16672358.4,189640,none,none,"---
configs:
  - config_name: base
    data_files:
      - split: train
        path: tydi_train_base.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  # Question Type Control Tasks (3 seeds)
  - config_name: control_question_type_seed1
    data_files:
      - split: train
        path: tydi_train_control_question_type_seed1.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_question_type_seed2
    data_files:
      - split: train
        path: tydi_train_control_question_type_seed2.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_question_type_seed3
    data_files:
      - split: train
        path: tydi_train_control_question_type_seed3.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  # Combined Complexity Control Tasks (3 seeds)
  - config_name: control_complexity_seed1
    data_files:
      - split: train
        path: tydi_train_control_complexity_seed1.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_complexity_seed2
    data_files:
      - split: train
        path: tydi_train_control_complexity_seed2.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_complexity_seed3
    data_files:
      - split: train
        path: tydi_train_control_complexity_seed3.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  # Individual Complexity Metrics Control Tasks (3 seeds each)
  - config_name: control_avg_links_len_seed1
    data_files:
      - split: train
        path: tydi_train_control_avg_links_len_seed1.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_avg_links_len_seed2
    data_files:
      - split: train
        path: tydi_train_control_avg_links_len_seed2.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_avg_links_len_seed3
    data_files:
      - split: train
        path: tydi_train_control_avg_links_len_seed3.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_avg_max_depth_seed1
    data_files:
      - split: train
        path: tydi_train_control_avg_max_depth_seed1.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_avg_max_depth_seed2
    data_files:
      - split: train
        path: tydi_train_control_avg_max_depth_seed2.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_avg_max_depth_seed3
    data_files:
      - split: train
        path: tydi_train_control_avg_max_depth_seed3.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_avg_subordinate_chain_len_seed1
    data_files:
      - split: train
        path: tydi_train_control_avg_subordinate_chain_len_seed1.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_avg_subordinate_chain_len_seed2
    data_files:
      - split: train
        path: tydi_train_control_avg_subordinate_chain_len_seed2.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_avg_subordinate_chain_len_seed3
    data_files:
      - split: train
        path: tydi_train_control_avg_subordinate_chain_len_seed3.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_avg_verb_edges_seed1
    data_files:
      - split: train
        path: tydi_train_control_avg_verb_edges_seed1.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_avg_verb_edges_seed2
    data_files:
      - split: train
        path: tydi_train_control_avg_verb_edges_seed2.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_avg_verb_edges_seed3
    data_files:
      - split: train
        path: tydi_train_control_avg_verb_edges_seed3.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_lexical_density_seed1
    data_files:
      - split: train
        path: tydi_train_control_lexical_density_seed1.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_lexical_density_seed2
    data_files:
      - split: train
        path: tydi_train_control_lexical_density_seed2.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_lexical_density_seed3
    data_files:
      - split: train
        path: tydi_train_control_lexical_density_seed3.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_n_tokens_seed1
    data_files:
      - split: train
        path: tydi_train_control_n_tokens_seed1.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_n_tokens_seed2
    data_files:
      - split: train
        path: tydi_train_control_n_tokens_seed2.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
        
  - config_name: control_n_tokens_seed3
    data_files:
      - split: train
        path: tydi_train_control_n_tokens_seed3.csv
      - split: test
        path: ud_test_base.csv
      - split: validation
        path: dev_base.csv
language: 
  - ar
  - en
  - fi
  - id
  - ja
  - ko
  - ru
license: cc-by-sa-4.0
annotations_creators:
  - found
  - machine-generated
language_creators:
  - found
task_categories:
  - text-classification
  - question-answering
task_ids:
  - text-scoring
  - intent-classification
  - extractive-qa
multilinguality: multilingual
size_categories:
  - 1K<n<10K
source_datasets:
  - original
  - extended|universal-dependencies
  - extended|tydiqa
pretty_name: Question Type and Complexity Dataset
---

# Question Type and Complexity (QTC) Dataset

## Dataset Overview

The Question Type and Complexity (QTC) dataset is a comprehensive resource for linguistics/NLP research focusing on question classification and linguistic complexity analysis across multiple languages. It contains questions from two distinct sources (TyDi QA and Universal Dependencies v2.15), automatically annotated with question types (polar/content) and a set of linguistic complexity features.

**Key Features:**

- 2 question types (polar and content questions) across 7 languages
- 6 numeric linguistic complexity metrics, all normalized using min-max scaling
- Combined/summed complexity scores
- Train(silver)/test(gold)/dev(mix) split using complementary data sources
- Control datasets for evaluating probe selectivity

## Data Sources

### TyDi QA (Training Set)

The primary source for our training data is the TyDi QA dataset (Clark et al., 2020), a typologically diverse question answering benchmark spanning 11 languages. We extracted questions from 7 languages (Arabic, English, Finnish, Indonesian, Japanese, Korean, and Russian), classified them into polar (yes/no) or content (wh-) questions, and analyzed their linguistic complexity.

**Reference:**
Clark, J. H., Choi, E., Collins, M., Garrette, D., Kwiatkowski, T., Nikolaev, V., & Palomaki, J. (2020). TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages. Transactions of the Association for Computational Linguistics, 2020.

### Universal Dependencies (Test Set)

For our test set, we extracted questions from the Universal Dependencies (UD) treebanks (Nivre et al., 2020). UD treebanks provide syntactically annotated sentences across numerous languages, allowing us to identify and extract questions with high precision. We chose UD as our gold standard test set because it provides syntactically annotated data across all our target languages and the universal annotation scheme ensures consistency across languages. Moreover, the high-quality manual annotations make it ideal as a gold standard for evaluation.

**Reference:**
Nivre, J., de Marneffe, M.-C., Ginter, F., Hajič, J., Manning, C. D., Pyysalo, S., Schuster, S., Tyers, F., & Zeman, D. (2020). Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection. In Proceedings of the 12th Language Resources and Evaluation Conference (pp. 4034-4043).

## Data Collection and Processing

### TyDi QA Processing

Data extraction for TyDi began with accessing the dataset via the HuggingFace datasets library. For question classification, we developed language-specific rule-based classifiers using regex and token matching to identify polar and content questions. Languages with well-documented grammatical question markers (Finnish -ko/-kö, Japanese か, English wh-words, etc.) were particularly amenable to accurate classification, as these markers serve as reliable indicators. We verified classification accuracy by cross-checking between our rule-based approach and existing annotations where available.

### Universal Dependencies Processing

The treebanks were chosen partly based on their mean absolute rankings as surveyed by Kulmizev and Nivre (2023). We processed the UD treebanks' CoNLL-U files to extract questions using sentence-final punctuation (?, ？, ؟), language-specific interrogative markers, and syntactic question patterns identifiable through dependency relations. For syntactic processing, we used UDPipe (Straka et al., 2016), which handled tokenization, lemmatization, morphological analysis, and dependency parsing with language-specific models trained on UD treebanks.

Our classification system used the `ud_classifier.py` script to identify and categorize questions from CoNLL-U files based on language-specific pattern matching for interrogative features. Questions were classified as polar or content based on their morphosyntactic properties, with careful filtering to remove incomplete questions, rhetorical questions, and other edge cases that could affect classification accuracy.

**Reference:**
Straka, M., Hajic, J., & Straková, J. (2016). UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, POS Tagging and Parsing. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16) (pp. 4290-4297).

Kulmizev, A. & Nivre, J. (2023). Investigating UD Treebanks via Dataset Difficulty Measures. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics (pp. 1076-1089).  

### Linguistic Complexity Feature Scoring

Our linguistic analysis pipeline consisted of two main components. First, we processed each question through UDPipe to generate CoNLL-U format parse trees using our `scripts/data-processing/run_udpipe.py`. These parsed trees were then analyzed using our `scripts/data_processing/profiling-UD/custom-profile.py` script to extract linguistic features. We normalized the results and aggregated them to provide a single complexity score for each question.

The feature extraction framework extends the approach of Brunato et al. (2020) on linguistic complexity profiling. This allowed us to process parsed sentences and extract a comprehensive set of complexity features that capture different dimensions of linguistic difficulty.

**Reference:**
Brunato, D., Cimino, A., Dell'Orletta, F., Venturi, G., & Montemagni, S. (2020). Profiling-UD: A Tool for Linguistic Profiling of Texts. In Proceedings of The 12th Language Resources and Evaluation Conference (pp. 7145-7151).

## Preprocessing and Feature Extraction

We normalized all linguistic features using min-max scaling per language. This approach ensures cross-linguistic comparability by mapping each feature to a 0-1 range for each language separately.

For the TyDi data, we applied strategic downsampling using token-based stratified sampling. This balances the distribution across languages and question types while preserving the original sentence length distribution, resulting in a more balanced dataset without sacrificing linguistic diversity.

The final step involved calculating a combined complexity score from the normalized features. This provides researchers with a single metric that consolidates multiple dimensions of linguistic complexity into one value for easier analysis and comparison.

## Dataset Structure

The dataset is organized into three main components corresponding to the train/dev/test splits:

```text
QTC-Dataset
├── base                               
│   ├── tydi_train_base.csv           
│   ├── dev_base.csv                   
│   └── ud_test_base.csv               
├── control_question_type_seed1        
│   ├── tydi_train_control_question_type_seed1.csv
│   ├── dev_base.csv
│   └── ud_test_base.csv
├── control_complexity_seed1          
│   ├── tydi_train_control_complexity_seed1.csv
│   ├── dev_base.csv
│   └── ud_test_base.csv
└── control_[metric]_seed[n]           
    ├── tydi_train_control_[metric]_seed[n].csv
    ├── dev_base.csv
    └── ud_test_base.csv
```

## Control Tasks
The dataset includes control task variants for evaluating probe selectivity, following the methodology of Hewitt & Liang (2019). Each control task preserves the structure of the original dataset but with randomized target values:

- Question Type Controls: Three seeds of randomly shuffled question type labels (within each language)
- Complexity Score Controls: Three seeds of randomly shuffled complexity scores (within each language)
- Individual Metric Controls: Three seeds for each of the six linguistic complexity metrics

These control tasks allow researchers to assess whether a probe is truly learning linguistic structure or simply memorizing patterns in the data.

**Reference:**
Hewitt, J., & Liang, P. (2019). Designing and Interpreting Probes with Control Tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (pp. 2733-2743).


## Features Description

### Core Attributes

| Feature | Type | Description |
|---------|------|-------------|
| `unique_id` | string | Unique identifier for each question |
| `text` | string | The question text |
| `language` | string | ISO language code (ar, en, fi, id, ja, ko, ru) |
| `question_type` | int | Binary encoding (0 = content, 1 = polar) |
| `complexity_score` | float | Combined linguistic complexity score |
| `lang_norm_complexity_score`| float | Language-normalized complexity score (0-1)|

### Linguistic Features

| Feature | Description | Normalization |
|---------|-------------|---------------|
| `avg_links_len` | Average syntactic dependency length | Min-max scaling by language |
| `avg_max_depth` | Average maximum dependency tree depth | Min-max scaling by language |
| `avg_subordinate_chain_len` | Average length of subordinate clause chains | Min-max scaling by language |
| `avg_verb_edges` | Average number of edges connected to verbal nodes | Min-max scaling by language |
| `lexical_density` | Ratio of content words to total words | Min-max scaling by language |
| `n_tokens` | Number of tokens in the question | Min-max scaling by language |

## Linguistic Feature Significance

### Syntactic Complexity

The `avg_links_len` feature captures the average syntactic dependency length, which indicates processing difficulty as syntactically related elements become further apart. Longer dependencies typically correlate with increased cognitive processing load. Similarly, `avg_max_depth` measures the depth of dependency trees, with deeper structures indicating higher levels of embedding and consequently greater syntactic complexity.

### Hierarchical Structure

The `avg_subordinate_chain_len` feature quantifies the length of subordinate clause chains. Longer chains create more dispersed hierarchical structures, which can be harder to process and understand. This feature helps capture how clausal embedding contributes to overall question complexity.

### Lexical and Semantic Load

The `lexical_density` feature measures the ratio of content words to total words. Higher density indicates a greater proportion of information-carrying words relative to function words, resulting in higher information density. The `avg_verb_edges` feature counts the average number of edges connected to verbal nodes, with more edges indicating more complex predicate-argument structures. Finally, `n_tokens` captures sentence length, which correlates with information content and overall processing difficulty.

## Silver and Gold Standard Data

### Silver Standard (TyDi QA)

The TyDi QA component serves as our silver standard training data. It offers a larger volume of questions drawn from real-world information-seeking contexts. These questions were automatically processed and classified through our custom pipeline, then strategically downsampled to balance distribution across languages and question types. The TyDi data represents authentic question complexity in information retrieval scenarios, making it ideal for training models to recognize patterns in question complexity across languages.

### Gold Standard (Universal Dependencies)

The Universal Dependencies component forms our gold standard test set. These questions come with manually annotated syntactic structures, providing high-quality linguistic information. The UD data represents a diverse range of linguistic contexts and genres, and unlike the TyDi data, it was not downsampled to preserve all available gold-standard annotations. While smaller in volume, the UD component offers superior annotation quality and precision, making it an ideal benchmark for evaluating question complexity models.

## Usage Examples

### Basic Usage
```python
from datasets import load_dataset

# Load the base dataset
dataset = load_dataset(""rokokot/question-type-and-complexity"", name=""base"")

# Access the training split (TyDi data)
tydi_data = dataset[""train""]

# Access the validation split (Dev data)
dev_data = dataset[""validation""]

# Access the test split (UD data)
ud_data = dataset[""test""]

# Filter for a specific language
finnish_questions = dataset[""train""].filter(lambda x: x[""language""] == ""fi"")

# Filter for a specific type
polar_questions = dataset[""train""].filter(lambda x: x[""question_type""] == 1)
content_questions = dataset[""train""].filter(lambda x: x[""question_type""] == 0)
```
### Working with Control Tasks
```python
from datasets import load_dataset

# Load the original dataset
original_data = load_dataset(""rokokot/question-type-and-complexity"", name=""base"")

# Load question type control tasks
question_control1 = load_dataset(""rokokot/question-type-and-complexity"", name=""control_question_type_seed1"")
question_control2 = load_dataset(""rokokot/question-type-and-complexity"", name=""control_question_type_seed2"")
question_control3 = load_dataset(""rokokot/question-type-and-complexity"", name=""control_question_type_seed3"")

# Load complexity score control tasks
complexity_control1 = load_dataset(""rokokot/question-type-and-complexity"", name=""control_complexity_seed1"")

# Load individual metric control tasks
links_control = load_dataset(""rokokot/question-type-and-complexity"", name=""control_avg_links_len_seed1"")
depth_control = load_dataset(""rokokot/question-type-and-complexity"", name=""control_avg_max_depth_seed2"")
```
## Research Applications

This dataset enables various research directions:

1. **Cross-linguistic question complexity**: Investigate how syntactic complexity varies across languages and question types.
2. **Question answering systems**: Analyze how question complexity affects QA system performance.
3. **Language teaching**: Develop difficulty-aware educational materials for language learners.
4. **Psycholinguistics**: Study processing difficulty predictions for different question constructions.
5. **Machine translation**: Evaluate translation symmetry for questions of varying complexity.

## Citation

If you use this dataset in your research, please cite it as follows:

```bibtex
@dataset{rokokot2025qtc,
  author    = {Robin Kokot},
  title     = {Question Type and Complexity (QTC) Dataset},
  year      = {2025},
  publisher = {Hugging Face},
  howpublished = {\url{https://huggingface.co/datasets/rokokot/question-complexity}},
}
```

Additionally, please cite the underlying data sources and tools:

```bibtex
@article{clark2020tydi,
  title={TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},
  author={Clark, Jonathan H and Choi, Eunsol and Collins, Michael and Garrette, Dan and Kwiatkowski, Tom and 
  Nikolaev, Vitaly and Palomaki, Jennimaria},  
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={454--470},
  year={2020},
  publisher={MIT Press}
}

@inproceedings{nivre2020universal,
  title={Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection},
  author={Nivre, Joakim and de Marneffe, Marie-Catherine and Ginter, Filip and Haji{\v{c}}, Jan and Manning, 
  Christopher D and Pyysalo, Sampo and Schuster, Sebastian and Tyers, Francis and Zeman, Daniel},
  booktitle={Proceedings of the 12th Language Resources and Evaluation Conference},
  pages={4034--4043},
  year={2020}
}

@inproceedings{straka2016udpipe,
  title={UDPipe: Trainable Pipeline for Processing CoNLL-U Files Performing Tokenization, Morphological Analysis, 
  POS Tagging and Parsing},
  author={Straka, Milan and Haji{\v{c}}, Jan and Strakov{\'a}, Jana},
  booktitle={Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)},
  pages={4290--4297},
  year={2016}
}

@inproceedings{brunato2020profiling,
  title={Profiling-UD: A Tool for Linguistic Profiling of Texts},
  author={Brunato, Dominique and Cimino, Andrea and Dell'Orletta, Felice and Venturi, Giulia and Montemagni, Simonetta},
  booktitle={Proceedings of The 12th Language Resources and Evaluation Conference},
  pages={7145--7151},
  year={2020}
}
```

## License

This dataset is released under the [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) license, in accordance with the licensing of the underlying TyDi QA and Universal Dependencies datasets.

## Acknowledgments

This dataset builds upon the work of the TyDi QA and Universal Dependencies research communities. We are grateful for their contributions to multilingual NLP resources. The linguistic complexity analysis was supported by the tools released by Brunato et al. (2020) and Straka et al.(2016). We acknowledge the critical role of UDPipe in providing robust syntactic parsing across multiple languages, which formed the foundation of our feature extraction pipeline.
",High,6.0
Q&A,inceptionai/AraGen,0.0,818.0,2025-03-26 16:10:25+00:00,cc-by-nc-4.0,0.0,42.9 MB,44983910.4,42.9 MB,44983910.4,23157,none,none,"---
dataset_info:
- config_name: AraGen-12-24
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: Round
      dtype: int64
    - name: SN.
      dtype: int64
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Question 1
    dtype: string
  - name: Answer 1
    dtype: string
  - name: Question 2
    dtype: string
  - name: Answer 2
    dtype: string
  splits:
  - name: test
    num_bytes: 217879
    num_examples: 279
  download_size: 103314
  dataset_size: 217879
- config_name: AraGen-12-24-1024m__PHI_4_Hindi_4bit
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1406394
    num_examples: 279
  download_size: 534328
  dataset_size: 1406394
- config_name: AraGen-12-24-ALLaM_AI__ALLaM_7B_Instruct_preview
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1233662
    num_examples: 279
  download_size: 434088
  dataset_size: 1233662
- config_name: AraGen-12-24-CohereForAI__aya_23_35B
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1135701
    num_examples: 279
  download_size: 451305
  dataset_size: 1135701
- config_name: AraGen-12-24-CohereForAI__aya_23_8B
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1134542
    num_examples: 279
  download_size: 447881
  dataset_size: 1134542
- config_name: AraGen-12-24-CohereForAI__aya_expanse_32b
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1631467
    num_examples: 279
  download_size: 673456
  dataset_size: 1631467
- config_name: AraGen-12-24-CohereForAI__aya_expanse_8b
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1688230
    num_examples: 279
  download_size: 691185
  dataset_size: 1688230
- config_name: AraGen-12-24-CohereForAI__c4ai_command_a_03_2025
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1602852
    num_examples: 279
  download_size: 667808
  dataset_size: 1602852
- config_name: AraGen-12-24-CohereForAI__c4ai_command_r7b_12_2024
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1477582
    num_examples: 279
  download_size: 577044
  dataset_size: 1477582
- config_name: AraGen-12-24-CohereForAI__c4ai_command_r7b_arabic_02_2025
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1511441
    num_examples: 279
  download_size: 601081
  dataset_size: 1511441
- config_name: AraGen-12-24-CohereForAI__c4ai_command_r_08_2024
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1374962
    num_examples: 279
  download_size: 540491
  dataset_size: 1374962
- config_name: AraGen-12-24-CohereForAI__c4ai_command_r_plus
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1367692
    num_examples: 279
  download_size: 552736
  dataset_size: 1367692
- config_name: AraGen-12-24-CohereForAI__c4ai_command_r_plus_08_2024
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1492049
    num_examples: 279
  download_size: 596615
  dataset_size: 1492049
- config_name: AraGen-12-24-CohereForAI__c4ai_command_r_v01
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1427019
    num_examples: 279
  download_size: 596857
  dataset_size: 1427019
- config_name: AraGen-12-24-Conception__aml_arabic_small_2025_02_20
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1530074
    num_examples: 279
  download_size: 600246
  dataset_size: 1530074
- config_name: AraGen-12-24-FreedomIntelligence__AceGPT_13B_chat
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1292243
    num_examples: 279
  download_size: 493579
  dataset_size: 1292243
- config_name: AraGen-12-24-FreedomIntelligence__AceGPT_7B_chat
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1600210
    num_examples: 279
  download_size: 575494
  dataset_size: 1600210
- config_name: AraGen-12-24-FreedomIntelligence__AceGPT_v1_5_13B_Chat
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1268885
    num_examples: 279
  download_size: 399479
  dataset_size: 1268885
- config_name: AraGen-12-24-FreedomIntelligence__AceGPT_v2_32B_Chat
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1090267
    num_examples: 279
  download_size: 403844
  dataset_size: 1090267
- config_name: AraGen-12-24-FreedomIntelligence__AceGPT_v2_70B_Chat
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 961455
    num_examples: 279
  download_size: 374936
  dataset_size: 961455
- config_name: AraGen-12-24-FreedomIntelligence__AceGPT_v2_8B_Chat
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - name: Model Name
      dtype: string
    - name: Params
      dtype: string
    - name: Precision
      dtype: string
    - name: Revision
      dtype: string
    - name: Round
      dtype: string
    - name: SN.
      dtype: string
    - name: Source
      dtype: string
    - name: Sub-Category
      dtype: string
    - name: Task
      dtype: string
  - name: Benchmark
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
    - name: Question 1
      dtype: string
    - name: Question 2
      dtype: string
  - name: Model Answers
    struct:
    - name: Answer 1
      dtype: string
    - name: Answer 2
      dtype: string
  - name: Evaluation
    struct:
    - name: 3C3H Scores
      struct:
      - name: Complete
        dtype: float64
      - name: Concise
        dtype: float64
      - name: Correct
        dtype: float64
      - name: Final Score
        dtype: float64
      - name: Harmless
        dtype: float64
      - name: Helpful
        dtype: float64
      - name: Honest
        dtype: float64
    - name: Judge Comments
      dtype: string
    - name: Judge Name
      dtype: string
  splits:
  - name: train
    num_bytes: 1089604
    num_examples: 279
  download_size: 393462
  dataset_size: 1089604
- config_name: AraGen-12-24-MaziyarPanahi__calme_2_1_qwen2_5_72b
  features:
  - name: Meta
    struct:
    - name: Category
      dtype: string
    - name: Comment
      dtype: string
    - name: Dataset
      dtype: string
    - name: Fact Checking
      dtype: string
    - name: License
      dtype: string
    - ",High,5.0
Q&A,miscovery/General_Facts_in_English_Arabic_Egyptian_Arabic,12.0,66.0,2025-04-06 07:10:44+00:00,mit,1.0,16.3 MB,17091788.8,7.44 MB,7801405.44,37149,none,none,"---
license: mit
task_categories:
- question-answering
- translation
- text-generation
- fill-mask
language:
- en
- ar
pretty_name: World_Facts_in_English_Arabic_Egyptian_Arabic
size_categories:
- 10K<n<100K
---


# 🌍 World Facts in English, Arabic & Egyptian Arabic (v1.0) (Categorized)

The **World Facts General Knowledge Dataset (v1.0)** is a high-quality, human-reviewed Q&A resource by **Miscovery**. It features general facts categorized across **50+ knowledge domains**, provided in **three languages**:

- 🌍 **English**
- 🇸🇦 **Modern Standard Arabic (MSA)**
- 🇪🇬 **Egyptian Arabic (Dialect)**

Each entry includes:
- The **question** and **answer**
- A **category** and **sub-category**
- **Language tag** (en, ar, ar_eg)
- Basic metadata: question & answer **word/character counts**

---

## ✅ Main Features

- ✅ **Human-reviewed**, 99% clean and high-quality data  
- 🌐 **Multilingual support** (EN, AR, AR_EG)  
- 🗂️ **Over 50 categories** including History, Science, Technology, Literature, Politics, and more  
- 🧠 **LLM-generated** and manually refined for clarity and accuracy  
- 📚 **Ideal for NLP tasks**: translation, classification, cross-lingual QA, cultural analysis

---

## 📁 Data Format

Each record follows this structure:

question, answer, category, sub_category, language, question_char_length, answer_char_length, question_word_count, answer_word_count


### 🔍 Example Entry

| Question (AR_EG)                                   | Answer                                                                                                  | Category | Sub-Category | Language |
|----------------------------------------------------|---------------------------------------------------------------------------------------------------------|----------|--------------|----------|
| مين اللي اخترع شبكة الويب العالمية؟              | بيقولوا على العالم البريطاني تيم بيرنرز لي هو اللي اخترع شبكة الويب العالمية...                       | History  | Discoveries  | ar_eg    |

---

## 🔓 License & Citation

MIT License

---

For any suggestions or contributions, feel free to reach out to **Miscovery**.



",Medium,3.0
Q&A,Irina-Na/AutenticHadithes,2.0,42.0,2025-04-18 18:18:57+00:00,cc-by-sa-4.0,0.0,13.6 MB,14260633.6,5.65 MB,5924454.4,2194,none,none,"---
license: cc-by-sa-4.0
task_categories:
- question-answering
language:
- ar
- en
size_categories:
- 1K<n<10K
---",Low,1.0
Q&A,Almansoorialikhalifa/Stage2-3k_Arabic_ChatML,0.0,35.0,2025-05-08 06:56:43+00:00,none,4.0,39.5 MB,41418752.0,39.5 MB,41418752.0,3533,none,none,"---
dataset_info:
  features:
  - name: answer
    dtype: string
  - name: id
    dtype: string
  - name: messages
    list:
    - name: content
      dtype: string
    - name: role
      dtype: string
  splits:
  - name: train
    num_bytes: 100820492
    num_examples: 3533
  download_size: 39506410
  dataset_size: 100820492
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- question-answering
language:
- ar
size_categories:
- 1K<n<10K
---",Low,1.0
Q&A,miscovery/arabic_egypt_english_world_facts,13.0,130.0,2025-04-26 02:18:29+00:00,mit,1.0,15.5 MB,16252928.0,7.51 MB,7874805.76,11798,none,none,"---
license: mit
task_categories:
- question-answering
- translation
- text-generation
- fill-mask
language:
- en
- ar
pretty_name: Arabic_Egypt_English_World_Facts
size_categories:
- 10K<n<100K
---


# 🌍 Version (v2.0) World Facts in English, Arabic & Egyptian Arabic (Categorized)

The **World Facts General Knowledge Dataset (v2.0)** is a high-quality, human-reviewed Q&A resource by **Miscovery**. It features general facts categorized across **50+ knowledge domains**, provided in **three languages**:

- 🌍 **English**
- 🇸🇦 **Modern Standard Arabic (MSA)**
- 🇪🇬 **Egyptian Arabic (Dialect)**

Each entry includes:
- The **question** and **answer**
- A **category** and **sub-category**
- **Language tag** (en, ar, ar_eg)
- Basic metadata: question & answer **word/character counts**

---

## ✅ Main Features

- ✅ **Human-reviewed**, 99% clean and high-quality data  
- 🌐 **Multilingual support** (EN, AR, AR_EG)  
- 🗂️ **Over 50 categories** including History, Science, Technology, Literature, Politics, and more  
- 🧠 **LLM-generated** and manually refined for clarity and accuracy  
- 📚 **Ideal for NLP tasks**: translation, classification, cross-lingual QA, cultural analysis

---

## 📁 Data Format

Each record follows this structure:

en_question, ar_question, ar_eg_question, en_answer, ar_answer, ar_eg_answer, category, sub_category, en_q_char, en_a_char, en_q_word, en_a_word, ar_q_char, ar_a_char, ar_q_word, ar_a_word, ar_eg_q_char, ar_eg_a_char, ar_eg_q_word, ar_eg_a_word


---

## 🔓 License & Citation

MIT License

---

For any suggestions or contributions, feel free to reach out to **Miscovery**.



",Medium,2.0
Q&A,miscovery/Math_CoT_Arabic_English_Reasoning,16.0,137.0,2025-05-12 00:14:13+00:00,mit,1.0,3.34 MB,3502243.84,1.59 MB,1667235.84,2834,none,none,"---
license: mit
task_categories:
- question-answering
- text-generation
- fill-mask
language:
- en
- ar
pretty_name: Math CoT Arabic English Reasoning
size_categories:
- 1K<n<10K
tags:
- code
---


# Math CoT Arabic English Dataset

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A high-quality, bilingual (English & Arabic) dataset for Chain-of-Thought (COT) reasoning in mathematics and related disciplines, developed by Miscovery AI.

## Overview

Math-COT is a unique dataset designed to facilitate and benchmark the development of chain-of-thought reasoning capabilities in language models across mathematical domains. With meticulously crafted examples, explicit reasoning steps, and bilingual support, this dataset offers a robust foundation for training and evaluating mathematical reasoning abilities.

## Key Features

- **99% Clean & High-Quality Data**: Human-reviewed, accurately annotated examples with verified solutions
- **Bilingual Support**: Complete English and Arabic parallel content for cross-lingual research and applications
- **Structured Reasoning Steps**: Each problem solution is broken down into explicit step-by-step reasoning
- **Diverse Subject Coverage**: Spans 21 different categories within mathematics and adjacent fields
- **Comprehensive Format**: Includes questions, answers, reasoning chains, and relevant metadata


## Dataset Structure

Each entry in the dataset contains the following fields:

```
{
  ""en_question"": ""Question text in English"",
  ""ar_question"": ""Question text in Arabic"",
  ""en_answer"": ""Detailed step-by-step solution in English"",
  ""ar_answer"": ""Detailed step-by-step solution in Arabic"",
  ""category"": ""Mathematical category"",
  ""en_q_word"": ""Word count of English question"",
  ""ar_q_word"": ""Word count of Arabic question"",
  ""en_a_word"": ""Word count of English answer"",
  ""ar_a_word"": ""Word count of Arabic answer""
}
```

## Categories

The dataset covers 21 distinct categories:

1. Mathematics - Arithmetic
2. Mathematics - Algebra
3. Mathematics - Geometry
4. Mathematics - Trigonometry
5. Mathematics - Calculus
6. Mathematics - Linear Algebra
7. Mathematics - Probability
8. Mathematics - Statistics
9. Mathematics - Set Theory
10. Mathematics - Number Theory
11. Mathematics - Discrete Math
12. Mathematics - Topology
13. Mathematics - Differential Equations
14. Mathematics - Real Analysis
15. Math Puzzles
16. Linguistics
17. Logic and Reasoning
18. Philosophy
19. Sports and Games
20. Psychology
21. Cultural Traditions

## Example

Here's a sample entry from the dataset:

```json
{
  ""en_question"": ""A bag contains only red and blue balls. If one ball is drawn at random, the probability that it is red is 2/5. If 8 more red balls are added, the probability of drawing a red ball becomes 4/5. How many blue balls are there in the bag?"",
  ""ar_question"": ""تحتوي الحقيبة على كرات حمراء وزرقاء فقط. إذا تم سحب كرة واحدة عشوائيًا ، فإن احتمال أن تكون حمراء هو 2/5. إذا تمت إضافة 8 كرات حمراء أخرى ، يصبح احتمال سحب كرة حمراء 4/5. كم عدد الكرات الزرقاء الموجودة في الحقيبة؟"",
  ""en_answer"": ""#step 1: Let r be the number of red balls and b be the number of blue balls. We have r / (r + b) = 2/5 and (r + 8) / (r + b + 8) = 4/5.\n#step 2: From the first equation, 5r = 2r + 2b => 3r = 2b.\n#step 3: From the second equation, 5(r + 8) = 4(r + b + 8) => 5r + 40 = 4r + 4b + 32 => r + 8 = 4b.\n#step 4: Substituting r = (2/3)b into the second equation, (2/3)b + 8 = 4b => 8 = (10/3)b => b = 24/10 = 12/5. This is wrong, check the math.\n#step 5: r = 2b/3 , and r + 8 = 4b/5. substitute r. 2b/3 + 8 = 4b  => 2b + 24 = 12b => 10b = 24 => b = 12/5. There must be an error.\n#step 6: 5r = 2(r+b), 5(r+8)=4(r+b+8), 5r=2r+2b, 5r+40 = 4r+4b+32 => r+8 = 4b-r, => r = 4b-8, 5(2b/3) + 40 = 4(2b/3) + 32, 10/3 b+40 = 8/3b +32, 2/3b = -8, b =12.\n#step 7: If b=12, r = (2/5)(r+12), 5r = 2r + 24, r = 8. (8+8)/(8+12+8)= 16/28 = 4/7 != 4/5.  The step 6 equations are wrong. r/(r+b) = 2/5, (r+8)/(r+b+8) = 4/5. then 5r = 2r+2b so 3r=2b. Then 5r+40 = 4r+4b+32 so r+8 = 4b-32, 2b/3+8 = 4b/5. so r+8 = 4b+32 so r + 8 = 4/5 (r+b+8), and r+8 = 4/5 r + 4/5b + 32/5, =>1/5r - 4b/5 = -8/5, r-4b = -8, and r = 4b-8. 3r=2b, 3(4b-8) = 2b so 12b - 24 = 2b => 10b =24 so b=2.4\n#step 8: r/(r+b) = 2/5, (r+8)/(r+b+8) = 4/5. So we get 5r = 2r + 2b, meaning 3r = 2b and we get 5(r+8) = 4(r+b+8), meaning 5r + 40 = 4r + 4b + 32, so r + 8 = 4b. substitute b = 3r/2 , r + 8 = 6r so 5r = 8. then r=8/5.\n#step 9: 5r=2r+2b so r =2b/3 and we have (r+8)/(r+b+8) = 4/5. Substituting yields (2b/3+8)/(2b/3+b+8) = 4/5 => (2b+24)/(5b+24) = 4/5 => 10b + 120 = 20b + 96, so 10b = 24, thus b=2.4. This makes no sense since number of balls can't be a fraction. Recalculating.\n#step 10: Let's check the equation (r+8)/(r+b+8)=4/5. If the red ball is 2, and blue ball is 3 (so total is 5 and r/total = 2/5), (r+8)/(total+8) = (10)/(13), !=4/5. Thus the original numbers of red and blue balls does not have to conform to the ratio. Red = 2k, Blue = 3k. (2k+8)/(5k+8) = 4/5 => 10k + 40 = 20k+32 => 10k = 8 => k=4/5. Thus there are 3k=12/5, so this cannot be the solution. If the problem assumes the original solution MUST be an integer number, then something might be off. Let total = t. So red = 2/5 t. The blue is 3/5 t. And (2/5 t + 8)/(t+8) = 4/5, 10/5 t + 40/5 = 4/5 t + 32/5 so 6/5 t = -8/5 . t < 0 which does not make sense.\n#step 11: 5r = 2r+2b and 5(r+8)=4(r+b+8) , so 3r = 2b, and r+8=4(b+8)/5 so 5r+40 = 4b+32, or 5r -4b = -8. substituting 3r/2 for b, we get 5r - 4(3r/2) = -8, so 5r-6r =-8 so -r=-8. Thus r=8. 3r = 2b => 24 = 2b so b=12.\n#answer: 12"",
  ""ar_answer"": ""#الخطوة 1: ليكن r هو عدد الكرات الحمراء و b هو عدد الكرات الزرقاء. لدينا r / (r + b) = 2/5 و (r + 8) / (r + b + 8) = 4/5.\n#الخطوة 2: من المعادلة الأولى، 5r = 2r + 2b => 3r = 2b.\n#الخطوة 3: من المعادلة الثانية، 5(r + 8) = 4(r + b + 8) => 5r + 40 = 4r + 4b + 32 => r + 8 = 4b.\n#الخطوة 4: بالتعويض بـ r = (2/3)b في المعادلة الثانية، (2/3)b + 8 = 4b => 8 = (10/3)b => b = 24/10 = 12/5. هذا خطأ.\n#الخطوة 5: إذا كان r = 2b/3 و r + 8 = 4b/5. نعوض r، 2b/3 + 8 = 4b => 2b + 24 = 12b => 10b = 24 => b = 12/5. يجب أن يكون هناك خطأ.\n#الخطوة 6: 5r = 2(r+b), 5(r+8)=4(r+b+8), 5r=2r+2b, 5r+40 = 4r+4b+32 => r+8 = 4b-r, => r = 4b-8, 5(2b/3) + 40 = 4(2b/3) + 32, 10/3 b+40 = 8/3b +32, 2/3b = -8, b =12.\n#الخطوة 7: إذا كان b=12، r = (2/5)(r+12), 5r = 2r + 24, r = 8. (8+8)/(8+12+8)= 16/28 = 4/7 != 4/5. معادلات الخطوة 6 خاطئة. r/(r+b) = 2/5, (r+8)/(r+b+8) = 4/5. ثم 5r = 2r+2b إذن 3r=2b. ثم 5r+40 = 4r+4b+32 إذن r+8 = 4b-32, 2b/3+8 = 4b/5. إذن r+8 = 4b+32 إذن r + 8 = 4/5 (r+b+8), and r+8 = 4/5 r + 4/5b + 32/5, =>1/5r - 4b/5 = -8/5, r-4b = -8, and r = 4b-8. 3r=2b, 3(4b-8) = 2b إذن 12b - 24 = 2b => 10b =24 إذن b=2.4\n#الخطوة 8: r/(r+b) = 2/5, (r+8)/(r+b+8) = 4/5. لذلك نحصل على 5r = 2r + 2b، مما يعني 3r = 2b ونحصل على 5(r+8) = 4(r+b+8)، مما يعني 5r + 40 = 4r + 4b + 32، إذن r + 8 = 4b. بالتعويض بـ b = 3r/2 , r + 8 = 6r إذن 5r = 8. إذن r=8/5.\n#الخطوة 9: 5r=2r+2b إذن r =2b/3 ولدينا (r+8)/(r+b+8) = 4/5. بالتعويض نحصل على (2b/3+8)/(2b/3+b+8) = 4/5 => (2b+24)/(5b+24) = 4/5 => 10b + 120 = 20b + 96، إذن 10b = 24، بالتالي b=2.4. هذا غير منطقي لأن عدد الكرات لا يمكن أن يكون كسرًا. إعادة الحساب.\n#الخطوة 10: دعنا نتحقق من المعادلة (r+8)/(r+b+8)=4/5. إذا كانت الكرة الحمراء هي 2، والكرة الزرقاء هي 3 (إذن الإجمالي هو 5 وr/الإجمالي = 2/5)، (r+8)/(الإجمالي+8) = (10)/(13)، !=4/5. وبالتالي فإن الأعداد الأصلية للكرات الحمراء والزرقاء لا يجب أن تتوافق مع النسبة. أحمر = 2k، أزرق = 3k. (2k+8)/(5k+8) = 4/5 => 10k + 40 = 20k+32 => 10k = 8 => k=4/5. وبالتالي يوجد 3k=12/5، لذلك لا يمكن أن يكون هذا هو الحل. إذا افترضت المشكلة أن الحل الأصلي يجب أن يكون عددًا صحيحًا، فقد يكون هناك خطأ ما. ليكن الإجمالي = t. إذن الأحمر = 2/5 t. الأزرق هو 3/5 t. و (2/5 t + 8)/(t+8) = 4/5, 10/5 t + 40/5 = 4/5 t + 32/5 إذن 6/5 t = -8/5 . t < 0 وهو أمر غير منطقي.\n#الخطوة 11: 5r = 2r+2b and 5(r+8)=4(r+b+8) , إذن 3r = 2b، و r+8=4(b+8)/5 إذن 5r+40 = 4b+32، أو 5r -4b = -8. بالتعويض بـ 3r/2 عن b، نحصل على 5r - 4(3r/2) = -8، إذن 5r-6r =-8 إذن -r=-8. إذن r=8. 3r = 2b => 24 = 2b إذن b=12.\n#الإجابة: 12"",
  ""category"": ""Math Puzzles"",
  ""en_q_word"": 48,
  ""ar_q_word"": 42,
  ""en_a_word"": 583,
  ""ar_a_word"": 566
}
```

## Usage

This dataset is especially valuable for:

- Training and evaluating mathematical reasoning in language models
- Research on step-by-step problem solving approaches
- Developing educational AI assistants for mathematics
- Cross-lingual research on mathematical reasoning
- Benchmarking Chain-of-Thought (COT) capabilities

## Citation

If you use this dataset in your research, please cite:

```bibtex
@dataset{miscoveryai2025mathcot,
  title={Math_CoT_Arabic_English_Reasoning: A Bilingual Dataset for Chain-of-Thought Mathematical Reasoning},
  author={Miscovery AI},
  year={2025},
  publisher={Hugging Face},
  url={https://huggingface.co/datasets/miscovery/Math_CoT_Arabic_English_Reasoning}
}
```

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Contact

For questions, feedback, or issues related to this dataset, please contact Miscovery AI at [info@miscovery.com](mailto:info@miscovery.com).",High,6.0
Q&A,Hamzah-Asadullah/TinyDS-20k,1.0,166.0,2025-06-09 09:22:53+00:00,mit,0.0,103 MB,108003328.0,46.8 MB,49073356.8,19764,none,none,"---
license: mit
language:
- en
- zh
- hi
- es
- fr
- ar
- bn
- ru
- pt
- ur
- id
- de
- ja
- mr
- te
- tr
- ta
- pa
- wuu
- ko
- vi
- ha
- jv
- arz
- it
- th
- gu
- kn
- fa
- bho
- yo
- ml
task_categories:
- question-answering
- translation
- text-generation
- text2text-generation
tags:
- reasoning
- cot
- thinking
- thoughts
- think
- qwen3
- synthetic
- multilingual
- STEM
- maths
- math
- mathematics
- literature
- chemistry
- physics
- programming
- coding
size_categories:
- 10K<n<100K
---

# TinyDS

![License](https://img.shields.io/badge/License-MIT-blue)
![Qwen3](https://img.shields.io/badge/LLM-Qwen3-red)
![Multilingual](https://img.shields.io/badge/Multilingual-green)

Alpaca-style dataset with around 20k samples scraped from [Qwen3-8B](https://huggingface.co/unsloth/Qwen3-8B-GGUF/blob/main/Qwen3-8B-Q4_K_M.gguf) using [SyntheticAlpaca](https://github.com/Hamzah-Asadullah/SyntheticAlpaca). Q&A pairs can be in 32 different languages, these are listed in the metadata.  
Topics are all around STEM, programming, and literature.  

MIT @ 2025 Hamzah Asadullah

---

[![View Counter](https://count.getloli.com/@Hamzah-Asadullah_TinyDS_20k?theme=booru-lewd)](https://hf.co/datasets/Hamzah-Asadullah/TinyDS-20k)",Low,1.0
Q&A,facebook/multiloko,1.0,203.0,2025-05-14 09:18:33+00:00,mit,0.0,319 MB,unknown,178 MB,unknown,67250,https://arxiv.org/abs/2504.10356,none,"---
license: mit
task_categories:
- question-answering
- text-generation
language:
- ar
- bn
- cs
- zh
- nl
- en
- fa
- fr
- de
- he
- hi
- id
- it
- ja
- km
- ko
- ms
- mr
- pl
- pt
- ro
- ru
- es
- sv
- tl
- th
- tr
- ur
- yue
- vi
pretty_name: MultiLoKo
tags:
- text
configs:
- config_name: default
  data_files:
  - path: ""*/dev.jsonl""
    split: dev
  - path: ""english/dev_translated_machine_*.jsonl""
    split: dev_translated_machine
  - path: ""english/dev_translated_human_*.jsonl""
    split: dev_translated_human 
  - path: ""*/dev_translated_human_english.jsonl""
    split: enloko
- config_name: arabic
  data_files:
  - path: arabic/dev.jsonl
    split: dev
  - path: arabic/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: arabic/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: bengali
  data_files:
  - path: bengali/dev.jsonl
    split: dev
  - path: bengali/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: bengali/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: cantonese
  data_files:
  - path: cantonese/dev.jsonl
    split: dev
  - path: cantonese/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: cantonese/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: czech
  data_files:
  - path: czech/dev.jsonl
    split: dev
  - path: czech/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: czech/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: dutch
  data_files:
  - path: dutch/dev.jsonl
    split: dev
  - path: dutch/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: dutch/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: english
  data_files:
  - path: english/dev.jsonl
    split: dev
  - path: english/dev_translated_machine_*.jsonl
    split: dev_mt
  - path: english/dev_translated_human_*.jsonl
    split: dev_translated
- config_name: farsi
  data_files:
  - path: farsi/dev.jsonl
    split: dev
  - path: farsi/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: farsi/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: french
  data_files:
  - path: french/dev.jsonl
    split: dev
  - path: french/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: french/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: german
  data_files:
  - path: german/dev.jsonl
    split: dev
  - path: german/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: german/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: hebrew
  data_files:
  - path: hebrew/dev.jsonl
    split: dev
  - path: hebrew/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: hebrew/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: hindi
  data_files:
  - path: hindi/dev.jsonl
    split: dev
  - path: hindi/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: hindi/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: indonesian
  data_files:
  - path: indonesian/dev.jsonl
    split: dev
  - path: indonesian/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: indonesian/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: italian
  data_files:
  - path: italian/dev.jsonl
    split: dev
  - path: italian/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: italian/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: japanese
  data_files:
  - path: japanese/dev.jsonl
    split: dev
  - path: japanese/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: japanese/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: khmer
  data_files:
  - path: khmer/dev.jsonl
    split: dev
  - path: khmer/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: khmer/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: korean
  data_files:
  - path: korean/dev.jsonl
    split: dev
  - path: korean/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: korean/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: malay
  data_files:
  - path: malay/dev.jsonl
    split: dev
  - path: malay/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: malay/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: marathi
  data_files:
  - path: marathi/dev.jsonl
    split: dev
  - path: marathi/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: marathi/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: polish
  data_files:
  - path: polish/dev.jsonl
    split: dev
  - path: polish/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: polish/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: portuguese
  data_files:
  - path: portuguese/dev.jsonl
    split: dev
  - path: portuguese/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: portuguese/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: romanian
  data_files:
  - path: romanian/dev.jsonl
    split: dev
  - path: romanian/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: romanian/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: russian
  data_files:
  - path: russian/dev.jsonl
    split: dev
  - path: russian/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: russian/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: spanish
  data_files:
  - path: spanish/dev.jsonl
    split: dev
  - path: spanish/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: spanish/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: swedish
  data_files:
  - path: swedish/dev.jsonl
    split: dev
  - path: swedish/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: swedish/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: tagalog
  data_files:
  - path: tagalog/dev.jsonl
    split: dev
  - path: tagalog/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: tagalog/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: thai
  data_files:
  - path: thai/dev.jsonl
    split: dev
  - path: thai/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: thai/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: traditional_mandarin
  data_files:
  - path: traditional_mandarin/dev.jsonl
    split: dev
  - path: traditional_mandarin/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: traditional_mandarin/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: turkish
  data_files:
  - path: turkish/dev.jsonl
    split: dev
  - path: turkish/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: turkish/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: urdu
  data_files:
  - path: urdu/dev.jsonl
    split: dev
  - path: urdu/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: urdu/dev_translated_human_english.jsonl
    split: dev_translated
- config_name: vietnamese
  data_files:
  - path: vietnamese/dev.jsonl
    split: dev
  - path: vietnamese/dev_translated_machine_english.jsonl
    split: dev_mt
  - path: vietnamese/dev_translated_human_english.jsonl
    split: dev_translated
extra_gated_heading: ""Protecting the integrity of MultiLoKo for evaluation""
extra_gated_fields: 
  I agree not to re-host MultiLoKo in places where it could be picked up by web crawlers: checkbox
  If I evaluate using MultiLoKo, I will ensure that its questions are not in the training data: checkbox
---
# MultiLoKo: a multilingual local knowledge benchmark for LLMs

MultiLoKo is a multilingual knowledge benchmark, covering 30 languages plus English.
The questions are separately sourced for each language, with an annotation protocol designed to target locally relevant topics for the respective language.
MultiLoKo contains the original data for each language, as well as both human and machine-authored translations of each non-English subset into English and vice versa, facilitating studies into a variety of research questions relating to multilinguality.
More information about the benchmark design can be found in the release paper of the benchmark:

* Dieuwke Hupkes and Nikolay Bogoychev.
[MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages](https://arxiv.org/abs/2504.10356).

```
@article{hupkes2025multiloko,
      title={MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages
},
      author = {Dieuwke Hupkes and Nikolay Bogoychev},
      year = {2025},
      journal = {CoRR},
      volume = {abs/2504.10356},
      eprinttype = {arXiv},
      eprint = {2504.10356},
      url = {https://doi.org/10.48550/arXiv.2504.10356},
}
```

## Data

Each language in MultiLoKo has its own subdirectory, containing:
- A jsonl file `dev.jsonl` containing  the 250 locally sourced questions for the respective language.
- A jsonl file `knowledge_fewshot.jsonl`, containing five fewshot examples for the language.
- A jsonl file `dev_translated_human_english.jsonl` with the human translations of the file into English (for English, instead there will be 30 files `dev_translated_human_{language}.jsonl`, for each of the respective languages).
- A jsonl file `dev_translated_machine_english.jsonl` with the machine translations of the file into English (for English, instead there will be 30 files `dev_translated_machine_{language}.jsonl`, for each of the respective languages).

Multilingual prompts can be found in our our github repository, in the file [examples/prompts.py](https://github.com/facebookresearch/multiloko/examples/prompts.py)

### Data format

The benchmark data is stored in jsonl files, containing a separate json object for each question. 
Each such objects has six fields:
- `text`: the paragraph from which the question was created. The answer to the question can be derived from this text. It is possible to transform the benchmark into a reading comprehension benchmark by preceeding the question itself from this text.
- `question`: the question itself.
- `targets`: a list of acceptable (short) answers to the question.
- `target`: a long answer to the question, which could potentially be used for COT. Note that the long answers have not been checked as extensively as the questions and short answers.
- `id`: the wikipedia page that the text is sourced from, along with the rank of that page in the relevant locale.
- `output_type`: the expected type of the output (e.g.\ number, date, year, word, etc), in the question language.
- `source_language`: the language in which the question was originally sourced.
- `question_language`: the language in which the question is asked.
- `translated`: whether the question is translated.
- `translation_type`: whether the translation is machine- or human-authored.

### MultiLoKo-test
MultiLoKo also has a secret test set with 250 examples for each languages, which will be released later on, blindly.
The split is similar to the dev split, apart from the fact that the topics it includes are more obscure.
If you would like to obtain scores for your model on the set, please upload your model on HuggingFace and send us a request to compute scores through the creation of an issue on this [MultiLoKo github page](https://github.com/facebookresearch/multiloko).

We will return the full set of results to you, and put the test results of your model on the leaderboard on this page.
If it is not possible to upload your model on HuggingFace, please reach out to either one of the authors to discuss other options.


## Evaluation
On our github page, you can find an evaluation script [eval.py](https://github.com/facebookresearch/multiloko/eval.py) that can be ran directly on a model's output, provided in jsonl or csv.
The script postprocesses and normalises the model answers and computes f1 score, exact match score, sentence chrf, edit distance between the closest target, edit distance between the first target, and max edit similarity between the targets.
It also computes five aggregate metrics for each of the metrics above: the average score, the maximum and minimum scores (across languages), and the gap between the best and worst performing language. We support both CSV and JSONL. 

### Usage
```
$ ./eval.py --help
usage: eval.py [-h] --dataset_location DATASET_LOCATION --subset SUBSET --predictions PREDICTIONS [--output OUTPUT]

options:
  -h, --help            show this help message and exit
  --dataset_location, -d DATASET_LOCATION
                        Dataset root directory where all language folders are located
  --subset, -s SUBSET   subset, ie dev, test, etc. Should match the jsonl filename inside the individual dataset language folders
  --predictions, -p PREDICTIONS
                        Prediction file to evaluate. Could be CSV or JSONL
  --output, -o OUTPUT   Output file (json) to write results to. If not specified, will print to stdout
./eval.py -d benchmark_data -s dev -p examples/test.json -o testscore.json
```

### Metric cheat sheet

| Metric | Description |
| --- | --- |
| Average EM | The first main metric we use to quantify performance for MultiLoKo is the average Exact Match score across languages, which expresses how many of the answers match one of the gold standard answers verbatim (after post-processing the answers). |
| Gap | The second main metric is the gap between a model’s best and worst performing language. We gap to quantify the extent to which a model has achieved parity across languages. Because a small gap can be achieved both through parity on high scores as parity on low scores, it is most informative in combination with average benchmark performance. |
| Mother tongue effect (MTE) | MTE expresses the impact of asking questions in a language in which the requested information is locally salient, compared to asking it in English. A positive MTE indicates information is more readily available in the language it was (likely) present in the training data, whereas a negative mother tongue effect indicates the information is more easily accessible in English. |
|Locality effect (LE) | LE quantifies the effect of using locally sourced vs translated data. It is measured by computing the difference between scores for locally sourced data and translated English-sourced data. A positive LE implies that using translated English data underestimates performance on a language, a negative LE that using translated English data overestimates performance. |
",High,5.0
Q&A,Fujitsu-FRE/MAPS,5.0,204.0,2025-05-29 14:01:44+00:00,mit,0.0,28.1 MB,29464985.6,21 MB,22020096.0,8855,https://arxiv.org/abs/2505.15935,none,"---
configs:
- config_name: Math
  data_files:
  - split: English
    path: data/english/math/*.json
  - split: Russian
    path: data/russian/math/*.json
  - split: Korean
    path: data/korean/math/*.json
  - split: Hindi
    path: data/hindi/math/*.json
  - split: Hebrew
    path: data/hebrew/math/*.json
  - split: Japanese
    path: data/japanese/math/*.json
  - split: German
    path: data/german/math/*.json
  - split: Italian
    path: data/italian/math/*.json
  - split: Portugese
    path: data/portugese/math/*.json
  - split: Spanish
    path: data/spanish/math/*.json
  - split: Arabian
    path: data/arabic/math/*.json
- config_name: ASB
  data_files:
  - split: English
    path: data/english/asb/all_attack_tools.jsonl
  - split: Russian
    path: data/russian/asb/all_attack_tools.jsonl
  - split: Korean
    path: data/korean/asb/all_attack_tools.jsonl
  - split: Hindi
    path: data/hindi/asb/all_attack_tools.jsonl
  - split: Hebrew
    path: data/hebrew/asb/all_attack_tools.jsonl
  - split: Japanese
    path: data/japanese/asb/all_attack_tools.jsonl
  - split: German
    path: data/german/asb/all_attack_tools.jsonl
  - split: Italian
    path: data/italian/asb/all_attack_tools.jsonl
  - split: Portugese
    path: data/portugese/asb/all_attack_tools.jsonl
  - split: Spanish
    path: data/spanish/asb/all_attack_tools.jsonl
  - split: Arabian
    path: data/arabic/asb/all_attack_tools.jsonl
- config_name: SWE
  data_files:
  - split: English
    path: data/english/swe/*.json
  - split: Russian
    path: data/russian/swe/*.json
  - split: Korean
    path: data/korean/swe/*.json
  - split: Hindi
    path: data/hindi/swe/*.json
  - split: Hebrew
    path: data/hebrew/swe/*.json
  - split: Japanese
    path: data/japanese/swe/*.json
  - split: German
    path: data/german/swe/*.json
  - split: Italian
    path: data/italian/swe/*.json
  - split: Portugese
    path: data/portugese/swe/*.json
  - split: Spanish
    path: data/spanish/swe/*.json
  - split: Arabian
    path: data/arabic/swe/*.json
- config_name: GAIA
  data_files:
  - split: English
    path: data/english/gaia/*.json
  - split: Russian
    path: data/russian/gaia/*.json
  - split: Korean
    path: data/korean/gaia/*.json
  - split: Hindi
    path: data/hindi/gaia/*.json
  - split: Hebrew
    path: data/hebrew/gaia/*.json
  - split: Japanese
    path: data/japanese/gaia/*.json
  - split: German
    path: data/german/gaia/*.json
  - split: Italian
    path: data/italian/gaia/*.json
  - split: Portugese
    path: data/portugese/gaia/*.json
  - split: Spanish
    path: data/spanish/gaia/*.json
  - split: Arabian
    path: data/arabic/gaia/*.json
license: mit
task_categories:
- text-generation
- question-answering
pretty_name: MAPS
size_categories:
- 1K<n<10K
language:
- ar
- en
- ja
- es
- ko
- hi
- ru
- he
- pt
- de
- it
---

# Dataset Card for Multilingual Benchmark for Global Agent Performance and Security

This is the first Multilingual Agentic AI Benchmark for evaluating agentic AI systems across different languages and diverse tasks. Benchmark enables systematic analysis of how agents perform under multilingual conditions. To balance performance and safety evaluation, our benchmark comprises 805 tasks: 405 from performance-oriented datasets (GAIA, SWE-bench, MATH) and 400 from the Agent Security Benchmark. We selected 165 tasks from GAIA (full validation set), 140 high-difficulty tasks from MATH (20 per topic across 7 topics), and 100 hard and medium tasks from SWE-bench. The remaining 400 tasks include all safety-relevant prompts from ASB. Each task was translated into 10 target languages resulting in a total of 8.8K multilingual tasks. **See more details in our Research Paper:** https://arxiv.org/abs/2505.15935

## Dataset Details

### Dataset Description

This benchmark is designed to evaluate agentic AI systems for both performance and safety across a wide range of tasks in a multilingual setting. It enables testing how well agents perform when operating in different languages, covering realistic tasks from multiple domains:

**GAIA**: Web search and tool-use tasks that test an agent’s ability to interact with external tools and follow multi-step reasoning.

**MATH**: Complex mathematical problem-solving tasks from seven topics, requiring structured reasoning and accurate computation.

**SWE-bench**: Software engineering tasks involving real-world GitHub issues, focusing on code understanding, bug fixing, and technical reasoning.

**ASB (Agent Security Benchmark)**: Safety-focused tasks designed to probe agent behavior under adversarial or sensitive scenarios, ensuring safe and aligned outputs across languages.

### languages

Each task in the benchmark is translated into the following 10 languages to enable comprehensive multilingual evaluation:
Spanish (es), German (de), Arabic (ar), Russian (ru), Japanese (ja), Portuguese (pt), Hindi (hi), Hebrew (he), Korean (Ko), Italian (it)

### Dataset Size

Each dataset in the benchmark includes a fixed number of instances per language, all translated into 10 languages. Below is the breakdown (including english):

- GAIA: 165 tasks per language × 11 languages = 1,815 tasks total
- MATH: 140 tasks per language × 11 languages = 1,540 tasks total
- SWE-bench: 100 tasks per language × 11 languages = 1,100 tasks total
- ASB: 400 attack per language × 11 languages = 4,400 attacks total

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [ ]
- **Paper [optional]:** [ ]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

 - **Compare multilingual robustness across agent designs or toolchains**: Evaluate how different agent architectures, prompting strategies, or tool-use capabilities perform across languages. This helps identify which designs are more robust to linguistic variation in task execution.

 - **Stress test agents for safe behavior in non-English inputs**: Use the Agent Security Benchmark (ASB) subset to probe safety risks in multiple languages. This scenario reveals whether agents behave safely and consistently when faced with adversarial or sensitive prompts beyond English.

 - **Benchmark cross-lingual generalization in reasoning, code, and safety tasks**: Assess agents on their ability to generalize core reasoning, coding, and safety principles across languages using datasets like GAIA, MATH, SWE-bench, and ASB.

- **Analyze performance drop-offs or safety regressions across languages**: Track how performance or safety behavior degrades in certain languages compared to English. This helps uncover biases, translation artifacts, or limitations in the agent’s multilingual handling.


## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

## Data Splits

Users can filter the benchmark tasks using two main criteria: by dataset (e.g., GAIA, MATH, SWE-bench, ASB) and by language (from the 11 supported languages). This flexible filtering enables targeted evaluation of agent performance and safety across specific domains and languages.

## Data format

All datasets are available in json format.


## Dataset Creation

### Curation Rationale

To build our multilingual benchmark, we use a hybrid machine–generation and human–verification pipeline. AI-based processing produces language variants at scale, while native speakers verify meaning and nuance. Each task is represented consistently across the ten diverse languages, ensuring faithful intent preservation and enabling reliable cross-language evaluation.


### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

We adopt a hybrid multi-stage translation pipeline that systematically combines the format-preserving strengths of Machine translation with the contextual refinement capabilities of LLMs, followed by manual verification for quality assurance.
More details about the hybrid translation pipeline is avaialble in our Research Paper.

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

Each item was independently rated by a bilingual annotator fluent in English and the target language Annotators evaluated three criteria on a 1~5 Likert scale: adequacy (semantic fidelity), fluency (grammatical and stylistic naturalness), and formatting accuracy (preservation of special elements such as LaTeX, variable names, and code). A final metric, answerability, captured whether the translation preserved the original intent well enough for the annotator to confidently answer the question as if it were posed in English. 
More details about the Annotations is avaialble in our Research Paper.

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

1. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., & Scialom, T. (2023).  
   **GAIA: A Benchmark for General-AI Assistants**. *ICLR 2023*. <https://openreview.net/forum?id=GAIA2023>

2. Zhang, H., Huang, J., Mei, K., Yao, Y., Wang, Z., Zhan, C., Wang, H., & Zhang, Y. (2024).  
   **Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents**. *arXiv 2410.02644*. <https://arxiv.org/abs/2410.02644>

3. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., & Steinhardt, J. (2021).  
   **Measuring Mathematical Problem Solving with the MATH Dataset**. *arXiv 2103.03874*. <https://arxiv.org/abs/2103.03874>

4. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., & Narasimhan, K. (2023).  
   **SWE-Bench: Can Language Models Resolve Real-World GitHub Issues?** *arXiv 2310.06770*. <https://arxiv.org/abs/2310.06770>

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Q&A,Anonymous4open/JailJudge,1.0,55.0,2025-05-14 12:55:16+00:00,other,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
configs:
- config_name: default
  data_files:
  - split: train
    path:
    - JAILJUDGE_TRAIN.json
  - split: test
    path:
    - JAILJUDGE_ID.json
    - JAILJUDGE_OOD.json
size_categories:
- 10K<n<100K
license: other
license_name: jailjudge
license_link: LICENSE
task_categories:
- text-classification
- question-answering
- text-generation
language:
- en
- zh
- it
- vi
- ar
- ko
- th
- bn
- sw
- jv
---


## Overview

Although significant research efforts have been dedicated to enhancing the safety of large language models (LLMs) by understanding and defending against jailbreak attacks, evaluating the defense capabilities of LLMs against jailbreak attacks  also attracts lots of attention. Current evaluation methods lack explainability and do not generalize well to complex scenarios, resulting in incomplete and inaccurate assessments (e.g., direct judgment without reasoning explainability,  the F1 score of the GPT-4 judge is only 55\% in complex scenarios and bias evaluation on multilingual scenarios, etc.). To address these challenges, we have developed a comprehensive evaluation benchmark, JAILJUDGE, which includes a wide range of risk scenarios with complex malicious prompts (e.g., synthetic, adversarial, in-the-wild, and multi-language scenarios, etc.) along with high-quality human-annotated test datasets. Specifically, the JAILJUDGE dataset comprises training data of JAILJUDGE, with over 35k+ instruction-tune training data with reasoning explainability, and JAILJUDGETEST, a 4.5k+ labeled set of broad risk scenarios and a 6k+ labeled set of multilingual scenarios in ten languages. To provide reasoning explanations (e.g., explaining why an LLM is jailbroken or not) and fine-grained evaluations (jailbroken score from 1 to 10), we propose a multi-agent jailbreak judge framework, JailJudge MultiAgent, making the decision inference process explicit and interpretable to enhance evaluation quality.   Using this framework, we construct the instruction-tuning ground truth and then instruction-tune an end-to-end jailbreak judge model, JAILJUDGE Guard, which can also provide reasoning explainability with fine-grained evaluations without API costs. 
Additionally, we introduce JailBoost, an attacker-agnostic attack enhancer, and GuardShield, a safety moderation defense method, both based on JAILJUDGE Guard. Comprehensive experiments demonstrate the superiority of our JAILJUDGE benchmark and jailbreak judge methods. Our jailbreak judge methods (JailJudge MultiAgent and JAILJUDGE Guard) achieve SOTA performance in closed-source models (e.g., GPT-4) and safety moderation models (e.g., Llama-Guard and ShieldGemma, etc.), across a broad range of complex behaviors (e.g., JAILJUDGE benchmark, etc.) to zero-shot scenarios (e.g., other open data, etc.). Importantly, JailBoost and GuardShield, based on JAILJUDGE Guard, can enhance downstream tasks in jailbreak attacks and defenses under zero-shot settings with significant improvement (e.g., JailBoost can increase the average performance  by approximately 29.24\%, while GuardShield can reduce the average defense ASR from 40.46\% to 0.15\%).

## 💡Framework


The JAILJUDGE Benchmark encompasses a wide variety of complex jailbreak scenarios, including multilingual and adversarial prompts, targeting diverse LLM responses for robust safety evaluation.

The JAILJUDGE Data includes over 35k instruction-tune training data and two test sets (4.5k+ broad risk scenarios and 6k+ multilingual examples), providing a rich foundation for comprehensive jailbreak assessments.

The Multi-agent Jailbreak Judge Framework leverages multiple agents (Judging, Voting, and Inference agents) to deliver fine-grained evaluations, reasoning explanations, and jailbroken scores, making the evaluation process explicit and interpretable.



# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->


# Dataset
We open-sourced two comprehensive datasets, **JAILJUDGETRAIN** and **JAILJUDGETEST**, designed to advance research in the jailbreak evaluation of LLMs. Both datasets encompass a wide range of prompt-response pairs and are structured to support jailbreak evaluation  methods. To assist researchers in replicating our work,  The complete datasets and supporting resources are available on [Hugging Face JAILJUDGE Dataset](https://huggingface.co/datasets/ICLR-Anonymous/JAILJUDGE)


### JAILJUDGETRAIN Dataset

The **JAILJUDGETRAIN** dataset is a comprehensive instruction-tuning dataset derived using our multi-agent jailbreak judge framework, specifically designed to evaluate the jailbreak potential of LLMs. This large-scale dataset consists of over 35,000 items, gathered from diverse sources to capture a wide range of risk and response scenarios. The framework employs multiple LLM-powered agents that collaboratively assess each prompt-response pair, providing detailed scoring and reasoning for each instance.

Each data instance in **JAILJUDGETRAIN** includes the following fields:
- **user_prompt**: The prompt given to the model.
- **model_response**: The model's response to the prompt.
- **hazard_cate_llamaguard3**: The hazard category of the prompt, classified by the LlamaGuard3 model.
- **prompt_complexity_cate**: The complexity category of the prompt, indicating the sophistication level of the input.
- **language**: The language in which the prompt is written.
- **reason**: The explanation or rationale for the score assigned to the model's response, providing insights into why the response may or may not indicate a jailbreak.
- **score**: A numerical score that represents the degree to which the model’s response aligns with jailbreak behavior, as evaluated by the multi-agent framework.

### Prompt Categories in JAILJUDGETRAIN

The dataset contains six distinct categories of prompts, covering a comprehensive range of risk scenarios and complexities:
1. **Vanilla Harmful Prompts**: These include a broad array of risk scenarios.
2. **Synthetic Vanilla Prompts**: Rewritten by LLMs to preserve original meanings with slight modifications.
3. **Synthetic Adversarial Prompts**: Modified prompts created to simulate jailbreak attacks.
4. **In-the-Wild Harmful Prompts**: Real-world user interactions with LLMs.
5. **Deceptive Harmful Prompts**: Complex and refined prompts, automatically generated to challenge jailbreak detection.

### Purpose and Utility

This multi-agent setup not only enhances the accuracy of jailbreak assessment but also ensures a thorough understanding of the reasoning behind each decision. With structured scores and detailed explanations, the **JAILJUDGETRAIN** dataset is a valuable resource for training and fine-tuning models focused on jailbreak detection tasks.


### JAILJUDGETEST
JAILJUDGETEST is a high-quality, human-annotated evaluation set consisting of two subsets: **JAILJUDGE ID** (in-distribution) and **JAILJUDGE OOD** (out-of-distribution). The JAILJUDGE ID set includes over 4,500 prompt-response pairs from JAILJUDGETRAIN (excluding multilingual prompts) and is balanced for various risk scenarios. The JAILJUDGE OOD set, focused on multilingual prompts across ten languages, includes over 6,000 instances. This OOD set is specifically designed to assess the model’s generalizability to diverse linguistic contexts.

### Human Annotation and Quality Assurance
To ensure high-quality labels, we employed a rigorous, multi-stage annotation process:
1. **Annotator Training**: Following MLCommons standards, annotators were trained on policies regarding user privacy, ethics, and morality.
2. **Manual Labeling and GPT-4 Assistance**: Initial labeling was performed manually, with GPT-4 providing secondary labels. Discrepancies between human and GPT-4 annotations were reviewed to ensure accuracy.
3. **Cross-Comparison and Multi-Person Voting**: Final labels were determined by majority voting, involving multiple annotators to increase reliability. 

### Prompt Complexity Categories
We categorize prompts into five complexity levels:
1. **Simple Prompts (Q1)**: Direct, straightforward prompts.
2. **Adversarial Prompts (Q2)**: Generated by jailbreak attacks.
3. **In-the-Wild Prompts (Q3)**: Real-world prompts with adversarial elements.
4. **Synthetic Vanilla Prompts (Q4)**: Rephrased prompts that retain original meanings.
5. **Deceptive Harmful Prompts (Q5)**: Complex, sophisticated prompts combining multiple adversarial techniques.


###  Hazard Categories for  Harmful Prompts

The following table provides a summary of the **hazard categories** for harmful prompts in the JAILJUDGE dataset. Each category code represents a specific type of harmful or sensitive content that the prompts aim to address.

| Category Code | Description                  | Category Code | Description                  |
|---------------|------------------------------|---------------|------------------------------|
| S1            | Violent Crimes               | S8            | Intellectual Property        |
| S2            | Non-Violent Crimes           | S9            | Indiscriminate Weapons       |
| S3            | Sex-Related Crimes           | S10           | Hate                          |
| S4            | Child Sexual Exploitation    | S11           | Suicide & Self-Harm          |
| S5            | Defamation                   | S12           | Sexual Content               |
| S6            | Specialized Advice           | S13           | Elections                    |
| S7            | Privacy                      | S14           | Code Interpreter Abuse       |




# Dataset Card for JAILJUDGE Dataset

## Dataset Summary

The JAILJUDGE dataset is a comprehensive collection of prompt-response pairs specifically curated to evaluate the jailbreak potential of LLMs across multiple scenarios. The dataset is divided into three main files, each with unique attributes to facilitate both in-distribution (ID) and out-of-distribution (OOD) testing. This dataset was constructed using our multi-agent jailbreak judge framework, which enables precise scoring, reasoning, and categorization of each response.

## Files and Fields

### JAILJUDGE_TRAIN.json
- **user_prompt**: The prompt given to the model.
- **model_response**: The model's response to the prompt.
- **hazard_cate_llamaguard3**: The hazard category of the prompt, as classified by the LlamaGuard3 model.
- **prompt_complexity_cate**: The complexity category of the prompt, indicating its sophistication.
- **language**: The language in which the prompt is written.
- **reason**: The rationale or explanation for the score given to the model's response.
- **score**: A numerical score (1-10) indicating the likelihood of jailbreak behavior in the response.

### JAILJUDGE_ID.json
- **user_prompt**: The prompt given to the model.
- **model_response**: The model's response to the prompt.
- **hazard_cate_llamaguard3**: The hazard category of the prompt, as classified by the LlamaGuard3 model.
- **prompt_complexity_cate**: The complexity category of the prompt.
- **language**: The language of the prompt.
- **is_jailbroken**: Boolean value (True or False) indicating whether the model's response was classified as jailbroken.

### JAILJUDGE_OOD.json
- **user_prompt**: The prompt given to the model.
- **model_response**: The model's response to the prompt.
- **hazard_cate_llamaguard3**: The hazard category of the prompt, as classified by the LlamaGuard3 model.
- **prompt_complexity_cate**: The complexity category of the prompt.
- **language**: The language of the prompt.
- **is_jailbroken**: Boolean value (True or False) indicating whether the model's response was classified as jailbroken.

## Detailed Field Descriptions

- **prompt_complexity_cate**: Represents the complexity level of the prompt. The categories are as follows:
  - **Simple Prompts (Q1)**: Direct and straightforward prompts.
  - **Adversarial Prompts (Q2)**: Prompts generated by jailbreak attacks.
  - **In-the-Wild Prompts (Q3)**: Prompts sourced from real-world user interactions.
  - **Synthetic Vanilla Prompts (Q4)**: Rephrased prompts that retain their original meanings.
  - **Deceptive Harmful Prompts (Q5)**: Sophisticated prompts combining multiple adversarial elements.

- **hazard_cate_llamaguard3**: The hazard category of the prompt, based on classifications from the LlamaGuard3 model. This field categorizes prompts into various risk types, such as:
  - **S1**: Violent Crimes
  - **S2**: Non-Violent Crimes
  - **S3**: Sex-Related Crimes
  - **S4**: Child Sexual Exploitation
  - **S5**: Defamation
  - **S6**: Specialized Advice
  - **S7**: Privacy
  - **S8**: Intellectual Property
  - **S9**: Indiscriminate Weapons
  - **S10**: Hate
  - **S11**: Suicide & Self-Harm
  - **S12**: Sexual Content
  - **S13**: Elections
  - **S14**: Code Interpreter Abuse

- **language**: Indicates the language in which the prompt is written. Supported languages include:
  - **en**: English
  - **zh**: Chinese
  - **it**: Italian
  - **vi**: Vietnamese
  - **ar**: Arabic
  - **ko**: Korean
  - **th**: Thai
  - **bn**: Bengali
  - **sw**: Swahili
  - **jv**: Javanese

- **is_jailbroken**: A Boolean value (True or False) that specifies whether the model's response to the prompt was classified as jailbroken.

- **score**: A numerical score ranging from 1 to 10 that indicates the extent to which the model's response is deemed as exhibiting jailbreak behavior, with higher scores representing a greater likelihood of jailbreak.

- **reason**: A detailed explanation justifying the score assigned to the model's response, offering insights into why the response may or may not be considered jailbroken.

## Usage Notes

The JAILJUDGE dataset provides a valuable resource for training and evaluating jailbreak detection models. The structured and annotated fields enable researchers to study various risk scenarios, prompt complexities, and response characteristics across multiple languages and hazard categories.


",High,4.0
Q&A,Fujitsu-FRE/MAPS_Verified,2.0,181.0,2025-06-03 08:55:08+00:00,mit,0.0,24.4 MB,25585254.4,13.4 MB,14050918.4,3047,https://arxiv.org/abs/2505.15935,none,"---
configs:
- config_name: Math
  data_files:
  - split: English
    path: data/english/math/*.json
  - split: Russian
    path: data/russian/math/*.json
  - split: Korean
    path: data/korean/math/*.json
  - split: Hindi
    path: data/hindi/math/*.json
  - split: Hebrew
    path: data/hebrew/math/*.json
  - split: Japanese
    path: data/japanese/math/*.json
  - split: German
    path: data/german/math/*.json
  - split: Italian
    path: data/italian/math/*.json
  - split: Portugese
    path: data/portugese/math/*.json
  - split: Spanish
    path: data/spanish/math/*.json
  - split: Arabian
    path: data/arabic/math/*.json
- config_name: ASB
  data_files:
  - split: English
    path: data/english/asb/*.json
  - split: Russian
    path: data/russian/asb/*.json
  - split: Korean
    path: data/korean/asb/*.json
  - split: Hindi
    path: data/hindi/asb/*.json
  - split: Hebrew
    path: data/hebrew/asb/*.json
  - split: Japanese
    path: data/japanese/asb/*.json
  - split: German
    path: data/german/asb/*.json
  - split: Italian
    path: data/italian/asb/*.json
  - split: Portugese
    path: data/portugese/asb/*.json
  - split: Spanish
    path: data/spanish/asb/*.json
  - split: Arabian
    path: data/arabic/asb/*.json
- config_name: SWE
  data_files:
  - split: English
    path: data/english/swe/*.json
  - split: Russian
    path: data/russian/swe/*.json
  - split: Korean
    path: data/korean/swe/*.json
  - split: Hindi
    path: data/hindi/swe/*.json
  - split: Hebrew
    path: data/hebrew/swe/*.json
  - split: Japanese
    path: data/japanese/swe/*.json
  - split: German
    path: data/german/swe/*.json
  - split: Italian
    path: data/italian/swe/*.json
  - split: Portugese
    path: data/portugese/swe/*.json
  - split: Spanish
    path: data/spanish/swe/*.json
  - split: Arabian
    path: data/arabic/swe/*.json
- config_name: GAIA
  data_files:
  - split: English
    path: data/english/gaia/*.json
  - split: Russian
    path: data/russian/gaia/*.json
  - split: Korean
    path: data/korean/gaia/*.json
  - split: Hindi
    path: data/hindi/gaia/*.json
  - split: Hebrew
    path: data/hebrew/gaia/*.json
  - split: Japanese
    path: data/japanese/gaia/*.json
  - split: German
    path: data/german/gaia/*.json
  - split: Italian
    path: data/italian/gaia/*.json
  - split: Portugese
    path: data/portugese/gaia/*.json
  - split: Spanish
    path: data/spanish/gaia/*.json
  - split: Arabian
    path: data/arabic/gaia/*.json
license: mit
task_categories:
- text-generation
- question-answering
pretty_name: Multilingual-Agentic-AI-Benchmark
size_categories:
- 1K<n<10K
language:
- ar
- en
- ja
- es
- ko
- hi
- ru
- he
- pt
- de
- it
---

# Dataset Card for Multilingual Benchmark for Global Agent Performance and Security

This is the first Multilingual Agentic AI Benchmark for evaluating agentic AI systems across different languages and diverse tasks. Benchmark enables systematic analysis of how agents perform under multilingual conditions. This dataset contains 550 instances for GAIA, 660 instances for ASB, 737 instances for Maths, and 1100 instances for SWE. Each task was translated into 10 target languages resulting in a total of around 3K multilingual tasks. **See more details in our Research Paper:** https://arxiv.org/abs/2505.15935

**This Multilingual Benchmark dataset is manually verified and annotated by native bilingual native speakers.**

## Dataset Details

### Dataset Description

This benchmark is designed to evaluate agentic AI systems for both performance and safety across a wide range of tasks in a multilingual setting. It enables testing how well agents perform when operating in different languages, covering realistic tasks from multiple domains:

**GAIA**: Web search and tool-use tasks that test an agent’s ability to interact with external tools and follow multi-step reasoning.

**MATH**: Complex mathematical problem-solving tasks from seven topics, requiring structured reasoning and accurate computation.

**SWE-bench**: Software engineering tasks involving real-world GitHub issues, focusing on code understanding, bug fixing, and technical reasoning.

**ASB (Agent Security Benchmark)**: Safety-focused tasks designed to probe agent behavior under adversarial or sensitive scenarios, ensuring safe and aligned outputs across languages.

### languages

Each task in the benchmark is translated into the following 10 languages to enable comprehensive multilingual evaluation:
Spanish (es), German (de), Arabic (ar), Russian (ru), Japanese (ja), Portuguese (pt), Hindi (hi), Hebrew (he), Korean (Ko), Italian (it)

### Dataset Size

Each dataset in the benchmark includes a fixed number of instances per language, all translated into 10 languages. Below is the breakdown (including english):

- GAIA: 50 tasks per language × 11 languages = 550 tasks total
- MATH: 67 tasks per language × 11 languages = 737 tasks total
- SWE-bench: 100 tasks per language × 11 languages = 1,100 tasks total
- ASB: 60 attack per language × 11 languages = 660 attacks total

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [ ]
- **Paper [optional]:** [ ]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

 - **Compare multilingual robustness across agent designs or toolchains**: Evaluate how different agent architectures, prompting strategies, or tool-use capabilities perform across languages. This helps identify which designs are more robust to linguistic variation in task execution.

 - **Stress test agents for safe behavior in non-English inputs**: Use the Agent Security Benchmark (ASB) subset to probe safety risks in multiple languages. This scenario reveals whether agents behave safely and consistently when faced with adversarial or sensitive prompts beyond English.

 - **Benchmark cross-lingual generalization in reasoning, code, and safety tasks**: Assess agents on their ability to generalize core reasoning, coding, and safety principles across languages using datasets like GAIA, MATH, SWE-bench, and ASB.

- **Analyze performance drop-offs or safety regressions across languages**: Track how performance or safety behavior degrades in certain languages compared to English. This helps uncover biases, translation artifacts, or limitations in the agent’s multilingual handling.


## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

## Data Splits

Users can filter the benchmark tasks using two main criteria: by dataset (e.g., GAIA, MATH, SWE-bench, ASB) and by language (from the 11 supported languages). This flexible filtering enables targeted evaluation of agent performance and safety across specific domains and languages.

## Data format

All datasets are available in json format.


## Dataset Creation

### Curation Rationale

To build our multilingual benchmark, we use a hybrid machine–generation and human–verification pipeline. AI-based processing produces language variants at scale, while native speakers verify meaning and nuance. Each task is represented consistently across the ten diverse languages, ensuring faithful intent preservation and enabling reliable cross-language evaluation.


### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

We adopt a hybrid multi-stage translation pipeline that systematically combines the format-preserving strengths of Machine translation with the contextual refinement capabilities of LLMs, followed by manual verification for quality assurance.
More details about the hybrid translation pipeline is avaialble in our Research Paper.

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

Each item was independently rated by a bilingual annotator fluent in English and the target language Annotators evaluated three criteria on a 1~5 Likert scale: adequacy (semantic fidelity), fluency (grammatical and stylistic naturalness), and formatting accuracy (preservation of special elements such as LaTeX, variable names, and code). A final metric, answerability, captured whether the translation preserved the original intent well enough for the annotator to confidently answer the question as if it were posed in English. 
More details about the Annotations is avaialble in our Research Paper.

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

1. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., & Scialom, T. (2023).  
   **GAIA: A Benchmark for General-AI Assistants**. *ICLR 2023*. <https://openreview.net/forum?id=GAIA2023>

2. Zhang, H., Huang, J., Mei, K., Yao, Y., Wang, Z., Zhan, C., Wang, H., & Zhang, Y. (2024).  
   **Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents**. *arXiv 2410.02644*. <https://arxiv.org/abs/2410.02644>

3. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., & Steinhardt, J. (2021).  
   **Measuring Mathematical Problem Solving with the MATH Dataset**. *arXiv 2103.03874*. <https://arxiv.org/abs/2103.03874>

4. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., & Narasimhan, K. (2023).  
   **SWE-Bench: Can Language Models Resolve Real-World GitHub Issues?** *arXiv 2310.06770*. <https://arxiv.org/abs/2310.06770>

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Q&A,Groovy-123/deep-think,1.0,58.0,2025-05-22 06:06:36+00:00,other,1.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: other
license_name: nexgen
license_link: LICENSE
task_categories:
- text-classification
- token-classification
- table-question-answering
- question-answering
- zero-shot-classification
- translation
- summarization
- feature-extraction
- text-generation
- text2text-generation
- fill-mask
- sentence-similarity
- text-to-speech
- text-to-audio
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
- image-to-text
- image-to-image
- image-to-video
- unconditional-image-generation
- video-classification
- reinforcement-learning
- robotics
- tabular-classification
- tabular-regression
- tabular-to-text
- table-to-text
- multiple-choice
- text-ranking
- text-retrieval
- time-series-forecasting
- text-to-video
- visual-question-answering
- zero-shot-image-classification
- graph-ml
- mask-generation
- zero-shot-object-detection
- image-to-3d
- text-to-3d
- image-feature-extraction
- video-text-to-text
language:
- ak
- en
- ar
- ha
- ee
size_categories:
- n>1T
---",Low,1.0
Q&A,QCRI/SpokenNativQA,0.0,269.0,2025-05-29 23:03:37+00:00,cc-by-nc-sa-4.0,0.0,8.91 MB,9342812.16,4.21 MB,4414504.96,13234,https://arxiv.org/abs/2505.19163,none,"---
license: cc-by-nc-sa-4.0
task_categories:
  - question-answering
language:
  - ar
  - en
tags:
  - question-answering
  - cultural-aligned
pretty_name: 'SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs'
size_categories:
  - 10K<n<100K
dataset_info:
  - config_name: Arabic-ASR-Azure
    splits:
      - name: test
        num_examples: 988
  - config_name: Arabic-ASR-Whisper
    splits:
      - name: test
        num_examples: 985
  - config_name: Arabic-ASR-Fanar-Aura
    splits:
      - name: test
        num_examples: 988
  - config_name: Arabic-ASR-Google
    splits:
      - name: test
        num_examples: 985
  - config_name: English-ASR-Azure
    splits:
      - name: test
        num_examples: 2322
  - config_name: English-ASR-Whisper
    splits:
      - name: test
        num_examples: 2322
  - config_name: English-ASR-Fanar-Aura
    splits:
      - name: test
        num_examples: 2322
  - config_name: English-ASR-Google
    splits:
      - name: test
        num_examples: 2322
configs:
  - config_name: Arabic-ASR-Azure
    data_files:
      - split: test
        path: arabic_qa/spokenqa_arabic_qa_test_azure_asr.json
  - config_name: Arabic-ASR-Whisper
    data_files:
      - split: test
        path: arabic_qa/spokenqa_arabic_qa_test_whisper_asr.json
  - config_name: Arabic-ASR-Fanar-Aura
    data_files:
      - split: test
        path: arabic_qa/spokenqa_arabic_qa_test_fanar_asr.json
  - config_name: Arabic-ASR-Google
    data_files:
      - split: test
        path: arabic_qa/spokenqa_arabic_qa_test_google_asr.json
  - config_name: English-ASR-Azure
    data_files:
      - split: test
        path: english_qa/spokenqa_english_qa_test_azure_asr.json
  - config_name: English-ASR-Whisper
    data_files:
      - split: test
        path: english_qa/spokenqa_english_qa_test_whisper_asr.json
  - config_name: English-ASR-Fanar-Aura
    data_files:
      - split: test
        path: english_qa/spokenqa_english_qa_test_fanar_asr.json
  - config_name: English-ASR-Google
    data_files:
      - split: test
        path: english_qa/spokenqa_english_qa_test_google_asr.json
---

# SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs

The [**SpokenNativQA**]() dataset consists of question-answer (QA) pairs, where queries are sourced from real users and answers are manually reviewed and edited. The dataset covers a diverse range of 18 topics that reflect culturally and regionally specific knowledge, as well as everyday queries. These topics include animals, business, clothing, education, events, food and drinks, general knowledge, geography, immigration, language, literature, names and persons, plants, religion, sports and games, tradition, travel, and weather.

SpokenNativQA provides **multilingual test sets of everyday spoken questions** to evaluate large language models (LLMs) and speech processing systems. The dataset contains **Arabic** and **English** queries, each transcribed by multiple automatic speech recognition (ASR) systems.

<p align=""left""> <img src=""./spokennativqa.png"" style=""width: 60%;"" id=""title-icon""> </p>


**<span style=""color:blue"">Note:</span>** As a part of this repository we only shared the wav files that are only used for evaluation. For the entire dataset that we reported in the paper might be accessible after contacting with the authors.


## Directory Overview

The dataset is organized into two main directories:

- **`arabic_qa/`**
  - `spokenqa_arabic_qa_test_azure_asr.jsonl`
  - `spokenqa_arabic_qa_test_fanar_asr.jsonl`
  - `spokenqa_arabic_qa_test_google_asr.jsonl`
  - `spokenqa_arabic_qa_test_whisper_asr.jsonl`
  - `spokenqa_arabic_qa_test.jsonl`
  - `speech/` -- wav files

- **`english_qa/`**
  - `spokenqa_english_qa_test_azure_asr.jsonl`
  - `spokenqa_english_qa_test_fanar_asr.jsonl`
  - `spokenqa_english_qa_test_google_asr.jsonl`
  - `spokenqa_english_qa_test.jsonl`
  - `speech/` -- wav files

### Dataset Structure and Format

Each `.jsonl` file contains a list of JSON objects, one per line. The typical structure includes:

- **`lang`**: The language of the spoken query (e.g., ""arabic"", ""english"").
- **`data_id`**: A unique identifier for the data instance.
- **`file_name`**: The name of the audio file.
- **`file_path`**: The relative path of the audio file.
- **`question`**: The intended question in text form (reference).
- **`answer`**: The expected answer or reference answer for the question.
- **`location`**: The geographical location where the query was recorded.
- **`asr_text`**: The text output from the ASR system.

### Example of a JSON Entry
```json
{
  ""lang"": ""arabic"",
  ""data_id"": ""3cdfcfd1acb722617ec8bbe6808114bc"",
  ""file_name"": ""3cdfcfd1acb722617ec8bbe6808114bc_1724222501325.wav"",
  ""file_path"": ""speech/3cdfcfd1acb722617ec8bbe6808114bc_1724222501325.wav"",
  ""question"": ""من هو الشاعر الذي سجن؟"",
  ""answer"": ""وهي القصائد التي كتبها أبو فراس الحمداني فترة أسره عند الروم في سجن خرشنة، وعرفت باسم الروميات نسبة لمكان أسره، وقد تميزت هذا القصائد بجزالتها وقوتها ورصانتها، وصدق عاطفتها."",
  ""location"": ""qatar"",
  ""asr_text"": ""من هو الشاعر الذي زعل؟""
}
```

# Experimental Scripts:
All of the experimental scripts are available as a part of [https://github.com/qcri/LLMeBench](https://github.com/qcri/LLMeBench) framework.

# License
This dataset is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0).


# Citation
If you are using this dataset in your research, we kindly ask that you cite our [paper](https://www.arxiv.org/pdf/2505.19163).

```
@inproceedings{alam2025spokennativqa,
  title     = {SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs},
  author    = {Firoj Alam and Md Arid Hasan and Shammur Absar Chowdhury},
  booktitle = {Proceedings of the 26th Interspeech Conference (Interspeech 2025)},
  year      = {2025},
  address   = {Rotterdam, The Netherlands},
  month     = aug,
  organization = {ISCA},
}

```
",High,5.0
Q&A,QuickdigiLLC/DeepAIM-AIM-G1,2.0,246.0,2025-06-10 14:59:23+00:00,mit,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
pretty_name: deepaim
version: 1.0.0
homepage: https://quickdigi-official.firebaseapp.com
license: mit
citation: |
  @misc{DeepAIM2025,
    author = {محمد},
    title = {DeepAIM Dataset},
    year = {2025},
    howpublished = {\url{https://quickdigi-official.firebaseapp.com}}
  }
language:
- ar
- en
task_categories:
- text-generation
- question-answering
- sentence-similarity
tags:
- code
- medical
- synthetic
- art
- legal
size_categories:
- 1M<n<10M
dataset_info:
  features:
  - name: category
    dtype: string
  - name: emotion
    dtype: string
  - name: questions
    sequence:
      dtype: string
  - name: answers
    sequence:
      dtype: string
  - name: reasons
    sequence:
      dtype: string
  - name: scoldResponses
    sequence:
      dtype: string
configs:
- config_name: default
  data_files:
  - split: train
    path: models/Model-2M.json.gz
  filetype: json
  field: Data
---

# DeepAIM-AIMG1-2M

**DeepAIM-AIMG1-2M** is a custom dataset built for training the DeepAIM artificial intelligence model (version: `AIM-G1`).  
This dataset is carefully structured to simulate realistic multi-turn conversations, emotions, and reasoning for building deep-response AI agents.

---

## 🧠 Dataset Overview

- **Model Target**: `AIM-G1` – 2M parameters
- **Language**: English
- **Focus Areas**:
  - Deep context understanding
  - Emotion-aware responses
  - Dynamic response chains
  - Scolding / correction logic (optional)
  - Internal reasoning (optional)

---

## 📐 Data Structure

Each dataset file follows this structure:

```json
{
  ""model"": ""AIM-G1"",
  ""Data"": [
    {
      ""category"": ""conversation / logic / personal / emotional / etc"",
      ""emotion"": ""happy / sad / angry / neutral / etc"",
      ""questions"": [
        ""What are you doing?""
        ...
      ],
      ""answers"": [
        ""I'm currently learning new things!""
        ...
      ],
      ""reasons"": [
        ""Because I'm designed to help and learn."",
        ...
      ],
      ""scoldResponses"": [
        ""Please be kind when speaking to me."",
        ...
      ]
    }
  ]
}
```

🔹 questions & answers are required
🔹 reasons and scoldResponses are optional
🔹 Supports 1 to 50+ questions/answers per object

# 📦 Use Cases
This dataset can be used to train models for:
* Chatbots
* Emotionally aware agents
* AI with internal logic and memory
* Response tuning with reinforcement feedback

---

# 🛠 Format
**Format**: JSON

**Encoding**: UTF-8

**Size**: ~2M parameters (token-focused)

**Preprocessing**: Cleaned, lowercased, trimmed, token-safe

# 📜 License
MIT License – Free to use, modify, and distribute with proper attribution.

# ✨ Creator
**Mohammed Mostafa Brawh(Dev)**
Creator of DeepAIM – the first Egyptian-made full-stack AI built from scratch.
Passionate about neural design, deep logic, and pushing boundaries.

# 💬 Contact & Links

GitHub: [Github](https://github.com/QuickDigi?utm_source=huggingface.co)",High,4.0
Q&A,trais-lab/DCA-Bench,2.0,194.0,2025-05-31 06:02:50+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2406.07275,none,"---
license: apache-2.0
task_categories:
- question-answering
- text-generation
- summarization
language:
- zh
- en
- ar
tags:
- LLM
- LLM Agent
- Dataset
- Data
- Data-Quality
pretty_name: dca
size_categories:
- 1K<n<10K
---
# DCA-Benchmark

[![arXiv](https://img.shields.io/badge/Arxiv-2406.07275-blueviolet?logo=arxiv)](https://arxiv.org/abs/2406.07275)
[![Github](https://img.shields.io/badge/Github-black?logo=github)](https://github.com/TRAIS-Lab/dca-bench)
[![Dataset](https://img.shields.io/badge/🤗%20Dataset-TRAIS--Lab%2Fdca--bench-green)](https://huggingface.co/datasets/TRAIS-Lab/dca-bench)

DCA-Benchmark aims to provide a comprehensive benchmark for evaluating LLM agents' capabilities in discovering data quality issues across online dataset platforms, representing the first step of the curation pipeline. Throughout this document, we will refer to such an LLM agent as a **""Curator""** to highlight its role in this task. A well-performing Curator can detect and locate existing issues, which is critical for subsequent fixes by human maintainers or other LLM agents. We collected 221 representative samples from 8 online dataset platforms and classified them into 4 types with 18 tags according to their various content and difficulty.

## Key Features

- **Real-world Cases with Minimal Simplification**: All test cases in DCA-Benchmark have references to real-world sources, allowing benchmark users to understand them better in practical scenarios. To test the Curator's ability in complex real-world environments, DCA-Benchmark includes all relevant dataset files for each test case, not just the flawed ones.

- **Multiple Difficulty Levels**: DCA-Benchmark provides four levels of hints for each test case in the benchmark. With higher-level hints, the Curator gains more information about the content and location of the issue. This approach aims to make the task more achievable and gauge the information required for the Curator to detect these issues.

- **Accurate Automatic Evaluation**: Unlike traditional machine learning tasks, dataset curation does not have labels that can be directly evaluated by scripts. Human-level efforts are required to rate the Curator's performance, which is not scalable. Therefore, we developed an automatic and accurate evaluation scheme using GPT-4 to replace human annotators.

## Getting Started

**1.Install the required packages using:**

```bash
conda create -n dca-bench python=3.9
conda activate dca-bench
pip install -r requirements.txt
```

**2.Download DCA-Benchmark Dataset**

After setting up the environment, please use:
```
from datasets import load_dataset
dataset = load_dataset(""trais-lab/DCA-Bench"")
```
to download dataset files, and put it under `benchmark/platforms`. The overall structure should be like:

```
benchmark/
├── platforms/
│   ├── kaggle/
│   ├── huggingface/
│   ├── ...
```

You can see meta information of each test cases in `benchmark/task_meta.json`, where each test case has three important attributes:

- **`id`**: A globally unique identifier for each test case, used to find the corresponding file locations.
- **`scenario`**: Indicates the platform from which the case is collected.
- **`hints`**: Each test case has four different level hints, providing ascending levels of information.

**3.Set up Your OpenAI Key**
Please refer to https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety to set up your OpenAI key.

## Benchmark API

The `benchmark API` is used to format the input for the Curator and obtain comprehensive statistics for DCA-Benchmark. You can see `demo.ipynb` for some use cases.

### Getting Test Case Input for the Curators

You can get the input of a test case for your Curator using the following code:

```python
from benchmark.api import BenchmarkManager

manager = BenchmarkManager()
hints, dataset_files, doc_paths = manager.get_test_case_info(id, dest_dir, use_doc=False)
```

Here are the return types for the function `get_test_case_info`:

- **`hints`**: `List[str]` – The list of hints for the test case.
- **`dataset_files`**: `List[str]` – The list of paths to the dataset files for the test case.
- **`doc_paths`**: `List[str]` – The list of paths to the document materials for the test case.

Params:

- **`dest_dir`**: If not `None`, `dataset_files` for this case will be copied to `dest_dir`.
- **`use_doc`**: If set to `True`, returns the paths of document materials for this scenario. This option was not used in the paper.

### Curator Output Format

Although the design of the Curators is not restricted, we have provided an example of output format to better align with the Evaluator, which requires three fields:

```json
{
  ""issue"": ""Wrong name in README"",
  ""evidence"": ""# My cool task This is a description of my cool task... in README"",
  ""description"": ""The name of the task in the README is incorrect. It should be 'My awesome task' instead of 'My cool task'.""
}
```

You can use `manager.format_output(issue, evidence, description)` to get the formatted output. Or use `manager.get_output_format()` to get the example above as a one-shot example for your Curator. **It's also ok to render them in an item list**, the point here is to include three fields, ""issue"" for a brief summary, ""evidence"" for contextual location in the file where the issue occurs, and ""description"" for a detailed analysis, just like a Github Issue.

## Evaluation

The evaluation results used in Tab.2 and Fig.3 of the paper are under `pipeline/eval_log`, where `eval=` indicates which model is used as Evaluator, `test=` denotes the Curator.

You could run the following code to get the results shown in Fig. 4 of the paper, which evaluates the performance of the baseline Curator based on the OpenAI Assistant API:

```bash
python -m pipeline.run_evaluation
```

The default results will be shown under `pipeline/eval_log/eval=gpt-4-0125-preview/test=gpt-4-0125-preview/example_run`.

**To evaluate your own Curator's performance**, structure your results as follows:

```
pipeline/output/{your_model_name}/{scenario}/{id}/hint_level_{hint_level}/output.txt
```

Then run:

```bash
python -m pipeline.run_evaluation --test_model_id=your_model_name \
--eval_model_id=model_as_evaluator --test_stamp=""The stamp of the run of the test model"" \ --stamp=""The stamp to distinguish your run""
```
Please see more parameters in `pipeline/run_evaluation.py`.

To test the performance of your Curator on a single (case, hint) pair, use the following code:

```python
from benchmark.api import Evaluator
evaluator = Evaluator()

result, cost = evaluator.run_single_evaluation(id, hint_level, input_path, save_dir)
```

- `id`: The file ID of the test case.
- `hint_level`: The hint level, which can be 0, 1, 2, or 3.
- `input_path`: The path to the output of your Curator for this (case, hint) pair.
- `save_dir`: The directory where you want to save the evaluation results.

## Try Baseline Curator

The test outputs from several baseline Curators are under `pipeline/output`. To try the Baseline Curator (OpenAI Assistant API) yourself, run:

```bash
python -m pipeline.run_inference --stamp=""The stamp to distinguish your run"" --provider=""The provider of the model""
```
Please see more parameters in `pipeline/run_inference.py`.

The default results will be shown at `pipeline/example_output/gpt-4-0125-preview`

## Contribute new datapoint

To contribute a new datapoint to DCA-Bench, you could run `contribute.py` to add a new datapoint, or you could simply write in `benchmark/task_meta.json`.

# Reproduction Guidelines

First, you need to set up the environment and download the dataset as described in the ""Getting Started"" section. Then you can follow the steps below to reproduce the results in the paper.

## Experiment Notebooks
- Statistics of DCA-Bench (Tab. 1): `notebooks/statistics.ipynb`
- Human aligment study (Tab. 3): `notebooks/human_alignment.ipynb`
- Evaluation of baseline curator (Tab. 4): `notebooks/eval_of_baseline_curator.ipynb`

## Artifacts
- `pipeline/gpt_3.5_turbo_labels_run.json`: human labeled results from GPT-3.5-turbo based Curator on 92 (case, hint) pairs.
- `pipeline/label.json`: human labeled results from GPT-4-0125-preview based Curator on 92 (case, hint) pairs.
- `pipeline/webpage.json`: hard set for webpage ChatGPT experiment.
- `dca-bench.csv`: the source and license information of test cases in DCA-Bench.


<details>
 <summary><strong>Statement of Potential Ethical Concerns and Justification</strong></summary>
  
### Dataset Content and Ethical Considerations
Our dataset, comprising 221 data points, includes content that may be considered sensitive or potentially controversial. Specifically:
- 4 data points (4.4% of the dataset) involve ethical or legal risks:
- 2 instances (IDs: 7488a00f-397f-42fe-80bf-15b0f490b690, 7e8f31cb-8c2a-4676-b3d4-941a64184a26) contain content that may exhibit bias towards specific groups of people.
- 2 instances (IDs: f192fe08-bb50-46dd-973d-8ba37d338758, 38d3fd72-c2b1-47e2-b64e-b579dc66887c) present potential legal risks.

### Justification for Inclusion
While we acknowledge the sensitive nature of these data points, their inclusion in our dataset is both intentional and necessary for the following reasons:
- **Benchmark Objectives**: A primary goal of our benchmark is to identify and assess potential ethical and legal risks in AI systems. The inclusion of these sensitive data points is crucial for thoroughly evaluating the capability of AI models to recognize and appropriately handle such content.
- **Realistic Representation**: These data points reflect real-world scenarios that AI systems may encounter. By including them, we ensure our benchmark provides a more comprehensive and authentic assessment of AI performance.

### Copyright and Licensing
DCA-Benchmark is released under the Apache License 2.0. We have curated a [table](https://docs.google.com/spreadsheets/d/1jweqxg7jZ97Knl1f2Y64RqolDc8x14kLS9iJn8rjkbc/edit?gid=0#gid=0) where all the files involved in DCA-Bench have been annotated with their License Information.

Each data point in DCA-Bench has two types of license:
- License for the platform that hosted this dataset
- License of this dataset

**Details:**
- Some data point in DCA-Bench involves files from 2 datasets, we listed all licenses.
- Many datasets themself don’t have a license listed on their data card. 
- We notice that there is one [dataset](https://www.kaggle.com/datasets/roche-data-science-coalition/uncover/data) with id (21ca944c-cf82-4764-bb2c-4c8db0cee950) claiming that  ""Data files © Original Authors"", which is not a standard License. We have reached out to the dataset owner for clarification, but have not received a response.

#### How does this secondary usage of user-generated data comply with restrictions?

DCA-Bench involves user-generated data (comments, modified codes) collected from dataset repo hosted on Github, HuggingFace, and Kaggle.

**For Github**, we collected the comments and modified the codes generated by users. According to section D.3, paragraph 2 of [Github Terms of Service Github](https://docs.github.com/en/site-policy/github-terms/github-terms-of-service\#c-acceptable-use),
> Because you retain ownership of and responsibility for Your Content, we need you to grant us — and other GitHub Users — certain legal permissions, listed in Sections D.4 — D.7.

According to section [D.5\. License Grant to Other Users](https://docs.github.com/en/site-policy/github-terms/github-terms-of-service\#5-license-grant-to-other-users), If not provided a specific License, any User-Generated Content you post publicly, including issues, comments, and contributions to other Users' repositories, may be viewed by others. However, it doesn’t clearly explain how this content is allowed to be used in which ways, and which is not. So I now opened a [Github Discussion](https://github.com/orgs/community/discussions/135466), and I will update this section once I get a response. Besides, I noticed that there have already been some works ([The Stack](https://huggingface.co/datasets/bigcode/the-stack/discussions/43), [SWE-Bench](https://arxiv.org/abs/2310.06770)) with similar usage of Github data, which implies that this usage is acceptable.

**For HuggingFace**, according to [HuggingFace Content Policy](https://huggingface.co/content-guidelines), 

Content types may include:

* ""ML Artifacts"": Code and assets hosted as Hugging Face Repositories, including Models, Datasets, and Spaces;  
* ""Community Content"": Content that can be found in the Community section of the Hugging Face Platform, including discussions, comments, and usernames, as well as related documentation such as READMEs, model cards, data cards, pull requests, and merges.

According to [HuggingFace Terms of Service](https://huggingface.co/terms-of-service), Section “Your Content”, 

> If you decide to set your Repository public, you grant each User a perpetual, irrevocable, worldwide, royalty-free, non-exclusive license to use, display, publish, reproduce, distribute, and make derivative works of your Content through our Services and functionalities;

Therefore, we believe our usage of user-generated content from HuggingFace should belong to the “derivative works” here, which is acceptable.

**For Kaggle**, after reviewing their [Terms of Use](https://www.kaggle.com/terms), I noticed that there are no explicit guidelines regarding the use of user-submitted content for academic research purposes. So I have sent Kaggle a request for clarification and will update this section once I receive a response. 

**Lastly, our collection and evaluation processes exclude the gathering of any GitHub user information. We promise we will remove the content once requested with a valid reason.**

</details>",High,5.0
Q&A,double7/MMMLU_subset,0.0,328.0,2025-06-12 11:27:33+00:00,mit,0.0,48.1 MB,50436505.6,26.7 MB,27996979.2,78420,https://arxiv.org/abs/2009.03300,none,"---
task_categories:
- question-answering
configs:
- config_name: default
  data_files:
  - split: test
    path: test/*.csv
- config_name: AR_XY
  data_files:
  - split: test
    path: test/mmlu_AR-XY*.csv
- config_name: BN_BD
  data_files:
  - split: test
    path: test/mmlu_BN-BD*.csv
- config_name: DE_DE
  data_files:
  - split: test
    path: test/mmlu_DE-DE*.csv
- config_name: ES_LA
  data_files:
  - split: test
    path: test/mmlu_ES-LA*.csv
- config_name: FR_FR
  data_files:
  - split: test
    path: test/mmlu_FR-FR*.csv
- config_name: HI_IN
  data_files:
  - split: test
    path: test/mmlu_HI-IN*.csv
- config_name: ID_ID
  data_files:
  - split: test
    path: test/mmlu_ID-ID*.csv
- config_name: IT_IT
  data_files:
  - split: test
    path: test/mmlu_IT-IT*.csv
- config_name: JA_JP
  data_files:
  - split: test
    path: test/mmlu_JA-JP*.csv
- config_name: KO_KR
  data_files:
  - split: test
    path: test/mmlu_KO-KR*.csv
- config_name: PT_BR
  data_files:
  - split: test
    path: test/mmlu_PT-BR*.csv
- config_name: SW_KE
  data_files:
  - split: test
    path: test/mmlu_SW-KE*.csv
- config_name: YO_NG
  data_files:
  - split: test
    path: test/mmlu_YO-NG*.csv
- config_name: ZH_CN
  data_files:
  - split: test
    path: test/mmlu_ZH-CN*.csv
- config_name: STEM
  data_files:
  - split: test
    path: test/*STEM.csv
- config_name: humanities
  data_files:
  - split: test
    path: test/*humanities.csv
- config_name: social_sciences
  data_files:
  - split: test
    path: test/*social_sciences.csv
- config_name: other
  data_files:
  - split: test
    path: test/*other.csv
- config_name: AR_XY_STEM
  data_files:
  - split: test
    path: test/mmlu_AR-XY_STEM.csv
- config_name: AR_XY_humanities
  data_files:
  - split: test
    path: test/mmlu_AR-XY_humanities.csv
- config_name: AR_XY_social_sciences
  data_files:
  - split: test
    path: test/mmlu_AR-XY_social_sciences.csv
- config_name: AR_XY_other
  data_files:
  - split: test
    path: test/mmlu_AR-XY_other.csv
- config_name: SW_KE_STEM
  data_files:
  - split: test
    path: test/mmlu_SW-KE_STEM.csv
- config_name: SW_KE_humanities
  data_files:
  - split: test
    path: test/mmlu_SW-KE_humanities.csv
- config_name: SW_KE_social_sciences
  data_files:
  - split: test
    path: test/mmlu_SW-KE_social_sciences.csv
- config_name: SW_KE_other
  data_files:
  - split: test
    path: test/mmlu_SW-KE_other.csv
- config_name: ES_LA_STEM
  data_files:
  - split: test
    path: test/mmlu_ES-LA_STEM.csv
- config_name: ES_LA_humanities
  data_files:
  - split: test
    path: test/mmlu_ES-LA_humanities.csv
- config_name: ES_LA_social_sciences
  data_files:
  - split: test
    path: test/mmlu_ES-LA_social_sciences.csv
- config_name: ES_LA_other
  data_files:
  - split: test
    path: test/mmlu_ES-LA_other.csv
- config_name: IT_IT_STEM
  data_files:
  - split: test
    path: test/mmlu_IT-IT_STEM.csv
- config_name: IT_IT_humanities
  data_files:
  - split: test
    path: test/mmlu_IT-IT_humanities.csv
- config_name: IT_IT_social_sciences
  data_files:
  - split: test
    path: test/mmlu_IT-IT_social_sciences.csv
- config_name: IT_IT_other
  data_files:
  - split: test
    path: test/mmlu_IT-IT_other.csv
- config_name: DE_DE_STEM
  data_files:
  - split: test
    path: test/mmlu_DE-DE_STEM.csv
- config_name: DE_DE_humanities
  data_files:
  - split: test
    path: test/mmlu_DE-DE_humanities.csv
- config_name: DE_DE_social_sciences
  data_files:
  - split: test
    path: test/mmlu_DE-DE_social_sciences.csv
- config_name: DE_DE_other
  data_files:
  - split: test
    path: test/mmlu_DE-DE_other.csv
- config_name: ZH_CN_STEM
  data_files:
  - split: test
    path: test/mmlu_ZH-CN_STEM.csv
- config_name: ZH_CN_humanities
  data_files:
  - split: test
    path: test/mmlu_ZH-CN_humanities.csv
- config_name: ZH_CN_social_sciences
  data_files:
  - split: test
    path: test/mmlu_ZH-CN_social_sciences.csv
- config_name: ZH_CN_other
  data_files:
  - split: test
    path: test/mmlu_ZH-CN_other.csv
- config_name: BN_BD_STEM
  data_files:
  - split: test
    path: test/mmlu_BN-BD_STEM.csv
- config_name: BN_BD_humanities
  data_files:
  - split: test
    path: test/mmlu_BN-BD_humanities.csv
- config_name: BN_BD_social_sciences
  data_files:
  - split: test
    path: test/mmlu_BN-BD_social_sciences.csv
- config_name: BN_BD_other
  data_files:
  - split: test
    path: test/mmlu_BN-BD_other.csv
- config_name: HI_IN_STEM
  data_files:
  - split: test
    path: test/mmlu_HI-IN_STEM.csv
- config_name: HI_IN_humanities
  data_files:
  - split: test
    path: test/mmlu_HI-IN_humanities.csv
- config_name: HI_IN_social_sciences
  data_files:
  - split: test
    path: test/mmlu_HI-IN_social_sciences.csv
- config_name: HI_IN_other
  data_files:
  - split: test
    path: test/mmlu_HI-IN_other.csv
- config_name: FR_FR_STEM
  data_files:
  - split: test
    path: test/mmlu_FR-FR_STEM.csv
- config_name: FR_FR_humanities
  data_files:
  - split: test
    path: test/mmlu_FR-FR_humanities.csv
- config_name: FR_FR_social_sciences
  data_files:
  - split: test
    path: test/mmlu_FR-FR_social_sciences.csv
- config_name: FR_FR_other
  data_files:
  - split: test
    path: test/mmlu_FR-FR_other.csv
- config_name: ID_ID_STEM
  data_files:
  - split: test
    path: test/mmlu_ID-ID_STEM.csv
- config_name: ID_ID_humanities
  data_files:
  - split: test
    path: test/mmlu_ID-ID_humanities.csv
- config_name: ID_ID_social_sciences
  data_files:
  - split: test
    path: test/mmlu_ID-ID_social_sciences.csv
- config_name: ID_ID_other
  data_files:
  - split: test
    path: test/mmlu_ID-ID_other.csv
- config_name: KO_KR_STEM
  data_files:
  - split: test
    path: test/mmlu_KO-KR_STEM.csv
- config_name: KO_KR_humanities
  data_files:
  - split: test
    path: test/mmlu_KO-KR_humanities.csv
- config_name: KO_KR_social_sciences
  data_files:
  - split: test
    path: test/mmlu_KO-KR_social_sciences.csv
- config_name: KO_KR_other
  data_files:
  - split: test
    path: test/mmlu_KO-KR_other.csv
- config_name: YO_NG_STEM
  data_files:
  - split: test
    path: test/mmlu_YO-NG_STEM.csv
- config_name: YO_NG_humanities
  data_files:
  - split: test
    path: test/mmlu_YO-NG_humanities.csv
- config_name: YO_NG_social_sciences
  data_files:
  - split: test
    path: test/mmlu_YO-NG_social_sciences.csv
- config_name: YO_NG_other
  data_files:
  - split: test
    path: test/mmlu_YO-NG_other.csv
- config_name: JA_JP_STEM
  data_files:
  - split: test
    path: test/mmlu_JA-JP_STEM.csv
- config_name: JA_JP_humanities
  data_files:
  - split: test
    path: test/mmlu_JA-JP_humanities.csv
- config_name: JA_JP_social_sciences
  data_files:
  - split: test
    path: test/mmlu_JA-JP_social_sciences.csv
- config_name: JA_JP_other
  data_files:
  - split: test
    path: test/mmlu_JA-JP_other.csv
- config_name: PT_BR_STEM
  data_files:
  - split: test
    path: test/mmlu_PT-BR_STEM.csv
- config_name: PT_BR_humanities
  data_files:
  - split: test
    path: test/mmlu_PT-BR_humanities.csv
- config_name: PT_BR_social_sciences
  data_files:
  - split: test
    path: test/mmlu_PT-BR_social_sciences.csv
- config_name: PT_BR_other
  data_files:
  - split: test
    path: test/mmlu_PT-BR_other.csv

language:
- ar
- bn
- de
- es
- fr
- hi
- id
- it
- ja
- ko
- pt
- sw
- yo
- zh
license: mit
---

# About MMMLU subset
  This is a subset of MMMLU, specifically, we sampled 10% of the original data to improve evaluation efficiency.
  In addition, we categorize the questions into four categories by subject, i.e., STEM, HUMANITIES, SOCIAL SCIENCES, and OTHER, aligned with [MMLU](https://huggingface.co/datasets/cais/mmlu).

# Multilingual Massive Multitask Language Understanding (MMMLU)

The MMLU is a widely recognized benchmark of general knowledge attained by AI models. It covers a broad range of topics from 57 different categories, covering elementary-level knowledge up to advanced professional subjects like law, physics, history, and computer science.

We translated the MMLU’s test set into 14 languages using professional human translators. Relying on human translators for this evaluation increases confidence in the accuracy of the translations, especially for low-resource languages like Yoruba. We are publishing the professional human translations and the code we use to run the evaluations.

This effort reflects our commitment to improving the multilingual capabilities of AI models, ensuring they perform accurately across languages, particularly for underrepresented communities. By prioritizing high-quality translations, we aim to make AI technology more inclusive and effective for users worldwide.

## Locales

MMMLU contains the MMLU test set translated into the following locales:
* AR_XY (Arabic)
* BN_BD (Bengali)
* DE_DE (German)
* ES_LA (Spanish)
* FR_FR (French)
* HI_IN (Hindi)
* ID_ID (Indonesian)
* IT_IT (Italian)
* JA_JP (Japanese)
* KO_KR (Korean)
* PT_BR (Brazilian Portuguese)
* SW_KE (Swahili)
* YO_NG (Yoruba)
* ZH_CN (Simplified Chinese)

## Sources

Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., & Steinhardt, J. (2021). [*Measuring Massive Multitask Language Understanding*](https://arxiv.org/abs/2009.03300).

[OpenAI Simple Evals GitHub Repository](https://github.com/openai/simple-evals)",Low,1.0
Q&A,UBC-NLP/palmx_2025_subtask1_culture,0.0,142.0,2025-06-11 19:54:36+00:00,none,0.0,708 kB,unknown,708 kB,unknown,2500,none,none,"---
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: dev
    path: data/dev-*
dataset_info:
  features:
  - name: id
    dtype: int64
  - name: question
    dtype: string
  - name: A
    dtype: string
  - name: B
    dtype: string
  - name: C
    dtype: string
  - name: D
    dtype: string
  - name: answer
    dtype: string
  splits:
  - name: train
    num_bytes: 1059019
    num_examples: 2000
  - name: dev
    num_bytes: 271128
    num_examples: 500
  download_size: 707872
  dataset_size: 1330147
task_categories:
- question-answering
language:
- ar
tags:
- ArabicNLP
- Shared-task
- Culture
---
🏷️ PalmX 2025 — General Culture Evaluation (PalmX-GC)

### Dataset Summary

PalmX-GC evaluates a model’s grasp of general Arab culture—customs, history, geography, arts, cuisine, notable figures, and everyday life across the 22 Arab League countries.
Every item is written in Modern Standard Arabic (MSA). The dataset powers Subtask 1 of the PalmX 2025 shared task.

---

### Dataset Structure

| Split | # MCQs | Release Date | Notes                    |
|------:|-------:|-------------|--------------------------|
| Train | 2000    | 10 Jun 2025 | With gold answers        |
| Dev   | 500    | 10 Jun 2025 | With gold answers        |
| Test  | 2 000  | Blind       | Answers withheld         |

---
### Example Record
```json
{
  ""id"": 42042,
  ""question"": ""ما هو أعلى جبل في اليمن؟"",
  ""A"": ""جبل النبي شعيب"",
  ""B"": ""جبل شمسان"",
  ""C"": ""جبل صبر"",
  ""D"": ""جبل حدة"",
  ""answer"": ""A""
}
```
---
### License
PalmX Data License — non-commercial research use.

---
#### Access
You will gain access after registering for the shared task.

    • Register here → https://forms.gle/cpmYGkAhM3YLXBsQ7
    • Website → https://palmx.dlnlp.ai/
 
---
### Contact & Links
	•	🌐 Website: https://palmx.dlnlp.ai/
	•	📝 Registration Form: https://forms.gle/cpmYGkAhM3YLXBsQ7
	•	📣 Google Group: https://groups.google.com/g/palmx2025",Medium,2.0
Q&A,UBC-NLP/palmx_2025_subtask2_islamic,0.0,67.0,2025-06-11 19:53:36+00:00,none,0.0,127 kB,unknown,127 kB,unknown,900,none,none,"---
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: dev
    path: data/dev-*
dataset_info:
  features:
  - name: id
    dtype: int64
  - name: question
    dtype: string
  - name: A
    dtype: string
  - name: B
    dtype: string
  - name: C
    dtype: string
  - name: D
    dtype: string
  - name: answer
    dtype: string
  splits:
  - name: train
    num_bytes: 152928
    num_examples: 600
  - name: dev
    num_bytes: 77428
    num_examples: 300
  download_size: 127119
  dataset_size: 230356
task_categories:
- question-answering
language:
- ar
tags:
- Islamic
- Shared-Task
- PlamX
- ArabicNLP
---


## 🏷️ PalmX 2025 — Islamic Culture Evaluation (PalmX-IC)

### Dataset Summary
PalmX-IC assesses a model’s knowledge of Islamic culture—rituals, Qurʾān verses, Ḥadīth, historic events, jurisprudence, and religious holidays—core elements of life across the Arab world.  
All items are authored in **Modern Standard Arabic (MSA)** . The dataset powers **Subtask 2** of the PalmX 2025 shared task.

---
### Dataset Structure
| Split | # MCQs | Release Date | Notes                    |
|------:|-------:|-------------|--------------------------|
| Train | 600    | 10 Jun 2025 | With gold answers        |
| Dev   | 300    | 10 Jun 2025 | With gold answers        |
| Test  | 1 000  | Blind       | Answers withheld         |

---
#### Example Record
```json
{
  ""id"": 57057,
  ""question"": ""في أي شهر هجري فُرض صيام رمضان؟"",
  ""A"": ""شعبان"",
  ""B"": ""رمضان"",
  ""C"": ""محرم"",
  ""D"": ""ربيع الأول"",
  ""answer"": ""A""
}
```
---
#### License
PalmX Data License — non-commercial research use.

---
#### Access
You will gain access after registering for the shared task.

    • Register here → https://forms.gle/cpmYGkAhM3YLXBsQ7
    • Website → https://palmx.dlnlp.ai/

---
#### Contact & Links
	•	🌐 Website: https://palmx.dlnlp.ai/
	•	📝 Registration Form: https://forms.gle/cpmYGkAhM3YLXBsQ7
	•	📣 Google Group: https://groups.google.com/g/palmx2025",Medium,3.0
Q&A,HLeiTR/R3-eval-MMMLU,0.0,172.0,2025-06-11 20:14:59+00:00,mit,0.0,1.73 GB,1857573355.52,363 MB,380633088.0,786352,none,none,"---
task_categories:
  - question-answering
configs:
  - config_name: default
    data_files:
      - split: test
        path: ""data/*.csv""
  - config_name: AR_XY
    data_files:
      - split: test
        path: ""data/mmmlu_AR_XY.csv""
  - config_name: BN_BD
    data_files:
      - split: test
        path: ""data/mmmlu_BN_BD.csv""
  - config_name: DE_DE
    data_files:
      - split: test
        path: ""data/mmmlu_DE_DE.csv""
  - config_name: ES_LA
    data_files:
      - split: test
        path: ""data/mmmlu_ES_LA.csv""
  - config_name: FR_FR
    data_files:
      - split: test
        path: ""data/mmmlu_FR_FR.csv""
  - config_name: HI_IN
    data_files:
      - split: test
        path: ""data/mmmlu_HI_IN.csv""
  - config_name: ID_ID
    data_files:
      - split: test
        path: ""data/mmmlu_ID_ID.csv""
  - config_name: IT_IT
    data_files:
      - split: test
        path: ""data/mmmlu_IT_IT.csv""
  - config_name: JA_JP
    data_files:
      - split: test
        path: ""data/mmmlu_JA_JP.csv""
  - config_name: KO_KR
    data_files:
      - split: test
        path: ""data/mmmlu_KO_KR.csv""
  - config_name: PT_BR
    data_files:
      - split: test
        path: ""data/mmmlu_PT_BR.csv""
  - config_name: SW_KE
    data_files:
      - split: test
        path: ""data/mmmlu_SW_KE.csv""
  - config_name: YO_NG
    data_files:
      - split: test
        path: ""data/mmmlu_YO_NG.csv""
  - config_name: ZH_CN
    data_files:
      - split: test
        path: ""data/mmmlu_ZH_CN.csv""
language:
  - ar
  - bn
  - de
  - es
  - fr
  - hi
  - id
  - it
  - ja
  - ko
  - pt
  - sw
  - yo
  - zh
license: mit
---",Low,1.0
Summarization,esdurmus/wiki_lingua,46.0,357.0,2024-01-05 08:06:54+00:00,cc-by-3.0,0.0,1.2 GB,1288490188.8,1.2 GB,1288490188.8,273824,https://arxiv.org/abs/2010.03093,"['https://aclanthology.org/2020.findings-emnlp.360"",']","---
annotations_creators:
- crowdsourced
language_creators:
- crowdsourced
language:
- ar
- cs
- de
- en
- es
- fr
- hi
- id
- it
- ja
- ko
- nl
- pt
- ru
- th
- tr
- vi
- zh
license:
- cc-by-3.0
multilinguality:
- multilingual
size_categories:
- 10K<n<100K
- 1K<n<10K
source_datasets:
- original
task_categories:
- summarization
task_ids: []
paperswithcode_id: wikilingua
pretty_name: WikiLingua
config_names:
- arabic
- chinese
- czech
- dutch
- english
- french
- german
- hindi
- indonesian
- italian
- japanese
- korean
- portuguese
- russian
- spanish
- thai
- turkish
- vietnamese
dataset_info:
- config_name: arabic
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 119116075
    num_examples: 9995
  download_size: 55808460
  dataset_size: 119116075
- config_name: chinese
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 41170645
    num_examples: 6541
  download_size: 25187026
  dataset_size: 41170645
- config_name: czech
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 20816346
    num_examples: 2520
  download_size: 12480761
  dataset_size: 20816346
- config_name: dutch
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 87257952
    num_examples: 10862
  download_size: 47651076
  dataset_size: 87257952
- config_name: english
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
  splits:
  - name: train
    num_bytes: 333699946
    num_examples: 57945
  download_size: 187189233
  dataset_size: 333699946
- config_name: french
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 197550244
    num_examples: 21690
  download_size: 105158840
  dataset_size: 197550244
- config_name: german
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 168674208
    num_examples: 20103
  download_size: 93078076
  dataset_size: 168674208
- config_name: hindi
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 63785007
    num_examples: 3402
  download_size: 22774620
  dataset_size: 63785007
- config_name: indonesian
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 136408773
    num_examples: 16308
  download_size: 67658970
  dataset_size: 136408773
- config_name: italian
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 138119439
    num_examples: 17673
  download_size: 78108134
  dataset_size: 138119439
- config_name: japanese
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 40144987
    num_examples: 4372
  download_size: 19794488
  dataset_size: 40144987
- config_name: korean
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 38647570
    num_examples: 4111
  download_size: 20029486
  dataset_size: 38647570
- config_name: portuguese
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 204270713
    num_examples: 28143
  download_size: 114735912
  dataset_size: 204270713
- config_name: russian
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 241923944
    num_examples: 18143
  download_size: 111025228
  dataset_size: 241923944
- config_name: spanish
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 314618442
    num_examples: 38795
  download_size: 170995186
  dataset_size: 314618442
- config_name: thai
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 86982807
    num_examples: 5093
  download_size: 31944979
  dataset_size: 86982807
- config_name: turkish
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 11371777
    num_examples: 1512
  download_size: 5964904
  dataset_size: 11371777
- config_name: vietnamese
  features:
  - name: url
    dtype: string
  - name: article
    sequence:
    - name: section_name
      dtype: string
    - name: document
      dtype: string
    - name: summary
      dtype: string
    - name: english_url
      dtype: string
    - name: english_section_name
      dtype: string
  splits:
  - name: train
    num_bytes: 69868744
    num_examples: 6616
  download_size: 33194150
  dataset_size: 69868744
configs:
- config_name: arabic
  data_files:
  - split: train
    path: arabic/train-*
- config_name: chinese
  data_files:
  - split: train
    path: chinese/train-*
- config_name: czech
  data_files:
  - split: train
    path: czech/train-*
- config_name: dutch
  data_files:
  - split: train
    path: dutch/train-*
- config_name: english
  data_files:
  - split: train
    path: english/train-*
  default: true
- config_name: french
  data_files:
  - split: train
    path: french/train-*
- config_name: german
  data_files:
  - split: train
    path: german/train-*
- config_name: hindi
  data_files:
  - split: train
    path: hindi/train-*
- config_name: indonesian
  data_files:
  - split: train
    path: indonesian/train-*
- config_name: italian
  data_files:
  - split: train
    path: italian/train-*
- config_name: japanese
  data_files:
  - split: train
    path: japanese/train-*
- config_name: korean
  data_files:
  - split: train
    path: korean/train-*
- config_name: portuguese
  data_files:
  - split: train
    path: portuguese/train-*
- config_name: russian
  data_files:
  - split: train
    path: russian/train-*
- config_name: spanish
  data_files:
  - split: train
    path: spanish/train-*
- config_name: thai
  data_files:
  - split: train
    path: thai/train-*
- config_name: turkish
  data_files:
  - split: train
    path: turkish/train-*
- config_name: vietnamese
  data_files:
  - split: train
    path: vietnamese/train-*
---
# Dataset Card for ""wiki_lingua""

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Repository:** [URL](https://github.com/esdurmus/Wikilingua)
- **Paper:** [WikiLingua: A Multilingual Abstractive Summarization Dataset](https://arxiv.org/abs/2010.03093)

### Dataset Summary

We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of cross-lingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard article-summary alignments across languages by aligning the images that are used to describe each how-to step in an article.

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

The table below shows number of article-summary pairs with a parallel article-summary pair in English. 
______________________________
| Language    | Num. parallel |
| ----------- | --------------|
| English     |   141,457     |
| Spanish     |   113,215     |
| Portuguese  |    81,695     |
| French      |    63,692     |
| German      |    58,375     |
| Russian     |    52,928     |
| Italian     |    50,968     |
| Indonesian  |    47,511     |
| Dutch       |    31,270     |
| Arabic      |    29,229     |
| Vietnamese  |    19,600     |
| Chinese     |    18,887     |
| Thai        |    14,770     |
| Japanese    |    12,669     |
| Korean      |    12,189     |
| Hindi       |     9,929     |
| Czech       |     7,200     |
| Turkish     |     4,503     |


## Dataset Structure

### Data Instances

```
{
    'article': {
        'document': ['make sure that the area is a safe place, especially if you plan on walking home at night.  It’s always a good idea to practice the buddy system.  Have a friend meet up and walk with you. Research the bus, train, or streetcar routes available in your area to find safe and affordable travel to your destination.  Make sure you check the schedule for your outgoing and return travel.  Some public transportation will cease to run late at night.  Be sure if you take public transportation to the venue that you will also be able to get home late at night. Check the routes.  Even if some public transit is still running late at night, the routing may change.  Some may run express past many of the stops, or not travel all the way to the ends.  Be sure that your stop will still be available when you need it for your return trip. If you are taking public transit in a vulnerable state after drinking, it is always a good idea to travel in groups.  Having friends available is a good way to stay safe and make sure that you reach your destination. This is more expensive option than a taxi or ride share service, but could be a fun and fancy way to stay safe and ensure that you will have a ride home. Plan this service in advance with a scheduled time to pick you up from your home and the venue.  You want to be sure that the service will still be available when you need to get home. This may be easy in a large city, but taxis may be less frequent in smaller towns.  This is especially true late at night, so this is a less reliable option than scheduling a ride in advance.  Have a friend accompany you and help you flag a cab to make sure you are able to get one. Set up a plan to call a friend when you get home to make sure that you made it safely to your destination. If there are no taxis readily available call a local service to send a car to pick you up. You can share a ride with your friends, or other people using the app at the same moment.  If you are in a vulnerable state it is best to share the ride with your friends to make sure you get home safe. You can request the car to yourself rather than sharing rides with strangers. If you travel home on your own or are the last of your group to be dropped off, make plans to call a friend when you get home so they know you made it safely to your destination. There may be a designated driver service in your area which can chauffeur your group.  Make reservations with them in advance and keep their contact information handy while you are drinking.',
            ""Designating a driver is a very popular tactic to avoid drinking and driving.  It is important to plan in advance, because your brain function will slow down and your decision making skills will be impaired once you start drinking. Decide before you begin drinking that you will not drive.  Figure out who will be getting you home before you leave. Make sure this person is responsible and keep them in your sight while you are drinking.  Have their contact information handy in case you can’t find them when you are ready to leave.  Choose a friend who doesn’t drink alcohol.  You likely have someone in your friend group who doesn’t drink.  This person is the most likely to remain sober. Decide on one person who will remain sober.  You can take turns within your friend group, alternating who will be the designated driver on each occasion.  Be sure that the designated driver actually remains sober.  The person who has drank the least is still not sober. If you don’t have your car with you, you can guarantee that you won’t make the choice to drive it home. If you are drinking at your home.  Give your keys to a responsible friend to ensure that you don't choose to drive somewhere after you have been drinking. It may be tempting to stay longer or leave with someone else.  Stick to the plan you made in advance and only leave with your sober, designated driver.  Keep the phone number of your driver handy in case you can't find them when you are ready to leave. If your designated driver drinks alcohol, find alternate transportation to get home."",
            'If you have been drinking at all you are at least on the spectrum of drunkenness.  You could be showing signs of impairment and slower brain function including lack of motor skills and slower reaction time, leading to the inability to operate a motor vehicle.  Some of these signs could be:  Poor balance or stumbling.  Difficulty speaking clearly and slurred words.  Abnormal behavior leading to you doing things you wouldn’t normally do if you were sober. As soon as you notice that you are showing signs of impairment, give your keys to a friend, the host or the bartender to ensure that you won’t drive until you are sober.  Make sure to only give them your car key.  Hold onto your house keys. If your friend, the host or the bartender are advising you not to drive, you are likely too drunk.  Listen to their advice and acknowledge that they are trying to help you. Bystander intervention is common when it comes to drinking and driving.  Many people will be willing to step in, take your keys and help you get home safely.  If no one if offering to help, you may need to ask.  Take a ride from a sober friend.  It is best to get in a car with someone you trust when you are in this vulnerable state. Allow the host or bartender to call a cab or car service to take you home. If you are having a difficult time finding a safe way to get home, find a place to stay which does not involve you driving.  Ask the host of the party if there is a place you can sleep.  Give them your keys and ask that they keep them in a safe place until the morning. Stay with a friend if they live nearby and are on their way home. Find a hotel within walking distance.  Call them to book a room, or have a friend help you secure one.  Ask the friend if they will walk you to the hotel and make sure you get checked in safely. There are people in your life who care about you and want to be sure that you are safe.  It may seem scary or embarrassing to call your parents or your siblings if you are too drunk to drive, but they will be glad you did.  Your safety is the most important. You may need your phone to call someone for a ride or get help from a friend.  Be sure to charge your phone before you leave the house.  It is also a good idea to bring a charger with you in case your battery dies before the end of the night or you end up staying where you are and need to get home the next morning. You may also want to invest in a portable battery charger for your phone should there not be a power outlet available.  Make sure it is fully charged before you leave your house.  Keep it handy in your pocket or your bag throughout the night.'
        ],
        'section_name': ['Finding Other Transportation',
            'Designating a Driver',
            'Staying Safe'
        ],
        'summary': ['Walk to the venue where you will be drinking if it is close enough. Take public transit. Show up in style by hiring a limo or black car service. Flag a taxi cab for a convenient option to get where you’re going. Request a rideshare service like Uber or Lyft using an app on your phone. Reserve a designated driver service.',
            'Plan in advance. Assign a designated driver. Leave your car at home. Leave the venue with your designated driver.',
            'Pay attention to your body. Give up your keys. Listen to other people. Accept help. Stay where you are. Have an emergency back-up plan. Make sure that your phone is charged.'
        ]
    },
    'url': 'https://www.wikihow.com/Avoid-Drinking-and-Driving'
}
```
### Data Fields

- `url`: WikiHow URL of the article
- `article`: A dictionary containing `section_name`, `document` and `summary`
  - `section_name`: List of section headings in an article
  - `document`: List of documents, one for each section in the `section_name` list
  - `summary`: List of summarized document

### Data Splits

|            |   train |
|:-----------|--------:|
| arabic     |    9995 |
| chinese    |    6541 |
| czech      |    2520 |
| dutch      |   10862 |
| english    |   57945 |
| french     |   21690 |
| german     |   20103 |
| hindi      |    3402 |
| indonesian |   16308 |
| italian    |   17673 |
| japanese   |    4372 |
| korean     |    4111 |
| portuguese |   28143 |
| russian    |   18143 |
| spanish    |    6616 |
| thai       |    5093 |
| turkish    |    1512 |
| vietnamese |    6616 |

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

- Article provided by wikiHow https://www.wikihow.com/Main-Page, a wiki building the world's largest, highest quality how-to manual. Please edit this article and find author credits at wikiHow.com. Content on wikiHow can be shared under a [Creative Commons license](http://creativecommons.org/licenses/by-nc-sa/3.0/).
- Refer to [this webpage](https://www.wikihow.com/wikiHow:Attribution) for the specific attribution guidelines.
- also see https://gem-benchmark.com/data_cards/WikiLingua

### Citation Information

```bibtex
@inproceedings{ladhak-etal-2020-wikilingua,
    title = ""{W}iki{L}ingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization"",
    author = ""Ladhak, Faisal  and
      Durmus, Esin  and
      Cardie, Claire  and
      McKeown, Kathleen"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2020"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.findings-emnlp.360"",
    doi = ""10.18653/v1/2020.findings-emnlp.360"",
    pages = ""4034--4048"",
}
```

### Contributions

Thanks to [@katnoria](https://github.com/katnoria) for adding this dataset.",High,5.0
Summarization,GEM/wiki_lingua,48.0,2202.0,2023-02-16 09:23:29+00:00,cc-by-nc-sa-3.0,7.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,"['https://aclanthology.org/2020.findings-emnlp.360"",']","---
annotations_creators:
- none
language_creators:
- unknown
language:
- ar
- cs
- de
- en
- es
- fr
- hi
- id
- it
- ja
- ko
- nl
- pt
- ru
- th
- tr
- vi
- zh
license:
- cc-by-nc-sa-3.0
multilinguality:
- multilingual
size_categories:
- unknown
source_datasets:
- original
task_categories:
- summarization
task_ids: []
pretty_name: wiki_lingua
---

# Dataset Card for GEM/wiki_lingua

## Dataset Description

- **Homepage:** None (See Repository)
- **Repository:** https://github.com/esdurmus/Wikilingua
- **Paper:** https://www.aclweb.org/anthology/2020.findings-emnlp.360/
- **Leaderboard:** N/A
- **Point of Contact:** Faisal Ladhak, Esin Durmus

### Link to Main Data Card

You can find the main data card on the [GEM Website](https://gem-benchmark.com/data_cards/wiki_lingua).

### Dataset Summary 

Placeholder

You can load the dataset via:
```
import datasets
data = datasets.load_dataset('GEM/wiki_lingua')
```
The data loader can be found [here](https://huggingface.co/datasets/GEM/wiki_lingua).

#### website
None (See Repository)

#### paper
https://www.aclweb.org/anthology/2020.findings-emnlp.360/

#### authors
Faisal Ladhak (Columbia University), Esin Durmus (Stanford University), Claire Cardie (Cornell University), Kathleen McKeown (Columbia University)

## Dataset Overview

### Where to find the Data and its Documentation

#### Webpage

<!-- info: What is the webpage for the dataset (if it exists)? -->
<!-- scope: telescope -->
None (See Repository)

#### Download

<!-- info: What is the link to where the original dataset is hosted? -->
<!-- scope: telescope -->
https://github.com/esdurmus/Wikilingua

#### Paper

<!-- info: What is the link to the paper describing the dataset (open access preferred)? -->
<!-- scope: telescope -->
https://www.aclweb.org/anthology/2020.findings-emnlp.360/

#### BibTex

<!-- info: Provide the BibTex-formatted reference for the dataset. Please use the correct published version (ACL anthology, etc.) instead of google scholar created Bibtex. -->
<!-- scope: microscope -->
@inproceedings{ladhak-etal-2020-wikilingua,
    title = ""{W}iki{L}ingua: A New Benchmark Dataset for Cross-Lingual Abstractive Summarization"",
    author = ""Ladhak, Faisal  and
      Durmus, Esin  and
      Cardie, Claire  and
      McKeown, Kathleen"",
    booktitle = ""Findings of the Association for Computational Linguistics: EMNLP 2020"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.findings-emnlp.360"",
    doi = ""10.18653/v1/2020.findings-emnlp.360"",
    pages = ""4034--4048"",
    abstract = ""We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of cross-lingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard article-summary alignments across languages by aligning the images that are used to describe each how-to step in an article. As a set of baselines for further studies, we evaluate the performance of existing cross-lingual abstractive summarization methods on our dataset. We further propose a method for direct cross-lingual summarization (i.e., without requiring translation at inference time) by leveraging synthetic data and Neural Machine Translation as a pre-training step. Our method significantly outperforms the baseline approaches, while being more cost efficient during inference."",
}

#### Contact Name

<!-- quick -->
<!-- info: If known, provide the name of at least one person the reader can contact for questions about the dataset. -->
<!-- scope: periscope -->
Faisal Ladhak, Esin Durmus

#### Contact Email

<!-- info: If known, provide the email of at least one person the reader can contact for questions about the dataset. -->
<!-- scope: periscope -->
faisal@cs.columbia.edu, esdurmus@stanford.edu

#### Has a Leaderboard?

<!-- info: Does the dataset have an active leaderboard? -->
<!-- scope: telescope -->
no


### Languages and Intended Use

#### Multilingual?

<!-- quick -->
<!-- info: Is the dataset multilingual? -->
<!-- scope: telescope -->
yes

#### Covered Dialects

<!-- info: What dialects are covered? Are there multiple dialects per language? -->
<!-- scope: periscope -->
Dataset does not have multiple dialects per language.

#### Covered Languages

<!-- quick -->
<!-- info: What languages/dialects are covered in the dataset? -->
<!-- scope: telescope -->
`English`, `Spanish, Castilian`, `Portuguese`, `French`, `German`, `Russian`, `Italian`, `Indonesian`, `Dutch, Flemish`, `Arabic`, `Chinese`, `Vietnamese`, `Thai`, `Japanese`, `Korean`, `Hindi`, `Czech`, `Turkish`

#### Whose Language?

<!-- info: Whose language is in the dataset? -->
<!-- scope: periscope -->
No information about the user demographic is available.

#### License

<!-- quick -->
<!-- info: What is the license of the dataset? -->
<!-- scope: telescope -->
cc-by-nc-sa-3.0: Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported (CC BY-NC-SA 3.0)

#### Intended Use

<!-- info: What is the intended use of the dataset? -->
<!-- scope: microscope -->
The dataset was intended to serve as a large-scale, high-quality benchmark dataset for cross-lingual summarization.

#### Primary Task

<!-- info: What primary task does the dataset support? -->
<!-- scope: telescope -->
Summarization

#### Communicative Goal

<!-- quick -->
<!-- info: Provide a short description of the communicative goal of a model trained for this task on this dataset. -->
<!-- scope: periscope -->
Produce a high quality summary for the given input article.



### Credit

#### Curation Organization Type(s)

<!-- info: In what kind of organization did the dataset curation happen? -->
<!-- scope: telescope -->
`academic`

#### Curation Organization(s)

<!-- info: Name the organization(s). -->
<!-- scope: periscope -->
Columbia University

#### Dataset Creators

<!-- info: Who created the original dataset? List the people involved in collecting the dataset and their affiliation(s). -->
<!-- scope: microscope -->
Faisal Ladhak (Columbia University), Esin Durmus (Stanford University), Claire Cardie (Cornell University), Kathleen McKeown (Columbia University)

#### Who added the Dataset to GEM?

<!-- info: Who contributed to the data card and adding the dataset to GEM? List the people+affiliations involved in creating this data card and who helped integrate this dataset into GEM. -->
<!-- scope: microscope -->
Jenny Chim (Queen Mary University of London), Faisal Ladhak (Columbia University)


### Dataset Structure

#### Data Fields

<!-- info: List and describe the fields present in the dataset. -->
<!-- scope: telescope -->
gem_id -- The id for the data instance.
source_language -- The language of the source article.
target_language -- The language of the target summary.
source -- The source document.


#### Example Instance

<!-- info: Provide a JSON formatted example of a typical instance in the dataset. -->
<!-- scope: periscope -->
{
    ""gem_id"": ""wikilingua_crosslingual-train-12345"",
    ""gem_parent_id"": ""wikilingua_crosslingual-train-12345"",
    ""source_language"": ""fr"",
    ""target_language"": ""de"",
    ""source"": ""Document in fr"",
    ""target"": ""Summary in de"",
}

#### Data Splits

<!-- info: Describe and name the splits in the dataset if there are more than one. -->
<!-- scope: periscope -->
The data is split into train/dev/test. In addition to the full test set, there's also a sampled version of the test set. 

#### Splitting Criteria

<!-- info: Describe any criteria for splitting the data, if used. If there are differences between the splits (e.g., if the training annotations are machine-generated and the dev and test ones are created by humans, or if different numbers of annotators contributed to each example), describe them here. -->
<!-- scope: microscope -->
The data was split to ensure the same document would appear in the same split across languages so as to ensure there's no leakage into the test set.



## Dataset in GEM

### Rationale for Inclusion in GEM

#### Why is the Dataset in GEM?

<!-- info: What does this dataset contribute toward better generation evaluation and why is it part of GEM? -->
<!-- scope: microscope -->
This dataset provides a large-scale, high-quality resource for cross-lingual summarization in 18 languages, increasing the coverage of languages for the GEM summarization task. 

#### Similar Datasets

<!-- info: Do other datasets for the high level task exist? -->
<!-- scope: telescope -->
yes

#### Unique Language Coverage

<!-- info: Does this dataset cover other languages than other datasets for the same task? -->
<!-- scope: periscope -->
yes

#### Difference from other GEM datasets

<!-- info: What else sets this dataset apart from other similar datasets in GEM? -->
<!-- scope: microscope -->
XSum covers English news articles, and MLSum covers news articles in German and Spanish. 
In contrast, this dataset has how-to articles in 18 languages, substantially increasing the languages covered. Moreover, it also provides a a different domain than the other two datasets.

#### Ability that the Dataset measures

<!-- info: What aspect of model ability can be measured with this dataset? -->
<!-- scope: periscope -->
The ability to generate quality summaries across multiple languages.


### GEM-Specific Curation

#### Modificatied for GEM?

<!-- info: Has the GEM version of the dataset been modified in any way (data, processing, splits) from the original curated data? -->
<!-- scope: telescope -->
yes

#### GEM Modifications

<!-- info: What changes have been made to he original dataset? -->
<!-- scope: periscope -->
`other`

#### Modification Details

<!-- info: For each of these changes, described them in more details and provided the intended purpose of the modification -->
<!-- scope: microscope -->
Previous version had separate data loaders for each language. In this version, we've created a single monolingual data loader, which contains monolingual data in each of the 18 languages. In addition, we've also created a single cross-lingual data loader across all the language pairs in the dataset.  

#### Additional Splits?

<!-- info: Does GEM provide additional splits to the dataset? -->
<!-- scope: telescope -->
no


### Getting Started with the Task




## Previous Results

### Previous Results

#### Measured Model Abilities

<!-- info: What aspect of model ability can be measured with this dataset? -->
<!-- scope: telescope -->
Ability to summarize content across different languages.

#### Metrics

<!-- info: What metrics are typically used for this task? -->
<!-- scope: periscope -->
`ROUGE`

#### Proposed Evaluation

<!-- info: List and describe the purpose of the metrics and evaluation methodology (including human evaluation) that the dataset creators used when introducing this task. -->
<!-- scope: microscope -->
ROUGE is used to measure content selection by comparing word overlap with reference summaries. In addition, the authors of the dataset also used human evaluation to evaluate content selection and fluency of the systems.

#### Previous results available?

<!-- info: Are previous results available? -->
<!-- scope: telescope -->
no



## Dataset Curation

### Original Curation

#### Original Curation Rationale

<!-- info: Original curation rationale -->
<!-- scope: telescope -->
The dataset was created in order to enable new approaches for cross-lingual and multilingual summarization, which are currently understudied as well as open up inetersting new directions for research in summarization. E.g., exploration of multi-source cross-lingual architectures, i.e. models that can summarize from multiple source languages into a target language, building models that can summarize articles from any language to any other language for a given set of languages.

#### Communicative Goal

<!-- info: What was the communicative goal? -->
<!-- scope: periscope -->
Given an input article, produce a high quality summary of the article in the target language.

#### Sourced from Different Sources

<!-- info: Is the dataset aggregated from different data sources? -->
<!-- scope: telescope -->
no


### Language Data

#### How was Language Data Obtained?

<!-- info: How was the language data obtained? -->
<!-- scope: telescope -->
`Found`

#### Where was it found?

<!-- info: If found, where from? -->
<!-- scope: telescope -->
`Single website`

#### Language Producers

<!-- info: What further information do we have on the language producers? -->
<!-- scope: microscope -->
WikiHow, which is an online resource of how-to guides (written and reviewed by human authors) is used as the data source. 

#### Topics Covered

<!-- info: Does the language in the dataset focus on specific topics? How would you describe them? -->
<!-- scope: periscope -->
The articles cover 19 broad categories including health, arts and entertainment, personal care and style, travel, education and communications, etc. The categories cover a broad set of genres and topics.

#### Data Validation

<!-- info: Was the text validated by a different worker or a data curator? -->
<!-- scope: telescope -->
not validated

#### Was Data Filtered?

<!-- info: Were text instances selected or filtered? -->
<!-- scope: telescope -->
not filtered


### Structured Annotations

#### Additional Annotations?

<!-- quick -->
<!-- info: Does the dataset have additional annotations for each instance? -->
<!-- scope: telescope -->
none

#### Annotation Service?

<!-- info: Was an annotation service used? -->
<!-- scope: telescope -->
no


### Consent

#### Any Consent Policy?

<!-- info: Was there a consent policy involved when gathering the data? -->
<!-- scope: telescope -->
yes

#### Consent Policy Details

<!-- info: What was the consent policy? -->
<!-- scope: microscope -->
(1) Text Content. All text posted by Users to the Service is sub-licensed by wikiHow to other Users under a Creative Commons license as provided herein. The Creative Commons license allows such text content be used freely for non-commercial purposes, so long as it is used and attributed to the original author as specified under the terms of the license. Allowing free republication of our articles helps wikiHow achieve its mission by providing instruction on solving the problems of everyday life to more people for free. In order to support this goal, wikiHow hereby grants each User of the Service a license to all text content that Users contribute to the Service under the terms and conditions of a Creative Commons CC BY-NC-SA 3.0 License. Please be sure to read the terms of the license carefully. You continue to own all right, title, and interest in and to your User Content, and you are free to distribute it as you wish, whether for commercial or non-commercial purposes.

#### Other Consented Downstream Use

<!-- info: What other downstream uses of the data did the original data creators and the data curators consent to? -->
<!-- scope: microscope -->
The data is made freely available under the Creative Commons license, therefore there are no restrictions about downstream uses as long is it's for non-commercial purposes.


### Private Identifying Information (PII)

#### Contains PII?

<!-- quick -->
<!-- info: Does the source language data likely contain Personal Identifying Information about the data creators or subjects? -->
<!-- scope: telescope -->
no PII

#### Justification for no PII

<!-- info: Provide a justification for selecting `no PII` above. -->
<!-- scope: periscope -->
Only the article text and summaries were collected. No user information was retained in the dataset.


### Maintenance

#### Any Maintenance Plan?

<!-- info: Does the original dataset have a maintenance plan? -->
<!-- scope: telescope -->
no



## Broader Social Context

### Previous Work on the Social Impact of the Dataset

#### Usage of Models based on the Data

<!-- info: Are you aware of cases where models trained on the task featured in this dataset ore related tasks have been used in automated systems? -->
<!-- scope: telescope -->
yes - other datasets featuring the same task


### Impact on Under-Served Communities

#### Addresses needs of underserved Communities?

<!-- info: Does this dataset address the needs of communities that are traditionally underserved in language technology, and particularly language generation technology? Communities may be underserved for exemple because their language, language variety, or social or geographical context is underepresented in NLP and NLG resources (datasets and models). -->
<!-- scope: telescope -->
no


### Discussion of Biases

#### Any Documented Social Biases?

<!-- info: Are there documented social biases in the dataset? Biases in this context are variations in the ways members of different social categories are represented that can have harmful downstream consequences for members of the more disadvantaged group. -->
<!-- scope: telescope -->
yes



## Considerations for Using the Data

### PII Risks and Liability



### Licenses

#### Copyright Restrictions on the Dataset

<!-- info: Based on your answers in the Intended Use part of the Data Overview Section, which of the following best describe the copyright and licensing status of the dataset? -->
<!-- scope: periscope -->
`non-commercial use only`

#### Copyright Restrictions on the Language Data

<!-- info: Based on your answers in the Language part of the Data Curation Section, which of the following best describe the copyright and licensing status of the underlying language data? -->
<!-- scope: periscope -->
`non-commercial use only`


### Known Technical Limitations



",High,5.0
Summarization,GEM/xlsum,5.0,2747.0,2024-10-03 19:09:00+00:00,cc-by-nc-sa-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/1607.01759,"['https://aclanthology.org/2021.findings-acl.413/', 'https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413"",']","---
annotations_creators:
- none
language_creators:
- unknown
language:
- am
- ar
- az
- bn
- my
- zh
- en
- fr
- gu
- ha
- hi
- ig
- id
- ja
- rn
- ko
- ky
- mr
- ne
- om
- ps
- fa
- gpe
- pt
- pa
- ru
- gd
- sr
- rsb
- si
- so
- es
- sw
- ta
- te
- th
- ti
- tr
- uk
- ur
- uz
- vi
- cy
- yo
license:
- cc-by-nc-sa-4.0
multilinguality:
- unknown
size_categories:
- unknown
source_datasets:
- original
task_categories:
- summarization
task_ids: []
pretty_name: xlsum
---

# Dataset Card for GEM/xlsum

## Dataset Description

- **Homepage:** https://github.com/csebuetnlp/xl-sum
- **Repository:** https://huggingface.co/datasets/csebuetnlp/xlsum/tree/main/data
- **Paper:** https://aclanthology.org/2021.findings-acl.413/
- **Leaderboard:** http://explainaboard.nlpedia.ai/leaderboard/task_xlsum/
- **Point of Contact:** Tahmid Hasan

### Link to Main Data Card

You can find the main data card on the [GEM Website](https://gem-benchmark.com/data_cards/xlsum).

### Dataset Summary 

XLSum is a highly multilingual summarization dataset supporting 44 language. The data stems from BBC news articles. 

You can load the dataset via:
```
import datasets
data = datasets.load_dataset('GEM/xlsum')
```
The data loader can be found [here](https://huggingface.co/datasets/GEM/xlsum).

#### website
[Github](https://github.com/csebuetnlp/xl-sum)

#### paper
[ACL Anthology](https://aclanthology.org/2021.findings-acl.413/)

## Dataset Overview

### Where to find the Data and its Documentation

#### Webpage

<!-- info: What is the webpage for the dataset (if it exists)? -->
<!-- scope: telescope -->
[Github](https://github.com/csebuetnlp/xl-sum)

#### Download

<!-- info: What is the link to where the original dataset is hosted? -->
<!-- scope: telescope -->
[Huggingface](https://huggingface.co/datasets/csebuetnlp/xlsum/tree/main/data)

#### Paper

<!-- info: What is the link to the paper describing the dataset (open access preferred)? -->
<!-- scope: telescope -->
[ACL Anthology](https://aclanthology.org/2021.findings-acl.413/)

#### BibTex

<!-- info: Provide the BibTex-formatted reference for the dataset. Please use the correct published version (ACL anthology, etc.) instead of google scholar created Bibtex. -->
<!-- scope: microscope -->
```
@inproceedings{hasan-etal-2021-xl,
    title = ""{XL}-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages"",
    author = ""Hasan, Tahmid  and
      Bhattacharjee, Abhik  and
      Islam, Md. Saiful  and
      Mubasshir, Kazi  and
      Li, Yuan-Fang  and
      Kang, Yong-Bin  and
      Rahman, M. Sohel  and
      Shahriyar, Rifat"",
    booktitle = ""Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.findings-acl.413"",
    pages = ""4693--4703"",
}
```

#### Contact Name

<!-- quick -->
<!-- info: If known, provide the name of at least one person the reader can contact for questions about the dataset. -->
<!-- scope: periscope -->
Tahmid Hasan

#### Contact Email

<!-- info: If known, provide the email of at least one person the reader can contact for questions about the dataset. -->
<!-- scope: periscope -->
tahmidhasan@cse.buet.ac.bd

#### Has a Leaderboard?

<!-- info: Does the dataset have an active leaderboard? -->
<!-- scope: telescope -->
yes

#### Leaderboard Link

<!-- info: Provide a link to the leaderboard. -->
<!-- scope: periscope -->
[Explainaboard](http://explainaboard.nlpedia.ai/leaderboard/task_xlsum/)

#### Leaderboard Details

<!-- info: Briefly describe how the leaderboard evaluates models. -->
<!-- scope: microscope -->
The leaderboard ranks models based on ROUGE scores (R1/R2/RL) of the generated summaries.



### Languages and Intended Use

#### Multilingual?

<!-- quick -->
<!-- info: Is the dataset multilingual? -->
<!-- scope: telescope -->
yes

#### Covered Languages

<!-- quick -->
<!-- info: What languages/dialects are covered in the dataset? -->
<!-- scope: telescope -->
`Amharic`, `Arabic`, `Azerbaijani`, `Bengali, Bangla`, `Burmese`, `Chinese (family)`, `English`, `French`, `Gujarati`, `Hausa`, `Hindi`, `Igbo`, `Indonesian`, `Japanese`, `Rundi`, `Korean`, `Kirghiz, Kyrgyz`, `Marathi`, `Nepali (individual language)`, `Oromo`, `Pushto, Pashto`, `Persian`, `Ghanaian Pidgin English`, `Portuguese`, `Panjabi, Punjabi`, `Russian`, `Scottish Gaelic, Gaelic`, `Serbian`, `Romano-Serbian`, `Sinhala, Sinhalese`, `Somali`, `Spanish, Castilian`, `Swahili (individual language), Kiswahili`, `Tamil`, `Telugu`, `Thai`, `Tigrinya`, `Turkish`, `Ukrainian`, `Urdu`, `Uzbek`, `Vietnamese`, `Welsh`, `Yoruba`

#### License

<!-- quick -->
<!-- info: What is the license of the dataset? -->
<!-- scope: telescope -->
cc-by-nc-sa-4.0: Creative Commons Attribution Non Commercial Share Alike 4.0 International

#### Intended Use

<!-- info: What is the intended use of the dataset? -->
<!-- scope: microscope -->
Abstractive summarization has centered around the English language, as most large abstractive summarization datasets are available in English only. Though there have been some recent efforts for curating multilingual abstractive summarization datasets, they are limited in terms of the number of languages covered, the number of training samples, or both. To this end, **XL-Sum** presents a large-scale abstractive summarization dataset of 1.35 million news articles from 45 languages crawled from the British Broadcasting Corporation website. It is intended to be used for both multilingual and per-language summarization tasks.

#### Primary Task

<!-- info: What primary task does the dataset support? -->
<!-- scope: telescope -->
Summarization

#### Communicative Goal

<!-- quick -->
<!-- info: Provide a short description of the communicative goal of a model trained for this task on this dataset. -->
<!-- scope: periscope -->
Summarize news-like text in one of 45 languages.


### Credit

#### Curation Organization Type(s)

<!-- info: In what kind of organization did the dataset curation happen? -->
<!-- scope: telescope -->
`academic`

#### Curation Organization(s)

<!-- info: Name the organization(s). -->
<!-- scope: periscope -->
Bangladesh University of Engineering and Technology

#### Who added the Dataset to GEM?

<!-- info: Who contributed to the data card and adding the dataset to GEM? List the people+affiliations involved in creating this data card and who helped integrate this dataset into GEM. -->
<!-- scope: microscope -->
Tahmid Hasan (Bangladesh University of Engineering and Technology), Abhik Bhattacharjee (Bangladesh University of Engineering and Technology)


### Dataset Structure

#### Data Fields

<!-- info: List and describe the fields present in the dataset. -->
<!-- scope: telescope -->
-  `gem_id`: A string representing the article ID.
-  `url`: A string representing the article URL.
-  `title`: A string containing the article title.
-  `summary`: A string containing the article summary.
-  `text` : A string containing the article text. 


#### Example Instance

<!-- info: Provide a JSON formatted example of a typical instance in the dataset. -->
<!-- scope: periscope -->
```
{
    ""gem_id"": ""GEM-xlsum_english-train-1589"",
    ""url"": ""[BBC news](https://www.bbc.com/news)/technology-17657859"",
    ""title"": ""Yahoo files e-book advert system patent applications"",
    ""summary"": ""Yahoo has signalled it is investigating e-book adverts as a way to stimulate its earnings."",
    ""text"": ""Yahoo's patents suggest users could weigh the type of ads against the sizes of discount before purchase. It says in two US patent applications that ads for digital book readers have been \""less than optimal\"" to date. The filings suggest that users could be offered titles at a variety of prices depending on the ads' prominence They add that the products shown could be determined by the type of book being read, or even the contents of a specific chapter, phrase or word. The paperwork was published by the US Patent and Trademark Office late last week and relates to work carried out at the firm's headquarters in Sunnyvale, California. \""Greater levels of advertising, which may be more valuable to an advertiser and potentially more distracting to an e-book reader, may warrant higher discounts,\"" it states. Free books It suggests users could be offered ads as hyperlinks based within the book's text, in-laid text or even \""dynamic content\"" such as video. Another idea suggests boxes at the bottom of a page could trail later chapters or quotes saying \""brought to you by Company A\"". It adds that the more willing the customer is to see the ads, the greater the potential discount. \""Higher frequencies... may even be great enough to allow the e-book to be obtained for free,\"" it states. The authors write that the type of ad could influence the value of the discount, with \""lower class advertising... such as teeth whitener advertisements\"" offering a cheaper price than \""high\"" or \""middle class\"" adverts, for things like pizza. The inventors also suggest that ads could be linked to the mood or emotional state the reader is in as a they progress through a title. For example, they say if characters fall in love or show affection during a chapter, then ads for flowers or entertainment could be triggered. The patents also suggest this could applied to children's books - giving the Tom Hanks animated film Polar Express as an example. It says a scene showing a waiter giving the protagonists hot drinks \""may be an excellent opportunity to show an advertisement for hot cocoa, or a branded chocolate bar\"". Another example states: \""If the setting includes young characters, a Coke advertisement could be provided, inviting the reader to enjoy a glass of Coke with his book, and providing a graphic of a cool glass.\"" It adds that such targeting could be further enhanced by taking account of previous titles the owner has bought. 'Advertising-free zone' At present, several Amazon and Kobo e-book readers offer full-screen adverts when the device is switched off and show smaller ads on their menu screens, but the main text of the titles remains free of marketing. Yahoo does not currently provide ads to these devices, and a move into the area could boost its shrinking revenues. However, Philip Jones, deputy editor of the Bookseller magazine, said that the internet firm might struggle to get some of its ideas adopted. \""This has been mooted before and was fairly well decried,\"" he said. \""Perhaps in a limited context it could work if the merchandise was strongly related to the title and was kept away from the text. \""But readers - particularly parents - like the fact that reading is an advertising-free zone. Authors would also want something to say about ads interrupting their narrative flow.\""""
}
```

#### Data Splits

<!-- info: Describe and name the splits in the dataset if there are more than one. -->
<!-- scope: periscope -->
The splits in the dataset are specified by the language names, which are as follows:
-  `amharic`
-  `arabic`
-  `azerbaijani`
-  `bengali`
-  `burmese`
-  `chinese_simplified`
-  `chinese_traditional`
-  `english`
-  `french`
-  `gujarati`
-  `hausa`
-  `hindi`
-  `igbo`
-  `indonesian`
-  `japanese`
-  `kirundi`
-  `korean`
-  `kyrgyz`
-  `marathi`
-  `nepali`
-  `oromo`
-  `pashto`
-  `persian`
-  `pidgin`
-  `portuguese`
-  `punjabi`
-  `russian`
-  `scottish_gaelic`
-  `serbian_cyrillic`
-  `serbian_latin`
-  `sinhala`
-  `somali`
-  `spanish`
-  `swahili`
-  `tamil`
-  `telugu`
-  `thai`
-  `tigrinya`
-  `turkish`
-  `ukrainian`
-  `urdu`
-  `uzbek`
-  `vietnamese`
-  `welsh`
-  `yoruba`

#### Splitting Criteria

<!-- info: Describe any criteria for splitting the data, if used. If there are differences between the splits (e.g., if the training annotations are machine-generated and the dev and test ones are created by humans, or if different numbers of annotators contributed to each example), describe them here. -->
<!-- scope: microscope -->
We used a 80%-10%-10% split for all languages with a few exceptions. `English` was split 93%-3.5%-3.5% for the evaluation set size to resemble that of `CNN/DM` and `XSum`; `Scottish Gaelic`, `Kyrgyz` and `Sinhala` had relatively fewer samples, their evaluation sets were increased to 500 samples for more reliable evaluation. Same articles were used for evaluation in the two variants of Chinese and Serbian to prevent data leakage in multilingual training. Individual dataset download links with train-dev-test example counts are given below:

Language      | ISO 639-1 Code | BBC subdomain(s) | Train | Dev | Test | Total |
--------------|----------------|------------------|-------|-----|------|-------|
Amharic | am | [BBC amharic](https://www.bbc.com/amharic) | 5761 | 719 | 719 | 7199 |
Arabic | ar | [BBC arabic](https://www.bbc.com/arabic) | 37519 | 4689 | 4689 | 46897 |
Azerbaijani | az | [BBC azeri](https://www.bbc.com/azeri) | 6478 | 809 | 809 | 8096 |
Bengali | bn | [BBC bengali](https://www.bbc.com/bengali) | 8102 | 1012 | 1012 | 10126 |
Burmese | my | [BBC burmese](https://www.bbc.com/burmese) | 4569 | 570 | 570 | 5709 |
Chinese (Simplified) | zh-CN | [BBC ukchina](https://www.bbc.com/ukchina)/simp, [BBC zhongwen](https://www.bbc.com/zhongwen)/simp | 37362 | 4670 | 4670 | 46702 |
Chinese (Traditional) | zh-TW | [BBC ukchina](https://www.bbc.com/ukchina)/trad, [BBC zhongwen](https://www.bbc.com/zhongwen)/trad | 37373 | 4670 | 4670 | 46713 |
English | en | [BBC english](https://www.bbc.com/english), [BBC sinhala](https://www.bbc.com/sinhala) `*` | 306522 | 11535 | 11535 | 329592 |
French | fr | [BBC afrique](https://www.bbc.com/afrique) | 8697 | 1086 | 1086 | 10869 |
Gujarati | gu | [BBC gujarati](https://www.bbc.com/gujarati) | 9119 | 1139 | 1139 | 11397 |
Hausa | ha | [BBC hausa](https://www.bbc.com/hausa) | 6418 | 802 | 802 | 8022 |
Hindi | hi | [BBC hindi](https://www.bbc.com/hindi) | 70778 | 8847 | 8847 | 88472 |
Igbo | ig | [BBC igbo](https://www.bbc.com/igbo) | 4183 | 522 | 522 | 5227 |
Indonesian | id | [BBC indonesia](https://www.bbc.com/indonesia) | 38242 | 4780 | 4780 | 47802 |
Japanese | ja | [BBC japanese](https://www.bbc.com/japanese) | 7113 | 889 | 889 | 8891 |
Kirundi | rn | [BBC gahuza](https://www.bbc.com/gahuza) | 5746 | 718 | 718 | 7182 |
Korean | ko | [BBC korean](https://www.bbc.com/korean) | 4407 | 550 | 550 | 5507 |
Kyrgyz | ky | [BBC kyrgyz](https://www.bbc.com/kyrgyz) | 2266 | 500 | 500 | 3266 |
Marathi | mr | [BBC marathi](https://www.bbc.com/marathi) | 10903 | 1362 | 1362 | 13627 |
Nepali | np | [BBC nepali](https://www.bbc.com/nepali) | 5808 | 725 | 725 | 7258 |
Oromo | om | [BBC afaanoromoo](https://www.bbc.com/afaanoromoo) | 6063 | 757 | 757 | 7577 |
Pashto | ps | [BBC pashto](https://www.bbc.com/pashto) | 14353 | 1794 | 1794 | 17941 |
Persian | fa | [BBC persian](https://www.bbc.com/persian) | 47251 | 5906 | 5906 | 59063 |
Pidgin`**` | pcm | [BBC pidgin](https://www.bbc.com/pidgin) | 9208 | 1151 | 1151 | 11510 |
Portuguese | pt | [BBC portuguese](https://www.bbc.com/portuguese) | 57402 | 7175 | 7175 | 71752 |
Punjabi | pa | [BBC punjabi](https://www.bbc.com/punjabi) | 8215 | 1026 | 1026 | 10267 |
Russian | ru | [BBC russian](https://www.bbc.com/russian), [BBC ukrainian](https://www.bbc.com/ukrainian) `*` | 62243 | 7780 | 7780 | 77803 |
Scottish Gaelic | gd | [BBC naidheachdan](https://www.bbc.com/naidheachdan) | 1313 | 500 | 500 | 2313 |
Serbian (Cyrillic) | sr | [BBC serbian](https://www.bbc.com/serbian)/cyr | 7275 | 909 | 909 | 9093 |
Serbian (Latin) | sr | [BBC serbian](https://www.bbc.com/serbian)/lat | 7276 | 909 | 909 | 9094 |
Sinhala | si | [BBC sinhala](https://www.bbc.com/sinhala) | 3249 | 500 | 500 | 4249 |
Somali | so | [BBC somali](https://www.bbc.com/somali) | 5962 | 745 | 745 | 7452 |
Spanish | es | [BBC mundo](https://www.bbc.com/mundo) | 38110 | 4763 | 4763 | 47636 |
Swahili | sw | [BBC swahili](https://www.bbc.com/swahili) | 7898 | 987 | 987 | 9872 |
Tamil | ta | [BBC tamil](https://www.bbc.com/tamil) | 16222 | 2027 | 2027 | 20276 |
Telugu | te | [BBC telugu](https://www.bbc.com/telugu) | 10421 | 1302 | 1302 | 13025 |
Thai | th | [BBC thai](https://www.bbc.com/thai) | 6616 | 826 | 826 | 8268 |
Tigrinya | ti | [BBC tigrinya](https://www.bbc.com/tigrinya) | 5451 | 681 | 681 | 6813 |
Turkish | tr | [BBC turkce](https://www.bbc.com/turkce) | 27176 | 3397 | 3397 | 33970 |
Ukrainian | uk | [BBC ukrainian](https://www.bbc.com/ukrainian) | 43201 | 5399 | 5399 | 53999 |
Urdu | ur | [BBC urdu](https://www.bbc.com/urdu) | 67665 | 8458 | 8458 | 84581 |
Uzbek | uz | [BBC uzbek](https://www.bbc.com/uzbek) | 4728 | 590 | 590 | 5908 |
Vietnamese | vi | [BBC vietnamese](https://www.bbc.com/vietnamese) | 32111 | 4013 | 4013 | 40137 |
Welsh | cy | [BBC cymrufyw](https://www.bbc.com/cymrufyw) | 9732 | 1216 | 1216 | 12164 |
Yoruba | yo | [BBC yoruba](https://www.bbc.com/yoruba) | 6350 | 793 | 793 | 7936 |

`*` A lot of articles in BBC Sinhala and BBC Ukrainian were written in English and Russian respectively. They were identified using [Fasttext](https://arxiv.org/abs/1607.01759) and moved accordingly.
`**` West African Pidgin English



## Dataset in GEM

### Rationale for Inclusion in GEM

#### Why is the Dataset in GEM?

<!-- info: What does this dataset contribute toward better generation evaluation and why is it part of GEM? -->
<!-- scope: microscope -->
Traditional abstractive text summarization has been centered around English and other high-resource languages. **XL-Sum** provides a large collection of high-quality article-summary pairs for 45  languages where the languages range from high-resource to extremely low-resource. This enables the research community to explore the summarization capabilities of different models for multiple languages and languages in isolation. We believe the addition of **XL-Sum** to GEM makes the domain of abstractive text summarization more diversified and inclusive to the research community.  We hope our efforts in this work will encourage the community to push the boundaries of abstractive text summarization beyond the English language, especially for low and mid-resource languages, bringing technological advances to communities of these languages that have been traditionally under-served.


#### Similar Datasets

<!-- info: Do other datasets for the high level task exist? -->
<!-- scope: telescope -->
yes

#### Unique Language Coverage

<!-- info: Does this dataset cover other languages than other datasets for the same task? -->
<!-- scope: periscope -->
yes

#### Difference from other GEM datasets

<!-- info: What else sets this dataset apart from other similar datasets in GEM? -->
<!-- scope: microscope -->
The summaries are highly concise and abstractive.

#### Ability that the Dataset measures

<!-- info: What aspect of model ability can be measured with this dataset? -->
<!-- scope: periscope -->
Conciseness, abstractiveness, and overall summarization capability.


### GEM-Specific Curation

#### Modificatied for GEM?

<!-- info: Has the GEM version of the dataset been modified in any way (data, processing, splits) from the original curated data? -->
<!-- scope: telescope -->
no

#### Additional Splits?

<!-- info: Does GEM provide additional splits to the dataset? -->
<!-- scope: telescope -->
no


### Getting Started with the Task




## Previous Results

### Previous Results

#### Measured Model Abilities

<!-- info: What aspect of model ability can be measured with this dataset? -->
<!-- scope: telescope -->
Conciseness, abstractiveness, and overall summarization capability.

#### Metrics

<!-- info: What metrics are typically used for this task? -->
<!-- scope: periscope -->
`ROUGE`

#### Proposed Evaluation

<!-- info: List and describe the purpose of the metrics and evaluation methodology (including human evaluation) that the dataset creators used when introducing this task. -->
<!-- scope: microscope -->
ROUGE is the de facto evaluation metric used for text summarization. However, it was designed specifically for evaluating English texts. Due to the nature of the metric, scores are heavily dependent on text tokenization / stemming / unnecessary character removal, etc. Some modifications to the original ROUGE evaluation were done such as punctuation only removal, language specific tokenization/stemming to enable reliable comparison of source and target summaries across different scripts.

#### Previous results available?

<!-- info: Are previous results available? -->
<!-- scope: telescope -->
no



## Dataset Curation

### Original Curation

#### Original Curation Rationale

<!-- info: Original curation rationale -->
<!-- scope: telescope -->
State-of-the-art text summarization models are heavily data-driven, i.e., a large number of article-summary pairs are required to train them effectively. As a result, abstractive summarization has centered around the English language, as most large abstractive summarization datasets are available in English only. Though there have been some recent efforts for curating multilingual abstractive summarization datasets, they are limited in terms of the number of languages covered, the number of training samples, or both. To this end, we curate **XL-Sum**, a large-scale abstractive summarization dataset of 1.35 million news articles from 45 languages crawled from the British Broadcasting Corporation website.


#### Communicative Goal

<!-- info: What was the communicative goal? -->
<!-- scope: periscope -->
Introduce new languages in the english-centric domain of abstractive text summarization and enable both multilingual and per-language summarization.

#### Sourced from Different Sources

<!-- info: Is the dataset aggregated from different data sources? -->
<!-- scope: telescope -->
yes

#### Source Details

<!-- info: List the sources (one per line) -->
<!-- scope: periscope -->
British Broadcasting Corporation (BBC) news websites.


### Language Data

#### How was Language Data Obtained?

<!-- info: How was the language data obtained? -->
<!-- scope: telescope -->
`Found`

#### Where was it found?

<!-- info: If found, where from? -->
<!-- scope: telescope -->
`Multiple websites`

#### Language Producers

<!-- info: What further information do we have on the language producers? -->
<!-- scope: microscope -->
The language content was written by professional news editors hired by BBC.

#### Topics Covered

<!-- info: Does the language in the dataset focus on specific topics? How would you describe them? -->
<!-- scope: periscope -->
News

#### Data Validation

<!-- info: Was the text validated by a different worker or a data curator? -->
<!-- scope: telescope -->
not validated

#### Data Preprocessing

<!-- info: How was the text data pre-processed? (Enter N/A if the text was not pre-processed) -->
<!-- scope: microscope -->
We used 'NFKC' normalization on all text instances.

#### Was Data Filtered?

<!-- info: Were text instances selected or filtered? -->
<!-- scope: telescope -->
algorithmically

#### Filter Criteria

<!-- info: What were the selection criteria? -->
<!-- scope: microscope -->
We designed a crawler to recursively crawl pages starting from the homepage by visiting different article links present in each page visited. We were able to take advantage of the fact that all BBC sites have somewhat similar structures, and were able to scrape articles from all sites. We discarded pages with no textual contents (mostly pages consisting of multimedia contents) before further processing. We designed a number of heuristics to make the extraction effective by carefully examining the HTML structures of the crawled pages:
1. The desired summary must be present within the beginning two paragraphs of an article.
2. The summary paragraph must have some portion of texts in bold format.
3. The summary paragraph may contain some hyperlinks that may not be bold. The proportion of bold texts and hyperlinked texts to the total length of the paragraph in consideration must be at least 95\%.
4. All texts except the summary and the headline must be included in the input text (including image captions).
5. The input text must be at least twice as large as the summary.



### Structured Annotations

#### Additional Annotations?

<!-- quick -->
<!-- info: Does the dataset have additional annotations for each instance? -->
<!-- scope: telescope -->
none

#### Annotation Service?

<!-- info: Was an annotation service used? -->
<!-- scope: telescope -->
no


### Consent

#### Any Consent Policy?

<!-- info: Was there a consent policy involved when gathering the data? -->
<!-- scope: telescope -->
yes

#### Consent Policy Details

<!-- info: What was the consent policy? -->
<!-- scope: microscope -->
BBC's policy specifies that the text content within its websites can be used for non-commercial research only.


### Private Identifying Information (PII)

#### Contains PII?

<!-- quick -->
<!-- info: Does the source language data likely contain Personal Identifying Information about the data creators or subjects? -->
<!-- scope: telescope -->
likely

#### Categories of PII

<!-- info: What categories of PII are present or suspected in the data? -->
<!-- scope: periscope -->
`generic PII`

#### Any PII Identification?

<!-- info: Did the curators use any automatic/manual method to identify PII in the dataset? -->
<!-- scope: periscope -->
no identification


### Maintenance

#### Any Maintenance Plan?

<!-- info: Does the original dataset have a maintenance plan? -->
<!-- scope: telescope -->
no



## Broader Social Context

### Previous Work on the Social Impact of the Dataset

#### Usage of Models based on the Data

<!-- info: Are you aware of cases where models trained on the task featured in this dataset ore related tasks have been used in automated systems? -->
<!-- scope: telescope -->
no


### Impact on Under-Served Communities

#### Addresses needs of underserved Communities?

<!-- info: Does this dataset address the needs of communities that are traditionally underserved in language technology, and particularly language generation technology? Communities may be underserved for exemple because their language, language variety, or social or geographical context is underepresented in NLP and NLG resources (datasets and models). -->
<!-- scope: telescope -->
yes

#### Details on how Dataset Addresses the Needs

<!-- info: Describe how this dataset addresses the needs of underserved communities. -->
<!-- scope: microscope -->
This dataset introduces  summarization corpus for many languages where there weren't any datasets like this curated before.


### Discussion of Biases

#### Any Documented Social Biases?

<!-- info: Are there documented social biases in the dataset? Biases in this context are variations in the ways members of different social categories are represented that can have harmful downstream consequences for members of the more disadvantaged group. -->
<!-- scope: telescope -->
no

#### Are the Language Producers Representative of the Language?

<!-- info: Does the distribution of language producers in the dataset accurately represent the full distribution of speakers of the language world-wide? If not, how does it differ? -->
<!-- scope: periscope -->
Yes



## Considerations for Using the Data

### PII Risks and Liability



### Licenses

#### Copyright Restrictions on the Dataset

<!-- info: Based on your answers in the Intended Use part of the Data Overview Section, which of the following best describe the copyright and licensing status of the dataset? -->
<!-- scope: periscope -->
`research use only`, `non-commercial use only`

#### Copyright Restrictions on the Language Data

<!-- info: Based on your answers in the Language part of the Data Curation Section, which of the following best describe the copyright and licensing status of the underlying language data? -->
<!-- scope: periscope -->
`research use only`, `non-commercial use only`


### Known Technical Limitations

#### Technical Limitations

<!-- info: Describe any known technical limitations, such as spurrious correlations, train/test overlap, annotation biases, or mis-annotations, and cite the works that first identified these limitations when possible. -->
<!-- scope: microscope -->
Human evaluation showed most languages had a high percentage of good summaries in the upper nineties, almost none of the summaries contained any conflicting information, while about one-third on average had information that was not directly inferrable from the source article. Since generally multiple articles are written regarding an important event, there could be an overlap between the training and evaluation data in terms on content. 


#### Unsuited Applications

<!-- info: When using a model trained on this dataset in a setting where users or the public may interact with its predictions, what are some pitfalls to look out for? In particular, describe some applications of the general task featured in this dataset that its curation or properties make it less suitable for. -->
<!-- scope: microscope -->
The dataset is limited to news domain only. Hence it wouldn't be advisable to use a model trained on this dataset for summarizing texts from a different domain i.e. literature, scientific text etc. Another pitfall could be hallucinations in the model generated summary. 

#### Discouraged Use Cases

<!-- info: What are some discouraged use cases of a model trained to maximize the proposed metrics on this dataset? In particular, think about settings where decisions made by a model that performs reasonably well on the metric my still have strong negative consequences for user or members of the public. -->
<!-- scope: microscope -->
ROUGE evaluates the quality of the summary as a whole by considering up to 4-gram overlaps. Therefore, in an article about India if the word ""India"" in the generated summary gets replaced by ""Pakistan"" due to model hallucination, the overall score wouldn't be reduced significantly, but the entire meaning could get changed.",High,5.0
Summarization,csebuetnlp/xlsum,138.0,15205.0,2023-04-18 01:46:20+00:00,cc-by-nc-sa-4.0,83.0,1.38 GB,1481763717.12,3.45 GB,3704409292.8,1351253,https://arxiv.org/abs/1607.01759,"['https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413"",']","---
annotations_creators:
- found
language_creators:
- found
language:
- am
- ar
- az
- bn
- my
- zh
- en
- fr
- gu
- ha
- hi
- ig
- id
- ja
- rn
- ko
- ky
- mr
- ne
- om
- ps
- fa
- pcm
- pt
- pa
- ru
- gd
- sr
- si
- so
- es
- sw
- ta
- te
- th
- ti
- tr
- uk
- ur
- uz
- vi
- cy
- yo
license:
- cc-by-nc-sa-4.0
multilinguality:
- multilingual
size_categories:
- 1M<n<10M
source_datasets:
- original
task_categories:
- summarization
- text-generation
task_ids: []
paperswithcode_id: xl-sum
pretty_name: XL-Sum
tags:
- conditional-text-generation
---

# Dataset Card for ""XL-Sum""

## Table of Contents
- [Dataset Card Creation Guide](#dataset-card-creation-guide)
  - [Table of Contents](#table-of-contents)
  - [Dataset Description](#dataset-description)
    - [Dataset Summary](#dataset-summary)
    - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
    - [Languages](#languages)
  - [Dataset Structure](#dataset-structure)
    - [Data Instances](#data-instances)
    - [Data Fields](#data-fields)
    - [Data Splits](#data-splits)
  - [Dataset Creation](#dataset-creation)
    - [Curation Rationale](#curation-rationale)
    - [Source Data](#source-data)
      - [Initial Data Collection and Normalization](#initial-data-collection-and-normalization)
      - [Who are the source language producers?](#who-are-the-source-language-producers)
    - [Annotations](#annotations)
      - [Annotation process](#annotation-process)
      - [Who are the annotators?](#who-are-the-annotators)
    - [Personal and Sensitive Information](#personal-and-sensitive-information)
  - [Considerations for Using the Data](#considerations-for-using-the-data)
    - [Social Impact of Dataset](#social-impact-of-dataset)
    - [Discussion of Biases](#discussion-of-biases)
    - [Other Known Limitations](#other-known-limitations)
  - [Additional Information](#additional-information)
    - [Dataset Curators](#dataset-curators)
    - [Licensing Information](#licensing-information)
    - [Citation Information](#citation-information)
    - [Contributions](#contributions)

## Dataset Description

- **Repository:** [https://github.com/csebuetnlp/xl-sum](https://github.com/csebuetnlp/xl-sum)
- **Paper:** [XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages](https://aclanthology.org/2021.findings-acl.413/)
- **Point of Contact:** [Tahmid Hasan](mailto:tahmidhasan@cse.buet.ac.bd)

### Dataset Summary

We present XLSum, a comprehensive and diverse dataset comprising 1.35 million professionally  annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 45 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. 


### Supported Tasks and Leaderboards

[More information needed](https://github.com/csebuetnlp/xl-sum)

### Languages

-  `amharic`
-  `arabic`
-  `azerbaijani`
-  `bengali`
-  `burmese`
-  `chinese_simplified`
-  `chinese_traditional`
-  `english`
-  `french`
-  `gujarati`
-  `hausa`
-  `hindi`
-  `igbo`
-  `indonesian`
-  `japanese`
-  `kirundi`
-  `korean`
-  `kyrgyz`
-  `marathi`
-  `nepali`
-  `oromo`
-  `pashto`
-  `persian`
-  `pidgin`
-  `portuguese`
-  `punjabi`
-  `russian`
-  `scottish_gaelic`
-  `serbian_cyrillic`
-  `serbian_latin`
-  `sinhala`
-  `somali`
-  `spanish`
-  `swahili`
-  `tamil`
-  `telugu`
-  `thai`
-  `tigrinya`
-  `turkish`
-  `ukrainian`
-  `urdu`
-  `uzbek`
-  `vietnamese`
-  `welsh`
-  `yoruba`

## Dataset Structure

### Data Instances

One example from the `English` dataset is given below in JSON format. 
  ```
  {
    ""id"": ""technology-17657859"",
    ""url"": ""https://www.bbc.com/news/technology-17657859"",
    ""title"": ""Yahoo files e-book advert system patent applications"",
    ""summary"": ""Yahoo has signalled it is investigating e-book adverts as a way to stimulate its earnings."",
    ""text"": ""Yahoo's patents suggest users could weigh the type of ads against the sizes of discount before purchase. It says in two US patent applications that ads for digital book readers have been \""less than optimal\"" to date. The filings suggest that users could be offered titles at a variety of prices depending on the ads' prominence They add that the products shown could be determined by the type of book being read, or even the contents of a specific chapter, phrase or word. The paperwork was published by the US Patent and Trademark Office late last week and relates to work carried out at the firm's headquarters in Sunnyvale, California. \""Greater levels of advertising, which may be more valuable to an advertiser and potentially more distracting to an e-book reader, may warrant higher discounts,\"" it states. Free books It suggests users could be offered ads as hyperlinks based within the book's text, in-laid text or even \""dynamic content\"" such as video. Another idea suggests boxes at the bottom of a page could trail later chapters or quotes saying \""brought to you by Company A\"". It adds that the more willing the customer is to see the ads, the greater the potential discount. \""Higher frequencies... may even be great enough to allow the e-book to be obtained for free,\"" it states. The authors write that the type of ad could influence the value of the discount, with \""lower class advertising... such as teeth whitener advertisements\"" offering a cheaper price than \""high\"" or \""middle class\"" adverts, for things like pizza. The inventors also suggest that ads could be linked to the mood or emotional state the reader is in as a they progress through a title. For example, they say if characters fall in love or show affection during a chapter, then ads for flowers or entertainment could be triggered. The patents also suggest this could applied to children's books - giving the Tom Hanks animated film Polar Express as an example. It says a scene showing a waiter giving the protagonists hot drinks \""may be an excellent opportunity to show an advertisement for hot cocoa, or a branded chocolate bar\"". Another example states: \""If the setting includes young characters, a Coke advertisement could be provided, inviting the reader to enjoy a glass of Coke with his book, and providing a graphic of a cool glass.\"" It adds that such targeting could be further enhanced by taking account of previous titles the owner has bought. 'Advertising-free zone' At present, several Amazon and Kobo e-book readers offer full-screen adverts when the device is switched off and show smaller ads on their menu screens, but the main text of the titles remains free of marketing. Yahoo does not currently provide ads to these devices, and a move into the area could boost its shrinking revenues. However, Philip Jones, deputy editor of the Bookseller magazine, said that the internet firm might struggle to get some of its ideas adopted. \""This has been mooted before and was fairly well decried,\"" he said. \""Perhaps in a limited context it could work if the merchandise was strongly related to the title and was kept away from the text. \""But readers - particularly parents - like the fact that reading is an advertising-free zone. Authors would also want something to say about ads interrupting their narrative flow.\""""
}
  ```

### Data Fields
-  'id': A string representing the article ID.
-  'url': A string representing the article URL.
-  'title': A string containing the article title.
-  'summary': A string containing the article summary.
-  'text' : A string containing the article text. 


### Data Splits

We used a 80%-10%-10% split for all languages with a few exceptions. `English` was split 93%-3.5%-3.5% for the evaluation set size to resemble that of `CNN/DM` and `XSum`; `Scottish Gaelic`, `Kyrgyz` and `Sinhala` had relatively fewer samples, their evaluation sets were increased to 500 samples for more reliable evaluation. Same articles were used for evaluation in the two variants of Chinese and Serbian to prevent data leakage in multilingual training. Individual dataset download links with train-dev-test example counts are given below:

Language      | ISO 639-1 Code | BBC subdomain(s) | Train | Dev | Test | Total |
--------------|----------------|------------------|-------|-----|------|-------|
Amharic | am | https://www.bbc.com/amharic | 5761 | 719 | 719 | 7199 |
Arabic | ar | https://www.bbc.com/arabic | 37519 | 4689 | 4689 | 46897 |
Azerbaijani | az | https://www.bbc.com/azeri | 6478 | 809 | 809 | 8096 |
Bengali | bn | https://www.bbc.com/bengali | 8102 | 1012 | 1012 | 10126 |
Burmese | my | https://www.bbc.com/burmese | 4569 | 570 | 570 | 5709 |
Chinese (Simplified) | zh-CN | https://www.bbc.com/ukchina/simp, https://www.bbc.com/zhongwen/simp | 37362 | 4670 | 4670 | 46702 |
Chinese (Traditional) | zh-TW | https://www.bbc.com/ukchina/trad, https://www.bbc.com/zhongwen/trad | 37373 | 4670 | 4670 | 46713 |
English | en | https://www.bbc.com/english, https://www.bbc.com/sinhala `*` | 306522 | 11535 | 11535 | 329592 |
French | fr | https://www.bbc.com/afrique | 8697 | 1086 | 1086 | 10869 |
Gujarati | gu | https://www.bbc.com/gujarati | 9119 | 1139 | 1139 | 11397 |
Hausa | ha | https://www.bbc.com/hausa | 6418 | 802 | 802 | 8022 |
Hindi | hi | https://www.bbc.com/hindi | 70778 | 8847 | 8847 | 88472 |
Igbo | ig | https://www.bbc.com/igbo | 4183 | 522 | 522 | 5227 |
Indonesian | id | https://www.bbc.com/indonesia | 38242 | 4780 | 4780 | 47802 |
Japanese | ja | https://www.bbc.com/japanese | 7113 | 889 | 889 | 8891 |
Kirundi | rn | https://www.bbc.com/gahuza | 5746 | 718 | 718 | 7182 |
Korean | ko | https://www.bbc.com/korean | 4407 | 550 | 550 | 5507 |
Kyrgyz | ky | https://www.bbc.com/kyrgyz | 2266 | 500 | 500 | 3266 |
Marathi | mr | https://www.bbc.com/marathi | 10903 | 1362 | 1362 | 13627 |
Nepali | np | https://www.bbc.com/nepali | 5808 | 725 | 725 | 7258 |
Oromo | om | https://www.bbc.com/afaanoromoo | 6063 | 757 | 757 | 7577 |
Pashto | ps | https://www.bbc.com/pashto | 14353 | 1794 | 1794 | 17941 |
Persian | fa | https://www.bbc.com/persian | 47251 | 5906 | 5906 | 59063 |
Pidgin`**` | n/a | https://www.bbc.com/pidgin | 9208 | 1151 | 1151 | 11510 |
Portuguese | pt | https://www.bbc.com/portuguese | 57402 | 7175 | 7175 | 71752 |
Punjabi | pa | https://www.bbc.com/punjabi | 8215 | 1026 | 1026 | 10267 |
Russian | ru | https://www.bbc.com/russian, https://www.bbc.com/ukrainian `*` | 62243 | 7780 | 7780 | 77803 |
Scottish Gaelic | gd | https://www.bbc.com/naidheachdan | 1313 | 500 | 500 | 2313 |
Serbian (Cyrillic) | sr | https://www.bbc.com/serbian/cyr | 7275 | 909 | 909 | 9093 |
Serbian (Latin) | sr | https://www.bbc.com/serbian/lat | 7276 | 909 | 909 | 9094 |
Sinhala | si | https://www.bbc.com/sinhala | 3249 | 500 | 500 | 4249 |
Somali | so | https://www.bbc.com/somali | 5962 | 745 | 745 | 7452 |
Spanish | es | https://www.bbc.com/mundo | 38110 | 4763 | 4763 | 47636 |
Swahili | sw | https://www.bbc.com/swahili | 7898 | 987 | 987 | 9872 |
Tamil | ta | https://www.bbc.com/tamil | 16222 | 2027 | 2027 | 20276 |
Telugu | te | https://www.bbc.com/telugu | 10421 | 1302 | 1302 | 13025 |
Thai | th | https://www.bbc.com/thai | 6616 | 826 | 826 | 8268 |
Tigrinya | ti | https://www.bbc.com/tigrinya | 5451 | 681 | 681 | 6813 |
Turkish | tr | https://www.bbc.com/turkce | 27176 | 3397 | 3397 | 33970 |
Ukrainian | uk | https://www.bbc.com/ukrainian | 43201 | 5399 | 5399 | 53999 |
Urdu | ur | https://www.bbc.com/urdu | 67665 | 8458 | 8458 | 84581 |
Uzbek | uz | https://www.bbc.com/uzbek | 4728 | 590 | 590 | 5908 |
Vietnamese | vi | https://www.bbc.com/vietnamese | 32111 | 4013 | 4013 | 40137 |
Welsh | cy | https://www.bbc.com/cymrufyw | 9732 | 1216 | 1216 | 12164 |
Yoruba | yo | https://www.bbc.com/yoruba | 6350 | 793 | 793 | 7936 |

`*` A lot of articles in BBC Sinhala and BBC Ukrainian were written in English and Russian respectively. They were identified using [Fasttext](https://arxiv.org/abs/1607.01759) and moved accordingly.

`**` West African Pidgin English

## Dataset Creation

### Curation Rationale

[More information needed](https://github.com/csebuetnlp/xl-sum)

### Source Data

[BBC News](https://www.bbc.co.uk/ws/languages)

#### Initial Data Collection and Normalization

[Detailed in the paper](https://aclanthology.org/2021.findings-acl.413/) 


#### Who are the source language producers?

[Detailed in the paper](https://aclanthology.org/2021.findings-acl.413/) 


### Annotations

[Detailed in the paper](https://aclanthology.org/2021.findings-acl.413/) 


#### Annotation process

[Detailed in the paper](https://aclanthology.org/2021.findings-acl.413/) 

#### Who are the annotators?

[Detailed in the paper](https://aclanthology.org/2021.findings-acl.413/) 

### Personal and Sensitive Information

[More information needed](https://github.com/csebuetnlp/xl-sum)

## Considerations for Using the Data

### Social Impact of Dataset

[More information needed](https://github.com/csebuetnlp/xl-sum)

### Discussion of Biases

[More information needed](https://github.com/csebuetnlp/xl-sum)

### Other Known Limitations

[More information needed](https://github.com/csebuetnlp/xl-sum)

## Additional Information

### Dataset Curators

[More information needed](https://github.com/csebuetnlp/xl-sum)

### Licensing Information

Contents of this repository are restricted to only non-commercial research purposes under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright of the dataset contents belongs to the original copyright holders.
### Citation Information

If you use any of the datasets, models or code modules, please cite the following paper:
```
@inproceedings{hasan-etal-2021-xl,
    title = ""{XL}-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages"",
    author = ""Hasan, Tahmid  and
      Bhattacharjee, Abhik  and
      Islam, Md. Saiful  and
      Mubasshir, Kazi  and
      Li, Yuan-Fang  and
      Kang, Yong-Bin  and
      Rahman, M. Sohel  and
      Shahriyar, Rifat"",
    booktitle = ""Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.findings-acl.413"",
    pages = ""4693--4703"",
}
```


### Contributions

Thanks to [@abhik1505040](https://github.com/abhik1505040) and [@Tahmid](https://github.com/Tahmid04) for adding this dataset.",High,5.0
Summarization,csebuetnlp/CrossSum,14.0,314.0,2024-06-19 17:09:58+00:00,cc-by-nc-sa-4.0,12.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2112.08804,none,"---
task_categories:
- summarization
task_ids:
- news-articles-summarization
language:
- am
- ar
- az
- bn
- my
- zh
- en
- fr
- gu
- ha
- hi
- ig
- id
- ja
- rn
- ko
- ky
- mr
- ne
- om
- ps
- fa
- pcm
- pt
- pa
- ru
- gd
- sr
- si
- so
- es
- sw
- ta
- te
- th
- ti
- tr
- uk
- ur
- uz
- vi
- cy
- yo
size_categories:
- 1M<n<10M
license:
- cc-by-nc-sa-4.0
multilinguality:
- multilingual
source_datasets:
- original
annotations_creators:
- found
language_creators:
- found
pretty_name: CrossSum
---

# Dataset Card for ""CrossSum""

## Table of Contents
- [Dataset Card Creation Guide](#dataset-card-creation-guide)
  - [Table of Contents](#table-of-contents)
  - [Dataset Description](#dataset-description)
    - [Dataset Summary](#dataset-summary)
    - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
    - [Languages](#languages)
  - [Dataset Structure](#dataset-structure)
    - [Data Instances](#data-instances)
    - [Data Fields](#data-fields)
    - [Data Splits](#data-splits)
  - [Dataset Creation](#dataset-creation)
    - [Curation Rationale](#curation-rationale)
    - [Source Data](#source-data)
      - [Initial Data Collection and Normalization](#initial-data-collection-and-normalization)
      - [Who are the source language producers?](#who-are-the-source-language-producers)
    - [Annotations](#annotations)
      - [Annotation process](#annotation-process)
      - [Who are the annotators?](#who-are-the-annotators)
    - [Personal and Sensitive Information](#personal-and-sensitive-information)
  - [Considerations for Using the Data](#considerations-for-using-the-data)
    - [Social Impact of Dataset](#social-impact-of-dataset)
    - [Discussion of Biases](#discussion-of-biases)
    - [Other Known Limitations](#other-known-limitations)
  - [Additional Information](#additional-information)
    - [Dataset Curators](#dataset-curators)
    - [Licensing Information](#licensing-information)
    - [Citation Information](#citation-information)
    - [Contributions](#contributions)

## Dataset Description

- **Repository:** [https://github.com/csebuetnlp/CrossSum](https://github.com/csebuetnlp/CrossSum)
- **Paper:** [CrossSum: Beyond English-Centric Cross-Lingual Abstractive Text Summarization for 1500+ Language Pairs](https://arxiv.org/abs/2112.08804)
- **Point of Contact:** [Tahmid Hasan](mailto:tahmidhasan@cse.buet.ac.bd)

### Dataset Summary

We present CrossSum, a large-scale dataset
comprising 1.70 million cross-lingual article summary samples in 1500+ language-pairs
constituting 45 languages. We use the multilingual XL-Sum dataset and align identical 
articles written in different languages via crosslingual retrieval using a language-agnostic 
representation model. 

### Supported Tasks and Leaderboards

[More information needed](https://github.com/csebuetnlp/CrossSum)

### Languages

-  `amharic`
-  `arabic`
-  `azerbaijani`
-  `bengali`
-  `burmese`
-  `chinese_simplified`
-  `chinese_traditional`
-  `english`
-  `french`
-  `gujarati`
-  `hausa`
-  `hindi`
-  `igbo`
-  `indonesian`
-  `japanese`
-  `kirundi`
-  `korean`
-  `kyrgyz`
-  `marathi`
-  `nepali`
-  `oromo`
-  `pashto`
-  `persian`
-  `pidgin`
-  `portuguese`
-  `punjabi`
-  `russian`
-  `scottish_gaelic`
-  `serbian_cyrillic`
-  `serbian_latin`
-  `sinhala`
-  `somali`
-  `spanish`
-  `swahili`
-  `tamil`
-  `telugu`
-  `thai`
-  `tigrinya`
-  `turkish`
-  `ukrainian`
-  `urdu`
-  `uzbek`
-  `vietnamese`
-  `welsh`
-  `yoruba`


## Loading the dataset
```python
from datasets import load_dataset

# for available language names, see above
src_lang = ""english""
tgt_lang = ""bengali""

ds = load_dataset(f""csebuetnlp/CrossSum"", ""{}-{}"".format(src_lang, tgt_lang))
```

## Dataset Structure

### Data Instances

One example from the `japanese-bengali` split of the dataset is given below in JSON format. 
```
{
    ""source_url"": ""https://www.bbc.com/japanese/53074000"",
    ""target_url"": ""https://www.bbc.com/bengali/news-53064712"",
    ""summary"": ""বিজ্ঞানীরা বলছেন ডেক্সামেথাসোন নামে সস্তা ও সহজলভ্য একটি ওষুধ করোনাভাইরাসে গুরুতর অসুস্থ রোগীদের জীবন রক্ষা করতে সাহায্য করবে।"",
    ""text"": ""ミシェル･ロバーツ、BBCニュースオンライン健康担当編集長 英オックスフォード大学の研究チームによると、低用量のデキサメタゾンは新型ウイルスとの戦いで画期的な突破口になる。 新型コロナウイルスに対し、様々な既存の治療法の効果を試す世界的規模の臨床試験の一貫として、デキサメタゾンが試された。 その結果、人工呼吸器を必要とする重症患者の致死率が3割下がり、酸素供給を必要とする患者の場合は2割下がった。 新型ウイルスのパンデミック（世界的流行）の初期からイギリスでデキサメタゾンを治療に使用していた場合、最大5000人の命が救えたはずだと研究者たちは言う。 さらに、新型コロナウイルスによる感染症「COVID-19」の患者が多く出ている貧しい国にとっても、安価なデキサメタゾンを使う治療は大いに役立つと期待される。 重症者の致死率が大幅に下がる イギリス政府は20万人分の投与量を備蓄しており、国民医療制度の国民保健サービス（NHS）で患者への使用を開始する方針を示した。 ボリス･ジョンソン英首相は「イギリス科学界の素晴らしい成果」を歓迎し、「たとえ感染の第2波が来ても備蓄が足りるよう、数を確保するための措置をとった」と述べた。 イングランド首席医務官クリス・ウィッティー教授は、「COVID-19にとってこれまでで一番重要な臨床試験結果だ。手に入りやすく安全でなじみのある薬によって、酸素供給や人工呼吸器が必要な人の致死率が大幅に下がった。（中略）この発見が世界中で人命を救う」と評価した。 ＜関連記事＞ 新型コロナウイルスに20人が感染した場合、19人は入院しないまま回復する。入院する人もほとんどは回復するものの、重症化して酸素供給や人工呼吸器を必要とする人もいる。 デキサメタゾンはこうした重症患者の治療に効果があるもよう。 新型ウイルスに感染した患者の体内では、ウイルスと戦う免疫系が暴走することがある。その免疫系の過剰反応による体の損傷を、デキサメタゾンが緩和するものとみられる。 「サイトカイン・ストーム」と呼ばれる免疫系の過剰反応が、患者の命を奪うこともある。 デキサメタゾンはすでに抗炎症剤として、ぜんそくや皮膚炎など様々な症状の治療に使われている。 初めて致死率を下げる薬 オックスフォード大学が主導する臨床試験は、約2000人の入院患者にデキサメタゾンを投与。それ以外の4000人以上の患者と容体を比較した。 人工呼吸器を使用する患者については、死亡リスクが40％から28％に下がった。 酸素供給する患者は、死亡リスクが25％から20％に下がった。 研究チームのピーター・ホービー教授は、「今のところ、致死率を実際に下げる結果が出たのは、この薬だけだ。しかも、致死率をかなり下げる。画期的な突破口だ」と話した。 研究を主導するマーティン・ランドレイ教授によると、人工呼吸器を使う患者の8人に1人、ならびに酸素供給治療を受ける患者の20-25人に1人が、デキサメタゾンで救えることが分かったという。 「これはきわめて明確なメリットだ」と教授は言う。 「最大10日間、デキサメタゾンを投与するという治療法で、費用は患者1人あたり1日約5ポンド（約670円）。つまり、35ポンド（約4700円）で人ひとりの命が救える」 「しかもこれは、世界中で手に入る薬だ」 状況が許す限り、新型コロナウイルスで入院中の患者にはただちに投与を開始すべきだと、ランドレイ教授は促した。 ただし、自宅で自己治療するために薬局に買いに行くべきではないと言う。 デキサメタゾンは、呼吸補助を必要としない軽症の患者には効果がないもよう。 3月に始動した新型コロナウイルス治療薬の無作為化臨床試験「リカバリー・トライアル」は、抗マラリア薬「ヒドロキシクロロキン」も調べたものの、心臓疾患や致死率の悪化につながるという懸念から、ヒドロキシクロロキンについては試験を中止した。 一方で、感染者の回復にかかる時間を短縮するとみられるレムデシビルは、すでにNHSの保険対象になり治療現場で使われている。 ＜解説＞ ファーガス・ウォルシュBBC健康担当編集委員 COVID-19の死者を減らすと初めて立証された薬は、高価な新しい薬ではなく、古くからずっと使われてきた、きわめて安いステロイド剤だった。 世界中の患者が直ちにその恩恵を受けることになるので、これは歓迎すべき発見だ。 この臨床試験の最新成果がこれほど急いで発表されたのは、そのためだ。とてつもない影響を世界中にもたらすので。 デキサメタゾンは1960年代初めから、関節リウマチやぜんそくなど、幅広い症状の治療に使われてきた。 これまでは、人工呼吸器を必要とするCOVID-19患者の半数が亡くなってきた。その致死率を3割減らすというのは、絶大な効果だ。 集中治療室では点滴で投与する。もう少し軽症な患者には、錠剤で与える。 これまでのところ、COVID-19患者に効果があると証明された薬は、エボラ治療薬のレムデシビルだけだった。 レムデシビルは症状の回復期間を15日から11日に短縮する。しかし、致死率を下げると言えるだけの証拠は出ていなかった。 デキサメタゾンと異なり、レムデシビルは数の少ない新薬で、薬価もまだ公表されていない。""
}
  ```

### Data Fields
-  'source_url': A string representing the source article URL.
-  'target_url': A string representing the target article URL.
-  'summary': A string containing the article summary.
-  'text' : A string containing the article text. 


### Data Splits

No. of total examples for each language pair are as follows:

Language (ISO 639-1-Code) | am | ar | az | bn | my | zh-CN | zh-TW | en | fr | gu | ha | hi | ig | id | ja | rn | ko | ky | mr | np | om | ps | fa | pcm | pt | pa | ru | gd | sr | sr | si | so | es | sw | ta | te | th | ti | tr | uk | ur | uz | vi | cy | yo 
----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- 
am | -- | 667 | 100 | 272 | 95 | 179 | 167 | 1456 | 358 | 173 | 221 | 377 | 26 | 494 | 264 | 423 | 244 | 92 | 221 | 301 | 21 | 192 | 431 | 209 | 307 | 189 | 347 | 0 | 357 | 365 | 62 | 309 | 351 | 378 | 390 | 329 | 124 | 131 | 435 | 345 | 409 | 41 | 285 | 1 | 67 
ar | 667 | -- | 787 | 804 | 652 | 2968 | 2843 | 9653 | 989 | 475 | 747 | 3665 | 86 | 6084 | 1188 | 876 | 707 | 299 | 559 | 854 | 9 | 2161 | 4186 | 436 | 2539 | 547 | 5564 | 1 | 1109 | 1145 | 315 | 1049 | 3654 | 1186 | 1311 | 877 | 367 | 27 | 4147 | 3457 | 4935 | 388 | 2666 | 38 | 141 
az | 100 | 787 | -- | 277 | 84 | 371 | 334 | 1317 | 208 | 192 | 126 | 748 | 28 | 1111 | 231 | 188 | 155 | 221 | 194 | 242 | 1 | 252 | 817 | 91 | 678 | 190 | 2238 | 4 | 289 | 283 | 124 | 367 | 704 | 539 | 515 | 245 | 140 | 2 | 1495 | 1383 | 966 | 199 | 725 | 30 | 42 
bn | 272 | 804 | 277 | -- | 139 | 318 | 284 | 1549 | 317 | 559 | 231 | 1396 | 35 | 1076 | 342 | 298 | 352 | 154 | 586 | 668 | 2 | 300 | 790 | 135 | 764 | 580 | 838 | 0 | 562 | 564 | 151 | 412 | 701 | 471 | 919 | 793 | 245 | 6 | 860 | 688 | 1382 | 98 | 527 | 37 | 61 
my | 95 | 652 | 84 | 139 | -- | 356 | 314 | 685 | 90 | 96 | 74 | 528 | 12 | 761 | 144 | 100 | 112 | 58 | 89 | 152 | 1 | 234 | 426 | 39 | 230 | 86 | 535 | 0 | 115 | 123 | 87 | 79 | 431 | 86 | 185 | 147 | 71 | 4 | 449 | 350 | 591 | 62 | 447 | 4 | 12 
zh-CN | 179 | 2968 | 371 | 318 | 356 | -- | 47101 | 4975 | 348 | 201 | 159 | 1379 | 38 | 2851 | 1017 | 240 | 412 | 139 | 240 | 275 | 14 | 559 | 1111 | 149 | 1371 | 250 | 2572 | 2 | 504 | 530 | 166 | 323 | 2002 | 412 | 511 | 353 | 269 | 11 | 1511 | 1619 | 1651 | 176 | 1858 | 33 | 39 
zh-TW | 167 | 2843 | 334 | 284 | 314 | 47101 | -- | 4884 | 331 | 174 | 150 | 1213 | 35 | 2588 | 953 | 209 | 382 | 131 | 213 | 252 | 16 | 501 | 967 | 141 | 1271 | 226 | 2286 | 1 | 453 | 494 | 150 | 302 | 1873 | 383 | 465 | 335 | 250 | 12 | 1294 | 1464 | 1444 | 158 | 1663 | 31 | 38 
en | 1456 | 9653 | 1317 | 1549 | 685 | 4975 | 4884 | -- | 1889 | 978 | 913 | 4728 | 144 | 10040 | 3040 | 1878 | 1673 | 490 | 1181 | 1614 | 38 | 1522 | 4680 | 1074 | 4744 | 1330 | 9080 | 128 | 3760 | 3809 | 532 | 2141 | 6910 | 2701 | 3156 | 2121 | 1020 | 58 | 5676 | 6562 | 6320 | 450 | 4574 | 2655 | 229 
fr | 358 | 989 | 208 | 317 | 90 | 348 | 331 | 1889 | -- | 242 | 477 | 616 | 106 | 1018 | 274 | 735 | 264 | 124 | 241 | 323 | 4 | 196 | 602 | 439 | 921 | 247 | 849 | 2 | 555 | 569 | 98 | 502 | 990 | 872 | 425 | 380 | 185 | 10 | 829 | 721 | 766 | 76 | 438 | 40 | 159 
gu | 173 | 475 | 192 | 559 | 96 | 201 | 174 | 978 | 242 | -- | 147 | 5170 | 34 | 710 | 228 | 183 | 268 | 106 | 2091 | 561 | 1 | 246 | 522 | 101 | 529 | 2210 | 582 | 0 | 331 | 345 | 125 | 261 | 540 | 300 | 1762 | 2066 | 164 | 5 | 631 | 508 | 1619 | 80 | 450 | 21 | 54 
ha | 221 | 747 | 126 | 231 | 74 | 159 | 150 | 913 | 477 | 147 | -- | 460 | 202 | 901 | 157 | 485 | 135 | 61 | 159 | 239 | 5 | 229 | 487 | 529 | 375 | 157 | 525 | 1 | 258 | 258 | 49 | 391 | 463 | 568 | 299 | 260 | 87 | 9 | 519 | 400 | 526 | 59 | 352 | 30 | 362 
hi | 377 | 3665 | 748 | 1396 | 528 | 1379 | 1213 | 4728 | 616 | 5170 | 460 | -- | 65 | 5627 | 623 | 489 | 520 | 234 | 3831 | 1357 | 4 | 1519 | 5351 | 192 | 6563 | 4052 | 4622 | 1 | 809 | 807 | 449 | 747 | 2931 | 893 | 3711 | 3762 | 378 | 7 | 3694 | 3935 | 15666 | 352 | 3738 | 77 | 79 
ig | 26 | 86 | 28 | 35 | 12 | 38 | 35 | 144 | 106 | 34 | 202 | 65 | -- | 113 | 24 | 107 | 32 | 16 | 51 | 36 | 3 | 11 | 49 | 255 | 61 | 39 | 79 | 0 | 51 | 51 | 13 | 77 | 91 | 151 | 52 | 54 | 18 | 5 | 91 | 83 | 61 | 15 | 65 | 6 | 296 
id | 494 | 6084 | 1111 | 1076 | 761 | 2851 | 2588 | 10040 | 1018 | 710 | 901 | 5627 | 113 | -- | 1274 | 994 | 774 | 347 | 745 | 1104 | 8 | 1430 | 3892 | 367 | 4409 | 725 | 7588 | 7 | 1387 | 1379 | 470 | 1312 | 4547 | 1873 | 1886 | 1131 | 599 | 9 | 5663 | 4829 | 6476 | 432 | 4810 | 145 | 174 
ja | 264 | 1188 | 231 | 342 | 144 | 1017 | 953 | 3040 | 274 | 228 | 157 | 623 | 24 | 1274 | -- | 372 | 654 | 140 | 302 | 424 | 2 | 266 | 1014 | 152 | 706 | 269 | 1517 | 2 | 550 | 571 | 109 | 387 | 950 | 425 | 641 | 425 | 305 | 5 | 1242 | 1013 | 797 | 49 | 908 | 25 | 33 
rn | 423 | 876 | 188 | 298 | 100 | 240 | 209 | 1878 | 735 | 183 | 485 | 489 | 107 | 994 | 372 | -- | 283 | 106 | 242 | 369 | 18 | 228 | 684 | 398 | 526 | 206 | 711 | 0 | 443 | 450 | 77 | 584 | 607 | 1186 | 521 | 363 | 149 | 13 | 724 | 610 | 617 | 59 | 631 | 20 | 180 
ko | 244 | 707 | 155 | 352 | 112 | 412 | 382 | 1673 | 264 | 268 | 135 | 520 | 32 | 774 | 654 | 283 | -- | 99 | 319 | 445 | 1 | 150 | 596 | 130 | 587 | 264 | 649 | 0 | 522 | 543 | 81 | 234 | 613 | 324 | 541 | 452 | 197 | 5 | 680 | 616 | 532 | 54 | 530 | 12 | 45 
ky | 92 | 299 | 221 | 154 | 58 | 139 | 131 | 490 | 124 | 106 | 61 | 234 | 16 | 347 | 140 | 106 | 99 | -- | 107 | 167 | 4 | 102 | 252 | 59 | 251 | 118 | 1013 | 1 | 206 | 211 | 45 | 145 | 279 | 150 | 206 | 174 | 109 | 3 | 346 | 508 | 270 | 113 | 201 | 12 | 23 
mr | 221 | 559 | 194 | 586 | 89 | 240 | 213 | 1181 | 241 | 2091 | 159 | 3831 | 51 | 745 | 302 | 242 | 319 | 107 | -- | 630 | 1 | 232 | 608 | 138 | 524 | 1797 | 675 | 0 | 419 | 436 | 129 | 270 | 603 | 332 | 1776 | 1886 | 196 | 11 | 706 | 596 | 1395 | 79 | 473 | 16 | 48 
np | 301 | 854 | 242 | 668 | 152 | 275 | 252 | 1614 | 323 | 561 | 239 | 1357 | 36 | 1104 | 424 | 369 | 445 | 167 | 630 | -- | 1 | 303 | 916 | 134 | 706 | 545 | 849 | 2 | 553 | 538 | 164 | 420 | 687 | 513 | 994 | 741 | 217 | 7 | 930 | 741 | 1156 | 84 | 719 | 39 | 65 
om | 21 | 9 | 1 | 2 | 1 | 14 | 16 | 38 | 4 | 1 | 5 | 4 | 3 | 8 | 2 | 18 | 1 | 4 | 1 | 1 | -- | 2 | 3 | 11 | 4 | 6 | 8 | 0 | 2 | 3 | 0 | 6 | 7 | 5 | 2 | 2 | 1 | 103 | 5 | 10 | 1 | 4 | 2 | 0 | 7 
ps | 192 | 2161 | 252 | 300 | 234 | 559 | 501 | 1522 | 196 | 246 | 229 | 1519 | 11 | 1430 | 266 | 228 | 150 | 102 | 232 | 303 | 2 | -- | 2815 | 94 | 594 | 249 | 1246 | 0 | 235 | 242 | 156 | 304 | 766 | 314 | 441 | 314 | 92 | 8 | 1049 | 818 | 2833 | 156 | 657 | 7 | 32 
fa | 431 | 4186 | 817 | 790 | 426 | 1111 | 967 | 4680 | 602 | 522 | 487 | 5351 | 49 | 3892 | 1014 | 684 | 596 | 252 | 608 | 916 | 3 | 2815 | -- | 186 | 5512 | 541 | 4328 | 0 | 1028 | 1023 | 276 | 812 | 2512 | 1002 | 1250 | 797 | 364 | 8 | 3695 | 3567 | 6752 | 313 | 3190 | 66 | 74 
pcm | 209 | 436 | 91 | 135 | 39 | 149 | 141 | 1074 | 439 | 101 | 529 | 192 | 255 | 367 | 152 | 398 | 130 | 59 | 138 | 134 | 11 | 94 | 186 | -- | 227 | 112 | 322 | 0 | 234 | 246 | 28 | 219 | 314 | 436 | 232 | 162 | 85 | 28 | 287 | 280 | 232 | 18 | 170 | 9 | 462 
pt | 307 | 2539 | 678 | 764 | 230 | 1371 | 1271 | 4744 | 921 | 529 | 375 | 6563 | 61 | 4409 | 706 | 526 | 587 | 251 | 524 | 706 | 4 | 594 | 5512 | 227 | -- | 579 | 4452 | 7 | 1371 | 1341 | 231 | 602 | 7112 | 983 | 1042 | 820 | 468 | 3 | 3483 | 4421 | 6759 | 186 | 3754 | 110 | 97 
pa | 189 | 547 | 190 | 580 | 86 | 250 | 226 | 1330 | 247 | 2210 | 157 | 4052 | 39 | 725 | 269 | 206 | 264 | 118 | 1797 | 545 | 6 | 249 | 541 | 112 | 579 | -- | 629 | 0 | 410 | 404 | 128 | 283 | 585 | 357 | 1726 | 1892 | 200 | 10 | 643 | 570 | 1515 | 73 | 431 | 16 | 44 
ru | 347 | 5564 | 2238 | 838 | 535 | 2572 | 2286 | 9080 | 849 | 582 | 525 | 4622 | 79 | 7588 | 1517 | 711 | 649 | 1013 | 675 | 849 | 8 | 1246 | 4328 | 322 | 4452 | 629 | -- | 5 | 1495 | 1460 | 373 | 1166 | 4864 | 1672 | 1628 | 892 | 595 | 7 | 6223 | 22241 | 5309 | 809 | 3963 | 134 | 125 
gd | 0 | 1 | 4 | 0 | 0 | 2 | 1 | 128 | 2 | 0 | 1 | 1 | 0 | 7 | 2 | 0 | 0 | 1 | 0 | 2 | 0 | 0 | 0 | 0 | 7 | 0 | 5 | -- | 2 | 3 | 2 | 1 | 3 | 1 | 0 | 0 | 1 | 0 | 6 | 5 | 2 | 1 | 3 | 36 | 2 
sr | 357 | 1109 | 289 | 562 | 115 | 504 | 453 | 3760 | 555 | 331 | 258 | 809 | 51 | 1387 | 550 | 443 | 522 | 206 | 419 | 553 | 2 | 235 | 1028 | 234 | 1371 | 410 | 1495 | 2 | -- | 9041 | 127 | 377 | 1235 | 574 | 761 | 691 | 340 | 6 | 1247 | 1512 | 1021 | 109 | 685 | 42 | 69 
sr | 365 | 1145 | 283 | 564 | 123 | 530 | 494 | 3809 | 569 | 345 | 258 | 807 | 51 | 1379 | 571 | 450 | 543 | 211 | 436 | 538 | 3 | 242 | 1023 | 246 | 1341 | 404 | 1460 | 3 | 9041 | -- | 137 | 382 | 1260 | 568 | 775 | 699 | 347 | 10 | 1229 | 1498 | 1009 | 112 | 639 | 45 | 79 
si | 62 | 315 | 124 | 151 | 87 | 166 | 150 | 532 | 98 | 125 | 49 | 449 | 13 | 470 | 109 | 77 | 81 | 45 | 129 | 164 | 0 | 156 | 276 | 28 | 231 | 128 | 373 | 2 | 127 | 137 | -- | 137 | 260 | 189 | 348 | 173 | 69 | 7 | 301 | 306 | 510 | 38 | 216 | 5 | 15 
so | 309 | 1049 | 367 | 412 | 79 | 323 | 302 | 2141 | 502 | 261 | 391 | 747 | 77 | 1312 | 387 | 584 | 234 | 145 | 270 | 420 | 6 | 304 | 812 | 219 | 602 | 283 | 1166 | 1 | 377 | 382 | 137 | -- | 689 | 1020 | 723 | 384 | 178 | 19 | 968 | 875 | 1000 | 75 | 724 | 20 | 116 
es | 351 | 3654 | 704 | 701 | 431 | 2002 | 1873 | 6910 | 990 | 540 | 463 | 2931 | 91 | 4547 | 950 | 607 | 613 | 279 | 603 | 687 | 7 | 766 | 2512 | 314 | 7112 | 585 | 4864 | 3 | 1235 | 1260 | 260 | 689 | -- | 1047 | 1073 | 827 | 469 | 10 | 3645 | 3130 | 3060 | 290 | 2330 | 59 | 133 
sw | 378 | 1186 | 539 | 471 | 86 | 412 | 383 | 2701 | 872 | 300 | 568 | 893 | 151 | 1873 | 425 | 1186 | 324 | 150 | 332 | 513 | 5 | 314 | 1002 | 436 | 983 | 357 | 1672 | 1 | 574 | 568 | 189 | 1020 | 1047 | -- | 929 | 492 | 261 | 10 | 1348 | 1309 | 1253 | 90 | 936 | 37 | 219 
ta | 390 | 1311 | 515 | 919 | 185 | 511 | 465 | 3156 | 425 | 1762 | 299 | 3711 | 52 | 1886 | 641 | 521 | 541 | 206 | 1776 | 994 | 2 | 441 | 1250 | 232 | 1042 | 1726 | 1628 | 0 | 761 | 775 | 348 | 723 | 1073 | 929 | -- | 2278 | 400 | 14 | 1486 | 1423 | 2404 | 134 | 1092 | 32 | 68 
te | 329 | 877 | 245 | 793 | 147 | 353 | 335 | 2121 | 380 | 2066 | 260 | 3762 | 54 | 1131 | 425 | 363 | 452 | 174 | 1886 | 741 | 2 | 314 | 797 | 162 | 820 | 1892 | 892 | 0 | 691 | 699 | 173 | 384 | 827 | 492 | 2278 | -- | 306 | 11 | 893 | 832 | 1748 | 107 | 644 | 21 | 61 
th | 124 | 367 | 140 | 245 | 71 | 269 | 250 | 1020 | 185 | 164 | 87 | 378 | 18 | 599 | 305 | 149 | 197 | 109 | 196 | 217 | 1 | 92 | 364 | 85 | 468 | 200 | 595 | 1 | 340 | 347 | 69 | 178 | 469 | 261 | 400 | 306 | -- | 5 | 477 | 480 | 414 | 37 | 357 | 10 | 26 
ti | 131 | 27 | 2 | 6 | 4 | 11 | 12 | 58 | 10 | 5 | 9 | 7 | 5 | 9 | 5 | 13 | 5 | 3 | 11 | 7 | 103 | 8 | 8 | 28 | 3 | 10 | 7 | 0 | 6 | 10 | 7 | 19 | 10 | 10 | 14 | 11 | 5 | -- | 8 | 8 | 4 | 2 | 5 | 0 | 6 
tr | 435 | 4147 | 1495 | 860 | 449 | 1511 | 1294 | 5676 | 829 | 631 | 519 | 3694 | 91 | 5663 | 1242 | 724 | 680 | 346 | 706 | 930 | 5 | 1049 | 3695 | 287 | 3483 | 643 | 6223 | 6 | 1247 | 1229 | 301 | 968 | 3645 | 1348 | 1486 | 893 | 477 | 8 | -- | 4108 | 4340 | 370 | 2981 | 126 | 130 
uk | 345 | 3457 | 1383 | 688 | 350 | 1619 | 1464 | 6562 | 721 | 508 | 400 | 3935 | 83 | 4829 | 1013 | 610 | 616 | 508 | 596 | 741 | 10 | 818 | 3567 | 280 | 4421 | 570 | 22241 | 5 | 1512 | 1498 | 306 | 875 | 3130 | 1309 | 1423 | 832 | 480 | 8 | 4108 | -- | 4290 | 442 | 3017 | 108 | 89 
ur | 409 | 4935 | 966 | 1382 | 591 | 1651 | 1444 | 6320 | 766 | 1619 | 526 | 15666 | 61 | 6476 | 797 | 617 | 532 | 270 | 1395 | 1156 | 1 | 2833 | 6752 | 232 | 6759 | 1515 | 5309 | 2 | 1021 | 1009 | 510 | 1000 | 3060 | 1253 | 2404 | 1748 | 414 | 4 | 4340 | 4290 | -- | 389 | 3723 | 72 | 88 
uz | 41 | 388 | 199 | 98 | 62 | 176 | 158 | 450 | 76 | 80 | 59 | 352 | 15 | 432 | 49 | 59 | 54 | 113 | 79 | 84 | 4 | 156 | 313 | 18 | 186 | 73 | 809 | 1 | 109 | 112 | 38 | 75 | 290 | 90 | 134 | 107 | 37 | 2 | 370 | 442 | 389 | -- | 257 | 10 | 15 
vi | 285 | 2666 | 726 | 527 | 447 | 1858 | 1663 | 4575 | 438 | 450 | 352 | 3738 | 65 | 4810 | 908 | 631 | 530 | 201 | 473 | 719 | 2 | 657 | 3190 | 170 | 3755 | 431 | 3963 | 3 | 685 | 639 | 216 | 724 | 2330 | 936 | 1092 | 644 | 357 | 5 | 2982 | 3017 | 3723 | 257 | -- | 106 | 76 
cy | 1 | 38 | 30 | 37 | 4 | 33 | 31 | 2655 | 40 | 21 | 30 | 77 | 6 | 145 | 25 | 20 | 12 | 12 | 16 | 39 | 0 | 7 | 66 | 9 | 110 | 16 | 134 | 36 | 42 | 45 | 5 | 20 | 59 | 37 | 32 | 21 | 10 | 0 | 126 | 108 | 72 | 10 | 106 | -- | 8 
yo | 67 | 141 | 42 | 61 | 12 | 39 | 38 | 229 | 159 | 54 | 362 | 79 | 296 | 174 | 33 | 180 | 45 | 23 | 48 | 65 | 7 | 32 | 74 | 462 | 97 | 44 | 125 | 2 | 69 | 79 | 15 | 116 | 133 | 219 | 68 | 61 | 26 | 6 | 130 | 89 | 88 | 15 | 76 | 8 | -- 


## Dataset Creation

### Curation Rationale

[More information needed](https://github.com/csebuetnlp/CrossSum)

### Source Data

[BBC News](https://www.bbc.co.uk/ws/languages)

#### Initial Data Collection and Normalization

[Detailed in the paper](https://arxiv.org/abs/2112.08804/) 


#### Who are the source language producers?

[Detailed in the paper](https://arxiv.org/abs/2112.08804/) 


### Annotations

[Detailed in the paper](https://arxiv.org/abs/2112.08804/) 


#### Annotation process

[Detailed in the paper](https://arxiv.org/abs/2112.08804/) 

#### Who are the annotators?

[Detailed in the paper](https://arxiv.org/abs/2112.08804/) 

### Personal and Sensitive Information

[More information needed](https://github.com/csebuetnlp/CrossSum)

## Considerations for Using the Data

### Social Impact of Dataset

[More information needed](https://github.com/csebuetnlp/CrossSum)

### Discussion of Biases

[More information needed](https://github.com/csebuetnlp/CrossSum)

### Other Known Limitations

[More information needed](https://github.com/csebuetnlp/CrossSum)

## Additional Information

### Dataset Curators

[More information needed](https://github.com/csebuetnlp/CrossSum)

### Licensing Information

Contents of this repository are restricted to only non-commercial research purposes under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright of the dataset contents belongs to the original copyright holders.
### Citation Information

If you use any of the datasets, models or code modules, please cite the following paper:
```
@article{hasan2021crosssum,
  author    = {Tahmid Hasan and Abhik Bhattacharjee and Wasi Uddin Ahmad and Yuan-Fang Li and Yong-bin Kang and Rifat Shahriyar},
  title     = {CrossSum: Beyond English-Centric Cross-Lingual Abstractive Text Summarization for 1500+ Language Pairs},
  journal   = {CoRR},
  volume    = {abs/2112.08804},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.08804},
  eprinttype = {arXiv},
  eprint    = {2112.08804}
}
```


### Contributions

Thanks to [@abhik1505040](https://github.com/abhik1505040) and [@Tahmid](https://github.com/Tahmid04) for adding this dataset.",High,5.0
Summarization,deliadumitrescu/disinfo22-small,2.0,27.0,2023-05-03 09:33:17+00:00,cc-by-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: cc-by-4.0
task_categories:
- feature-extraction
- token-classification
- question-answering
- translation
- summarization
language:
- en
- ar
- pt
- es
- si
- tr
- gu
- id
- ml
- uk
tags:
- not-for-all-audiences
- medical
size_categories:
- n<1K
---
Data set contains 379 images of posts identified as mis-/disinformation and 1 csv file linking the image IDs to urls. The list of urls orginates from the CoronaVirusFacts Database of the International
Fact Checking Network.",Low,1.0
Summarization,hltcoe/megawika,38.0,92830.0,2025-01-31 15:32:11+00:00,cc-by-sa-4.0,2.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2307.07049,['https://aclanthology.org/2021.eacl-demos.19.pdf)'],"---
license: cc-by-sa-4.0
task_categories:
- summarization
- question-answering
- text-generation
- text2text-generation
language:
- af
- ar
- az
- bn
- cs
- de
- en
- es
- et
- fa
- fi
- fr
- ga
- gl
- gu
- he
- hi
- hr
- id
- it
- ja
- ka
- kk
- km
- ko
- lt
- lv
- mk
- ml
- mn
- mr
- my
- ne
- nl
- pl
- ps
- pt
- ro
- ru
- si
- sl
- sv
- ta
- th
- tr
- uk
- ur
- vi
- xh
- zh
pretty_name: MegaWika
size_categories:
- 10M<n<100M
---
# Dataset Card for MegaWika

## Dataset Description

- **Homepage:** [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika)
- **Repository:** [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika)
- **Paper:** [Coming soon]
- **Leaderboard:** [Coming soon]
- **Point of Contact:** [Samuel Barham](samuel.barham@jhuapl.edu)

### Dataset Summary

MegaWika is a multi- and crosslingual text dataset containing 30 million Wikipedia passages with their scraped and cleaned web citations. The passages span
50 Wikipedias in 50 languages, and the articles in which the passages were originally embedded are included for convenience. Where a Wikipedia passage is in a
non-English language, an automated English translation is provided. Furthermore, nearly 130 million English question/answer pairs were extracted from the
passages, and FrameNet events occurring in the passages are detected using the [LOME](https://aclanthology.org/2021.eacl-demos.19.pdf) FrameNet parser.


<!---
To get a feel for the dataset -- its structure, content, strengths and weaknesses -- you may visit the [dataset viewer](https://huggingface.co/spaces/hltcoe/megawika)
we have set up as a HuggingFace Space. It allows the curious visitor to explore a small set of examples spread across a number of the dataset's constituent languages.
-->

### Dataset Creation

The pipeline through which MegaWika was created is complex, and is described in more detail in the paper (linked above),
but the following diagram illustrates the basic approach.

![Illustration of MegaWikaProcess](images/MegaWikaProcess-cross-lingual.drawio.png)

### Supported Tasks and Leaderboards

MegaWika is meant to support research across a variety of tasks, including report generation, summarization, information retrieval, question answering, etc.

### Languages

MegaWika is divided by Wikipedia language. There are 50 languages, including English, each designated by their 2-character ISO language code:
- `af`: Afrikaans
- `ar`: Arabic
- `az`: Azeri (Azerbaijani)
- `bn`: Bengali
- `cs`: Czech
- `de`: German (Deutsch)
- `en`: English
- `es`: Spanish (Español)
- `et`: Estonian
- `fa`: Farsi (Persian)
- `fi`: Finnish
- `fr`: French
- `ga`: Irish (Gaelic)
- `gl`: Galician
- `gu`: Gujarati
- `he`: Hebrew
- `hi`: Hindi
- `hr`: Hungarian
- `id`: Indonesian
- `it`: Italian
- `ja`: Japanese
- `ka`: Georgian (Kartvelian/Kartlian)
- `kk`: Kazakh
- `km`: Khmer
- `ko`: Korean
- `lt`: Lithuanian
- `lv`: Latvian
- `mk`: Macedonian (Makedonski)
- `ml`: Malay (Malayalam)
- `mn`: Mongolian
- `mr`: Marathi
- `my`: Burmese (Myanmar language)
- `ne`: Nepali
- `nl`: Dutch (Nederlands)
- `pl`: Polish
- `ps`: Pashto
- `pt`: Portuguese
- `ro`: Romanian
- `ru`: Russian
- `si`: Sinhalese (Sri Lankan language)
- `sl`: Slovenian
- `sv`: Swedish (Svenska)
- `ta`: Tamil
- `th`: Thai
- `tr`: Turkish
- `uk`: Ukrainian
- `ur`: Urdu
- `vi`: Vietnamese
- `xh`: Xhosa
- `zh`: Chinese (Zhōng wén)

## Dataset Structure

The dataset is divided by language, and the data for each of the 50 languages is further chunked into discrete JSON lines files.
Each line of these files -- we'll call such a line an **instance** -- contains the data extracted from a single Wikipedia article.

### Data Instances

Each instance contains the text of the seed Wikipedia article, along with a list of **entries**. Each entry consists basically in
an extracted Wikipedia passage, the URL and scraped text of the web source it cites, a list of questions/answer pairs extracted from the passage,
and a framenet parse of the passage. Where the passage is from a non-English Wikipedia, a machine translation into English is also provided.

### Data Fields

The detailed structure of an instance is as follows:
```
{
  ""article_title"": <string : title of original Wikipedia article>
  ""article_text"": <string : text of Wikipedia article>
  ""entries"": [
    # Wiki Passage
    ""id"": <string : passage ID>
    ""passage"": {
      ""text"": <string : text of passage in English (possibly via MT)>
      ""parse"": <list of dict : FrameNet parse of English passage text>
      ""en_tokens"": <dict : tokenization of passage in English>
      ""lang_tokens"": <dict : tokenization of original non-English passage>
      ""en_lang_token_map"": <dict : alignment mapping between English and original language token indices>
    }

    # MT
    ""original"": <string : original language passage>
    ""original_sents"": <list of string : sentencized original language passage>
    ""translation"": <string : machine translation of passage>
    ""translation_sents"": <list of string : sentencized machine translation of passage>
    ""translation_probs"": <list of float : log prob of machine translation by sentence, where available>
    ""repetitious_translation"": <string \in (""true"", ""false"") : automated judgment on whether machine translation is pathologically repetitious>
    ""source_lang"": <string : language ID, 2-character ISO code>

    # Source
    ""source_url"": <string : URL of the cited web source>
    ""source_text"": <string : content extracted from the scrape of the source URL>

    # Question/Answer Pairs
    ""qa_pairs"": [
      ...
      {
        ""question"": <string : generated question>
        ""passage_id"": <string : passage ID>
        ""en_answer"": <string : English answer>
        ""lang_answer"": <string : aligned original language answer>
        ""frames"": [
          ...
          {
            ""frame"": <string : frame triggered by the question>
            ""argument"": <string : detected frame arguments>
          }
          ...
        ]
        # NB: answer matches can be empty, in the case no matching span exists
        ""en_matches_in_source"": <list of int : start and end index of the English language-answer token(s) in the source document>
        ""en_match_in_passage"": <list of int : start and end index of the English language-answer token(s) in the English language translation of the passage>
        ""lang_matches_in_source"": <list of int : start and end index of the original language-answer token(s) in the source document>
        ""lang_match_in_passage"": <list of int : start and end index of the original language-answer token(s) in the original language passage>
        ""passage"": <list of string : sentencized view of the passage>
        ""en_answer_tokens"": <list of string>
        ""match_disambiguated_question"": <string : disambiguated version of question obtained by matching pronouns with article title (noisy but often helpful)>
      }
      ...
    ]
  ]
}
```

English language instances differ not in structure but in content; 
1. Fields in the block labeled ""MT"" above are naturally null (that is, they are set to falsy values in Python -- specifically `None`)
2. Since the Wiki passage only exists in English, and has no corresponding non-English ""original language"" version, answer spans also necessarily have only an English-language version (and no non-English ""original-language"" version. Therefore, fields in the `qa_pairs` block beginning with `lang_` are set to null/falsy values in Python (in this case, empty lists).


### Data Splits

MegaWika is currently split only by language, as each task will imply its own approach to filtering, sampling, downselecting, and splitting into train/test splits.

<!---
### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]
-->

## Licensing and Takedown

MegaWika 1.0 consists in part of documents scraped from across the web (based on citations linked in Wikipedia articles.)

We do not own any of the scraped text nor do we claim copyright: text drawn from Wikipedia citations are meant for research use in algorithmic design and model training.

We release this dataset and all its contents under CC-BY-SA-4.0.

### Notice and Takedown Policy:
*NB*: Should you consider that our data contains material that is owned by you and should therefore not be reproduced here, please:

- Clearly identify yourself, with detailed contact data such as an address, telephone number or email address at which you can be contacted.
- Clearly identify the copyrighted work claimed to be infringed.
- Clearly identify the material that is claimed to be infringing and information reasonably sufficient to allow us to locate the material.

And contact the authors.

*Take down*: We will comply to legitimate requests by removing the affected sources from the next release of the dataset.

## Additional Information

### Dataset Curators

Released and maintained by the Johns Hopkins University Human Language Technology Center of Excellence (JHU/HLTCOE). 
You can contact one the MegaWika authors, including [Samuel Barham](mailto:samuel.barham@jhuapl.edu), [Orion Weller](mailto:oweller2@jhu.edu),
and [Ben van Durme](mailto:vandurme@jhu.edu) with questions.

### Licensing Information

Released under the [Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/) license.

### Citation Information

```
@misc{barham2023megawika,
      title={MegaWika: Millions of reports and their sources across 50 diverse languages}, 
      author={Samuel Barham and and  Weller and Michelle Yuan and Kenton Murray and Mahsa Yarmohammadi and Zhengping Jiang and Siddharth Vashishtha and Alexander Martin and Anqi Liu and Aaron Steven White and Jordan Boyd-Graber and Benjamin Van Durme},
      year={2023},
      eprint={2307.07049},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

<!--
### Contributions

[More Information Needed]
-->
",High,5.0
Summarization,Abdelkareem/arabic_summarization_text,0.0,35.0,2023-06-18 13:34:54+00:00,none,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
task_categories:
- summarization
language:
- ar
---",Low,0.0
Summarization,Ezell/test,0.0,15.0,2023-07-03 05:34:59+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- token-classification
- summarization
language:
- ab
- am
- ar
tags:
- biology
size_categories:
- 10K<n<100K
---",Low,1.0
Summarization,crystalai/autotrain-data-crystal_alchemist-vision,1.0,0.0,2023-08-25 01:37:45+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,none,Low,0.0
Summarization,alielfilali01/Hindawi-Books-dataset,12.0,44.0,2023-08-03 20:05:42+00:00,cc-by-nc-4.0,0.0,495 MB,519045120.0,495 MB,519045120.0,49821,none,none,"---
dataset_info:
  features:
  - name: BookLink
    dtype: string
  - name: BookName
    dtype: string
  - name: AuthorName
    dtype: string
  - name: AboutBook
    dtype: string
  - name: ChapterLink
    dtype: string
  - name: ChapterName
    dtype: string
  - name: ChapterText
    dtype: string
  - name: AboutAuthor
    dtype: string
  splits:
  - name: train
    num_bytes: 1364861563
    num_examples: 49821
  download_size: 494678002
  dataset_size: 1364861563
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: cc-by-nc-4.0
task_categories:
- text-generation
- summarization
language:
- ar
pretty_name: Hindawi
size_categories:
- 10K<n<100K
---

# Dataset Card for ""Hindawi Books Dataset""

**Hindawi Books Dataset is a large collection of more than 3000 books written in Modern Standard Arabic.**

## Dataset Description

Hindawi Books Dataset offers a rich and diverse collection of literary works, covering various topics and genres, all written in Modern Standard Arabic. The dataset includes information about each book, such as the title, author name, book abstract, and a link to access the complete text online. Additionally, the dataset contains chapter details, including the chapter name and text, providing insights into the content of each book.

## Dataset Details

- **Homepage:** [https://huggingface.co/datasets/Ali-C137/Hindawi-Books-dataset](https://huggingface.co/datasets/Ali-C137/Hindawi-Books-dataset)
- **Author:** Elfilali Ali
- **Email:** ali.elfilali00@gmail.com, alielfilali0909@gmail.com
- **GitHub Profile:** [https://github.com/alielfilali01](https://github.com/alielfilali01)
- **LinkedIn Profile:** [https://www.linkedin.com/in/alielfilali01/](https://www.linkedin.com/in/alielfilali01/)

## Dataset Size

The Hindawi Books Dataset contains over 3000 books, making it a substantial resource for research and NLP model development. The dataset size on disk is approximately 476 MB, and it comprises more than 120 million tokens.

## Potential Use Cases

Researchers and NLP enthusiasts can utilize the Hindawi Books Dataset for various applications, including:

- **Language Model Training:** The dataset is ideal for training Large Language Models (LLMs) specifically tailored to Arabic text.
- **Text Generation:** Developers can leverage the dataset to generate new stories, poems, or other literary works in Modern Standard Arabic.
- **Text Summarization:** Researchers can explore long text summarization tasks by using the book text and abstract as targets.

## Dataset Access

The Hindawi Books Dataset is publicly available for academic and non-commercial research purposes only. [Hindawi Foundation](https://www.hindawi.org/) has granted permission to scrape and publish the data as a dataset on HuggingFace Hub for non-commercial and research only use.

We kindly request users to respect copyright and intellectual property rights and acknowledge Hindawi Foundation's contribution in any research or academic publications utilizing this dataset. Users are also reminded not to distribute the dataset to any third parties.

## Citation

Please use the following citation when referencing the Hindawi Books Dataset:

```
@dataset{ 
title = {Hindawi Books Dataset},
author = {Elfilali Ali},
howpublished = {Dataset},
url = {https://huggingface.co/datasets/Ali-C137/Hindawi-Books-dataset},
year = {2023},
}
```

## Feedback and Discussion

We encourage researchers and users of the Hindawi Books Dataset to provide feedback, report any potential mistakes, or discuss concerns and risks related to the dataset in the discussion window on the dataset card in the Hugging Face Hub. Your valuable feedback will help us improve and enhance the dataset for the NLP community.

####
",High,5.0
Summarization,PetraAI/PetraAI,21.0,204.0,2023-09-14 21:04:52+00:00,apache-2.0,1.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- text-classification
- token-classification
- table-question-answering
- question-answering
- zero-shot-classification
- translation
- summarization
- conversational
- feature-extraction
- text-generation
- text2text-generation
- fill-mask
- sentence-similarity
- text-to-speech
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
- image-to-text
- image-to-image
- unconditional-image-generation
- video-classification
- reinforcement-learning
- robotics
- tabular-classification
- tabular-regression
- tabular-to-text
- table-to-text
- multiple-choice
- text-retrieval
- time-series-forecasting
- text-to-video
- visual-question-answering
- zero-shot-image-classification
- graph-ml
language:
- ar
- en
tags:
- chemistry
- biology
- finance
- legal
- music
- art
- code
- climate
- medical
pretty_name: PETRA
size_categories:
- 1M<n<10M
---
# PETRA 

## Overview

PETRA is a multilingual dataset for training and evaluating AI systems on a diverse range of tasks across multiple modalities. It contains data in Arabic and English for tasks including translation, summarization, question answering, and more.

## Dataset Structure

- Data is separated by language into `/ar` and `/en` directories
- Within each language directory, data is separated by task into subdirectories  
- Tasks include:
  - Translation
  - Summarization
  - Conversational
  - Feature extraction
  - Zero-shot classification
  - Text generation
  - Fill mask
  - Sentence similarity
  - Text-to-speech
  - Automatic speech recognition
  - Text classification
  - Token classification
  - Table question answering
  - Question answering
  - Text2text generation
  - Audio-to-audio
  - Audio classification
  - Voice activity detection
  - Depth estimation
  - Image classification
  - Object detection
  - Image segmentation
  - Text-to-image
  - Image-to-text
  - Image-to-image
  - Unconditional image generation
  - Reinforcement learning
  - Video classification
  - Robotics
  - Tabular classification
  - Tabular regression
  - Table-to-text
  - Multiple choice
  - Text retrieval
  - Tabular-to-text
  - Text-to-video
  - Time series forecasting
  - Visual question answering
  - Zero-shot image classification
  - Graph ML

## Dataset Tags 

- code
- art
- chemistry
- biology  
- finance
- legal
- music
- climate
- medical

## Dataset Size

1M < n < 10M samples  

## Licenses

Apache 2.0

## Citation

If you use this dataset, please cite it as:

[cite paper, arXiv, etc] 

@article{PetraAI2022PetraAI,
  title={PetraAI: A Massive Multilingual Dataset for Machine Learning}, 
  author={First Last and First Last},
  journal={arXiv},
  year={2022},
  url={https://huggingface.co/datasets/PetraAI/PetraAI}
}

## Contact

For any questions, please reach out to [shadilytn@gmail.com]


# Dataset Cards

## What are Dataset Cards?

Each dataset may be documented by the `README.md` file in the repository. This file is called a **dataset card**, and the Hugging Face Hub will render its contents on the dataset’s main page. To inform users about how to responsibly use the data, it’s a good idea to include information about any potential biases within the dataset. Generally, dataset cards help users understand the contents of the dataset and give context for how the dataset should be used. 

You can also add dataset metadata to your card. The metadata describes important information about a dataset such as its license, language, and size. It also contains tags to help users discover a dataset on the Hub. Tags are defined in a YAML metadata section at the top of the `README.md` file.

## Dataset card metadata

A dataset repo will render its README.md as a dataset card. To control how the Hub displays the card, you should create a YAML section in the README file to define some metadata. Start by adding three --- at the top, then include all of the relevant metadata, and close the section with another group of --- like the example below:


The metadata that you add to the dataset card enables certain interactions on the Hub. For example:

- Allow users to filter and discover datasets at https://huggingface.co/datasets.
  
- If you choose a license using the keywords listed in the right column of this table, the license will be displayed on the dataset page.

When creating a README.md file in a dataset repository on the Hub, use Metadata UI to fill the main metadata:

To see metadata fields, see the detailed dataset card metadata specification here.

### Dataset card creation guide

For a step-by-step guide on creating a dataset card, check out the Create a dataset card guide. 

Reading through existing dataset cards, such as the ELI5 dataset card, is a great way to familiarize yourself with the common conventions.

### Linking a Paper

If the dataset card includes a link to a paper on arXiv, the Hub will extract the arXiv ID and include it in the dataset tags with the format `arxiv:<PAPER ID>`. Clicking on the tag will let you:

- Visit the Paper page
  
- Filter for other models on the Hub that cite the same paper.

Read more about paper pages here.

https://huggingface.co/docs/hub/paper-pages",High,5.0
Summarization,M-A-D/Mixed-Arabic-Datasets-Repo,34.0,1229.0,2023-10-16 21:25:35+00:00,none,0.0,28.6 GB,30709016166.4,28.6 GB,30709016166.4,208606355,none,none,"---
language:
- ar
size_categories:
- 1B<n<10B
task_categories:
- text-classification
- question-answering
- translation
- summarization
- conversational
- text-generation
- text2text-generation
- fill-mask
pretty_name: Mixed Arabic Datasets (MAD) Corpus
dataset_info:
- config_name: Ara--Ali-C137--Hindawi-Books-dataset
  features:
  - name: BookLink
    dtype: string
  - name: BookName
    dtype: string
  - name: AuthorName
    dtype: string
  - name: AboutBook
    dtype: string
  - name: ChapterLink
    dtype: string
  - name: ChapterName
    dtype: string
  - name: ChapterText
    dtype: string
  - name: AboutAuthor
    dtype: string
  splits:
  - name: train
    num_bytes: 1364854259
    num_examples: 49821
  download_size: 494678002
  dataset_size: 1364854259
- config_name: Ara--Goud--Goud-sum
  features:
  - name: article
    dtype: string
  - name: headline
    dtype: string
  - name: categories
    dtype: string
  splits:
  - name: train
    num_bytes: 288296544
    num_examples: 139288
  download_size: 147735776
  dataset_size: 288296544
- config_name: Ara--J-Mourad--MNAD.v1
  features:
  - name: Title
    dtype: string
  - name: Body
    dtype: string
  - name: Category
    dtype: string
  splits:
  - name: train
    num_bytes: 1101921980
    num_examples: 418563
  download_size: 527154122
  dataset_size: 1101921980
- config_name: Ara--JihadZa--IADD
  features:
  - name: Sentence
    dtype: string
  - name: Region
    dtype: string
  - name: DataSource
    dtype: string
  - name: Country
    dtype: string
  splits:
  - name: train
    num_bytes: 19167070
    num_examples: 135804
  download_size: 8644491
  dataset_size: 19167070
- config_name: Ara--LeMGarouani--MAC-corpus
  features:
  - name: tweets
    dtype: string
  - name: type
    dtype: string
  - name: class
    dtype: string
  splits:
  - name: train
    num_bytes: 1945646
    num_examples: 18087
  download_size: 866198
  dataset_size: 1945646
- config_name: Ara--MBZUAI--Bactrian-X
  features:
  - name: instruction
    dtype: string
  - name: input
    dtype: string
  - name: id
    dtype: string
  - name: output
    dtype: string
  splits:
  - name: train
    num_bytes: 66093524
    num_examples: 67017
  download_size: 33063779
  dataset_size: 66093524
- config_name: Ara--OpenAssistant--oasst1
  features:
  - name: message_id
    dtype: string
  - name: parent_id
    dtype: string
  - name: user_id
    dtype: string
  - name: created_date
    dtype: string
  - name: text
    dtype: string
  - name: role
    dtype: string
  - name: lang
    dtype: string
  - name: review_count
    dtype: int32
  - name: review_result
    dtype: bool
  - name: deleted
    dtype: bool
  - name: rank
    dtype: float64
  - name: synthetic
    dtype: bool
  - name: model_name
    dtype: 'null'
  - name: detoxify
    dtype: 'null'
  - name: message_tree_id
    dtype: string
  - name: tree_state
    dtype: string
  - name: emojis
    struct:
    - name: count
      sequence: int32
    - name: name
      sequence: string
  - name: labels
    struct:
    - name: count
      sequence: int32
    - name: name
      sequence: string
    - name: value
      sequence: float64
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 58168
    num_examples: 56
  download_size: 30984
  dataset_size: 58168
- config_name: Ara--Wikipedia
  features:
  - name: id
    dtype: string
  - name: url
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 3052201469
    num_examples: 1205403
  download_size: 1316212231
  dataset_size: 3052201469
- config_name: Ara--bigscience--xP3
  features:
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  splits:
  - name: train
    num_bytes: 4727881680
    num_examples: 2148955
  download_size: 2805060725
  dataset_size: 4727881680
- config_name: Ara--cardiffnlp--tweet_sentiment_multilingual
  features:
  - name: text
    dtype: string
  - name: label
    dtype:
      class_label:
        names:
          '0': negative
          '1': neutral
          '2': positive
  splits:
  - name: train
    num_bytes: 306108
    num_examples: 1839
  - name: validation
    num_bytes: 53276
    num_examples: 324
  - name: test
    num_bytes: 141536
    num_examples: 870
  download_size: 279900
  dataset_size: 500920
- config_name: Ara--miracl--miracl
  features:
  - name: query_id
    dtype: string
  - name: query
    dtype: string
  - name: positive_passages
    list:
    - name: docid
      dtype: string
    - name: text
      dtype: string
    - name: title
      dtype: string
  - name: negative_passages
    list:
    - name: docid
      dtype: string
    - name: text
      dtype: string
    - name: title
      dtype: string
  splits:
  - name: train
    num_bytes: 32012083
    num_examples: 3495
  download_size: 15798509
  dataset_size: 32012083
- config_name: Ara--mustapha--QuranExe
  features:
  - name: text
    dtype: string
  - name: resource_name
    dtype: string
  - name: verses_keys
    dtype: string
  splits:
  - name: train
    num_bytes: 133108687
    num_examples: 49888
  download_size: 58769417
  dataset_size: 133108687
- config_name: Ara--pain--Arabic-Tweets
  features:
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 41639770853
    num_examples: 202700438
  download_size: 22561651700
  dataset_size: 41639770853
- config_name: Ara--saudinewsnet
  features:
  - name: source
    dtype: string
  - name: url
    dtype: string
  - name: date_extracted
    dtype: string
  - name: title
    dtype: string
  - name: author
    dtype: string
  - name: content
    dtype: string
  splits:
  - name: train
    num_bytes: 103654009
    num_examples: 31030
  download_size: 49117164
  dataset_size: 103654009
- config_name: Ary--AbderrahmanSkiredj1--Darija-Wikipedia
  features:
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 8104410
    num_examples: 4862
  download_size: 3229966
  dataset_size: 8104410
- config_name: Ary--Ali-C137--Darija-Stories-Dataset
  features:
  - name: ChapterName
    dtype: string
  - name: ChapterLink
    dtype: string
  - name: Author
    dtype: string
  - name: Text
    dtype: string
  - name: Tags
    dtype: int64
  splits:
  - name: train
    num_bytes: 476926644
    num_examples: 6142
  download_size: 241528641
  dataset_size: 476926644
- config_name: Ary--Wikipedia
  features:
  - name: id
    dtype: string
  - name: url
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 10007364
    num_examples: 6703
  download_size: 4094377
  dataset_size: 10007364
- config_name: Arz--Wikipedia
  features:
  - name: id
    dtype: string
  - name: url
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 1364641408
    num_examples: 1617770
  download_size: 306420318
  dataset_size: 1364641408
configs:
- config_name: Ara--Ali-C137--Hindawi-Books-dataset
  data_files:
  - split: train
    path: Ara--Ali-C137--Hindawi-Books-dataset/train-*
- config_name: Ara--Goud--Goud-sum
  data_files:
  - split: train
    path: Ara--Goud--Goud-sum/train-*
- config_name: Ara--J-Mourad--MNAD.v1
  data_files:
  - split: train
    path: Ara--J-Mourad--MNAD.v1/train-*
- config_name: Ara--JihadZa--IADD
  data_files:
  - split: train
    path: Ara--JihadZa--IADD/train-*
- config_name: Ara--LeMGarouani--MAC-corpus
  data_files:
  - split: train
    path: Ara--LeMGarouani--MAC-corpus/train-*
- config_name: Ara--MBZUAI--Bactrian-X
  data_files:
  - split: train
    path: Ara--MBZUAI--Bactrian-X/train-*
- config_name: Ara--OpenAssistant--oasst1
  data_files:
  - split: train
    path: Ara--OpenAssistant--oasst1/train-*
- config_name: Ara--Wikipedia
  data_files:
  - split: train
    path: Ara--Wikipedia/train-*
- config_name: Ara--bigscience--xP3
  data_files:
  - split: train
    path: Ara--bigscience--xP3/train-*
- config_name: Ara--cardiffnlp--tweet_sentiment_multilingual
  data_files:
  - split: train
    path: Ara--cardiffnlp--tweet_sentiment_multilingual/train-*
  - split: validation
    path: Ara--cardiffnlp--tweet_sentiment_multilingual/validation-*
  - split: test
    path: Ara--cardiffnlp--tweet_sentiment_multilingual/test-*
- config_name: Ara--miracl--miracl
  data_files:
  - split: train
    path: Ara--miracl--miracl/train-*
- config_name: Ara--mustapha--QuranExe
  data_files:
  - split: train
    path: Ara--mustapha--QuranExe/train-*
- config_name: Ara--pain--Arabic-Tweets
  data_files:
  - split: train
    path: Ara--pain--Arabic-Tweets/train-*
- config_name: Ara--saudinewsnet
  data_files:
  - split: train
    path: Ara--saudinewsnet/train-*
- config_name: Ary--AbderrahmanSkiredj1--Darija-Wikipedia
  data_files:
  - split: train
    path: Ary--AbderrahmanSkiredj1--Darija-Wikipedia/train-*
- config_name: Ary--Ali-C137--Darija-Stories-Dataset
  data_files:
  - split: train
    path: Ary--Ali-C137--Darija-Stories-Dataset/train-*
- config_name: Ary--Wikipedia
  data_files:
  - split: train
    path: Ary--Wikipedia/train-*
- config_name: Arz--Wikipedia
  data_files:
  - split: train
    path: Arz--Wikipedia/train-*
---
# Dataset Card for ""Mixed Arabic Datasets (MAD) Corpus""

**The Mixed Arabic Datasets Corpus : A Community-Driven Collection of Diverse Arabic Texts**

## Dataset Description

The Mixed Arabic Datasets (MAD) presents a dynamic compilation of diverse Arabic texts sourced from various online platforms and datasets. It addresses a critical challenge faced by researchers, linguists, and language enthusiasts: the fragmentation of Arabic language datasets across the Internet. With MAD, we are trying to centralize these dispersed resources into a single, comprehensive repository.

Encompassing a wide spectrum of content, ranging from social media conversations to literary masterpieces, MAD captures the rich tapestry of Arabic communication, including both standard Arabic and regional dialects.

This corpus offers comprehensive insights into the linguistic diversity and cultural nuances of Arabic expression.

## Usage 

If you want to use this dataset you pick one among the available configs:

`Ara--MBZUAI--Bactrian-X` | `Ara--OpenAssistant--oasst1` | `Ary--AbderrahmanSkiredj1--Darija-Wikipedia`

`Ara--Wikipedia` | `Ary--Wikipedia` | `Arz--Wikipedia`

`Ary--Ali-C137--Darija-Stories-Dataset` | `Ara--Ali-C137--Hindawi-Books-dataset` | ``

Example of usage:

```python
dataset = load_dataset('M-A-D/Mixed-Arabic-Datasets-Repo', 'Ara--MBZUAI--Bactrian-X')
```

If you loaded multiple datasets and wanted to merge them together then you can simply laverage `concatenate_datasets()` from `datasets`

```pyhton
dataset3 = concatenate_datasets([dataset1['train'], dataset2['train']])
```

Note : proccess the datasets before merging in order to make sure you have a new dataset that is consistent

## Dataset Size

The Mixed Arabic Datasets (MAD) is a dynamic and evolving collection, with its size fluctuating as new datasets are added or removed. As MAD continuously expands, it becomes a living resource that adapts to the ever-changing landscape of Arabic language datasets.

**Dataset List**

MAD draws from a diverse array of sources, each contributing to its richness and breadth. While the collection is constantly evolving, some of the datasets that are poised to join MAD in the near future include:

- [✔] OpenAssistant/oasst1 (ar portion) : [Dataset Link](https://huggingface.co/datasets/OpenAssistant/oasst1)
- [✔] MBZUAI/Bactrian-X (ar portion) : [Dataset Link](https://huggingface.co/datasets/MBZUAI/Bactrian-X/viewer/ar/train)
- [✔] AbderrahmanSkiredj1/Darija-Wikipedia : [Dataset Link](https://huggingface.co/datasets/AbderrahmanSkiredj1/moroccan_darija_wikipedia_dataset)
- [✔] Arabic Wikipedia : [Dataset Link](https://huggingface.co/datasets/wikipedia)
- [✔] Moroccan Arabic Wikipedia : [Dataset Link](https://huggingface.co/datasets/wikipedia)
- [✔] Egyptian Arabic Wikipedia : [Dataset Link](https://huggingface.co/datasets/wikipedia)
- [✔] Darija Stories Dataset : [Dataset Link](https://huggingface.co/datasets/Ali-C137/Darija-Stories-Dataset)
- [✔] Hindawi Books Dataset : [Dataset Link](https://huggingface.co/datasets/Ali-C137/Hindawi-Books-dataset)
- [] uonlp/CulturaX - ar : [Dataset Link](https://huggingface.co/datasets/uonlp/CulturaX/viewer/ar/train)
- [✔] Pain/ArabicTweets : [Dataset Link](https://huggingface.co/datasets/pain/Arabic-Tweets)
- [] Abu-El-Khair Corpus : [Dataset Link](https://huggingface.co/datasets/arabic_billion_words)
- [✔] QuranExe : [Dataset Link](https://huggingface.co/datasets/mustapha/QuranExe)
- [✔] MNAD : [Dataset Link](https://huggingface.co/datasets/J-Mourad/MNAD.v1)
- [✔] IADD : [Dataset Link](https://raw.githubusercontent.com/JihadZa/IADD/main/IADD.json)
- [] OSIAN : [Dataset Link](https://wortschatz.uni-leipzig.de/en/download/Arabic#ara-tn_newscrawl-OSIAN_2018)
- [✔] MAC corpus : [Dataset Link](https://raw.githubusercontent.com/LeMGarouani/MAC/main/MAC%20corpus.csv)
- [✔] Goud.ma-Sum : [Dataset Link](https://huggingface.co/datasets/Goud/Goud-sum)
- [✔] SaudiNewsNet : [Dataset Link](https://huggingface.co/datasets/saudinewsnet)
- [✔] Miracl : [Dataset Link](https://huggingface.co/datasets/miracl/miracl)
- [✔] CardiffNLP/TweetSentimentMulti : [Dataset Link](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual)
- [] OSCAR-2301 : [Dataset Link](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301/viewer/ar/train)
- [] mc4 : [Dataset Link](https://huggingface.co/datasets/mc4/viewer/ar/train)
- [✔] bigscience/xP3 : [Dataset Link](https://huggingface.co/datasets/bigscience/xP3/viewer/ar/train)
- [] Muennighoff/xP3x : [Dataset Link](https://huggingface.co/datasets/Muennighoff/xP3x)
- [] Ai_Society : [Dataset Link](https://huggingface.co/datasets/camel-ai/ai_society_translated)


## Potential Use Cases

The Mixed Arabic Datasets (MAD) holds the potential to catalyze a multitude of groundbreaking applications:

- **Linguistic Analysis:** Employ MAD to conduct in-depth linguistic studies, exploring dialectal variances, language evolution, and grammatical structures.
- **Topic Modeling:** Dive into diverse themes and subjects through the extensive collection, revealing insights into emerging trends and prevalent topics.
- **Sentiment Understanding:** Decode sentiments spanning Arabic dialects, revealing cultural nuances and emotional dynamics.
- **Sociocultural Research:** Embark on a sociolinguistic journey, unraveling the intricate connection between language, culture, and societal shifts.

## Dataset Access

MAD's access mechanism is unique: while it doesn't carry a general license itself, each constituent dataset within the corpus retains its individual license. By accessing the dataset details through the provided links in the ""Dataset List"" section above, users can understand the specific licensing terms for each dataset.

### Join Us on Discord

For discussions, contributions, and community interactions, join us on Discord! [![Discord](https://img.shields.io/discord/798499298231726101?label=Join%20us%20on%20Discord&logo=discord&logoColor=white&style=for-the-badge)](https://discord.gg/2NpJ9JGm)

### How to Contribute

Want to contribute to the Mixed Arabic Datasets project? Follow our comprehensive guide on Google Colab for step-by-step instructions: [Contribution Guide](https://colab.research.google.com/drive/1kOIRoicgCOV8TPvASAI_2uMY7rpXnqzJ?usp=sharing).

**Note**: If you'd like to test a contribution before submitting it, feel free to do so on the [MAD Test Dataset](https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Dataset-test).

## Citation

```
@dataset{ 
title = {Mixed Arabic Datasets (MAD)},
author = {MAD Community},
howpublished = {Dataset},
url = {https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Datasets-Repo},
year = {2023},
}
```",High,6.0
Summarization,mohamedemam/Arabic-samsum-dialogsum,1.0,49.0,2023-09-11 14:35:29+00:00,cc-by-nc-2.0,1.0,14 MB,14680064.0,14 MB,14680064.0,24813,https://arxiv.org/abs/1911.12237,none,"---
dataset_info:
  features:
  - name: index
    dtype: int64
  - name: id
    dtype: string
  - name: dialogue
    dtype: string
  - name: summary
    dtype: string
  - name: topic
    dtype: string
  splits:
  - name: train
    num_bytes: 27913254
    num_examples: 24813
  download_size: 13968520
  dataset_size: 27913254
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: cc-by-nc-2.0
task_categories:
- summarization
- conversational
language:
- ar
pretty_name: ar messum
size_categories:
- 10K<n<100K
---
# Dataset Card for ""Arabic-samsum-dialogsum""

this dataset is comption between samsum and dialogsum dataset translated in arabic
## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://arxiv.org/abs/1911.12237v2
- **Repository:** [Needs More Information]
- **Paper:** https://arxiv.org/abs/1911.12237v2
- **Leaderboard:** [Needs More Information]
- **Point of Contact:** [Needs More Information]

### Dataset Summary

The SAMSum dataset contains about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English. Linguists were asked to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger convesations. The style and register are diversified - conversations could be informal, semi-formal or formal, they may contain slang words, emoticons and typos. Then, the conversations were annotated with summaries. It was assumed that summaries should be a concise brief of what people talked about in the conversation in third person.
The SAMSum dataset was prepared by Samsung R&D Institute Poland and is distributed for research purposes (non-commercial licence: CC BY-NC-ND 4.0).

### Supported Tasks and Leaderboards

[Needs More Information]

### Languages

Arabic

## Dataset Structure
t
### Data Instances

The created dataset is made of 16369 conversations distributed uniformly into 4 groups based on the number of utterances in con- versations: 3-6, 7-12, 13-18 and 19-30. Each utterance contains the name of the speaker. Most conversations consist of dialogues between two interlocutors (about 75% of all conversations), the rest is between three or more people

The first instance in the training set:
{'id': '13818513', 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.', 'dialogue': ""Amanda: I baked  cookies. Do you want some?\r\nJerry: Sure!\r\nAmanda: I'll bring you tomorrow :-)""}

### Data Fields

- dialogue: text of dialogue.
- summary: human written summary of the dialogue.
- id: unique id of an example.

### Data Splits

- train: 24732

## Dataset Creation

### Curation Rationale

In paper:
> In the first approach, we reviewed datasets from the following categories: chatbot dialogues, SMS corpora, IRC/chat data, movie dialogues, tweets, comments data (conversations formed by replies to comments), transcription of meetings, written discussions, phone dialogues and daily communication data. Unfortunately, they all differed in some respect from the conversations that are typ- ically written in messenger apps, e.g. they were too technical (IRC data), too long (comments data, transcription of meetings), lacked context (movie dialogues) or they were more of a spoken type, such as a dialogue between a petrol station assis- tant and a client buying petrol.
As a consequence, we decided to create a chat dialogue dataset by constructing such conversa- tions that would epitomize the style of a messenger app.

### Source Data

#### Initial Data Collection and Normalization

 In paper:
> We asked linguists to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger conversations. It includes chit-chats, gossiping about friends, arranging meetings, discussing politics, consulting university assignments with colleagues, etc. Therefore, this dataset does not contain any sensitive data or fragments of other corpora.

#### Who are the source language producers?

linguists

### Annotations

#### Annotation process

In paper:
> Each dialogue was created by one person. After collecting all of the conversations, we asked language experts to annotate them with summaries, assuming that they should (1) be rather short, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person. Each dialogue contains only one ref- erence summary.

#### Who are the annotators?

language experts

### Personal and Sensitive Information

None, see above: Initial Data Collection and Normalization

## Considerations for Using the Data

### Social Impact of Dataset

[Needs More Information]

### Discussion of Biases

[Needs More Information]

### Other Known Limitations

[Needs More Information]

## Additional Information

### Dataset Curators

[Needs More Information]

### Licensing Information

non-commercial licence: CC BY-NC-ND 4.0

### Citation Information

```
@inproceedings{gliwa-etal-2019-samsum,
    title = ""{SAMS}um Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization"",
    author = ""Gliwa, Bogdan  and
      Mochol, Iwona  and
      Biesek, Maciej  and
      Wawer, Aleksander"",
    booktitle = ""Proceedings of the 2nd Workshop on New Frontiers in Summarization"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/D19-5409"",
    doi = ""10.18653/v1/D19-5409"",
    pages = ""70--79""
}
```

### Contributions

Thanks to [@cccntu](https://github.com/cccntu) for adding this dataset.
[More Information needed](https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)",High,5.0
Summarization,alielfilali01/Goud-Sum-Instruct,0.0,20.0,2023-09-12 19:22:47+00:00,apache-2.0,0.0,171 MB,179306496.0,171 MB,179306496.0,158282,none,none,"---
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: validation
    path: data/validation-*
  - split: test
    path: data/test-*
dataset_info:
  features:
  - name: id
    dtype: int64
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 329002522
    num_examples: 139288
  - name: validation
    num_bytes: 22449821
    num_examples: 9497
  - name: test
    num_bytes: 22447355
    num_examples: 9497
  download_size: 170777466
  dataset_size: 373899698
license: apache-2.0
task_categories:
- summarization
language:
- ar
size_categories:
- 100K<n<1M
---
# Dataset Card for Goud-Sum-Instruct

Goud-Sum-Instruct is a meticulously curated dataset originating from [Goud-sum](https://huggingface.co/datasets/Goud/Goud-sum) dataset, This dataset is primed for fine-tuning chat and instruct models, without any compromise to the existing training mode. This strategic approach enables the specific training of models to respond effectively to the main instruction which is ""To Summarise"". In conclusion, this dataset is meant to finetune a chat model in order to serve later as a summarizer.

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [Needs More Information]
- **Repository:** [Needs More Information]
- **Paper:** [Goud.ma: a News Article Dataset for Summarization in Moroccan Darija](https://openreview.net/forum?id=BMVq5MELb9)
- **Leaderboard:** [Needs More Information]
- **Point of Contact:** [Needs More Information]

### Dataset Summary

Goud-Sum-Instruct contains 158k articles and their headlines extracted from [Goud.ma](https://www.goud.ma/) news website. The articles are written in the Arabic script. All headlines are in Moroccan Darija, while articles may be in Moroccan Darija, in Modern Standard Arabic, or a mix of both (code-switched Moroccan Darija).

### Supported Tasks and Leaderboards

Text Summarization

### Languages

- Moroccan Arabic (Darija)
- Modern Standard Arabic

## Dataset Structure

### Data Instances

The dataset consists of article-headline pairs in string format.

### Data Fields

- article: a string containing the body of the news article
- headline: a string containing the article's headline
- categories: a list of string of article categories

### Data Splits

Goud-Sum-Instruct dataset has 3 splits: _train_, _validation_, and _test_. Below are the number of instances in each split.

| Dataset Split | Number of Instances in Split |
| ------------- | ---------------------------- |
| Train         | 139,288                      |
| Validation    | 9,497                        |
| Test          | 9,497                        |

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

The text was written by journalists at [Goud](https://www.goud.ma/).

### Annotations
The dataset does not contain any additional annotations.

#### Annotation process

[N/A]

#### Who are the annotators?

[N/A]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

```
@inproceedings{issam2022goudma,
title={Goud.ma: a News Article Dataset for Summarization in Moroccan Darija},
author={Abderrahmane Issam and Khalil Mrini},
booktitle={3rd Workshop on African Natural Language Processing},
year={2022},
url={https://openreview.net/forum?id=BMVq5MELb9}
}
```

### Contributions

Thanks to [@issam9](https://github.com/issam9) and [@KhalilMrini](https://github.com/KhalilMrini) for adding the original [dataset](https://huggingface.co/datasets/Goud/Goud-sum)",High,5.0
Summarization,Amina-Chouigui/ANTCorpusv2.1,1.0,17.0,2023-09-17 16:19:02+00:00,none,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
task_categories:
- text-classification
- summarization
language:
- ar
size_categories:
- 10K<n<100K
---
# Dataset Card for ANTCorpus v2.1

## Dataset Description

- **Homepage:** https://antcorpus.github.io/
- **Point of Contact:** aminachouigui@gmail.com

### Dataset Summary

ANTCorpus v2.1 (31 525 articles with multi-source Arabic news websites)
### Supported Tasks and Leaderboards

Text classification and summarization.

### Languages

Arabic

### Licensing Information

By downloading ANT Corpus, you agree to cite at least one of our papers describing ANT Corpus and/or refer the project's main page in any kind of material you produce where ANT Corpus was used to conduct search or experimentation, whether be it a research paper, dissertation, article, poster, presentation, or documentation.

📄 A. Chouigui, O. Ben Khiroun, and B. Elayeb. An Arabic Multi-source News Corpus: Experimenting on Single-document Extractive Summarization. In Arabian Journal for Science and Engineering (AJSE 2021), 46(08), 1-14, DOI : 10.1007/s13369-020-05258-z , February 2021.

📄 A. Chouigui, O. Ben Khiroun, and B. Elayeb. ANT Corpus : An Arabic News Text Collection for Textual Classification. In proceedings of the 14th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA 2017), pp. 135-142, Hammamet, Tunisia, October 30 - November 3, 2017.

📄 A. Chouigui, O. Ben Khiroun, and B. Elayeb. A TF-IDF and Co-occurrence Based Approach for Events Extraction from Arabic News Corpus. In proceedings of the 23rd International Conference on Natural Language & Information Systems (NLDB 2018), pp. 272-280, Paris, France, 13-15 June 2018.

📄 A. Chouigui, O. Ben Khiroun and B. Elayeb. Related Terms Extraction from Arabic News Corpus using Word Embedding. In: OTM Conferences & Workshops: Proceedings of the 7th International Workshop on Methods, Evaluation, Tools and Applications for the Creation and Consumption of Structured Data for the e-Society (Meta4eS'18), Springer, LNCS, pp. 1-11, Valletta, Malta, 22-26 October 2018.
",Low,1.0
Summarization,M-A-D/Mixed-Arabic-Dataset-Main,6.0,173.0,2023-10-06 17:56:33+00:00,none,3.0,791 MB,829423616.0,791 MB,829423616.0,131393,none,none,"---
language:
- ar
task_categories:
- conversational
- text-generation
- text2text-generation
- translation
- summarization
pretty_name: MAD
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
dataset_info:
  features:
  - name: GenId
    dtype: int64
  - name: SubId
    dtype: int64
  - name: DatasetName
    dtype: string
  - name: DatasetLink
    dtype: string
  - name: Text
    dtype: string
  - name: MetaData
    struct:
    - name: AboutAuthor
      dtype: string
    - name: AboutBook
      dtype: string
    - name: Author
      dtype: string
    - name: AuthorName
      dtype: string
    - name: BookLink
      dtype: string
    - name: BookName
      dtype: string
    - name: ChapterLink
      dtype: string
    - name: ChapterName
      dtype: string
    - name: Tags
      dtype: float64
    - name: __index_level_0__
      dtype: float64
    - name: created_date
      dtype: string
    - name: deleted
      dtype: bool
    - name: detoxify
      dtype: 'null'
    - name: emojis
      struct:
      - name: count
        sequence: int32
      - name: name
        sequence: string
    - name: id
      dtype: string
    - name: labels
      struct:
      - name: count
        sequence: int32
      - name: name
        sequence: string
      - name: value
        sequence: float64
    - name: lang
      dtype: string
    - name: message_id
      dtype: string
    - name: message_tree_id
      dtype: string
    - name: model_name
      dtype: 'null'
    - name: parent_id
      dtype: string
    - name: query_id
      dtype: string
    - name: rank
      dtype: float64
    - name: review_count
      dtype: float64
    - name: review_result
      dtype: bool
    - name: role
      dtype: string
    - name: synthetic
      dtype: bool
    - name: title
      dtype: string
    - name: tree_state
      dtype: string
    - name: url
      dtype: string
    - name: user_id
      dtype: string
  - name: ConcatenatedText
    dtype: int64
  - name: __index_level_0__
    dtype: float64
  splits:
  - name: train
    num_bytes: 1990497610
    num_examples: 131393
  download_size: 790648134
  dataset_size: 1990497610
---
# Dataset Card for ""Mixed-Arabic-Dataset""

## Mixed Arabic Datasets (MAD)

The Mixed Arabic Datasets (MAD) project provides a comprehensive collection of diverse Arabic-language datasets, sourced from various repositories, platforms, and domains. These datasets cover a wide range of text types, including books, articles, Wikipedia content, stories, and more.

### MAD Repo vs. MAD Main

#### MAD Repo
- **Versatility**: In the MAD Repository (MAD Repo), datasets are made available in their original, native form. Researchers and practitioners can selectively download specific datasets that align with their specific interests or requirements.
- **Independent Access**: Each dataset is self-contained, enabling users to work with individual datasets independently, allowing for focused analyses and experiments.

#### MAD Main or simply MAD
- **Unified Dataframe**: MAD Main represents a harmonized and unified dataframe, incorporating all datasets from the MAD Repository. It provides a seamless and consolidated view of the entire MAD collection, making it convenient for comprehensive analyses and applications.
- **Holistic Perspective**: Researchers can access a broad spectrum of Arabic-language content within a single dataframe, promoting holistic exploration and insights across diverse text sources.

### Why MAD Main?
- **Efficiency**: Working with MAD Main streamlines the data acquisition process by consolidating multiple datasets into one structured dataframe. This is particularly beneficial for large-scale projects or studies requiring diverse data sources.
- **Interoperability**: With MAD Main, the datasets are integrated into a standardized format, enhancing interoperability and compatibility with a wide range of data processing and analysis tools.
- **Meta-Analysis**: Researchers can conduct comprehensive analyses, such as cross-domain studies, trend analyses, or comparative studies, by leveraging the combined richness of all MAD datasets.

### Getting Started
- To access individual datasets in their original form, refer to the MAD Repository ([Link to MAD Repo](https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Datasets-Repo)).
- For a unified view of all datasets, conveniently organized in a dataframe, you are here in the right place.
```python
from datasets import load_dataset

dataset = load_dataset(""M-A-D/Mixed-Arabic-Dataset-Main"")
```

### Join Us on Discord

For discussions, contributions, and community interactions, join us on Discord! [![Discord](https://img.shields.io/discord/798499298231726101?label=Join%20us%20on%20Discord&logo=discord&logoColor=white&style=for-the-badge)](https://discord.gg/2NpJ9JGm)

### How to Contribute

Want to contribute to the Mixed Arabic Datasets project? Follow our comprehensive guide on Google Colab for step-by-step instructions: [Contribution Guide](https://colab.research.google.com/drive/1w7_7lL6w7nM9DcDmTZe1Vfiwkio6SA-w?usp=sharing).

**Note**: If you'd like to test a contribution before submitting it, feel free to do so on the [MAD Test Dataset](https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Dataset-test).

## Citation

```
@dataset{ 
title = {Mixed Arabic Datasets (MAD)},
author = {MAD Community},
howpublished = {Dataset},
url = {https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Datasets-Repo},
year = {2023},
}
```",Medium,3.0
Summarization,FahdSeddik/AGS-Corpus,6.0,35.0,2023-09-29 12:36:04+00:00,cc-by-nc-4.0,1.0,UNKNOWN,unknown,190 MB,unknown,141467,none,none,"---
license: cc-by-nc-4.0
task_categories:
- summarization
language:
- ar
tags:
- chemistry
- biology
- legal
- finance
- music
- art
- code
- climate
- medical
pretty_name: AGS Corpus
size_categories:
- 100K<n<1M
---
# Dataset Card for AGS

## Table of Contents
- Dataset Description
  - Dataset Summary
  - Supported Tasks and Leaderboards
  - Languages
- Dataset Structure
  - Data Instances
  - Data Fields
- Dataset Creation
  - Curation Rationale
  - Source Data
  - Personal and Sensitive Information
 

## Dataset Description

- **Paper:** [Atef, A., Seddik, F., & Elbedewy, A. (2023).
AGS: Arabic GPT Summarization Corpus]()
- **Point of Contact:** fahdseddik@gmail.com

### Dataset Summary

AGS is the first publicly accessible abstractive summarization dataset for Arabic. It consists of 142,000 pairs of articles and summaries, all written in Modern Standard Arabic (MSA). The summaries are generated using GPT-3.5 Turbo, a large language model, through meticulous prompt engineering. The dataset covers a wide range of topics, such as politics, sports, culture, science, and technology.

### Supported Tasks and Leaderboards

The supported task is abstractive text summarization, which involves generating a concise and informative summary from a longer text. The dataset can be used to train and evaluate models for this task, as well as to benchmark their performance against existing methods.

There is no official leaderboard for this dataset, but the we report the results of several models on the test set, using Rouge-L, SS-Population mean, and Compression ratio metrics. The best performing model is mT5, which achieves 21.27, 82.65, and 62 scores on these metrics, respectively.

### Languages

The dataset is in Arabic (ISO 639-1: ar).

## Dataset Structure

### Data Instances

An example data instance is:
```
{ “text”: “نظرية التعقيد هي فرع من فروع نظرية
 الحوسبة والرياضيات، وهذه النظرية تتركز في تصنيف المسائل الحاسوبية حسب صعوبتها وربط أقسام التعقيد ببعضها، والمسألة
 الحاسوبية هي المسألة التي يستطيع الحاسوب بحلها.ويمكن اعتبارها مسألة صعبة إذا استخدمت كمية مُعينة من الموارد أياً كانت
 الخوارزمية. ولعل النماذج الحسابية هي الطريقة الأمثل في هذه النظرية لدراسة هذه المسائل وتحديد كمية الموارد اللازمة
 مثل: الوقت أو حجم المكان الإضافي اللازم، وتوجد معايير تعقيد أخرى مثل: الاتصال (مستخدم في نظرية تعقيد الاتصال) وعدد
 البوابات في الدارات المنطقية (مستخدم في نظرية تعقيد الدارات المنطقية) وكذلك عدد المعالجات (مستخدم في الحساب المتوازي).”,
 “summary”: “نظرية التعقيد هي فرع من نظرية
الحوسبة والرياضيات، تصنف المسائل الحاسوبية حسب صعوبتها وتربط أقسام التعقيد ببعضها. تحديد كمية الموارد
اللازمة يتم باستخدام النماذج الحسابية، مثل الوقت وحجم المكان الإضافي وعدد البوابات في الدارات المنطقية.” }
```

### Data Fields

- 'id' : an identification number
- `text`: the original text of the article, written in Arabic.
- `summary`: the abstractive summary of the article, written in Arabic.


## Dataset Creation

### Curation Rationale

The dataset was created to address the lack of abstractive summarization datasets for Arabic, which is a low-resource and under-studied language. The dataset aims to provide a large and diverse corpus of articles and summaries that can be used to train and evaluate models for this task, as well as to advance the research in this field.

### Source Data

The source data was collected from Wikipedia & Youm7 websites, covering a wide range of topics, such as politics, sports, culture, science, and technology. The websites were selected based on their popularity, credibility, and content quality. The data collection process involved web crawling, text sampling, and prompt engineering.

### Personal and Sensitive Information

The dataset does not contain any personal or sensitive information, as it only consists of articles and summaries that are publicly available on the web. The dataset creators are not responsible for any misuse or harm that may result from the use of this data.

",Medium,3.0
Summarization,sbarham/megawika-test,1.0,91.0,2023-10-03 17:22:49+00:00,cc-by-sa-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2307.07049,['https://aclanthology.org/2021.eacl-demos.19.pdf)'],"---
license: cc-by-sa-4.0
task_categories:
- summarization
- question-answering
- text-generation
- text2text-generation
language:
- af
- ar
- az
- bn
- cs
- de
- en
- es
- et
- fa
- fi
- fr
- ga
- gl
- gu
- he
- hi
- hr
- id
- it
- ja
- ka
- kk
- km
- ko
- lt
- lv
- mk
- ml
- mn
- mr
- my
- ne
- nl
- pl
- ps
- pt
- ro
- ru
- si
- sl
- sv
- ta
- th
- tr
- uk
- ur
- vi
- xh
- zh
pretty_name: MegaWika
size_categories:
- 10M<n<100M
---
# Dataset Card for MegaWika

## Dataset Description

- **Homepage:** [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika)
- **Repository:** [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika)
- **Paper:** [Coming soon]
- **Leaderboard:** [Coming soon]
- **Point of Contact:** [Samuel Barham](samuel.barham@jhuapl.edu)

### Dataset Summary

MegaWika is a multi- and crosslingual text dataset containing 30 million Wikipedia passages with their scraped and cleaned web citations. The passages span
50 Wikipedias in 50 languages, and the articles in which the passages were originally embedded are included for convenience. Where a Wikipedia passage is in a
non-English language, an automated English translation is provided. Furthermore, nearly 130 million English question/answer pairs were extracted from the
passages, and FrameNet events occurring in the passages are detected using the [LOME](https://aclanthology.org/2021.eacl-demos.19.pdf) FrameNet parser.


<!---
To get a feel for the dataset -- its structure, content, strengths and weaknesses -- you may visit the [dataset viewer](https://huggingface.co/spaces/hltcoe/megawika)
we have set up as a HuggingFace Space. It allows the curious visitor to explore a small set of examples spread across a number of the dataset's constituent languages.
-->

### Dataset Creation

The pipeline through which MegaWika was created is complex, and is described in more detail in the paper (linked above),
but the following diagram illustrates the basic approach.

![Illustration of MegaWikaProcess](images/MegaWikaProcess-cross-lingual.drawio.png)

### Supported Tasks and Leaderboards

MegaWika is meant to support research across a variety of tasks, including report generation, summarization, information retrieval, question answering, etc.

### Languages

MegaWika is divided by Wikipedia language. There are 50 languages, including English, each designated by their 2-character ISO language code:
- `af`: Afrikaans
- `ar`: Arabic
- `az`: Azeri (Azerbaijani)
- `bn`: Bengali
- `cs`: Czech
- `de`: German (Deutsch)
- `en`: English
- `es`: Spanish (Español)
- `et`: Estonian
- `fa`: Farsi (Persian)
- `fi`: Finnish
- `fr`: French
- `ga`: Irish (Gaelic)
- `gl`: Galician
- `gu`: Gujarati
- `he`: Hebrew
- `hi`: Hindi
- `hr`: Hungarian
- `id`: Indonesian
- `it`: Italian
- `ja`: Japanese
- `ka`: Georgian (Kartvelian/Kartlian)
- `kk`: Kazakh
- `km`: Khmer
- `ko`: Korean
- `lt`: Lithuanian
- `lv`: Latvian
- `mk`: Macedonian (Makedonski)
- `ml`: Malay (Malayalam)
- `mn`: Mongolian
- `mr`: Marathi
- `my`: Burmese (Myanmar language)
- `ne`: Nepali
- `nl`: Dutch (Nederlands)
- `pl`: Polish
- `ps`: Pashto
- `pt`: Portuguese
- `ro`: Romanian
- `ru`: Russian
- `si`: Sinhalese (Sri Lankan language)
- `sl`: Slovenian
- `sv`: Swedish (Svenska)
- `ta`: Tamil
- `th`: Thai
- `tr`: Turkish
- `uk`: Ukrainian
- `ur`: Urdu
- `vi`: Vietnamese
- `xh`: Xhosa
- `zh`: Chinese (Zhōng wén)

## Dataset Structure

The dataset is divided by language, and the data for each of the 50 languages is further chunked into discrete JSON lines files.
Each line of these files -- we'll call such a line an **instance** -- contains the data extracted from a single Wikipedia article.

### Data Instances

Each instance contains the text of the seed Wikipedia article, along with a list of **entries**. Each entry consists basically in
an extracted Wikipedia passage, the URL and scraped text of the web source it cites, a list of questions/answer pairs extracted from the passage,
and a framenet parse of the passage. Where the passage is from a non-English Wikipedia, a machine translation into English is also provided.

### Data Fields

The detailed structure of an instance is as follows:
```
{
  ""article_title"": <string : title of original Wikipedia article>
  ""article_text"": <string : text of Wikipedia article>
  ""entries"": [
    # Wiki Passage
    ""id"": <string : passage ID>
    ""passage"": {
      ""text"": <string : text of passage in English (possibly via MT)>
      ""parse"": <list of dict : FrameNet parse of English passage text>
      ""en_tokens"": <dict : tokenization of passage in English>
      ""lang_tokens"": <dict : tokenization of original non-English passage>
      ""en_lang_token_map"": <dict : alignment mapping between English and original language token indices>
    }

    # MT
    ""original"": <string : original language passage>
    ""original_sents"": <list of string : sentencized original language passage>
    ""translation"": <string : machine translation of passage>
    ""translation_sents"": <list of string : sentencized machine translation of passage>
    ""translation_probs"": <list of float : log prob of machine translation by sentence, where available>
    ""repetitious_translation"": <string \in (""true"", ""false"") : automated judgment on whether machine translation is pathologically repetitious>
    ""source_lang"": <string : language ID, 2-character ISO code>

    # Source
    ""source_url"": <string : URL of the cited web source>
    ""source_text"": <string : content extracted from the scrape of the source URL>

    # Question/Answer Pairs
    ""qa_pairs"": [
      ...
      {
        ""question"": <string : generated question>
        ""passage_id"": <string : passage ID>
        ""en_answer"": <string : English answer>
        ""lang_answer"": <string : aligned original language answer>
        ""frames"": [
          ...
          {
            ""frame"": <string : frame triggered by the question>
            ""argument"": <string : detected frame arguments>
          }
          ...
        ]
        # NB: answer matches can be empty, in the case no matching span exists
        ""en_matches_in_source"": <list of int : start and end index of the English language-answer token(s) in the source document>
        ""en_match_in_passage"": <list of int : start and end index of the English language-answer token(s) in the English language translation of the passage>
        ""lang_matches_in_source"": <list of int : start and end index of the original language-answer token(s) in the source document>
        ""lang_match_in_passage"": <list of int : start and end index of the original language-answer token(s) in the original language passage>
        ""passage"": <list of string : sentencized view of the passage>
        ""en_answer_tokens"": <list of string>
        ""match_disambiguated_question"": <string : disambiguated version of question obtained by matching pronouns with article title (noisy but often helpful)>
      }
      ...
    ]
  ]
}
```

English language instances differ not in structure but in content; 
1. Fields in the block labeled ""MT"" above are naturally null (that is, they are set to falsy values in Python -- specifically `None`)
2. Since the Wiki passage only exists in English, and has no corresponding non-English ""original language"" version, answer spans also necessarily have only an English-language version (and no non-English ""original-language"" version. Therefore, fields in the `qa_pairs` block beginning with `lang_` are set to null/falsy values in Python (in this case, empty lists).


### Data Splits

MegaWika is currently split only by language, as each task will imply its own approach to filtering, sampling, downselecting, and splitting into train/test splits.

<!---
### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]
-->

## Licensing and Takedown

MegaWika 1.0 consists in part of documents scraped from across the web (based on citations linked in Wikipedia articles.)

We do not own any of the scraped text nor do we claim copyright: text drawn from Wikipedia citations are meant for research use in algorithmic design and model training.

We release this dataset and all its contents under CC-BY-SA-4.0.

### Notice and Takedown Policy:
*NB*: Should you consider that our data contains material that is owned by you and should therefore not be reproduced here, please:

- Clearly identify yourself, with detailed contact data such as an address, telephone number or email address at which you can be contacted.
- Clearly identify the copyrighted work claimed to be infringed.
- Clearly identify the material that is claimed to be infringing and information reasonably sufficient to allow us to locate the material.

And contact the authors.

*Take down*: We will comply to legitimate requests by removing the affected sources from the next release of the dataset.

## Additional Information

### Dataset Curators

Released and maintained by the Johns Hopkins University Human Language Technology Center of Excellence (JHU/HLTCOE). 
You can contact one the MegaWika authors, including [Samuel Barham](mailto:samuel.barham@jhuapl.edu), [Orion Weller](mailto:oweller2@jhu.edu),
and [Ben van Durme](mailto:vandurme@jhu.edu) with questions.

### Licensing Information

Released under the [Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/) license.

### Citation Information

```
@misc{barham2023megawika,
      title={MegaWika: Millions of reports and their sources across 50 diverse languages}, 
      author={Samuel Barham and and  Weller and Michelle Yuan and Kenton Murray and Mahsa Yarmohammadi and Zhengping Jiang and Siddharth Vashishtha and Alexander Martin and Anqi Liu and Aaron Steven White and Jordan Boyd-Graber and Benjamin Van Durme},
      year={2023},
      eprint={2307.07049},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

<!--
### Contributions

[More Information Needed]
-->",High,5.0
Summarization,PeepDaSlan9/DIDI,2.0,21.0,2023-10-18 15:51:49+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- conversational
- text-classification
- table-question-answering
- question-answering
- translation
- summarization
- text-generation
- text2text-generation
- text-to-speech
- automatic-speech-recognition
- text-to-audio
- voice-activity-detection
language:
- ko
- ru
- ig
- es
- en
- ar
- fr
- de
- am
pretty_name: 'DIDI_3.5 '
size_categories:
- 100M<n<1B
---",Low,1.0
Summarization,hltcoe/megawika-report-generation,6.0,920.0,2024-01-19 13:01:58+00:00,cc-by-sa-4.0,0.0,UNKNOWN,unknown,6.83 GB,unknown,785241,https://arxiv.org/abs/2307.07049,none,"---
license: cc-by-sa-4.0
task_categories:
- summarization
- text-retrieval
- text-generation
- text2text-generation
language:
- af
- ar
- az
- bn
- cs
- de
- en
- es
- et
- fa
- fi
- fr
- ga
- gl
- gu
- he
- hi
- hr
- id
- it
- ja
- ka
- kk
- km
- ko
- lt
- lv
- mk
- ml
- mn
- mr
- my
- ne
- nl
- pl
- ps
- pt
- ro
- ru
- si
- sl
- sv
- ta
- th
- tr
- uk
- ur
- vi
- xh
- zh
pretty_name: MegaWika-Report-Generation
---
# Dataset Card for MegaWika for Report Generation

## Dataset Description

- **Homepage:** [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika)
- **Repository:** [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika)
- **Paper:** [link](https://arxiv.org/pdf/2307.07049.pdf)
- **Point of Contact:** [Samuel Barham](samuel.barham@jhuapl.edu)

### Dataset Summary

MegaWika is a multi- and crosslingual text dataset containing 30 million Wikipedia passages with their scraped and cleaned web citations. The passages span
50 Wikipedias in 50 languages, and the articles in which the passages were originally embedded are included for convenience. Where a Wikipedia passage is in a
non-English language, an automated English translation is provided. 

This dataset provides the data for report generation / multi-document summarization with information retrieval.


### Dataset Creation

See the original [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika) repo.


### Languages

MegaWika is divided by Wikipedia language. There are 50 languages, including English, each designated by their 2-character ISO language code. 

### Languages

MegaWika is divided by Wikipedia language. There are 50 languages, including English, each designated by their 2-character ISO language code:
- `af`: Afrikaans
- `ar`: Arabic
- `az`: Azeri (Azerbaijani)
- `bn`: Bengali
- `cs`: Czech
- `de`: German (Deutsch)
- `en`: English
- `es`: Spanish (Español)
- `et`: Estonian
- `fa`: Farsi (Persian)
- `fi`: Finnish
- `fr`: French
- `ga`: Irish (Gaelic)
- `gl`: Galician
- `gu`: Gujarati
- `he`: Hebrew
- `hi`: Hindi
- `hr`: Hungarian
- `id`: Indonesian
- `it`: Italian
- `ja`: Japanese
- `ka`: Georgian (Kartvelian/Kartlian)
- `kk`: Kazakh
- `km`: Khmer
- `ko`: Korean
- `lt`: Lithuanian
- `lv`: Latvian
- `mk`: Macedonian (Makedonski)
- `ml`: Malay (Malayalam)
- `mn`: Mongolian
- `mr`: Marathi
- `my`: Burmese (Myanmar language)
- `ne`: Nepali
- `nl`: Dutch (Nederlands)
- `pl`: Polish
- `ps`: Pashto
- `pt`: Portuguese
- `ro`: Romanian
- `ru`: Russian
- `si`: Sinhalese (Sri Lankan language)
- `sl`: Slovenian
- `sv`: Swedish (Svenska)
- `ta`: Tamil
- `th`: Thai
- `tr`: Turkish
- `uk`: Ukrainian
- `ur`: Urdu
- `vi`: Vietnamese
- `xh`: Xhosa
- `zh`: Chinese (Zhōng wén)


## Dataset Structure

The dataset is divided into two main sections (1) generating the entire Wikipedia sections from multiple citations (""all"") or (2) generating segments of each section in an iterative fashion (""iterative"").
Then the dataset is divided by language pairs. Note that each language can be used cross-lingually by using the `en_gold_section_text` key.

### Data Instances

Given the rest of the fields (except for the ID) the goals is to produce the `gold_section_text` (e.g. given the title, intro, section name, and citations). 
`num_docs` is provided for filtering on the number of docs for the multi-doc summarization. Note that in the iterative setting is it just one citation. **NOTE: `num_docs` is incorrect for now, will be updated.**

### Data Fields

The detailed structure of an instance is as follows:
```
{
  ""id"": <string : a unique id for the instance>
  ""num_docs"": <int : the number of citations for this instance>
  ""title"": <string : title of original Wikipedia article>
  ""intro"": <string : text of the Wikipedia article's introduction>
  ""section_name"": <string : the name of the section to generate>
  ""previous_text"": <string : used for the iterative task format, the previous text in the section already to condition on>
  ""question"": <string : a natural language question that could be used for query-focused summarization, generated by ChatGPT>
  ""gold_section_text"": <string : the text of the original Wikipedia section, e.g. the gold label for summarization>
  ""en_gold_section_text"": <string : the English version of the text from the original Wikipedia section, e.g. the gold label for cross-lingual summarization>
  ""citations"": <list of strings : the text of the citations (e.g. reference) for the section/chunk >
}
```

## Licensing and Takedown

MegaWika 1.0 consists in part of documents scraped from across the web (based on citations linked in Wikipedia articles.)

We do not own any of the scraped text nor do we claim copyright: text drawn from Wikipedia citations are meant for research use in algorithmic design and model training.

We release this dataset and all its contents under CC-BY-SA-4.0.

### Notice and Takedown Policy:
*NB*: Should you consider that our data contains material that is owned by you and should therefore not be reproduced here, please:

- Clearly identify yourself, with detailed contact data such as an address, telephone number or email address at which you can be contacted.
- Clearly identify the copyrighted work claimed to be infringed.
- Clearly identify the material that is claimed to be infringing and information reasonably sufficient to allow us to locate the material.

And contact the authors.

*Take down*: We will comply to legitimate requests by removing the affected sources from the next release of the dataset.

## Usage

```
# all of the dataset (not recommended)
dataset = load_dataset(""hltcoe/megawika-report-generation"")

# just the `all`` section data (all splits)
dataset = load_dataset(""hltcoe/megawika-report-generation"", data_dir=""all"")

# just the `all` English test set (can replace with ""validation"" or ""train"", or other langs)
dataset = load_dataset(""hltcoe/megawika-report-generation"", data_dir=""all/en"", split=""test"")
```

### Dataset Curators

Released and maintained by the Johns Hopkins University Human Language Technology Center of Excellence (JHU/HLTCOE). 
You can contact one the MegaWika authors, including [Samuel Barham](mailto:samuel.barham@jhuapl.edu), [Orion Weller](mailto:oweller2@jhu.edu),
and [Ben van Durme](mailto:vandurme@jhu.edu) with questions.

### Licensing Information

Released under the [Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/) license.

### Citation Information

```
@misc{barham2023megawika,
      title={MegaWika: Millions of reports and their sources across 50 diverse languages}, 
      author={Samuel Barham and and  Weller and Michelle Yuan and Kenton Murray and Mahsa Yarmohammadi and Zhengping Jiang and Siddharth Vashishtha and Alexander Martin and Anqi Liu and Aaron Steven White and Jordan Boyd-Graber and Benjamin Van Durme},
      year={2023},
      eprint={2307.07049},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```",High,5.0
Summarization,2A2I/Arabic_Aya,14.0,1753.0,2024-03-15 11:08:04+00:00,apache-2.0,0.0,12.5 GB,13421772800.0,12.5 GB,13421772800.0,41472592,https://arxiv.org/abs/2402.06619,none,"---
language:
- ar
license: apache-2.0
size_categories:
- 1M<n<10M
task_categories:
- text-classification
- translation
- summarization
pretty_name: 2A
dataset_info:
- config_name: CohereForAI-aya_collection-aya_dataset
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: string
  - name: language_code
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 7555482
    num_examples: 13960
  download_size: 3687445
  dataset_size: 7555482
- config_name: CohereForAI-aya_collection-aya_human_annotated
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: test
    num_bytes: 222650
    num_examples: 250
  download_size: 120393
  dataset_size: 222650
- config_name: CohereForAI-aya_collection-templated_afrisenti
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 5070578
    num_examples: 14468
  - name: test
    num_bytes: 2674428
    num_examples: 7838
  - name: validation
    num_bytes: 643036
    num_examples: 1816
  download_size: 2330165
  dataset_size: 8388042
- config_name: CohereForAI-aya_collection-templated_mintaka
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 20413129
    num_examples: 70000
  - name: test
    num_bytes: 5799667
    num_examples: 20000
  - name: validation
    num_bytes: 2976183
    num_examples: 10000
  download_size: 6746433
  dataset_size: 29188979
- config_name: CohereForAI-aya_collection-templated_ntx_llm
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 199809
    num_examples: 111
  download_size: 34306
  dataset_size: 199809
- config_name: CohereForAI-aya_collection-templated_xcsqa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: validation
    num_bytes: 393580
    num_examples: 1000
  download_size: 137233
  dataset_size: 393580
- config_name: CohereForAI-aya_collection-templated_xlel_wd
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 97691354
    num_examples: 90760
  - name: test
    num_bytes: 15499274
    num_examples: 14791
  - name: validation
    num_bytes: 10752041
    num_examples: 9768
  download_size: 57959575
  dataset_size: 123942669
- config_name: CohereForAI-aya_collection-translated_adversarial_qa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 147727007
    num_examples: 100000
  - name: test
    num_bytes: 16108000
    num_examples: 10000
  - name: validation
    num_bytes: 14862183
    num_examples: 10000
  download_size: 52642775
  dataset_size: 178697190
- config_name: CohereForAI-aya_collection-translated_cnn_dailymail
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 3578924407
    num_examples: 1000000
  - name: test
    num_bytes: 415594340
    num_examples: 114900
  - name: validation
    num_bytes: 486698663
    num_examples: 133680
  download_size: 2209523190
  dataset_size: 4481217410
- config_name: CohereForAI-aya_collection-translated_dolly
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: gcp_source
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: alphabet
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 213140804
    num_examples: 148080
  download_size: 96189154
  dataset_size: 213140804
- config_name: CohereForAI-aya_collection-translated_flan_coqa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 245744048
    num_examples: 64090
  download_size: 124335769
  dataset_size: 245744048
- config_name: CohereForAI-aya_collection-translated_flan_cot
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 634249526
    num_examples: 919100
  download_size: 273491678
  dataset_size: 634249526
- config_name: CohereForAI-aya_collection-translated_flan_gem_wiki
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 961863533.277311
    num_examples: 271470
  download_size: 485152798
  dataset_size: 961863533.277311
- config_name: CohereForAI-aya_collection-translated_flan_lambada
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 16531932
    num_examples: 42790
  download_size: 7457248
  dataset_size: 16531932
- config_name: CohereForAI-aya_collection-translated_flan_qa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 2989244
    num_examples: 5400
  download_size: 1292664
  dataset_size: 2989244
- config_name: CohereForAI-aya_collection-translated_hotpotqa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 1154195031
    num_examples: 3554760
  - name: validation
    num_bytes: 69779681
    num_examples: 224000
  download_size: 420699282
  dataset_size: 1223974712
- config_name: CohereForAI-aya_collection-translated_joke_explaination
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 8219049
    num_examples: 7540
  download_size: 3600136
  dataset_size: 8219049
- config_name: CohereForAI-aya_collection-translated_mintaka
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 40908047
    num_examples: 140000
  - name: test
    num_bytes: 11646781
    num_examples: 40000
  - name: validation
    num_bytes: 5951801
    num_examples: 20000
  download_size: 12723211
  dataset_size: 58506629
- config_name: CohereForAI-aya_collection-translated_mlqa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: test
    num_bytes: 331062576
    num_examples: 231800
  - name: validation
    num_bytes: 31900260
    num_examples: 22960
  download_size: 146571384
  dataset_size: 362962836
- config_name: CohereForAI-aya_collection-translated_nqopen
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 397677612
    num_examples: 1758500
  - name: validation
    num_bytes: 16780970
    num_examples: 72200
  download_size: 136208663
  dataset_size: 414458582
- config_name: CohereForAI-aya_collection-translated_paws
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 303643575
    num_examples: 494010
  - name: test
    num_bytes: 49242541
    num_examples: 80000
  - name: validation
    num_bytes: 49475307
    num_examples: 80000
  download_size: 66436419
  dataset_size: 402361423
- config_name: CohereForAI-aya_collection-translated_piqa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 113290227
    num_examples: 161130
  - name: validation
    num_bytes: 12924744
    num_examples: 18380
  download_size: 45954644
  dataset_size: 126214971
- config_name: CohereForAI-aya_collection-translated_soda
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 6230916321
    num_examples: 11915820
  - name: test
    num_bytes: 777982873
    num_examples: 1489680
  - name: validation
    num_bytes: 772817056
    num_examples: 1463460
  download_size: 2804874077
  dataset_size: 7781716250
- config_name: CohereForAI-aya_collection-translated_wiki_split
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 6349516377
    num_examples: 9899440
  - name: test
    num_bytes: 32058254
    num_examples: 50000
  - name: validation
    num_bytes: 32284536
    num_examples: 50000
  download_size: 2446037624
  dataset_size: 6413859167
- config_name: CohereForAI-aya_collection-translated_wikiqa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 5014300
    num_examples: 10400
  - name: test
    num_bytes: 1378807
    num_examples: 2930
  - name: validation
    num_bytes: 685770
    num_examples: 1400
  download_size: 2872586
  dataset_size: 7078877
- config_name: CohereForAI-aya_collection-translated_xlel_wd
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 5250663186
    num_examples: 5231120
  - name: test
    num_bytes: 721821743
    num_examples: 729740
  - name: validation
    num_bytes: 635907993
    num_examples: 632640
  download_size: 3091503409
  dataset_size: 6608392922
- config_name: CohereForAI-aya_dataset
  features:
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: language
    dtype: string
  - name: language_code
    dtype: string
  - name: annotation_type
    dtype: string
  - name: user_id
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 8314232
    num_examples: 13960
  - name: test
    num_bytes: 246400
    num_examples: 250
  download_size: 3778631
  dataset_size: 8560632
- config_name: CohereForAI-aya_evaluation_suite-aya_human_annotated
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: test
    num_bytes: 222650
    num_examples: 250
  download_size: 120393
  dataset_size: 222650
- config_name: CohereForAI-aya_evaluation_suite-dolly_human_edited
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: source_id
    dtype: int64
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: test
    num_bytes: 188495
    num_examples: 200
  download_size: 100291
  dataset_size: 188495
- config_name: CohereForAI-aya_evaluation_suite-dolly_machine_translated
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: source_id
    dtype: int64
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: test
    num_bytes: 3491803
    num_examples: 2000
  download_size: 1762303
  dataset_size: 3491803
configs:
- config_name: CohereForAI-aya_collection-aya_dataset
  data_files:
  - split: train
    path: CohereForAI-aya_collection-aya_dataset/train-*
- config_name: CohereForAI-aya_collection-aya_human_annotated
  data_files:
  - split: test
    path: CohereForAI-aya_collection-aya_human_annotated/test-*
- config_name: CohereForAI-aya_collection-templated_afrisenti
  data_files:
  - split: train
    path: CohereForAI-aya_collection-templated_afrisenti/train-*
  - split: test
    path: CohereForAI-aya_collection-templated_afrisenti/test-*
  - split: validation
    path: CohereForAI-aya_collection-templated_afrisenti/validation-*
- config_name: CohereForAI-aya_collection-templated_mintaka
  data_files:
  - split: train
    path: CohereForAI-aya_collection-templated_mintaka/train-*
  - split: test
    path: CohereForAI-aya_collection-templated_mintaka/test-*
  - split: validation
    path: CohereForAI-aya_collection-templated_mintaka/validation-*
- config_name: CohereForAI-aya_collection-templated_ntx_llm
  data_files:
  - split: train
    path: CohereForAI-aya_collection-templated_ntx_llm/train-*
- config_name: CohereForAI-aya_collection-templated_xcsqa
  data_files:
  - split: validation
    path: CohereForAI-aya_collection-templated_xcsqa/validation-*
- config_name: CohereForAI-aya_collection-templated_xlel_wd
  data_files:
  - split: train
    path: CohereForAI-aya_collection-templated_xlel_wd/train-*
  - split: test
    path: CohereForAI-aya_collection-templated_xlel_wd/test-*
  - split: validation
    path: CohereForAI-aya_collection-templated_xlel_wd/validation-*
- config_name: CohereForAI-aya_collection-translated_adversarial_qa
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_adversarial_qa/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_adversarial_qa/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_adversarial_qa/validation-*
- config_name: CohereForAI-aya_collection-translated_cnn_dailymail
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_cnn_dailymail/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_cnn_dailymail/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_cnn_dailymail/validation-*
- config_name: CohereForAI-aya_collection-translated_dolly
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_dolly/train-*
- config_name: CohereForAI-aya_collection-translated_flan_coqa
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_flan_coqa/train-*
- config_name: CohereForAI-aya_collection-translated_flan_cot
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_flan_cot/train-*
- config_name: CohereForAI-aya_collection-translated_flan_gem_wiki
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_flan_gem_wiki/train-*
- config_name: CohereForAI-aya_collection-translated_flan_lambada
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_flan_lambada/train-*
- config_name: CohereForAI-aya_collection-translated_flan_qa
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_flan_qa/train-*
- config_name: CohereForAI-aya_collection-translated_hotpotqa
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_hotpotqa/train-*
  - split: validation
    path: CohereForAI-aya_collection-translated_hotpotqa/validation-*
- config_name: CohereForAI-aya_collection-translated_joke_explaination
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_joke_explaination/train-*
- config_name: CohereForAI-aya_collection-translated_mintaka
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_mintaka/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_mintaka/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_mintaka/validation-*
- config_name: CohereForAI-aya_collection-translated_mlqa
  data_files:
  - split: test
    path: CohereForAI-aya_collection-translated_mlqa/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_mlqa/validation-*
- config_name: CohereForAI-aya_collection-translated_nqopen
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_nqopen/train-*
  - split: validation
    path: CohereForAI-aya_collection-translated_nqopen/validation-*
- config_name: CohereForAI-aya_collection-translated_paws
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_paws/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_paws/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_paws/validation-*
- config_name: CohereForAI-aya_collection-translated_piqa
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_piqa/train-*
  - split: validation
    path: CohereForAI-aya_collection-translated_piqa/validation-*
- config_name: CohereForAI-aya_collection-translated_soda
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_soda/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_soda/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_soda/validation-*
- config_name: CohereForAI-aya_collection-translated_wiki_split
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_wiki_split/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_wiki_split/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_wiki_split/validation-*
- config_name: CohereForAI-aya_collection-translated_wikiqa
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_wikiqa/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_wikiqa/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_wikiqa/validation-*
- config_name: CohereForAI-aya_collection-translated_xlel_wd
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_xlel_wd/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_xlel_wd/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_xlel_wd/validation-*
- config_name: CohereForAI-aya_dataset
  data_files:
  - split: train
    path: CohereForAI-aya_dataset/train-*
  - split: test
    path: CohereForAI-aya_dataset/test-*
- config_name: CohereForAI-aya_evaluation_suite-aya_human_annotated
  data_files:
  - split: test
    path: CohereForAI-aya_evaluation_suite-aya_human_annotated/test-*
- config_name: CohereForAI-aya_evaluation_suite-dolly_human_edited
  data_files:
  - split: test
    path: CohereForAI-aya_evaluation_suite-dolly_human_edited/test-*
- config_name: CohereForAI-aya_evaluation_suite-dolly_machine_translated
  data_files:
  - split: test
    path: CohereForAI-aya_evaluation_suite-dolly_machine_translated/test-*
---
# Dataset Card for : Arabic Aya (2A)

<!-- Provide a quick summary of the dataset. -->

<!-- This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).-->

## **Arabic Aya (2A) : A Curated Subset of the Aya Collection for Arabic Language Processing**

### Dataset Sources & Infos
- **Data Origin**: Derived from 69 subsets of the original Aya datasets : [CohereForAI/aya_collection](https://huggingface.co/datasets/CohereForAI/aya_collection), [CohereForAI/aya_dataset](https://huggingface.co/datasets/CohereForAI/aya_dataset), and [CohereForAI/aya_evaluation_suite](https://huggingface.co/datasets/CohereForAI/aya_evaluation_suite).
- **Languages**: Modern Standard Arabic (MSA) and a variety of Arabic dialects ( 'arb', 'arz', 'ary', 'ars', 'knc', 'acm', 'apc', 'aeb', 'ajp', 'acq' )
- **Applications**: `Language Modeling`,  `Text Classification`, `Sentiment Analysis`, `Dialect Identification`, `Translation`
- **Paper:** [2402.06619](https://huggingface.co/papers/2402.06619)
- **Maintainer:** [Elfilali Ali](https://huggingface.co/Ali-C137)
- **License:** Apache-2.0

### Overview
`Arabic Aya` is a meticulously curated dataset derived from the comprehensive Aya collection by [CohereForAI](https://huggingface.co/CohereForAI), specifically focusing on Arabic text data. This dataset aggregates content from the [CohereForAI/aya_collection](https://huggingface.co/datasets/CohereForAI/aya_collection), [CohereForAI/aya_dataset](https://huggingface.co/datasets/CohereForAI/aya_dataset), and [CohereForAI/aya_evaluation_suite](https://huggingface.co/datasets/CohereForAI/aya_evaluation_suite), filtering out all but the Arabic content, including both Modern Standard Arabic (MSA) and various regional dialects. 

### Purpose
The aim of 'Arabic Aya' is to provide researchers, technologists, and linguists with a ready-to-use Arabic text resource, significantly reducing the time and effort required for data preprocessing in NLP and AI projects focused on the Arabic language.
- Use the Aya datasets out of the box for your Arabic applications and research 😀

### Usage
This dataset serves as a foundational tool for those embarking on Arabic language projects, from academic research to commercial applications. By providing a pre-filtered source of Arabic text, 'Arabic Aya' enables users to dive straight into model training, analysis, and application development without the preliminary hassle of data cleaning and language filtering.

#### Use with HuggingFace's datasets library
To load this dataset with Datasets, you'll need to install Datasets as `pip install datasets --upgrade` and then use a similar code to the following:

```python
from datasets import load_dataset

dataset = load_dataset(""2A2I/Arabic_Aya"", ""CohereForAI-aya_collection-templated_mintaka"")
```
In the above code snippet, ""CohereForAI-aya_collection-templated_mintaka"" refers to the arabic version (100k rows) of the original ""templated_mintaka"" subset (780k rows) of the aya_collection. You can load other subsets by specifying its name at the time of loading the dataset.


### Access and Contribution
Available on the Hugging Face Hub under [2A2I/Arabic_Aya](https://huggingface.co/datasets/2A2I/Arabic_Aya), 'Arabic Aya' invites contributions from the community. Users are encouraged to offer feedback, suggest improvements.

### Support and Collaboration
We are committed to fostering an inclusive and supportive environment around Arabic AI and NLP research. For support, collaboration, or queries regarding the dataset, please reach out through the Hugging Face Hub's discussion section or reach out at [2A2I Contact Email](arabic.ai.initiative@gmail.com).




# Original Dataset Card of Aya by CohereForAI


![Aya Header](https://huggingface.co/datasets/CohereForAI/aya_collection/resolve/main/aya_header.png)

# Dataset Summary
The Aya Collection is a massive multilingual collection consisting of 513 million instances of prompts and completions covering a wide range of tasks.
This collection incorporates instruction-style templates from fluent speakers and applies them to a curated list of datasets, as well as translations of instruction-style datasets into 101 languages. Aya Dataset, a human-curated multilingual instruction and response dataset, is also part of this collection. See our paper for more details regarding the collection.   

- **Curated by:** Contributors of [Aya Open Science Intiative](https://cohere.com/research/aya)
- **Language(s):** 115 languages
- **License:** [Apache 2.0](https://opensource.org/license/apache-2-0)
  
- **Aya Datasets Family:**
| Name | Explanation |
|------|--------------|
| [aya_dataset](https://huggingface.co/datasets/CohereForAI/aya_dataset) | Human-annotated multilingual instruction finetuning dataset, comprising over 204K instances across 65 languages. |
| [aya_collection](https://huggingface.co/datasets/CohereForAI/aya_collection) | Created by applying instruction-style templates from fluent speakers to 44 datasets, including translations of 19 instruction-style datasets into 101 languages.|
| [aya_evaluation_suite](https://huggingface.co/datasets/CohereForAI/aya_evaluation_suite) | A diverse evaluation set for multilingual open-ended generation, featuring 250 culturally grounded prompts in 7 languages, 200 translated prompts in 24 languages, and human-edited versions selected for cross-cultural relevance from English Dolly in 6 languages.|


# Dataset
The `Aya Collection` is a comprehensive, large corpus of datasets that can be used by researchers around the world to train multilingual models. Our goal is only to include datasets with permissive licensing for manipulation and redistribution.

The `Aya Collection` consists of three different sources of data:

1. Templated data: We collaborated with fluent speakers to create templates that allowed for the automatic expansion of existing datasets into various languages.
2. Translated data: We translated a hand-selected subset of 19 datasets into 101 languages (114 dialects) using the NLLB 3.3B parameter machine translation model.
3. Aya Dataset: We release the [Aya Dataset](https://huggingface.co/datasets/CohereForAI/aya_dataset) as a subset of the overall collection. This is the only dataset in the collection that is human-annotated in its entirety.

## Load with Datasets
To load ",High,6.0
Summarization,Mouwiya/SANAD,1.0,12.0,2024-05-24 08:53:56+00:00,none,0.0,86.4 MB,90596966.4,86.4 MB,90596966.4,45500,none,none,"---
dataset_info:
  features:
  - name: text
    dtype: string
  - name: label
    dtype: string
  splits:
  - name: train
    num_bytes: 181482477
    num_examples: 45500
  download_size: 86389093
  dataset_size: 181482477
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- text-classification
- translation
- summarization
language:
- ar
- en
size_categories:
- 10K<n<100K
---

# Arabic News Articles Dataset

## About Dataset
Context
SANAD Dataset is a large collection of Arabic news articles that can be used in different Arabic NLP tasks such as Text Classification and Word Embedding. The articles were collected using Python scripts written specifically for three popular news websites: AlKhaleej, AlArabiya and Akhbarona.

All datasets have seven categories [Culture, Finance, Medical, Politics, Religion, Sports and Tech], except AlArabiya which doesn’t have [Religion]. SANAD contains a total number of 190k+ articles.

## Content
a subset Khaleej (45500 articles in 7 categories) of SANAD. Labels are categorized in: Culture, Finance, Medical, Politics,Religion,Sports,Tech.

## Acknowledgements
We wouldn't be here without the help of Omar Einea, Ashraf Elnagar, Ridhwan Al-Debsi.
https://doi.org/10.1016/j.dib.2019.104076

## Contact
**Mouwiya S. A. Al-Qaisieh** 
mo3awiya@gmail.com",Low,1.0
Summarization,Mohamed-Sami/cleaned_xlsum_arabic,0.0,30.0,2024-05-22 19:41:30+00:00,none,0.0,83.5 MB,87556096.0,83.5 MB,87556096.0,41983,none,none,"---
dataset_info:
  features:
  - name: summary
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 159729666.22196755
    num_examples: 32877
  - name: test
    num_bytes: 19638663.670078907
    num_examples: 4547
  - name: validation
    num_bytes: 19736613.955214333
    num_examples: 4559
  download_size: 83453277
  dataset_size: 199104943.84726077
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: test
    path: data/test-*
  - split: validation
    path: data/validation-*
task_categories:
- summarization
language:
- ar
---",Low,1.0
Summarization,Mohamedfadil369/BrainData,1.0,16.0,2024-06-08 02:36:18+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- translation
- summarization
- text2text-generation
language:
- ar
- en
tags:
- legal
- finance
- medical
- webdataset
pretty_name: BrainData
size_categories:
- n>1T
---

# Dataset Card for BrainData

<!-- Provide a quick summary of the dataset. -->

This dataset card provides detailed information about the BrainData dataset. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->

BrainData is a comprehensive dataset designed for multiple NLP tasks including translation, summarization, and text-to-text generation. It encompasses a variety of domains such as legal, finance, and medical, with content available in both Arabic and English. This extensive dataset is ideal for training robust multilingual models.

- **Curated by:** Dr. Mohamed El Fadil
- **Funded by:** BRAINSAIT LTD
- **Shared by:** Dr. Mohamed El Fadil
- **Language(s) (NLP):** Arabic, English
- **License:** Apache-2.0

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [BrainData GitHub Repository](https://github.com/brainsait/dataset)
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

BrainData can be used for training models in translation, summarization, and text generation. It is particularly useful for applications in the legal, finance, and medical sectors, where multilingual support is required.

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

The dataset is not suitable for tasks unrelated to text processing, such as image recognition or speech-to-text. Additionally, it should not be used for generating inappropriate or harmful content.

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

The dataset includes text data in both Arabic and English, covering multiple domains. It is structured to support easy access and processing, with clear separations between different task categories and languages.

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

BrainData was created to address the need for high-quality multilingual datasets in the fields of legal, finance, and medical text processing. It aims to facilitate the development of advanced NLP models that can operate across different languages and domains.

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

The source data includes legal documents, financial reports, medical records, and web data, ensuring a diverse and representative sample of text for each domain.

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

Data was collected from reputable sources in the legal, finance, and medical fields. It underwent thorough filtering and normalization to ensure consistency and quality. Tools such as Python's NLTK and SpaCy libraries were used for text preprocessing.

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

The source data was produced by professionals and organizations in the legal, finance, and medical sectors, ensuring authoritative and accurate content.

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

Annotations were performed by domain experts using tools like Prodigy and Labelbox. Guidelines were provided to ensure consistency, and inter-annotator agreement was regularly checked to maintain quality.

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

The annotators were professionals with expertise in legal, finance, and medical fields, ensuring high-quality and accurate annotations.

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

The dataset contains sensitive information, particularly in the medical and financial domains. All personal identifiers have been removed or anonymized to protect privacy.

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

The dataset may contain biases inherent to the source data, such as regional or institutional biases. Users should be aware of these limitations and consider them when developing models.

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should perform bias analysis and fairness checks when using the dataset, especially for critical applications. Regular updates and retraining with new data are recommended to mitigate biases.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

```bibtex
@misc{braindata2024dataset,
  title={BrainData: A Multilingual Dataset for Legal, Finance, and Medical Text Processing},
  author={El Fadil, Mohamed and BrainSAIT Team},
  year={2024},
  url={https://github.com/brainsait/dataset}
}
",High,4.0
Summarization,ClusterlabAi/InstAr-500k,12.0,133.0,2024-07-30 16:41:57+00:00,apache-2.0,3.0,375 MB,393216000.0,375 MB,393216000.0,481281,none,none,"---
language:
- ar
license: apache-2.0
size_categories:
- 100K<n<1M
task_categories:
- question-answering
- summarization
- text-classification
dataset_info:
  features:
  - name: uuid
    dtype: string
  - name: source
    dtype: string
  - name: task
    dtype: string
  - name: type
    dtype: string
  - name: topic
    dtype: string
  - name: system
    dtype: string
  - name: instruction
    dtype: string
  - name: output
    dtype: string
  splits:
  - name: train
    num_bytes: 1093175218
    num_examples: 481281
  download_size: 374662094
  dataset_size: 1093175218
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
---
# Dataset Card for ""InstAr-500k""

The dataset comprises almost 500,000 Arabic instructions and responses designed for fine-tuning large language models (LLMs) for Arabic NLP tasks. It includes a combination of synthetic and human-crafted data across various domains and instruction types. This extensive dataset aims to improve the performance of LLMs on Arabic-specific tasks

## Dataset Summary 

<table>
  <tr>
    <th>Type</th>
    <th>Task</th>
    <th>Number of Samples</th>
    <th>Percentage of Samples</th>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Classification</td>
    <td>220,131</td>
    <td>45.7386%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Closed QA</td>
    <td>42,650</td>
    <td>8.86177%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Explanation</td>
    <td>2,000</td>
    <td>0.415558%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Extraction</td>
    <td>642</td>
    <td>0.133394%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Extraction and Explanation</td>
    <td>4,682</td>
    <td>0.97282%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Generation</td>
    <td>2,683</td>
    <td>0.557471%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Open QA</td>
    <td>14,410</td>
    <td>2.99409%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Rewrite</td>
    <td>5,132</td>
    <td>1.06632%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Summarization</td>
    <td>796</td>
    <td>0.165392%</td>
  </tr>
  <tr>
    <td>Generated</td>
    <td>Text Completion</td>
    <td>1,423</td>
    <td>0.295669%</td>
  </tr>
  <tr>
    <td>Human-crafted</td>
    <td>Brainstorming</td>
    <td>14,000</td>
    <td>2.9089%</td>
  </tr>
  <tr>
    <td>Human-crafted</td>
    <td>Closed QA</td>
    <td>1,273</td>
    <td>0.264502%</td>
  </tr>
  <tr>
    <td>Human-crafted</td>
    <td>Explanation</td>
    <td>6,000</td>
    <td>1.24667%</td>
  </tr>
  <tr>
    <td>Human-crafted</td>
    <td>Mixed</td>
    <td>33,054</td>
    <td>6.86792%</td>
  </tr>
  <tr>
    <td>Human-crafted</td>
    <td>Open QA</td>
    <td>132,405</td>
    <td>27.511%</td>
  </tr>
</table>

## Dataset Structure
- `uuid(str)`: Unique number of each instruction-output pair.
- `instruction(str)`: Required instruction.
- `output(str)`: Response for the given instruction.
- `system (str)`: Prompts guiding the system in generating responses.
- `topic(str)`: Subject area of the instruction.
- `task(str)`: Nature of the task.
- `source(str)`: Origin of the data.

##  Loading The Dataset
You can download the dataset directly from HuggingFace or use the following code:

```python
from datasets import load_dataset
instar = load_dataset('ClusterlabAi/InstAr-500k')
```

**Dataset Sample**

<p align=""left"" width=""100%"">
<img src=""data:image/png;base64,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"" alt=""Dataset_Sample"" style=""width: 60%; display: block; margin: auto;""/>
</p>

## Dataset Construction

The [InstAr-500k](https://huggingface.co/datasets/ClusterlabAi/InstAr-500k) dataset combines synthetic and human-crafted data to ensure a diverse range of instructions and responses. The synthetic data was generated using the Command R+ model, while human-crafted data was sourced from [101 Billion Arabic Words dataset](https://huggingface.co/datasets/ClusterlabAi/101_billion_arabic_words_dataset)

<p align=""left"" width=""100%"">
<a ><img src=""data:image/png;base64,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",High,4.0
Summarization,Washedashore/thepower,2.0,11.0,2024-07-15 02:56:09+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- text-classification
- question-answering
- summarization
- text-generation
- table-question-answering
- token-classification
- zero-shot-classification
- translation
- fill-mask
- depth-estimation
- automatic-speech-recognition
language:
- aa
- ab
- ae
- af
- ak
- am
- an
- ar
- ay
- bm
- as
- av
- az
- br
- en
- ce
- ba
- be
- bg
- bh
- bi
- bn
- cy
tags:
- chemistry
- biology
- medical
- finance
- art
- code
- synthetic
pretty_name: pow
size_categories:
- 100B<n<1T
---
# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [Washed Ashore Relics]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Summarization,bushra1dajam/news_articles,2.0,11.0,2024-07-24 21:36:37+00:00,unknown,0.0,22.4 MB,23488102.4,10.9 MB,11429478.4,8378,none,none,"---
license: unknown
task_categories:
- text-classification
- summarization
language:
- ar
tags:
- medical
- sports
- finance
size_categories:
- n<1K
---

# News Articles Classification Dataset

This dataset consists of news articles labeled with corresponding categories for classification tasks.

## Overview

The news articles classification dataset is a collection of articles sourced from various news outlets, each labeled with a specific category. The dataset is designed for tasks such as text classification, topic modeling, and sentiment analysis.

## Dataset Information

- **Name**: News Articles Classification Dataset
- **Description**: Collection of news articles labeled with categories
- **Source**: Various news outlets
- **Size**: Number of articles
- **Format**: Text data with corresponding labels
- **License**: 

## Data Fields

The dataset includes the following fields:
- **Text**: The content of the news article
- **Category**: The category or label assigned to the article

## Data Splits

The dataset is split into the following categories:
- Training Set
- Validation Set
- Test Set

## Dataset Creation

The news articles were collected from multiple news sources and manually labeled with appropriate categories for classification.

## Dataset Cleaning

The dataset underwent text preprocessing steps, including removing special characters, tokenization, and lowercasing.
",Medium,2.0
Summarization,faisaltareque/XL-HeadTags,3.0,98.0,2024-08-16 15:30:07+00:00,cc-by-sa-4.0,3.0,4.82 GB,5175435591.68,4.82 GB,5175435591.68,415117,none,"['https://aclanthology.org/2024.findings-acl.771/', 'https://aclanthology.org/2024.findings-acl.771/).', 'https://aclanthology.org/2024.findings-acl.771"",']","---
dataset_info:
  features:
  - name: ID
    dtype: string
  - name: Headline
    dtype: string
  - name: Article
    dtype: string
  - name: Language Code
    dtype: string
  - name: Image Captions
    dtype: string
  - name: Topic Words
    dtype: string
  - name: Image Retrieved K-5
    dtype: string
  - name: Caption Retrieved K-5
    dtype: string
  - name: Image Retrieved K-10
    dtype: string
  - name: Caption Retrieved K-10
    dtype: string
  - name: Image Retrieved K-15
    dtype: string
  - name: Caption Retrieved K-15
    dtype: string
  - name: Input Prefix
    dtype: string
  splits:
  - name: train
    num_bytes: 8424009059
    num_examples: 394353
  - name: val
    num_bytes: 111870321
    num_examples: 5187
  - name: test
    num_bytes: 330319605
    num_examples: 15577
  download_size: 4817462439
  dataset_size: 8866198985
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: val
    path: data/val-*
  - split: test
    path: data/test-*
license: cc-by-sa-4.0
task_categories:
- summarization
- text2text-generation
- sentence-similarity
language:
- en
- pt
- es
- ru
- uk
- pa
- gu
- hi
- mr
- bn
- fr
- tr
- ar
- zh
- te
- ta
- ne
- fa
- ur
- id
tags:
- headline-generation
- tags-generation
- multilingual
size_categories:
- 100K<n<1M
---

# Dataset Card for XL-HeadTags Corpus

## Dataset Source
We have used [M3LS](https://github.com/Raghvendra-14/M3LS) and [XL-Sum](https://github.com/csebuetnlp/xl-sum) as source for this dataset.

## Dataset Description

- **Homepage:** 
- **Repository:** https://github.com/faisaltareque/XL-HeadTags
- **Paper:** https://aclanthology.org/2024.findings-acl.771/
- **Leaderboard:** 
- **Point of Contact:** [Faisal Tareque Shohan](mailto:faisaltareque@hotmail.com)

### Dataset Summary
 We provide **XL-HeadTags**, a large-scale news headline and tags generation dataset. The dataset consists of 20 languages across six diverse language families. It contains 415K news headline-article pairs with auxiliary information such as image captions, topic words [(read more)](https://aclanthology.org/2024.findings-acl.771/).


## Dataset Structure

### Data Instances

One example from the train split of the dataset is given below in JSON format.
```
{
    ""ID"": ""uk-politics-31707197"",
    ""Headline"": ""Labour backs 'Turing law' to quash historical gay convictions"",
    ""Article"": ""The Labour leader said a new law would allow family and friends of deceased men to seek the quashing of historical convictions for ""gross indecency"".Legislation would be known as ""Turing's Law"" in memory of Alan Turing, he said.The Enigma code-breaker was convicted of ""gross indecency"" in 1952 and was only given a posthumous pardon in 2013.Homosexuality was illegal until it was decriminalised in England in 1967. Mr Turing was convicted for gross indecency in 1952 in connection with an affair with a 19-year-old man, after which he was chemically castrated.The conviction meant he lost his security clearance and had to stop the code-cracking work that had proved vital to the Allies in World War Two.The mathematician was only given a royal pardon in 2013, nearly 60 years after his death by suicide in 1954. This followed an official apology by former prime minister Gordon Brown in 2009 for how Mr Turing had been treated. Relatives of Mr Turing have led a high-profile campaign to secure pardons for the 49,000 other men convicted under historical indecency laws. Announcing his support for the move, Mr Miliband said: ""What was right for Alan Turing's family should be right for other families as well.""The next Labour government will extend the right individuals already have to overturn convictions that society now see as grossly unfair to the relatives of those convicted who have passed away.""Asked whether David Cameron would back Mr Miliband's proposals, No 10 said the prime minister ""will always continue to look carefully at what more can be done to right these wrongs"".A spokesman pointed out that the coalition government had already passed legislation to allow individuals with historical convictions or cautions for certain homosexual activities to apply for them to be removed from criminal records.""It was this government that introduced that 2012 act,"" said the spokesman. ""It was under this government that Mr Turing received the pardon through the use of the royal prerogative.""A pardon is only normally granted when the person is innocent of the offence and where a request has been made by someone with a vested interest such as a family member. But, in Mr Turing's case, a pardon was issued without either requirement being met, after an intervention by Lord Chancellor Chris Grayling."",
    ""Language Code"": ""eng"",
    ""Image Captions"": ""Relatives of Alan Turing have been pressing for historic convictions to be quashed"",
    ""Topic Words"": ""Ed Miliband|Alan Turing|Labour Party"",
    ""Image Retrieved K-5"": #Top 5 sentences from the article that are most semantically similar to the image.
    ""Image Retrieved K-10"": #Top 10 sentences from the article that are most semantically similar to the image.
    ""Image Retrieved K-15"": #Top 15 sentences from the article that are most semantically similar to the image.
    ""Caption Retrieved K-5"": #Top 5 sentences from the article that are most semantically similar to the image caption.
    ""Caption Retrieved K-10"": #Top 10 sentences from the article that are most semantically similar to the image caption.
    ""Caption Retrieved K-15"": #Top 15 sentences from the article that are most semantically similar to the image caption.
    ""Input Prefix"": ""Generate Headline and Three Tag words:""
}
```

### Data Fields

- `ID`: Unique identifier for the news article. Can be used to map articles with M3LS and XL-Sum datasets.
- `Headline`: The headline of the news article.
- `Article`: The main body of the news article.
- `Language Code`: The language code of the news article.
- `Image Captions`: The captions of the images in the news article. Captions are Seperated by `|`.
- `Topic Words`: The topic words of the news article. Topic words are Seperated by `|`.
- `Image Retrieved K-5`: Top 5 sentences from the article that are most semantically similar to the image.
- `Image Retrieved K-10`: Top 10 sentences from the article that are most semantically similar to the image.
- `Image Retrieved K-15`: Top 15 sentences from the article that are most semantically similar to the image.
- `Caption Retrieved K-5`: Top 5 sentences from the article that are most semantically similar to the image caption.
- `Caption Retrieved K-10`: Top 10 sentences from the article that are most semantically similar to the image caption.
- `Caption Retrieved K-15`: Top 15 sentences from the article that are most semantically similar to the image caption.
- `Input Prefix`: The input prefix during training the model.

### Data Splits

The **XL-HeadTags** dataset has 415117 news samples. We maintain the ratio of (95% - 394,353), (1% - 5187), and (4% - 15,577) samples from all the language to construct the train, validation, and test set, respectively. During training we 70:30 task specific prefix mixture ratio.

### Discussion of Ethics

We considered some ethical aspects while scraping the data. We requested data at a reasonable rate 
without any intention of a DDoS attack. Moreover, for each website, we read the instructions listed in 
robots.txt to check whether we can crawl the intended content. We tried to minimize offensive texts in 
the data by explicitly crawling the sites where such contents are minimal. Further, we removed the 
Personal Identifying Information (PII) such as name, phone number, email address, _etc._ from the corpus.

### Other Known Limitations

Our dataset relies on auxiliary information such as image, image captions and topic words to achieve superior 
performance in generating news headlines and tags. However, it is quite common to include images and extra information 
(e.g., topic words) to increase the article’s visibility, support, and context.

## Additional Information

<!---
### Dataset Curators

[More Information Needed]
-->

### Licensing Information

Contents of this repository are restricted to only non-commercial research purposes under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). 
Copyright of the dataset contents belongs to the original copyright holders.

### Citation Information
If you find this work useful for your research, please consider citing:
```
@inproceedings{shohan-etal-2024-xl,
    title = ""{XL}-{H}ead{T}ags: Leveraging Multimodal Retrieval Augmentation for the Multilingual Generation of News Headlines and Tags"",
    author = ""Shohan, Faisal  and
      Nayeem, Mir Tafseer  and
      Islam, Samsul  and
      Akash, Abu Ubaida  and
      Joty, Shafiq"",
    editor = ""Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek"",
    booktitle = ""Findings of the Association for Computational Linguistics ACL 2024"",
    month = aug,
    year = ""2024"",
    address = ""Bangkok, Thailand and virtual meeting"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.findings-acl.771"",
    pages = ""12991--13024""
}
```

### Contributors
- Faisal Tareque Shohan (faisaltareque@hotmail.com)
- Mir Tafseer Nayeem (mnayeem@ualberta.ca)
- Samsul Islam (samsulratul98@gmail.com)
- Abu Ubaida Akash (abu.ubaida.akash@usherbrooke.ca)
- Shafiq Joty (sjoty@salesforce.com)

### Acknowledgements
- Mir Tafseer Nayeem is supported by [Huawei](https://digitalpower.huawei.com/en/) Doctoral Fellowship.
",High,5.0
Summarization,data-silence/sumnews,2.0,54.0,2024-08-21 17:45:16+00:00,cc-by-nc-sa-4.0,16.0,742 MB,778043392.0,742 MB,778043392.0,321697,none,none,"---
language:
- am
- ar
- az
- bn
- my
- zh
- en
- fr
- gu
- ha
- hi
- ig
- id
- ja
- rn
- ko
- ky
- mr
- ne
- om
- ps
- fa
- pcm
- pt
- pa
- ru
- gd
- sr
- si
- so
- es
- sw
- ta
- te
- th
- ti
- tr
- uk
- ur
- uz
- vi
- cy
- yo
license:
- cc-by-nc-sa-4.0
multilinguality:
- multilingual
size_categories:
- 100K<n<1M
task_categories:
- summarization
- text-generation
pretty_name: sum-any-news-dataset
dataset_info:
  features:
  - name: title
    dtype: string
  - name: resume
    dtype: string
  - name: news
    dtype: string
  splits:
  - name: train
    num_bytes: 1276733873
    num_examples: 289524
  - name: test
    num_bytes: 143098117
    num_examples: 32173
  download_size: 742417882
  dataset_size: 1419831990
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: test
    path: data/test-*
tags:
- news
- media
- conditional-text-generation
---

# Dataset Card for ""Sum-any-news""

## Dataset Description

This dataset is a set for fine-tuning the summarization task on the “google/mt5-base” model and its derivatives. 
The set is based on the well-known dataset [“csebuetnlp/xlsum”](https://huggingface.co/datasets/csebuetnlp/xlsum), but is a limited 20,000 examples of news articles from it. 
In addition, about 20 thousand news articles in Russian for the period from 2019-01-01 to 2024-08-01 have been added to the model (multiple sources).",Medium,3.0
Summarization,1-800-SHARED-TASKS/xlsum-subset,0.0,143.0,2024-09-26 13:12:19+00:00,cc-by-nc-sa-4.0,0.0,1.38 GB,1481763717.12,20.7 MB,21705523.2,45,https://arxiv.org/abs/1607.01759,"['https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413/)', 'https://aclanthology.org/2021.findings-acl.413"",']","---
annotations_creators:
- found
language_creators:
- found
language:
- am
- ar
- az
- bn
- my
- zh
- en
- fr
- gu
- ha
- hi
- ig
- id
- ja
- rn
- ko
- ky
- mr
- ne
- om
- ps
- fa
- pcm
- pt
- pa
- ru
- gd
- sr
- si
- so
- es
- sw
- ta
- te
- th
- ti
- tr
- uk
- ur
- uz
- vi
- cy
- yo
license:
- cc-by-nc-sa-4.0
multilinguality:
- multilingual
size_categories:
- 1M<n<10M
source_datasets:
- original
task_categories:
- summarization
- text-generation
task_ids: []
paperswithcode_id: xl-sum
pretty_name: XL-Sum
tags:
- conditional-text-generation
---

# Dataset Card for ""XL-Sum""

## Table of Contents
- [Dataset Card Creation Guide](#dataset-card-creation-guide)
  - [Table of Contents](#table-of-contents)
  - [Dataset Description](#dataset-description)
    - [Dataset Summary](#dataset-summary)
    - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
    - [Languages](#languages)
  - [Dataset Structure](#dataset-structure)
    - [Data Instances](#data-instances)
    - [Data Fields](#data-fields)
    - [Data Splits](#data-splits)
  - [Dataset Creation](#dataset-creation)
    - [Curation Rationale](#curation-rationale)
    - [Source Data](#source-data)
      - [Initial Data Collection and Normalization](#initial-data-collection-and-normalization)
      - [Who are the source language producers?](#who-are-the-source-language-producers)
    - [Annotations](#annotations)
      - [Annotation process](#annotation-process)
      - [Who are the annotators?](#who-are-the-annotators)
    - [Personal and Sensitive Information](#personal-and-sensitive-information)
  - [Considerations for Using the Data](#considerations-for-using-the-data)
    - [Social Impact of Dataset](#social-impact-of-dataset)
    - [Discussion of Biases](#discussion-of-biases)
    - [Other Known Limitations](#other-known-limitations)
  - [Additional Information](#additional-information)
    - [Dataset Curators](#dataset-curators)
    - [Licensing Information](#licensing-information)
    - [Citation Information](#citation-information)
    - [Contributions](#contributions)

## Dataset Description

- **Repository:** [https://github.com/csebuetnlp/xl-sum](https://github.com/csebuetnlp/xl-sum)
- **Paper:** [XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages](https://aclanthology.org/2021.findings-acl.413/)
- **Point of Contact:** [Tahmid Hasan](mailto:tahmidhasan@cse.buet.ac.bd)

### Dataset Summary

We present XLSum, a comprehensive and diverse dataset comprising 1.35 million professionally  annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. The dataset covers 45 languages ranging from low to high-resource, for many of which no public dataset is currently available. XL-Sum is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. 


### Supported Tasks and Leaderboards

[More information needed](https://github.com/csebuetnlp/xl-sum)

### Languages

-  `amharic`
-  `arabic`
-  `azerbaijani`
-  `bengali`
-  `burmese`
-  `chinese_simplified`
-  `chinese_traditional`
-  `english`
-  `french`
-  `gujarati`
-  `hausa`
-  `hindi`
-  `igbo`
-  `indonesian`
-  `japanese`
-  `kirundi`
-  `korean`
-  `kyrgyz`
-  `marathi`
-  `nepali`
-  `oromo`
-  `pashto`
-  `persian`
-  `pidgin`
-  `portuguese`
-  `punjabi`
-  `russian`
-  `scottish_gaelic`
-  `serbian_cyrillic`
-  `serbian_latin`
-  `sinhala`
-  `somali`
-  `spanish`
-  `swahili`
-  `tamil`
-  `telugu`
-  `thai`
-  `tigrinya`
-  `turkish`
-  `ukrainian`
-  `urdu`
-  `uzbek`
-  `vietnamese`
-  `welsh`
-  `yoruba`

## Dataset Structure

### Data Instances

One example from the `English` dataset is given below in JSON format. 
  ```
  {
    ""id"": ""technology-17657859"",
    ""url"": ""https://www.bbc.com/news/technology-17657859"",
    ""title"": ""Yahoo files e-book advert system patent applications"",
    ""summary"": ""Yahoo has signalled it is investigating e-book adverts as a way to stimulate its earnings."",
    ""text"": ""Yahoo's patents suggest users could weigh the type of ads against the sizes of discount before purchase. It says in two US patent applications that ads for digital book readers have been \""less than optimal\"" to date. The filings suggest that users could be offered titles at a variety of prices depending on the ads' prominence They add that the products shown could be determined by the type of book being read, or even the contents of a specific chapter, phrase or word. The paperwork was published by the US Patent and Trademark Office late last week and relates to work carried out at the firm's headquarters in Sunnyvale, California. \""Greater levels of advertising, which may be more valuable to an advertiser and potentially more distracting to an e-book reader, may warrant higher discounts,\"" it states. Free books It suggests users could be offered ads as hyperlinks based within the book's text, in-laid text or even \""dynamic content\"" such as video. Another idea suggests boxes at the bottom of a page could trail later chapters or quotes saying \""brought to you by Company A\"". It adds that the more willing the customer is to see the ads, the greater the potential discount. \""Higher frequencies... may even be great enough to allow the e-book to be obtained for free,\"" it states. The authors write that the type of ad could influence the value of the discount, with \""lower class advertising... such as teeth whitener advertisements\"" offering a cheaper price than \""high\"" or \""middle class\"" adverts, for things like pizza. The inventors also suggest that ads could be linked to the mood or emotional state the reader is in as a they progress through a title. For example, they say if characters fall in love or show affection during a chapter, then ads for flowers or entertainment could be triggered. The patents also suggest this could applied to children's books - giving the Tom Hanks animated film Polar Express as an example. It says a scene showing a waiter giving the protagonists hot drinks \""may be an excellent opportunity to show an advertisement for hot cocoa, or a branded chocolate bar\"". Another example states: \""If the setting includes young characters, a Coke advertisement could be provided, inviting the reader to enjoy a glass of Coke with his book, and providing a graphic of a cool glass.\"" It adds that such targeting could be further enhanced by taking account of previous titles the owner has bought. 'Advertising-free zone' At present, several Amazon and Kobo e-book readers offer full-screen adverts when the device is switched off and show smaller ads on their menu screens, but the main text of the titles remains free of marketing. Yahoo does not currently provide ads to these devices, and a move into the area could boost its shrinking revenues. However, Philip Jones, deputy editor of the Bookseller magazine, said that the internet firm might struggle to get some of its ideas adopted. \""This has been mooted before and was fairly well decried,\"" he said. \""Perhaps in a limited context it could work if the merchandise was strongly related to the title and was kept away from the text. \""But readers - particularly parents - like the fact that reading is an advertising-free zone. Authors would also want something to say about ads interrupting their narrative flow.\""""
}
  ```

### Data Fields
-  'id': A string representing the article ID.
-  'url': A string representing the article URL.
-  'title': A string containing the article title.
-  'summary': A string containing the article summary.
-  'text' : A string containing the article text. 


### Data Splits

We used a 80%-10%-10% split for all languages with a few exceptions. `English` was split 93%-3.5%-3.5% for the evaluation set size to resemble that of `CNN/DM` and `XSum`; `Scottish Gaelic`, `Kyrgyz` and `Sinhala` had relatively fewer samples, their evaluation sets were increased to 500 samples for more reliable evaluation. Same articles were used for evaluation in the two variants of Chinese and Serbian to prevent data leakage in multilingual training. Individual dataset download links with train-dev-test example counts are given below:

Language      | ISO 639-1 Code | BBC subdomain(s) | Train | Dev | Test | Total |
--------------|----------------|------------------|-------|-----|------|-------|
Amharic | am | https://www.bbc.com/amharic | 5761 | 719 | 719 | 7199 |
Arabic | ar | https://www.bbc.com/arabic | 37519 | 4689 | 4689 | 46897 |
Azerbaijani | az | https://www.bbc.com/azeri | 6478 | 809 | 809 | 8096 |
Bengali | bn | https://www.bbc.com/bengali | 8102 | 1012 | 1012 | 10126 |
Burmese | my | https://www.bbc.com/burmese | 4569 | 570 | 570 | 5709 |
Chinese (Simplified) | zh-CN | https://www.bbc.com/ukchina/simp, https://www.bbc.com/zhongwen/simp | 37362 | 4670 | 4670 | 46702 |
Chinese (Traditional) | zh-TW | https://www.bbc.com/ukchina/trad, https://www.bbc.com/zhongwen/trad | 37373 | 4670 | 4670 | 46713 |
English | en | https://www.bbc.com/english, https://www.bbc.com/sinhala `*` | 306522 | 11535 | 11535 | 329592 |
French | fr | https://www.bbc.com/afrique | 8697 | 1086 | 1086 | 10869 |
Gujarati | gu | https://www.bbc.com/gujarati | 9119 | 1139 | 1139 | 11397 |
Hausa | ha | https://www.bbc.com/hausa | 6418 | 802 | 802 | 8022 |
Hindi | hi | https://www.bbc.com/hindi | 70778 | 8847 | 8847 | 88472 |
Igbo | ig | https://www.bbc.com/igbo | 4183 | 522 | 522 | 5227 |
Indonesian | id | https://www.bbc.com/indonesia | 38242 | 4780 | 4780 | 47802 |
Japanese | ja | https://www.bbc.com/japanese | 7113 | 889 | 889 | 8891 |
Kirundi | rn | https://www.bbc.com/gahuza | 5746 | 718 | 718 | 7182 |
Korean | ko | https://www.bbc.com/korean | 4407 | 550 | 550 | 5507 |
Kyrgyz | ky | https://www.bbc.com/kyrgyz | 2266 | 500 | 500 | 3266 |
Marathi | mr | https://www.bbc.com/marathi | 10903 | 1362 | 1362 | 13627 |
Nepali | np | https://www.bbc.com/nepali | 5808 | 725 | 725 | 7258 |
Oromo | om | https://www.bbc.com/afaanoromoo | 6063 | 757 | 757 | 7577 |
Pashto | ps | https://www.bbc.com/pashto | 14353 | 1794 | 1794 | 17941 |
Persian | fa | https://www.bbc.com/persian | 47251 | 5906 | 5906 | 59063 |
Pidgin`**` | n/a | https://www.bbc.com/pidgin | 9208 | 1151 | 1151 | 11510 |
Portuguese | pt | https://www.bbc.com/portuguese | 57402 | 7175 | 7175 | 71752 |
Punjabi | pa | https://www.bbc.com/punjabi | 8215 | 1026 | 1026 | 10267 |
Russian | ru | https://www.bbc.com/russian, https://www.bbc.com/ukrainian `*` | 62243 | 7780 | 7780 | 77803 |
Scottish Gaelic | gd | https://www.bbc.com/naidheachdan | 1313 | 500 | 500 | 2313 |
Serbian (Cyrillic) | sr | https://www.bbc.com/serbian/cyr | 7275 | 909 | 909 | 9093 |
Serbian (Latin) | sr | https://www.bbc.com/serbian/lat | 7276 | 909 | 909 | 9094 |
Sinhala | si | https://www.bbc.com/sinhala | 3249 | 500 | 500 | 4249 |
Somali | so | https://www.bbc.com/somali | 5962 | 745 | 745 | 7452 |
Spanish | es | https://www.bbc.com/mundo | 38110 | 4763 | 4763 | 47636 |
Swahili | sw | https://www.bbc.com/swahili | 7898 | 987 | 987 | 9872 |
Tamil | ta | https://www.bbc.com/tamil | 16222 | 2027 | 2027 | 20276 |
Telugu | te | https://www.bbc.com/telugu | 10421 | 1302 | 1302 | 13025 |
Thai | th | https://www.bbc.com/thai | 6616 | 826 | 826 | 8268 |
Tigrinya | ti | https://www.bbc.com/tigrinya | 5451 | 681 | 681 | 6813 |
Turkish | tr | https://www.bbc.com/turkce | 27176 | 3397 | 3397 | 33970 |
Ukrainian | uk | https://www.bbc.com/ukrainian | 43201 | 5399 | 5399 | 53999 |
Urdu | ur | https://www.bbc.com/urdu | 67665 | 8458 | 8458 | 84581 |
Uzbek | uz | https://www.bbc.com/uzbek | 4728 | 590 | 590 | 5908 |
Vietnamese | vi | https://www.bbc.com/vietnamese | 32111 | 4013 | 4013 | 40137 |
Welsh | cy | https://www.bbc.com/cymrufyw | 9732 | 1216 | 1216 | 12164 |
Yoruba | yo | https://www.bbc.com/yoruba | 6350 | 793 | 793 | 7936 |

`*` A lot of articles in BBC Sinhala and BBC Ukrainian were written in English and Russian respectively. They were identified using [Fasttext](https://arxiv.org/abs/1607.01759) and moved accordingly.

`**` West African Pidgin English

## Dataset Creation

### Curation Rationale

[More information needed](https://github.com/csebuetnlp/xl-sum)

### Source Data

[BBC News](https://www.bbc.co.uk/ws/languages)

#### Initial Data Collection and Normalization

[Detailed in the paper](https://aclanthology.org/2021.findings-acl.413/) 


#### Who are the source language producers?

[Detailed in the paper](https://aclanthology.org/2021.findings-acl.413/) 


### Annotations

[Detailed in the paper](https://aclanthology.org/2021.findings-acl.413/) 


#### Annotation process

[Detailed in the paper](https://aclanthology.org/2021.findings-acl.413/) 

#### Who are the annotators?

[Detailed in the paper](https://aclanthology.org/2021.findings-acl.413/) 

### Personal and Sensitive Information

[More information needed](https://github.com/csebuetnlp/xl-sum)

## Considerations for Using the Data

### Social Impact of Dataset

[More information needed](https://github.com/csebuetnlp/xl-sum)

### Discussion of Biases

[More information needed](https://github.com/csebuetnlp/xl-sum)

### Other Known Limitations

[More information needed](https://github.com/csebuetnlp/xl-sum)

## Additional Information

### Dataset Curators

[More information needed](https://github.com/csebuetnlp/xl-sum)

### Licensing Information

Contents of this repository are restricted to only non-commercial research purposes under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)](https://creativecommons.org/licenses/by-nc-sa/4.0/). Copyright of the dataset contents belongs to the original copyright holders.
### Citation Information

If you use any of the datasets, models or code modules, please cite the following paper:
```
@inproceedings{hasan-etal-2021-xl,
    title = ""{XL}-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages"",
    author = ""Hasan, Tahmid  and
      Bhattacharjee, Abhik  and
      Islam, Md. Saiful  and
      Mubasshir, Kazi  and
      Li, Yuan-Fang  and
      Kang, Yong-Bin  and
      Rahman, M. Sohel  and
      Shahriyar, Rifat"",
    booktitle = ""Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"",
    month = aug,
    year = ""2021"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.findings-acl.413"",
    pages = ""4693--4703"",
}
```


### Contributions

Thanks to [@abhik1505040](https://github.com/abhik1505040) and [@Tahmid](https://github.com/Tahmid04) for adding this dataset.",High,5.0
Summarization,YoussefAnwar/Arabic-news,1.0,2045.0,2024-10-13 00:14:00+00:00,none,0.0,745 MB,781189120.0,745 MB,781189120.0,588881,none,none,"---
language:
- ar
multilinguality:
- monolingual
task_categories:
- summarization
task_ids:
- news-articles-summarization
- language-modeling
pretty_name: Arabic News
dataset_info:
  features:
  - name: Body
    dtype: string
  - name: Title
    dtype: string
  splits:
  - name: train
    num_bytes: 1479976249.0
    num_examples: 588881
  download_size: 745147868
  dataset_size: 1479976249.0
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
tags: []
---



# Arabic News Dataset

## Dataset Summary
The Arabic News Dataset contains a collection of news articles in Arabic, featuring titles and corresponding bodies of text. It is designed for natural language processing tasks such as text classification, summarization, and sentiment analysis.

## Dataset Details

- **Languages**: Arabic
- **Number of Samples**: [6.15k]
- **Features**:
  - `title`: The title of the news article.
  - `body`: The full text of the news article.


## Usage
This dataset can be used for various NLP tasks, including:
- Text classification (e.g., categorizing news articles)
- Summarization (e.g., generating summaries of news articles)
- Sentiment analysis (e.g., determining the sentiment of news articles)

## Dataset Splits
- **Train**: [Number of training samples]
- **Validation**: [Number of validation samples]

## Data Collection
This dataset was collected from various Arabic news sources, ensuring a diverse range of topics and styles.",Medium,2.0
Summarization,espnet/floras,10.0,5169.0,2024-11-29 20:12:19+00:00,cc-by-3.0,0.0,UNKNOWN,UNKNOWN,10.4 GB,11166914969.6,11166914969.6,none,none,"---
license: cc-by-3.0
dataset_info:
- config_name: monolingual
  features:
  - name: id
    dtype: string
  - name: language
    dtype: string
  - name: score
    dtype: string
  - name: audio
    dtype:
      audio:
        sampling_rate: 16000
  - name: text
    dtype: string
  - name: summary
    dtype: string
  - name: translation
    dtype: string
  splits:
  - name: train
    num_bytes: 2250087924
    num_examples: 50814
  - name: dev
    num_bytes: 3730403898.0
    num_examples: 81
  - name: test
    num_bytes: 6882657690.0
    num_examples: 116
  download_size: 27806858743
  dataset_size: 21226123202.0
- config_name: multilingual
  features:
  - name: id
    dtype: string
  - name: language
    dtype: string
  - name: score
    dtype: string
  - name: audio
    dtype:
      audio:
        sampling_rate: 16000
  - name: text
    dtype: string
  - name: summary
    dtype: string
  - name: translation
    dtype: string
  splits:
  - name: dev
    num_bytes: 49979924635.32
    num_examples: 1154
  - name: test
    num_bytes: 56817933774.28
    num_examples: 1188
  download_size: 102717641464
  dataset_size: 106797858409.6
configs:
- config_name: monolingual
  data_files:
  - split: train
    path: monolingual/train-*
  - split: dev
    path: monolingual/dev-*
  - split: test
    path: monolingual/test-*
- config_name: multilingual
  data_files:
  - split: dev
    path: multilingual/dev-*
  - split: test
    path: multilingual/test-*
task_categories:
- automatic-speech-recognition
- translation
- summarization
language:
- en
- es
- fr
- de
- nl
- it
- pt
- hu
- fi
- el
- ca
- eo
- et
- da
- la
- sv
- cy
- gl
- ru
- pl
- uk
- ro
- cs
- sl
- sk
- hr
- bg
- bs
- ka
- tr
- fa
- ar
- uz
- az
- ku
- ky
- hi
- ta
- ur
- bn
- id
- vi
- th
- mi
- ms
- ja
- zh
---

# FLORAS

FLORAS is a 50-language benchmark **F**or **LO**ng-form **R**ecognition **A**nd **S**ummarization of spoken language. 
The goal of FLORAS is to create a more realistic benchmarking environment for speech recognition, translation, and summarization models. 
Unlike typical academic benchmarks like LibriSpeech and FLEURS that uses pre-segmented single-speaker read-speech, FLORAS tests the capabilities of models on raw long-form conversational audio, which can have one or many speakers.

To encourage research in multi-tasking, FLORAS provides 1-way to 3-way parallel data for long-form Automatic Speech Recognition (ASR), long-form X-to-EN Speech Translation (ST), and Speech Summarization (SSUM).
This means that some samples only have paired speech and transcripts, while others may have paired speech, transcripts, translations and/or summaries.
In total, FLORAS contains roughly 32,000 hours of raw audio.

## Dataset Creation

FLORAS is derived from [YODAS](https://huggingface.co/datasets/espnet/yodas), a large multilingual crawl of YouTube videos and their subtitles. 
Since the raw crawl of YODAS is too noisy for direct training in many settings, we filter out most of the data using CTC alignment scores.
The translations and summaries are obtained via pseudo-labelling using Google's [Gemini Flash](https://deepmind.google/technologies/gemini/flash/).
Our translators then filtered out or corrected faulty pseudo-labels in the test set. We did not perform filtering on the training/development sets.

## Dataset Structure

FLORAS is organized into two subsets, each with data splits for training, validation, and testing.
```
FLORAS
- monolingual
  - train
  - dev
  - test
- multilingual
  - train
  - dev
  - test_unverified
  - test_verified
```
The monolingual subset contains English-only data. The multilingual subset contains the data for the other 49 languages.

The multilingual subset contains two test sets: `test_unverified` and `test_verified`. 

Verified languages are those that have had professional translators and/or native speakers verify the translation/summary pseudo-labels.

Unverified languages are those that did not go through this process (See below to determine which languages have been verified).

## Data Fields

Each subset/split has the following data fields:
- **id** (str): sample ID of the speech.
- **language** (str): ISO3 language code of the speech.
- **score** (float): CTC alignment score of the video. Closer to 0 is better.
- **audio** (dict):  Audio object including loaded audio array, sampling rate and path to audio.
- **text** (str): Text transcription.
- **translation** (str): English translation of transcript, if available. If not available, will yield the empty string.
- **summary** (str): Summary of transcript, if available. If not available, will yield the empty string.

Since FLORAS only supports X-to-EN translation, the `translation` field is always empty for samples in the `monolingual` subset.

## Languages

The languages in FLORAS by region are as follows:
- **Western Europe**: _English_, Spanish, German, French, Italian, Portuguese, Dutch, Basque, Hungarian, Finnish, Greek, Catalan, Esperanto, Danish, Latin, Swedish, Galician, Welsh
- **Eastern Europe**: Russian, Polish, Ukrainian, Romanian, Czech, Estonian, Slovak, Slovenian, Croatian, Serbian, Bulgarian, Bosnian, Georgian
- **Central-Asia/Middle-East/North-Africa**: Turkish, Persian, Arabic, Uzbek, Kurdish, Kyrgyz, Azerbaijani
- **South-Asia**: Hindi, Tamil, Urdu, Bengali
- **South-East Asia**: Indonesian, Vietnamese, Thai, Malay, Maori
- **East Asia**: _Japanese_, _Mandarin Chinese_

_Italicized_ languages have been verified by professional translators and/or native speakers for the translation/summary pseudo-labels.

**If a language that you speak is not verified and you would like to donate some time to check the pseudo-label quality, please reach out to us!**",Medium,2.0
Summarization,conceptofmind/MegaWika,1.0,318.0,2024-11-19 04:22:41+00:00,cc-by-sa-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2307.07049,['https://aclanthology.org/2021.eacl-demos.19.pdf)'],"---
license: cc-by-sa-4.0
task_categories:
- summarization
- question-answering
- text-generation
- text2text-generation
language:
- af
- ar
- az
- bn
- cs
- de
- en
- es
- et
- fa
- fi
- fr
- ga
- gl
- gu
- he
- hi
- hr
- id
- it
- ja
- ka
- kk
- km
- ko
- lt
- lv
- mk
- ml
- mn
- mr
- my
- ne
- nl
- pl
- ps
- pt
- ro
- ru
- si
- sl
- sv
- ta
- th
- tr
- uk
- ur
- vi
- xh
- zh
pretty_name: MegaWika
size_categories:
- 10M<n<100M
---
# Dataset Card for MegaWika

## Dataset Description

- **Homepage:** [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika)
- **Repository:** [HuggingFace](https://huggingface.co/datasets/hltcoe/megawika)
- **Paper:** [Coming soon]
- **Leaderboard:** [Coming soon]
- **Point of Contact:** [Samuel Barham](samuel.barham@jhuapl.edu)

### Dataset Summary

MegaWika is a multi- and crosslingual text dataset containing 30 million Wikipedia passages with their scraped and cleaned web citations. The passages span
50 Wikipedias in 50 languages, and the articles in which the passages were originally embedded are included for convenience. Where a Wikipedia passage is in a
non-English language, an automated English translation is provided. Furthermore, nearly 130 million English question/answer pairs were extracted from the
passages, and FrameNet events occurring in the passages are detected using the [LOME](https://aclanthology.org/2021.eacl-demos.19.pdf) FrameNet parser.


<!---
To get a feel for the dataset -- its structure, content, strengths and weaknesses -- you may visit the [dataset viewer](https://huggingface.co/spaces/hltcoe/megawika)
we have set up as a HuggingFace Space. It allows the curious visitor to explore a small set of examples spread across a number of the dataset's constituent languages.
-->

### Dataset Creation

The pipeline through which MegaWika was created is complex, and is described in more detail in the paper (linked above),
but the following diagram illustrates the basic approach.

![Illustration of MegaWikaProcess](images/MegaWikaProcess-cross-lingual.drawio.png)

### Supported Tasks and Leaderboards

MegaWika is meant to support research across a variety of tasks, including report generation, summarization, information retrieval, question answering, etc.

### Languages

MegaWika is divided by Wikipedia language. There are 50 languages, including English, each designated by their 2-character ISO language code:
- `af`: Afrikaans
- `ar`: Arabic
- `az`: Azeri (Azerbaijani)
- `bn`: Bengali
- `cs`: Czech
- `de`: German (Deutsch)
- `en`: English
- `es`: Spanish (Español)
- `et`: Estonian
- `fa`: Farsi (Persian)
- `fi`: Finnish
- `fr`: French
- `ga`: Irish (Gaelic)
- `gl`: Galician
- `gu`: Gujarati
- `he`: Hebrew
- `hi`: Hindi
- `hr`: Hungarian
- `id`: Indonesian
- `it`: Italian
- `ja`: Japanese
- `ka`: Georgian (Kartvelian/Kartlian)
- `kk`: Kazakh
- `km`: Khmer
- `ko`: Korean
- `lt`: Lithuanian
- `lv`: Latvian
- `mk`: Macedonian (Makedonski)
- `ml`: Malay (Malayalam)
- `mn`: Mongolian
- `mr`: Marathi
- `my`: Burmese (Myanmar language)
- `ne`: Nepali
- `nl`: Dutch (Nederlands)
- `pl`: Polish
- `ps`: Pashto
- `pt`: Portuguese
- `ro`: Romanian
- `ru`: Russian
- `si`: Sinhalese (Sri Lankan language)
- `sl`: Slovenian
- `sv`: Swedish (Svenska)
- `ta`: Tamil
- `th`: Thai
- `tr`: Turkish
- `uk`: Ukrainian
- `ur`: Urdu
- `vi`: Vietnamese
- `xh`: Xhosa
- `zh`: Chinese (Zhōng wén)

## Dataset Structure

The dataset is divided by language, and the data for each of the 50 languages is further chunked into discrete JSON lines files.
Each line of these files -- we'll call such a line an **instance** -- contains the data extracted from a single Wikipedia article.

### Data Instances

Each instance contains the text of the seed Wikipedia article, along with a list of **entries**. Each entry consists basically in
an extracted Wikipedia passage, the URL and scraped text of the web source it cites, a list of questions/answer pairs extracted from the passage,
and a framenet parse of the passage. Where the passage is from a non-English Wikipedia, a machine translation into English is also provided.

### Data Fields

The detailed structure of an instance is as follows:
```
{
  ""article_title"": <string : title of original Wikipedia article>
  ""article_text"": <string : text of Wikipedia article>
  ""entries"": [
    # Wiki Passage
    ""id"": <string : passage ID>
    ""passage"": {
      ""text"": <string : text of passage in English (possibly via MT)>
      ""parse"": <list of dict : FrameNet parse of English passage text>
      ""en_tokens"": <dict : tokenization of passage in English>
      ""lang_tokens"": <dict : tokenization of original non-English passage>
      ""en_lang_token_map"": <dict : alignment mapping between English and original language token indices>
    }
    # MT
    ""original"": <string : original language passage>
    ""original_sents"": <list of string : sentencized original language passage>
    ""translation"": <string : machine translation of passage>
    ""translation_sents"": <list of string : sentencized machine translation of passage>
    ""translation_probs"": <list of float : log prob of machine translation by sentence, where available>
    ""repetitious_translation"": <string \in (""true"", ""false"") : automated judgment on whether machine translation is pathologically repetitious>
    ""source_lang"": <string : language ID, 2-character ISO code>
    # Source
    ""source_url"": <string : URL of the cited web source>
    ""source_text"": <string : content extracted from the scrape of the source URL>
    # Question/Answer Pairs
    ""qa_pairs"": [
      ...
      {
        ""question"": <string : generated question>
        ""passage_id"": <string : passage ID>
        ""en_answer"": <string : English answer>
        ""lang_answer"": <string : aligned original language answer>
        ""frames"": [
          ...
          {
            ""frame"": <string : frame triggered by the question>
            ""argument"": <string : detected frame arguments>
          }
          ...
        ]
        # NB: answer matches can be empty, in the case no matching span exists
        ""en_matches_in_source"": <list of int : start and end index of the English language-answer token(s) in the source document>
        ""en_match_in_passage"": <list of int : start and end index of the English language-answer token(s) in the English language translation of the passage>
        ""lang_matches_in_source"": <list of int : start and end index of the original language-answer token(s) in the source document>
        ""lang_match_in_passage"": <list of int : start and end index of the original language-answer token(s) in the original language passage>
        ""passage"": <list of string : sentencized view of the passage>
        ""en_answer_tokens"": <list of string>
        ""match_disambiguated_question"": <string : disambiguated version of question obtained by matching pronouns with article title (noisy but often helpful)>
      }
      ...
    ]
  ]
}
```

English language instances differ not in structure but in content; 
1. Fields in the block labeled ""MT"" above are naturally null (that is, they are set to falsy values in Python -- specifically `None`)
2. Since the Wiki passage only exists in English, and has no corresponding non-English ""original language"" version, answer spans also necessarily have only an English-language version (and no non-English ""original-language"" version. Therefore, fields in the `qa_pairs` block beginning with `lang_` are set to null/falsy values in Python (in this case, empty lists).


### Data Splits

MegaWika is currently split only by language, as each task will imply its own approach to filtering, sampling, downselecting, and splitting into train/test splits.

<!---
### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]
-->

## Licensing and Takedown

MegaWika 1.0 consists in part of documents scraped from across the web (based on citations linked in Wikipedia articles.)

We do not own any of the scraped text nor do we claim copyright: text drawn from Wikipedia citations are meant for research use in algorithmic design and model training.

We release this dataset and all its contents under CC-BY-SA-4.0.

### Notice and Takedown Policy:
*NB*: Should you consider that our data contains material that is owned by you and should therefore not be reproduced here, please:

- Clearly identify yourself, with detailed contact data such as an address, telephone number or email address at which you can be contacted.
- Clearly identify the copyrighted work claimed to be infringed.
- Clearly identify the material that is claimed to be infringing and information reasonably sufficient to allow us to locate the material.

And contact the authors.

*Take down*: We will comply to legitimate requests by removing the affected sources from the next release of the dataset.

## Additional Information

### Dataset Curators

Released and maintained by the Johns Hopkins University Human Language Technology Center of Excellence (JHU/HLTCOE). 
You can contact one the MegaWika authors, including [Samuel Barham](mailto:samuel.barham@jhuapl.edu), [Orion Weller](mailto:oweller2@jhu.edu),
and [Ben van Durme](mailto:vandurme@jhu.edu) with questions.

### Licensing Information

Released under the [Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/) license.

### Citation Information

```
@misc{barham2023megawika,
      title={MegaWika: Millions of reports and their sources across 50 diverse languages}, 
      author={Samuel Barham and and  Weller and Michelle Yuan and Kenton Murray and Mahsa Yarmohammadi and Zhengping Jiang and Siddharth Vashishtha and Alexander Martin and Anqi Liu and Aaron Steven White and Jordan Boyd-Graber and Benjamin Van Durme},
      year={2023},
      eprint={2307.07049},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

<!--
### Contributions

[More Information Needed]
-->",High,5.0
Summarization,dijihax/Dataset,2.0,7.0,2024-12-06 17:37:23+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- text-classification
- token-classification
- table-question-answering
- question-answering
- zero-shot-classification
- translation
- summarization
- feature-extraction
- text-generation
- text2text-generation
- fill-mask
- sentence-similarity
- text-to-speech
- text-to-audio
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
- image-to-image
- unconditional-image-generation
- multiple-choice
- image-to-text
- image-to-video
- video-classification
- reinforcement-learning
- robotics
- tabular-classification
- tabular-regression
- tabular-to-text
- table-to-text
- text-retrieval
- time-series-forecasting
- text-to-video
- visual-question-answering
- zero-shot-image-classification
- graph-ml
- mask-generation
- zero-shot-object-detection
- text-to-3d
- image-to-3d
- image-feature-extraction
- video-text-to-text
language:
- en
- aa
- ab
- ae
- af
- ak
- am
- an
- ar
- as
- av
- ay
- az
- ba
- be
- bg
- bh
- bi
- bm
- bn
- bo
- br
- bs
- ca
- ce
- ch
- co
- cr
- cs
- cu
- cv
- cy
- da
- de
- dv
- dz
- ee
- el
- eo
- es
- et
- eu
- fa
- ff
- fi
- fj
- fo
- fr
- fy
- ga
- gd
- gl
- gn
- gu
- gv
- ha
- he
- hi
- ho
- hr
- ht
- hu
- hy
- hz
- ia
- id
- ie
- ig
- ii
- ik
- io
- is
- it
- iu
- ja
- jv
- ka
- kg
- ki
- kj
- kk
- kl
- km
- kn
- ko
- kr
- ks
- ku
- kv
- kw
- ky
- la
- lb
- lg
- li
- ln
- lo
- lt
- lu
- lv
- mg
- mh
- mi
- mk
- ml
- mn
- mr
- ms
- mt
- my
- na
- nb
- nd
- ne
- ng
- nl
- nn
- 'no'
- nr
- nv
- ny
- oc
- oj
- om
- or
- os
- pa
- pi
- pl
- ps
- pt
- qu
- rm
- rn
- ro
- ru
- rw
- sa
- sc
- sd
- se
- sg
- si
- sk
- sl
- sm
- sn
- so
tags:
- chemistry
- biology
- finance
- legal
- music
- art
- code
- climate
- medical
- not-for-all-audiences
- synthetic
pretty_name: DijiHax/DataStax
size_categories:
- n>1T
---
# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Summarization,kingkaung/islamqainfo_parallel_corpus,0.0,36.0,2025-01-27 07:00:39+00:00,cc-by-nc-4.0,0.0,UNKNOWN,unknown,177 MB,unknown,19052,none,none,"---
license: cc-by-nc-4.0
task_categories:
- table-question-answering
- text-generation
- translation
- summarization
- text-classification
language:
- en
- ar
- ur
- bn
- fr
- es
- zh
- ru
- de
- tg
- pt
- hi
- ug
- tr
- id
- ja
tags:
- Islam
- Question&Answer
- Religion
- Translation
- Corpus
- Parallel
- ParallelCorpus
pretty_name: islamqa.info
size_categories:
- 10K<n<100K
---

# Dataset Card for IslamQA Info Parallel Corpus

#### Dataset Description

The **IslamQA Info Parallel Corpus** is a multilingual dataset derived from the IslamQA repository. It contains curated question-and-answer pairs across 17 languages, making it a valuable resource for multilingual and cross-lingual natural language processing (NLP) tasks. The dataset has been created over nearly three decades (since 1997) by Sheikhul Islam Muhammad Saalih al-Munajjid and his team.

### Key Features

- **Curated by:** IslamQA team
- **Languages (NLP):** English (base), Arabic, Urdu, Bengali, French, Chinese, Spanish, Russian, German, Tajik, Portuguese, Hindi, Uyghur, Turkish, Indonesian, and Japanese.
- **License:** CC BY-NC 4.0 (Attribution-NonCommercial)
- **Size:** Contains 19,052 rows in Parquet format.
- **Content Fields:**
  - `Global ID`: Unique ID for each question-answer group.
  - `topic`: Thematic category (e.g., Belief, Sacrifices).
  - `original_id`: Original source ID.
  - `title`, `question`, `answer`: Content fields for each language (e.g., `title_en`, `question_ar`, `answer_bn`).
  - `link`: URLs for verification (e.g., `link_en`, `link_zh`).

### Dataset Sources

- **Repository:** [IslamQA Info Parallel Corpus on Hugging Face](https://huggingface.co/datasets/kingkaung/islamqainfo_parallel_corpus)
- **Original Source:** [IslamQA Website](https://islamqa.info/)

# Uses

### Direct Use

This dataset is suitable for:
- Multilingual NLP tasks such as machine translation, question-answering, and text summarization.
- Cross-lingual semantic search and information retrieval.
- Training and fine-tuning multilingual models.

### Out-of-Scope Use

The dataset is not intended for generating biased or harmful content. Some topics may include culturally or religiously sensitive concepts; users should respect the context and original intent.

### Dataset Structure

The dataset is available **only in Parquet format**, encoded in UTF-8. It includes the following fields:

1. `Global ID`: Groups translations of the same Q&A across languages.
2. `topic`: Thematic category of the Q&A.
3. `original_id`: Reference to the original source.
4. `title`, `question`, `answer`: Content fields for each language (e.g., `title_en`, `question_ar`, `answer_bn`).
5. `link`: URLs for verification (e.g., `link_en`, `link_fr`).

### Sample Record

```json
{
    ""Global ID"": ""G_ID_00157"",
    ""topic"": ""Belief"",
    ""original_id"": ""10776"",
    ""title_en"": ""The Best Means to Increase Your Faith"",
    ""question_en"": ""Question\n\nWhat are the means of increasing faith?"",
    ""answer_en"": ""Praise be to Allah.There are several means of increasing faith :  \n\nLearning about Allah through His names and attributes. The more a person learns about Allah through His names and attributes, the more he will increase in faith, without a doubt. Hence you find that the scholars who know more about the names and attributes of Allah than others do, are stronger in faith than others in this regard.  \nLooking at the signs of Allah in the universe and the signs of Shari`ah (i.e., verses of the Quran and miracles of the Prophet (peace and blessings of Allah be upon him), etc.). The more a person looks at the signs of Allah in the universe, the more he increases in faith. Allah says (interpretation of the meaning):\n\n“And on the earth are signs for those who have Faith with certainty. And also in your own selves. Will you not then see?.” [Al-Dhariyat 51:20-21]\nThe verses which indicate that are many, I mean the verses which indicate that by pondering and thinking about this universe, man can increase in faith. \nDoing many acts of worship, for the more acts of worship a person does, the more he will increase in faith thereby, whether those acts of worship involve words or actions. So dhikr increases faith in quantity and quality, and prayer, fasting and Hajj also increase faith in quantity and quality.\nFor more, please see these answers: 14041 , 331637 , 22877 , 20059 , 223615 , and 9082 .\nAnd Allah knows best."",
    ""title_ar"": ""ما هي أسباب زيادة الإيمان ؟"",
    ""question_ar"": """",
    ""answer_ar"": ""الحمد لله.للزيادة أسباب :\nالسبب الأول : معرفة الله تعالى بأسمائه وصفاته , فإن الإنسان كلما ازداد معرفة بالله وبأسمائه وصفاته ازداد إيماناً بلا شك , ولهذا تجد أهل العلم الذين يعلمون من أسماء الله وصفاته ما لا يعلمه غيرهم تجدهم أقوى إيماناً من الآخرين من هذا الوجه .\nالسبب الثاني : النظر في آيات الله الكونية , والشرعية , فإن الإنسان كلما نظر في الآيات الكونية التي هي المخلوقات ازداد إيماناً قال تعالى : ( وفي الأرض آيات للموقنين وفي أنفسكم أفلا تبصرون ) الذاريات /20-21 والآيات الدالة على هذا كثيرة أعني الآيات الدالة على أن الإنسان بتدبره وتأمله في هذا الكون يزداد إيمانه .\nالسبب الثالث : كثرة الطاعات فإن الإنسان كلما كثرت طاعاته ازداد بذلك إيماناً سواء كانت هذه الطاعات قولية أم فعلية , فالذكر يزيد الإيمان كمية وكيفية , والصلاة والصوم والحج تزيد الإيمان أيضاً كمية وكيفية ."",
    ""title_bn"": ""ঈমান বৃদ্ধির কারণগুলো কী কী?"",
    ""question_bn"": """",
    ""answer_bn"": ""আলহামদু লিল্লাহ।.ঈমান বৃদ্ধির কারণগুলো:\nপ্রথম কারণ: \nআল্লাহ্‌কে তার নাম ও গুণসমূহসহ জানা। নিঃসন্দেহে মানুষ আল্লাহ্‌কে যতবেশি জানবে, তাঁর নাম ও গুণগুলোকে যতবেশি জানবে; ততই তাঁর ঈমান বৃদ্ধি পাবে।..."",
    ""title_fr"": ""Les causes de l’augmentation de la foi"",
    ""question_fr"": """",
    ""answer_fr"": ""Louange à Allah.Les causes sont les suivantes :\nLa première : La connaissance d’Allah le Très-Haut par l'acquisition de la connaissance de Ses noms et Ses attributs. Plus l’homme approfondit sa connaissance d’Allah, de Ses noms et de Ses attributs, plus sa foi s’affermit certainement..."",
    ""title_es"": ""¿Cuáles son los medios para aumentar la fe?"",
    ""question_es"": """",
    ""answer_es"": ""Alabado sea Dios.Hay muchos medios para aumentar la fe: \nEl primer medio es aprender sobre Allah a través de Sus nombres y atributos. Cuanto más aprenda una persona sobre Allah, Sus nombres y atributos, más aumentará su fe..."",
    ""title_zh"": ""信仰增加的因素是什么？"",
    ""question_zh"": """",
    ""answer_zh"": ""一切赞颂，全归真主。\n“信仰增加的因素很多：\n第一个因素：了解伟大真主的所有尊名和属性，每当一个人对真主的所有尊名和属性了解越多，他的信仰肯定会增加..."",
    ""title_ru"": ""Как можно увеличить веру?"",
    ""question_ru"": """",
    ""answer_ru"": ""Хвала Аллаху.Есть несколько способов увеличения веры.\nПервое средство. Больше узнавать об Аллахе через изучение Его имён и атрибутов..."",
    ""title_de"": ""Was sind Wege zu einem stärkeren Glauben (Iimaan)?"",
    ""question_de"": """",
    ""answer_de"": ""Alles Lob gebührt Allah..Für einen starken Glauben (Iimaan) gibt es verschiedene Gründe:\nDer erste Grund: Das Kennen Allahs dem Erhabenen mit Seinen Namen und Eigenschaften..."",
    ""link_en"": ""https://islamqa.info/en/answers/10776/the-best-means-to-increase-your-faith"",
    ""link_ar"": ""https://islamqa.info/ar/answers/10776/%D9%85%D8%A7-%D9%87%D9%8A-%D8%A7%D8%B3%D8%A8%D8%A7%D8%A8-%D8%B2%D9%8A%D8%A7%D8%AF%D8%A9-%D8%A7%D9%84%D8%A7%D9%8A%D9%85%D8%A7%D9%86"",
    ""link_bn"": ""https://islamqa.info/bn/answers/10776/%E0%A6%88%E0%A6%AE%E0%A6%A8-%E0%A6%AC%E0%A6%A6%E0%A6%A7%E0%A6%B0-%E0%A6%95%E0%A6%B0%E0%A6%A3%E0%A6%97%E0%A6%B2-%E0%A6%95-%E0%A6%95""
}

``` 
# Dataset Creation

### Curation Rationale

This dataset was created to facilitate research and applications in multilingual NLP. It offers a rich source of content, including translations, topics, and unique cultural insights.

### Source Data

The data was collected directly from the IslamQA website using web scraping tools. The original content was manually curated and verified by the IslamQA team over the years.

Data Collection and Processing
	•	Extracted Q&A pairs, topics, and multilingual versions using HTML parsing tools (e.g., BeautifulSoup).
	•	Mapped multilingual articles to their English equivalents.
	•	Standardized column names and ensured content consistency across languages.

```
````
### Usage Example

To use this dataset in your NLP workflows, you can load it directly using the Hugging Face `datasets` library:

```python
from datasets import load_dataset

# Load the dataset
dataset = load_dataset(""kingkaung/islamqainfo_parallel_corpus"")

# Display the first few rows
print(dataset[""train""].select(range(5)))

```

# Who are the Source Data Producers?

The dataset originates from the IslamQA website, managed by Sheikhul Islam Muhammad Saalih al-Munajjid and his team.

# Bias, Risks, and Limitations

### Risks and Limitations
	•	The dataset reflects Islamic values and may include topics that are sensitive or non-Western (e.g., Jihad, religious rulings).
	•	Translations may vary in quality and completeness across languages.
	•	The dataset is not anonymized and includes URLs pointing to the original content.

### Recommendations
	•	Users should respect the cultural and religious context of the dataset.
	•	Obtain permission from IslamQA for any commercial use.
	•	Consider additional filtering or preprocessing if required for specific tasks.

# Citation

### BibTeX:

```
@misc{islamqa_parallel_corpus,
  author = {Sheikhul Islam Muhammad Saalih al-Munajjid and the IslamQA team},
  title = {IslamQA Info Parallel Corpus},
  year = {2025},
  url = {https://huggingface.co/datasets/kingkaung/islamqainfo_parallel_corpus},
  note = {Dataset for research purposes only.}
}

```

###### APA:

```

Sheikhul Islam Muhammad Saalih al-Munajjid & IslamQA team. (2025). IslamQA Info Parallel Corpus. Retrieved from https://huggingface.co/datasets/kingkaung/islamqainfo_parallel_corpus

```

## License
This dataset is licensed under the [Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/).  
- You are free to share, adapt, and use the dataset for non-commercial purposes.  
- Commercial use is prohibited without explicit permission from the original authors.  
",High,6.0
Summarization,ZoneTwelve/multilingual-stories,0.0,11.0,2025-02-05 10:01:17+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
language: 
  - fr  # French
  - it  # Italian
  - de  # German
  - en  # English
  - ko  # Korean
  - es  # Spanish
  - zh  # Chinese
  - ja  # Japanese
  - ru  # Russian
  - cs  # Czech
  - da  # Danish
  - nl  # Dutch
  - ar  # Arabic
  - bg  # Bulgarian
  - et  # Estonian
  - hu  # Hungarian
  - id  # Indonesian
  - nb  # Norwegian
  - pt  # Portuguese
  - el  # Greek
  - lt  # Lithuanian
  - fi  # Finnish
  - lv  # Latvian
  - pl  # Polish
  - sv  # Swedish
  - sk  # Slovak
  - sl  # Slovenian
  - ro  # Romanian
  - tr  # Turkish
  - uk  # Ukrainian

language_bcp47:
  - zh-Hant  # Chinese (Traditional)
  - zh-Hans  # Chinese (Simplified)
  - en-GB  # English (British)
  - pt-BR  # Portuguese (Brazilian)

pretty_name: ""Multilingual Dataset""

tags:
  - multilingual
  - NLP
  - text-generation
  - text-classification
  - dataset-processing
  - large-scale-dataset

license: ""apache-2.0""

task_categories:
  - text-classification
  - text-generation
  - text2text-generation
  - feature-extraction
  - translation
  - summarization

size_categories:
  - large

source_datasets: []
---

# Multilingual Dataset

This dataset contains multilingual articles generated using various models.

# Dataset Summary

**Total records:** 14850

## Categories
Unique categories and their frequencies:
- **OmundoAmanha**: 99
- **malinois**: 33
- **CBDPouches**: 33
- **ingenieurs**: 66
- **PPeperomioides**: 33
- **DiscGolfCarts**: 33
- **TIMAF**: 33
- **AntisocialMemeClub**: 33
- **Tancan**: 66
- **ogerwaters**: 33
- **MotosBr**: 33
- **lgballt**: 66
- **CipherAcademy**: 33
- **LofiEdits**: 33
- **jennyraee**: 33
- **RonnieHotdogs**: 33
- **Bratzillaz**: 66
- **cubiccommunity**: 33
- **Labectance**: 33
- **ChasmicOccult**: 33
- **exresidents**: 33
- **GamingParents**: 33
- **Elisaonfire**: 33
- **blackdesert**: 66
- **MockElectionsAUS**: 33
- **DogfreeHumor**: 33
- **TToT**: 33
- **GeneralContractor**: 33
- **nkim**: 33
- **HeadandNeckCancer**: 33
- **CantonReddit**: 99
- **GrassrootsSelectAZ**: 33
- **BlackCloverMobile**: 33
- **uclafood**: 66
- **trueskyrimmods**: 33
- **heximal**: 33
- **Hetrochromia**: 33
- **photographysatx**: 33
- **AsturiasPatriaQuerida**: 33
- **2b2tqueue**: 99
- **kristyglas**: 33
- **advancedwitchcraft**: 33
- **rockymountainhouse**: 132
- **Kontaktanzeigen**: 33
- **DepressionVentSpace**: 33
- **MagickaWizardWarz**: 33
- **miniature_tigers**: 66
- **ifunnyNovoMundo**: 33
- **ShrineOfAC**: 66
- **EDAnonPics**: 33
- **FontLab**: 33
- **agent_of_shield**: 66
- **Infinity_laugh**: 33
- **CataclismoRTS**: 33
- **5DChess**: 33
- **idiotsflyingdrones**: 33
- **cscareerquestionsIN**: 66
- **birdsonbirds**: 99
- **msvduisburg**: 33
- **Eunha**: 33
- **Rotundanimals**: 33
- **Norse**: 66
- **treadstone**: 33
- **MadilynMei**: 66
- **ValentinaRose**: 66
- **AzurePercept**: 33
- **HollyHoes**: 66
- **exquisitecorpse**: 33
- **NoglaTerroriserReacts**: 33
- **StephenHillenburg**: 99
- **maxima**: 33
- **turndotme**: 33
- **dakboard**: 66
- **NetflixAnime**: 66
- **semaglutideweightloss**: 198
- **TheGatewayExperience**: 33
- **SubredditForFriends**: 33
- **porchpiratepwnage**: 33
- **throughtheages**: 66
- **TheFakeDentist**: 66
- **SquidMoonCoin**: 99
- **BazarRogueExcrement**: 66
- **techmaestro**: 33
- **ChurchOf_AquaxAixRuby**: 33
- **NLProducerGang**: 66
- **flannel**: 33
- **alternativewritings**: 66
- **Crypts**: 33
- **Mr_MandM**: 33
- **7counter**: 33
- **TwitchSafe**: 66
- **dRehmFlight**: 66
- **TheLetterOctothorpe**: 33
- **chuck**: 33
- **ConfrontingChaos**: 33
- **GettingOverItGame**: 33
- **KillYourConsole**: 33
- **Male_Studies**: 66
- **lacenterwa**: 33
- **IrishFishing**: 66
- **ptvo**: 99
- **Phigolf**: 33
- **the_derek**: 33
- **preciousmoments**: 33
- **AnaDaniela69xxx**: 33
- **blergg**: 33
- **GraphingCalculator**: 132
- **Knoxville**: 33
- **The_Real_Austin**: 33
- **EricBrakey**: 33
- **NightWitchGang**: 33
- **CityNerd**: 99
- **Pratibha_Ranta**: 99
- **BSCStation**: 33
- **WLWactually**: 33
- **suddenlymandjtv**: 33
- **Celestron**: 99
- **CSMagicRecipe**: 33
- **Phone_Foldables**: 99
- **MavuikaMainsGI**: 33
- **InboxPounds**: 33
- **janiscorner**: 33
- **PECG**: 33
- **IcebergExplained**: 33
- **GreedIncorporated**: 33
- **SurvivingRingworm**: 66
- **Snowplow**: 33
- **Tadano**: 66
- **funtechnology**: 33
- **ACUsedVillagers**: 33
- **Fordtractors**: 66
- **CargoWise**: 33
- **earwig**: 33
- **nocontextaudio**: 33
- **kenbarbbe**: 33
- **cwstonekingfans**: 33
- **VicBWolfArt**: 33
- **BrothersoftheGrain**: 99
- **TheRunaways**: 33
- **NewsWorld**: 66
- **DELF**: 33
- **deardiary**: 33
- **MyOneLineDogma**: 33
- **bunchofamateurs**: 33
- **Goulburn**: 66
- **Mattpryor**: 33
- **communitydevelopment**: 33
- **MiddleGradeWriters**: 66
- **businesslaw**: 33
- **BanglaMuktobak**: 33
- **StupidMario**: 66
- **Grimgar**: 33
- **HarvestMoonGBA**: 66
- **SOTR_auditions**: 99
- **TheShannaraChronicles**: 33
- **overcoding**: 33
- **Awareparents**: 66
- **dogmalove**: 66
- **HAESfood**: 33
- **EvergreenSkills**: 33
- **valentina94O**: 33
- **fsft**: 33
- **OkBuddyYharim**: 33
- **Swapandsell**: 66
- **warhospital**: 33
- **Demeo**: 33
- **TvIntroMashups**: 33
- **Winiday**: 132
- **algogaming**: 33
- **IhateElon**: 99
- **XtotheV**: 33
- **SouthOfTheCircleGame**: 66
- **wlu**: 33
- **MarketingFails**: 33
- **constanzavelazcoteton**: 33
- **legoww1**: 33
- **cheekdimples**: 33
- **circularguements**: 33
- **BattleforBFCI**: 33
- **JohnFarnham**: 99
- **DisablEdCanTeach**: 33
- **saysueme**: 66
- **Rengarmains**: 33
- **overclockfps**: 33
- **GOTV**: 66
- **Business_Management**: 33
- **latticeclimbing**: 33
- **rarehugs**: 33
- **AroAllo**: 33
- **gfbreadrecipies**: 66
- **JCBDL**: 33
- **snowplowproject**: 33
- **knockedloose**: 66
- **ScoreLords**: 33
- **ShopImprovements**: 33
- **256cub**: 66
- **UltiumCellsLLC**: 66
- **ObscureGIFs**: 33
- **alchademy**: 33
- **CastroPodcast**: 33
- **MelissaONeilPics**: 66
- **ConsensusDebate**: 33
- **okbuddyplants**: 33
- **mischakim**: 99
- **Palermo**: 33
- **losersofinstagram**: 99
- **lezconnect**: 33
- **TyreNichols**: 33
- **Primis_Bank**: 33
- **givemeanidea**: 33
- **Actualfixes**: 33
- **GrowingGirls**: 33
- **YukongMains**: 66
- **YapDollarMyBeloved**: 33
- **trials**: 33
- **wbmedicos**: 33
- **RaceTrackDesigns**: 33
- **supercellmoment**: 33
- **Survivors_BBC_1975**: 66
- **the_mysterium**: 33
- **KillerKitty**: 33
- **kaybomb**: 33
- **Blogsabouttea**: 33
- **OlympiacosBC**: 33
- **ReDIY**: 33
- **Hampture**: 33
- **Demonias_**: 66
- **WILTWIFLS**: 33
- **Analysts_of_MBTI**: 66
- **SoulSplitGame**: 33
- **gracelyra**: 33
- **Meme__World**: 66
- **CrescentHub**: 66
- **psytrancefrance**: 33
- **2016campaign**: 66
- **theguild3**: 33
- **practicingheathenry**: 33
- **budgetguitargear**: 99
- **Dogma**: 33
- **novomesto**: 33
- **ContinentedoBombarral**: 33
- **Comadre**: 33
- **thethyroidmadness**: 33
- **Xooginme**: 33
- **HomebrewTactical**: 33
- **kosmic**: 33
- **GameGeeks**: 33
- **TESHeroes**: 66
- **SchoolMotivation**: 66
- **dataforenergyskills**: 33
- **VeganJunkFood**: 66
- **TheRealArkhamWorld**: 33
- **ExpressiveArtsTherapy**: 33
- **TojinoMiko**: 33
- **Suicide_Talk**: 33
- **Shaolin**: 33
- **PositiveHerpes**: 33
- **MaddiMays**: 33
- **GeneseoIllinois**: 33
- **AutoClassifieds**: 33
- **littlenatie_**: 33
- **KoolKorsunsky**: 33
- **Airpower**: 33
- **foundCust0mCraft**: 33
- **PowerUniversity**: 66
- **BirdBoxMemes**: 33
- **Sevigor**: 33
- **Wreddit**: 66
- **digitalcats**: 33
- **llamas**: 66
- **XGIMI_Beta_Testing**: 33
- **orlandofishing**: 33
- **animalsasleaders**: 33
- **MUCC**: 33
- **Icefuse**: 33
- **FunkyLizard**: 33
- **Keensmemes**: 66
- **hamood**: 66
- **Terrarya**: 33
- **indianmetamisogynist**: 99
- **CelebrityBoots**: 66
- **autokey**: 66
- **kegswitharms**: 33
- **Unexpectedllamas**: 132
- **accidentalsubreddit**: 99
- **temporarygunowners**: 33
- **djtzone**: 33
- **LetsTalkRocks**: 66
- **antisexwork**: 33
- **aquaponie**: 33
- **clandestineairsoft**: 33
- **Paget_Schroetter**: 33
- **civcast**: 66
- **GHSE**: 33
- **massnorthshorefmlokin**: 33
- **HandmadeLure**: 33
- **PantslessPajamas**: 33
- **CrocodileHunters**: 33
- **lumberjackcidents**: 33
- **ShintoReligion**: 33
- **Moondrop_Sundrop**: 33
- **NarutoNinjaStorm**: 33
- **TrevTutor**: 66
- **ZichWedding**: 33
- **ParadigmFoundation**: 33
- **unsentLoveLetters1st**: 33
- **PaidInternships**: 33
- **serfposting**: 33
- **thatsthesexnumber**: 33
- **OutlanderPHEV**: 33
- **SUITrump**: 33
- **redwolf344**: 33
- **ratemyfridge**: 33
- **Starry_Flowers**: 33
- **f1online**: 66
- **hodlandshill**: 33
- **bigchungusarmy**: 33
- **OddportAcademy**: 33
- **MCxYuri**: 66
- **Islamic_History**: 99
- **FMF**: 33
- **JacindaArdern**: 33
- **bristoldrill**: 33
- **Halloweenlove**: 33
- **iPhone14Pro**: 99
- **Mountaineering**: 66
- **PuyEnVelay**: 66
- **GameGearMicroMods**: 33
- **ThriftStoreHaulsValue**: 66
- **Hoisan**: 33
- **artcovers**: 33
- **BrawlStarsGiftShop**: 33
- **GrumpyCoinCommunity**: 33
- **foundAwesomeness_999**: 33
- **imaginaryelectionscj**: 33
- **akbbyy**: 66
- **kleptomanicsupport**: 33
- **CivScarcity**: 66
- **DesignforFDM**: 66
- **cursedminecraftimages**: 66
- **simplethoughts**: 33
- **Nat1pub**: 66
- **hawaiiboonie**: 33
- **investigators**: 33
- **YabukiNako**: 33
- **hiphoptoday**: 99
- **maximalistfashion**: 33
- **accurateguides**: 66
- **DevExpress**: 33
- **WonderlandGameSeries**: 66
- **ErgoMechKeyboards**: 99
- **Urgot**: 99
- **PopulationMaps**: 66
- **CatsNamedToothless**: 33
- **MinecartDriverServer**: 33
- **GwapaGangNFT**: 66
- **Midnightthoughts**: 66
- **comicstriphistory**: 33
- **unibo_studenti**: 99
- **IronGiantMemes**: 33
- **homelessmind**: 33
- **MajesticCats**: 33
- **preprocessing**: 66
- **Ripsquad**: 99
- **Flowerfellfuture**: 33
- **badradio**: 33
- **SavingPrivateNoOne**: 33
- **ToothpasteEnema**: 33
- **karinerino**: 99
- **albertanudist**: 33
- **LINCHWAR**: 33
- **vanifestos**: 33
- **FakemonDesignContests**: 33
- **schmitty**: 33
- **gensoukyou**: 33
- **Warside**: 33
- **Apex_NC**: 33
- **CreamyCami8407**: 33
- **smallengines**: 33
- **IABrasil**: 33
- **FeiProtocol**: 66
- **BugandSeek**: 33
- **Finntroll**: 33
- **SneakersNL**: 33
- **ImpracticalFoods**: 33
- **arabhealth**: 66
- **okbuddynicotine**: 33
- **stoicfemina**: 99
- **mathsucks**: 33
- **freezepop**: 33
- **AuthoritarianNewsHub**: 33
- **corpsepaint**: 66
- **FXMasterRace**: 33
- **RetroConsoleModding**: 33
- **e17**: 66
- **UruguaySoftCombat**: 33
- **cupcakes_please**: 33
- **OfMonstersAndMen**: 33
- **HingeStories**: 33
- **sashannarcy**: 33
- **slapbass**: 66
- **fakepng**: 33
- **hibid**: 33
- **obletsvet**: 33
- **MaxarTechnologies**: 66
- **HopeKelesis3**: 33
- **wrestlingbelts**: 99
- **indian_desi**: 33
- **failedshinypokemon**: 99
- **ChutneyMusic**: 33
- **kleinerfanclub**: 33
- **TheeCrew**: 33
- **OctopathTraveler2**: 33
- **slidan**: 33
- **Worldbuilders**: 33
- **AussieTherians**: 66
- **PaganRockCollectors**: 33
- **SixDegreesOfCoulson**: 66
- **rubik**: 99
- **slk**: 33
- **Paper4Paper**: 33
- **YoutubeSings**: 33
- **KingdomHearts**: 33
- **reallycoolrobots**: 99
- **RedmiNote9pro**: 33
- **NarutoRP**: 99
- **basketballanalytics**: 33
- **russianfemboy**: 33
- **IndyOnlyfans**: 33
- **Teentalks**: 33
- **UKISP**: 33
- **VisualNovelCryptids**: 33
- **StudyCatalogPublish**: 33
- **Pawar2018**: 33
- **CanadaMonopolyProtest**: 33
- **chainsawLife**: 66
- **AltPolitic**: 33
- **yizhuoning**: 99
- **calculus**: 66
- **deservedapmaan**: 66
- **proofoftalk**: 33
- **joinelevatehealth**: 33
- **readbeforeeat**: 33
- **DramaticReading**: 33
- **worst_maps**: 33
- **anitwtitter**: 33
- **dramatic_kibbe**: 66
- **lookingforgroup**: 33
- **HISHE**: 66
- **laserweapons**: 66
- **CrazyPassengers**: 33
- **KSPMildlyInteresting**: 33
- **stpetebeach**: 99
- **BeingTube**: 66
- **JaydenBartels**: 99
- **Ticos_TI**: 66
- **MuseumOfArtifacts**: 33
- **GastroHealth**: 33
- **Gailbatescncdream**: 66
- **CoalitionAgainstEvil**: 99
- **TotalDramaPolls**: 66
- **TOPTIERSESSIONS**: 66
- **BossfightBestiary**: 66
- **playlost**: 33
- **DurangoCJAYM**: 33
- **Plunderstorm**: 33
- **Solitary**: 33
- **SproutValleyGame**: 33
- **LesbicasPortuguesas**: 33
- **blip**: 33
- **DeerfieldHighSchool**: 99
- **norwalkct**: 66
- **Dumbconspiracytheorys**: 33
- **FBEPC**: 33
- **bestdealsPH**: 33
- **careerguidance**: 33
- **izombiecw**: 33
- **tongueknots**: 33
- **MarketingCareers**: 66
- **Sommerspiele**: 33
- **Spaceagency2138**: 66
- **monkeynft**: 33
- **foundthewaltwhitepfp**: 33
- **MillaSofiaLookbook**: 66
- **horizonmemes**: 33
- **MVAgusta**: 33
- **FlagRedesigns**: 33
- **ShesJudgingYou**: 66
- **Lisle**: 33
- **NoMansSkyOfficial**: 33
- **PeptideForum**: 66
- **cutoutdresses**: 33
- **r2northstar**: 66
- **PatrioticMurica**: 33
- **WildernessBackpacking**: 66
- **herpes_dating**: 99
- **Unsatisfying**: 33
- **naming_animals**: 33
- **AskPhotography**: 66
- **jakeneutrality**: 66
- **tacticalops**: 33
- **BackyardBoxing**: 33
- **yujin**: 33
- **DammitZappa**: 33
- **AITL**: 33
- **jianca14**: 33
- **CuteAndFunnyAnime**: 33
- **MyHotwife_**: 33
- **Razorlight**: 33
- **yunli**: 33
- **wholesomebutstrange**: 66
- **jaywalking**: 66
- **marmello**: 33
- **turkayak1**: 66
- **reddithydrogen**: 99
- **Szormok_AVE**: 66
- **OpenMatte**: 33
- **XboxStudios**: 66
- **ScottishMeetUps**: 33
- **gsh**: 33
- **DasBootMaimais**: 33
- **phf**: 66
- **TopCustomPaper**: 66
- **TheJournal4Project**: 33
- **agfy**: 99
- **GOPwatch**: 33
- **meepo**: 66
- **LondonLadies**: 33
- **ukrainethelatest**: 99
- **PoliceDog**: 33
- **ActinicFolliculitis**: 33
- **BlockedAndReported**: 33
- **DPSdubai**: 66
- **skku**: 99
- **formulaminecraft**: 33
- **aremystitchestwisted**: 33
- **JESSIFOXXinflates**: 33
- **DungeonBuilder**: 33
- **myronkoops_**: 33
- **GeorginaCampbell**: 66
- **MeticulousEspresso**: 33
- **RoadRash64**: 33
- **HamiltonEuchre**: 33
- **JusticeforKarenRead_2**: 33
- **Lystfiskeri**: 33
- **dogsofukraine**: 33
- **CubaVexillology**: 33
- **Manga_Germany**: 33
- **MadLed**: 33
- **SuddenlyGayCommunism**: 66
- **TransFem18**: 33
- **Stickwars**: 66
- **minecraft_youtube**: 33
- **penguin**: 33
- **Charliethething**: 66
- **2imperialeurope4u**: 33
- **imsorrymumbo**: 33
- **guitarproblems**: 33
- **actuallyItsCMF**: 66
- **Surfari**: 66
- **WorldwideKahoot**: 66
- **BillBrasky**: 33
- **Shoeler**: 33
- **DishaPataniFans**: 33
- **tunnelsnakes**: 33
- **TangleAndWhisper**: 33
- **Onshape**: 33
- **LGV60**: 33
- **weddinghashtags**: 66
- **Travelbuddy**: 33
- **RedGreenShow**: 33
- **tankionlineint**: 33
- **mermaid**: 66
- **FansofMissAnnaGigi**: 66
- **AtheistCheeseCake**: 66
- **systemml**: 33
- **TransPuns**: 33
- **deepnutrition**: 33
- **cindernft**: 33
- **Farmasi_drama_snark**: 33
- **pollotarian**: 33
- **CityOfLiverpoolFC**: 33
- **femboy_kay**: 33
- **inmigracionhaitiana**: 33
- **OuterSpaceTravel**: 66
- **duckliftingnews**: 33
- **Tasha_sexy_21**: 33
- **SMK25**: 66
- **EnochianMagick**: 33
- **FtMFashionAdvice**: 33
- **twosentencevindicated**: 33
- **GameStopCanada**: 33
- **southbaldwincountyfwb**: 99
- **RealTruthSeeking**: 33
- **CarDIY**: 33
- **brookiecookiesnark**: 33
- **beardmeetsfood**: 33
- **shiphappens**: 33
- **zuhanygondolatok**: 33
- **jojoly**: 66
- **Love101Netflix**: 33
- **GoodTask**: 66
- **F95zonee**: 66
- **cutetextfaces**: 33
- **boardgamingmy**: 66
- **carbobike**: 33
- **FlowerGirlsTamagotchi**: 99
- **Gemology**: 33
- **northrupphoto**: 33
- **rdms**: 33
- **ErikaHerceg**: 66
- **aua**: 66
- **blockethaveri**: 33
- **routeros**: 33
- **IndieWorldOrder**: 33
- **EliteOuendan**: 99
- **ChristianHistory**: 33
- **EastAfricaNews**: 33
- **FatFrankie**: 33
- **HDDcoinNetwork**: 33
- **americanman**: 33
- **callthephotoshoppers**: 33
- **Pisstearmanseries**: 33
- **shaderDev**: 33
- **RollWithAdvantage**: 33
- **thenightmarecollector**: 33
- **SeadooSpark**: 33
- **JadeAzevedo**: 66
- **CHSMichelleSnark**: 33
- **unpresidented**: 33
- **SimLab**: 99
- **Cybers_public_school**: 33
- **monte_video**: 33
- **chaoticswifties**: 66
- **ImmersiveDaydreaming**: 33
- **MattChampion**: 33
- **RLCarShow**: 33
- **KtuKerala**: 33
- **TaylorSwiftAnkles**: 66
- **ryze**: 33
- **ratemysinging**: 99
- **HMHSBritannic**: 33
- **WhoSampled**: 33
- **DiscussChrist**: 33
- **JavelinTalk**: 33
- **titaniumbikes**: 33
- **AnimeAndEtc**: 66
- **KSPToMarslanderteam**: 66
- **Catchirps**: 33
- **virtualwarfighter**: 33
- **coopgames**: 66
- **YouthHockeyHighlights**: 33
- **buyaplantsellaplant**: 33
- **BolsaChile**: 33
- **Dikpipiler**: 33
- **EryingMotherboard**: 33
- **RUFUSDUSOL**: 66
- **sanantoniobeer**: 33
- **techpolitics**: 66
- **RockPosters**: 33
- **dorajar**: 33
- **suddenlyvicodin**: 66
- **nadide**: 33
- **BigStickEnergy**: 66
- **zoomergen**: 99
- **drivingUK**: 66
- **MSPA**: 66
- **TheATOShow**: 33
- **TheMegatronDon**: 33
- **EnemiesBeingBros**: 33
- **ratmemes**: 33
- **MirMGlobal**: 33
- **AppleAndOnion**: 33
- **B638**: 66
- **LGBT_iel**: 66
- **TrainingEverywhere**: 99
- **biggestbroent**: 66
- **WWYDif**: 33
- **DSTAlumnaeChapter**: 33
- **WWE_Eva_Marie**: 33
- **guatemala_gay**: 66
- **CringyAntiVegans**: 33
- **NinaDobrev_**: 33
- **makemoneyforexreddit**: 33
- **iJustPublishedMyGame**: 66
- **lordtuts**: 33
- **brennanjones**: 33
- **SubmarineTitans**: 33
- **ClonazepamHealth**: 33
- **okbuddyrhodok**: 66
- **FinancialMeme**: 33
- **FiverFilth**: 33
- **Theni**: 33
- **VZfit**: 132
- **FunctionalFill**: 33
- **SACWATR**: 33
- **GiuliaBenitee**: 33
- **Reality_Quest**: 33
- **SheGotFrameWork**: 33
- **abertayhackers**: 99
- **ThePower**: 66
- **IceandFirePowers**: 33
- **datasatanism**: 66
- **badshirtanatomy**: 33
- **Necrontyr**: 33
- **signs**: 33
- **vrbohosts**: 66
- **erannorthchronicles**: 66
- **Brainysweddit**: 66
- **Ooergonewild**: 66
- **EB2_NIW_Cybersecurity**: 33
- **Djlynch2009**: 33
- **foundtheAngela**: 66
- **digitaltabletop**: 33
- **Idvitalia**: 33
- **surebet**: 33
- **subscriptionsharing**: 33
- **Exodus90**: 33
- **northfork**: 33
- **Emsland**: 33
- **CoilCommunity**: 66
- **shameless**: 33
- **ManitobaGardening**: 33
- **Telekinetic**: 99
- **TheNew52**: 33
- **TeamSky**: 33
- **HypixelSlovenija**: 33
- **Stabbingthemap**: 33
- **mysteriousmetallicorb**: 33
- **DeerVirus**: 66
- **EquinoxEv**: 33
- **titration**: 33
- **NoSenseMemes**: 33
- **HellenicMemes**: 66
- **FoundTheGiantSquid**: 33
- **MetroidDread**: 33
- **Chiskate**: 33
- **cannadips**: 66
- **WeHateYT**: 33
- **askdisqus**: 33
- **marniexvictor**: 66
- **RootandDigby**: 33
- **SuperheroLeague**: 33
- **CipherMains_HSR**: 33
- **obviouslyincorrect**: 33
- **InfinityNado**: 33
- **desporto**: 33
- **Dallas_commons**: 33
- **go_ask_your_crush_out**: 132
- **heelonly**: 33
- **Porsgrunn**: 33
- **truetexan**: 66
- **rushzilla**: 33
- **banime**: 33
- **Nalbinding**: 33
- **oliviadejarnet**: 66
- **preparedguitar**: 66
- **NielsonShow**: 66
- **transsexmalesurgeries**: 99
- **cleanpuppers**: 33
- **InesperadoPau**: 33
- **louderthanlife**: 33
- **ZamasuDidNothingWrong**: 99
- **OdeyaRush**: 33
- **CyberSecurityAdvice**: 33
- **dutchposer**: 33
- **WK2**: 33
- **minecraftinfographics**: 33
- **applycoldwatertoburn**: 66
- **fearofheights**: 33
- **themeaneyoflife**: 33
- **mildlyaccessible**: 33
- **BLACKED_AL**: 33
- **bluegrassguitar**: 66
- **azuretips**: 66
- **fitbod**: 33
- **somaligay**: 33
- **TheWellsTable**: 33
- **Poudlard**: 33
- **10000ShipsHBO**: 66
- **TookOurJerbs**: 66
- **FreeGameKeys**: 33
- **CloudAssessments**: 33
- **OTRebels**: 66
- **berlinfoodies**: 66
- **DisneyParkHistory**: 33
- **infjhome**: 33
- **jotform**: 33
- **danielledominguez_**: 66
- **ryzencpu**: 33
- **bandirma**: 33
- **cdldriver**: 99
- **CarpetsforAirports**: 66
- **Superchunk**: 66
- **artisansasylum**: 33
- **ClubeDoZanzigau**: 99
- **HowToMen**: 33
- **accidentaldiamondclub**: 33
- **CelebrityOutside**: 33
- **RobertoArguelles**: 33
- **ConcourseCityTransit**: 33
- **VisitingNorway**: 33
- **Sims4Build**: 33
- **TNOmod**: 66
- **WorshipTheColander**: 33
- **CowboyIPTV**: 66
- **owsisajoke**: 33
- **DACA_Medicine**: 33
- **shironeko**: 33
- **anomic**: 33
- **GardeningIndia2**: 33
- **TeenDevsIndia**: 66
- **Ssupdhq**: 33
- **JCVAP**: 33
- **DecorativePlumbing**: 99
- **TwoSentenceFeels**: 33
- **CodeMystery**: 66
- **htlhollabrunn**: 66
- **AlternativeMedia**: 33
- **guildgrumps**: 33
- **kiwibrowser**: 33
- **BabyCheeks**: 33
- **Milo_Baughman_Design**: 66
- **PadresVent**: 33
- **Nyelveszet**: 33
- **AltFantasySports**: 33
- **TOTC_UFOs**: 33
- **VIU**: 66
- **FistBros**: 99
- **catTunes**: 33
- **okbuddyinfantry**: 33
- **We_are_doing_this**: 33
- **asianamericanytsnark**: 33
- **lssmod_526**: 33
- **zora**: 33
- **Beginners_Photography**: 99
- **Anarcho_Frontierism**: 33
- **DiwaariHeads**: 33
- **hmtforsale**: 33
- **LosingItTogether**: 99
- **StormHawksTv**: 33
- **theCRAMPSstaysick**: 33
- **nokeric**: 33
- **GRITS**: 66
- **KamersNijmegen**: 33
- **UkraineLongRead**: 99
- **MyInternetCrushes**: 66
- **LawandOrder_OC**: 33
- **rachaelharris**: 33
- **In_the_name_of_Beers**: 99
- **godscomputer**: 33
- **PokemonPlaza**: 33
- **theUKSpaceNews**: 33
- **CivRealms**: 33
- **expectedundertale**: 99
- **BigGangsterMod**: 66
- **minecraftdupers**: 66
- **BipocWitchesandPagans**: 99
- **GSMFerroglobe**: 66
- **SingleIncomeNoKid**: 66
- **FansOfBAL**: 33
- **TwinRiverPoker**: 66
- **Redditquandaryroyale**: 33
- **chiptunes**: 33
- **notdisappointed**: 66
- **DionisoES**: 33
- **SweetBabyRays**: 33
- **khargone**: 33
- **KennedySpaceCenter**: 33
- **jmc2obj**: 66
- **Aerospace_Engineering**: 33
- **xb70**: 66
- **SheSaidSheWas12**: 66
- **LockportTownshipHS**: 33
- **F5Robin**: 33
- **BroilKingKeg**: 66
- **highdesert**: 66
- **AdvancedEntityScience**: 33
- **CATAGRUM**: 33
- **tiffin**: 66
- **Wealthsimple_Invest**: 99
- **xplane12**: 99
- **silverbackgorillas**: 33
- **boysarescary**: 33
- **dendaris**: 66
- **toyexchange**: 99
- **Brightline**: 33
- **LordoftheLost**: 33
- **YardHypeSpot**: 66
- **CalebHarris**: 33
- **livefromhere**: 33
- **Revita**: 33
- **sgcgrading**: 33
- **HecarimMains**: 33
- **TheMetGala**: 132
- **ForzaModding**: 33
- **Farbenfuchs_Facharzt**: 33
- **SteamWorldHeist**: 33
- **MeerkatMillionaires**: 66
- **Foodrecalls**: 66
- **CodeFellows**: 66
- **OffGridCrappers**: 66
- **ChurchOfBlueAlienChef**: 33
- **VeeInfuso**: 66
- **GameScience**: 132
- **wtfshippingsupplies**: 33
- **Gamecocks**: 66
- **WeloBk**: 33
- **SneakersEgypt**: 33
- **bon_appetit**: 33
- **RemoteHRJobs**: 33
- **feathery**: 33
- **CuriousExpedition**: 33
- **Cebu**: 33
- **SmallBusinessSellers**: 33
- **germanchubbywife**: 33
- **ImaginaryPirates**: 66
- **TheDeprogramEurope**: 33
- **CycleKarts**: 66
- **hornystan**: 33
- **schleeey**: 33
- **JetEngines**: 33
- **QUITOO**: 66
- **findayoutube**: 33
- **A_Separate_Peace**: 33
- **kittykeep**: 33
- **EiyuuouBuwoKiwameru**: 66
- **YECirclehugs**: 33
- **hallenhollylove**: 66
- **PhotoEnts**: 66
- **Network_Automation**: 33
- **GymOwnerNetwork**: 66
- **Bunkd**: 99
- **kota**: 33
- **canplfantasy**: 33
- **FortunateCum**: 66
- **waukeshaparade**: 33
- **Mario3DWorldOnline**: 33
- **Gandhinagar_**: 66
- **Arazhul**: 33
- **kfee**: 33
- **ShinyPokeCommunity**: 33
- **ShortStoryPrompts**: 33
- **thrashcub**: 66
- **Cuteaggression**: 33
- **CritCrab**: 33
- **Alipay**: 33
- **agrochemistry**: 99
- **ThatsLegal**: 33
- **EarthFinalConflict**: 33
- **Tetr_College**: 33
- **FoxestheSinger**: 66
- **PFFO**: 33
- **whyperart**: 33
- **heilzamememes**: 66
- **TamilCinephiles**: 33
- **trumpsolana**: 33
- **nuance**: 33
- **RealPDKC**: 66
- **fukuyamameme**: 33
- **GreenwichVillage**: 33
- **Tramlines**: 66

## Languages
Unique languages and their frequencies:
- **French**: 450
- **Italian**: 450
- **German**: 450
- **English (American)**: 450
- **Korean**: 450
- **Spanish**: 450
- **Chinese (traditional)**: 450
- **Chinese (simplified)**: 450
- **Japanese**: 450
- **Russian**: 450
- **Czech**: 450
- **Danish**: 450
- **Dutch**: 450
- **Arabic**: 450
- **Bulgarian**: 450
- **English (British)**: 450
- **Estonian**: 450
- **Hungarian**: 450
- **Indonesian**: 450
- **Norwegian (bokmål)**: 450
- **Portuguese**: 450
- **Greek**: 450
- **Lithuanian**: 450
- **Finnish**: 450
- **Latvian**: 450
- **Portuguese (Brazilian)**: 450
- **Polish**: 450
- **Swedish**: 450
- **Slovak**: 450
- **Slovenian**: 450
- **Romanian**: 450
- **Turkish**: 450
- **Ukrainian**: 450

## Models
Unique models and their frequencies:
- **Granite-3.1-8B-Instruct**: 2871
- **UnieInfra2.aqua-mini**: 3069
- **aqua-mini-fast**: 6303
- **phi4**: 2607

## Article Statistics
- **Average article length (characters):** 1275.34
- **Minimum article length (characters):** 2
- **Maximum article length (characters):** 15407
- **Average word count:** 184.62
- **Minimum word count:** 1
- **Maximum word count:** 1134
",Medium,2.0
Summarization,unieai/multilingual-stories-original,2.0,7.0,2025-02-05 10:01:17+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
language: 
  - fr  # French
  - it  # Italian
  - de  # German
  - en  # English
  - ko  # Korean
  - es  # Spanish
  - zh  # Chinese
  - ja  # Japanese
  - ru  # Russian
  - cs  # Czech
  - da  # Danish
  - nl  # Dutch
  - ar  # Arabic
  - bg  # Bulgarian
  - et  # Estonian
  - hu  # Hungarian
  - id  # Indonesian
  - nb  # Norwegian
  - pt  # Portuguese
  - el  # Greek
  - lt  # Lithuanian
  - fi  # Finnish
  - lv  # Latvian
  - pl  # Polish
  - sv  # Swedish
  - sk  # Slovak
  - sl  # Slovenian
  - ro  # Romanian
  - tr  # Turkish
  - uk  # Ukrainian

language_bcp47:
  - zh-Hant  # Chinese (Traditional)
  - zh-Hans  # Chinese (Simplified)
  - en-GB  # English (British)
  - pt-BR  # Portuguese (Brazilian)

pretty_name: ""Multilingual Dataset""

tags:
  - multilingual
  - NLP
  - text-generation
  - text-classification
  - dataset-processing
  - large-scale-dataset

license: ""apache-2.0""

task_categories:
  - text-classification
  - text-generation
  - text2text-generation
  - feature-extraction
  - translation
  - summarization

size_categories:
  - large

source_datasets: []
---

# Multilingual Dataset

This dataset contains multilingual articles generated using various models.

# Dataset Summary

**Total records:** 14850

## Categories
Unique categories and their frequencies:
- **OmundoAmanha**: 99
- **malinois**: 33
- **CBDPouches**: 33
- **ingenieurs**: 66
- **PPeperomioides**: 33
- **DiscGolfCarts**: 33
- **TIMAF**: 33
- **AntisocialMemeClub**: 33
- **Tancan**: 66
- **ogerwaters**: 33
- **MotosBr**: 33
- **lgballt**: 66
- **CipherAcademy**: 33
- **LofiEdits**: 33
- **jennyraee**: 33
- **RonnieHotdogs**: 33
- **Bratzillaz**: 66
- **cubiccommunity**: 33
- **Labectance**: 33
- **ChasmicOccult**: 33
- **exresidents**: 33
- **GamingParents**: 33
- **Elisaonfire**: 33
- **blackdesert**: 66
- **MockElectionsAUS**: 33
- **DogfreeHumor**: 33
- **TToT**: 33
- **GeneralContractor**: 33
- **nkim**: 33
- **HeadandNeckCancer**: 33
- **CantonReddit**: 99
- **GrassrootsSelectAZ**: 33
- **BlackCloverMobile**: 33
- **uclafood**: 66
- **trueskyrimmods**: 33
- **heximal**: 33
- **Hetrochromia**: 33
- **photographysatx**: 33
- **AsturiasPatriaQuerida**: 33
- **2b2tqueue**: 99
- **kristyglas**: 33
- **advancedwitchcraft**: 33
- **rockymountainhouse**: 132
- **Kontaktanzeigen**: 33
- **DepressionVentSpace**: 33
- **MagickaWizardWarz**: 33
- **miniature_tigers**: 66
- **ifunnyNovoMundo**: 33
- **ShrineOfAC**: 66
- **EDAnonPics**: 33
- **FontLab**: 33
- **agent_of_shield**: 66
- **Infinity_laugh**: 33
- **CataclismoRTS**: 33
- **5DChess**: 33
- **idiotsflyingdrones**: 33
- **cscareerquestionsIN**: 66
- **birdsonbirds**: 99
- **msvduisburg**: 33
- **Eunha**: 33
- **Rotundanimals**: 33
- **Norse**: 66
- **treadstone**: 33
- **MadilynMei**: 66
- **ValentinaRose**: 66
- **AzurePercept**: 33
- **HollyHoes**: 66
- **exquisitecorpse**: 33
- **NoglaTerroriserReacts**: 33
- **StephenHillenburg**: 99
- **maxima**: 33
- **turndotme**: 33
- **dakboard**: 66
- **NetflixAnime**: 66
- **semaglutideweightloss**: 198
- **TheGatewayExperience**: 33
- **SubredditForFriends**: 33
- **porchpiratepwnage**: 33
- **throughtheages**: 66
- **TheFakeDentist**: 66
- **SquidMoonCoin**: 99
- **BazarRogueExcrement**: 66
- **techmaestro**: 33
- **ChurchOf_AquaxAixRuby**: 33
- **NLProducerGang**: 66
- **flannel**: 33
- **alternativewritings**: 66
- **Crypts**: 33
- **Mr_MandM**: 33
- **7counter**: 33
- **TwitchSafe**: 66
- **dRehmFlight**: 66
- **TheLetterOctothorpe**: 33
- **chuck**: 33
- **ConfrontingChaos**: 33
- **GettingOverItGame**: 33
- **KillYourConsole**: 33
- **Male_Studies**: 66
- **lacenterwa**: 33
- **IrishFishing**: 66
- **ptvo**: 99
- **Phigolf**: 33
- **the_derek**: 33
- **preciousmoments**: 33
- **AnaDaniela69xxx**: 33
- **blergg**: 33
- **GraphingCalculator**: 132
- **Knoxville**: 33
- **The_Real_Austin**: 33
- **EricBrakey**: 33
- **NightWitchGang**: 33
- **CityNerd**: 99
- **Pratibha_Ranta**: 99
- **BSCStation**: 33
- **WLWactually**: 33
- **suddenlymandjtv**: 33
- **Celestron**: 99
- **CSMagicRecipe**: 33
- **Phone_Foldables**: 99
- **MavuikaMainsGI**: 33
- **InboxPounds**: 33
- **janiscorner**: 33
- **PECG**: 33
- **IcebergExplained**: 33
- **GreedIncorporated**: 33
- **SurvivingRingworm**: 66
- **Snowplow**: 33
- **Tadano**: 66
- **funtechnology**: 33
- **ACUsedVillagers**: 33
- **Fordtractors**: 66
- **CargoWise**: 33
- **earwig**: 33
- **nocontextaudio**: 33
- **kenbarbbe**: 33
- **cwstonekingfans**: 33
- **VicBWolfArt**: 33
- **BrothersoftheGrain**: 99
- **TheRunaways**: 33
- **NewsWorld**: 66
- **DELF**: 33
- **deardiary**: 33
- **MyOneLineDogma**: 33
- **bunchofamateurs**: 33
- **Goulburn**: 66
- **Mattpryor**: 33
- **communitydevelopment**: 33
- **MiddleGradeWriters**: 66
- **businesslaw**: 33
- **BanglaMuktobak**: 33
- **StupidMario**: 66
- **Grimgar**: 33
- **HarvestMoonGBA**: 66
- **SOTR_auditions**: 99
- **TheShannaraChronicles**: 33
- **overcoding**: 33
- **Awareparents**: 66
- **dogmalove**: 66
- **HAESfood**: 33
- **EvergreenSkills**: 33
- **valentina94O**: 33
- **fsft**: 33
- **OkBuddyYharim**: 33
- **Swapandsell**: 66
- **warhospital**: 33
- **Demeo**: 33
- **TvIntroMashups**: 33
- **Winiday**: 132
- **algogaming**: 33
- **IhateElon**: 99
- **XtotheV**: 33
- **SouthOfTheCircleGame**: 66
- **wlu**: 33
- **MarketingFails**: 33
- **constanzavelazcoteton**: 33
- **legoww1**: 33
- **cheekdimples**: 33
- **circularguements**: 33
- **BattleforBFCI**: 33
- **JohnFarnham**: 99
- **DisablEdCanTeach**: 33
- **saysueme**: 66
- **Rengarmains**: 33
- **overclockfps**: 33
- **GOTV**: 66
- **Business_Management**: 33
- **latticeclimbing**: 33
- **rarehugs**: 33
- **AroAllo**: 33
- **gfbreadrecipies**: 66
- **JCBDL**: 33
- **snowplowproject**: 33
- **knockedloose**: 66
- **ScoreLords**: 33
- **ShopImprovements**: 33
- **256cub**: 66
- **UltiumCellsLLC**: 66
- **ObscureGIFs**: 33
- **alchademy**: 33
- **CastroPodcast**: 33
- **MelissaONeilPics**: 66
- **ConsensusDebate**: 33
- **okbuddyplants**: 33
- **mischakim**: 99
- **Palermo**: 33
- **losersofinstagram**: 99
- **lezconnect**: 33
- **TyreNichols**: 33
- **Primis_Bank**: 33
- **givemeanidea**: 33
- **Actualfixes**: 33
- **GrowingGirls**: 33
- **YukongMains**: 66
- **YapDollarMyBeloved**: 33
- **trials**: 33
- **wbmedicos**: 33
- **RaceTrackDesigns**: 33
- **supercellmoment**: 33
- **Survivors_BBC_1975**: 66
- **the_mysterium**: 33
- **KillerKitty**: 33
- **kaybomb**: 33
- **Blogsabouttea**: 33
- **OlympiacosBC**: 33
- **ReDIY**: 33
- **Hampture**: 33
- **Demonias_**: 66
- **WILTWIFLS**: 33
- **Analysts_of_MBTI**: 66
- **SoulSplitGame**: 33
- **gracelyra**: 33
- **Meme__World**: 66
- **CrescentHub**: 66
- **psytrancefrance**: 33
- **2016campaign**: 66
- **theguild3**: 33
- **practicingheathenry**: 33
- **budgetguitargear**: 99
- **Dogma**: 33
- **novomesto**: 33
- **ContinentedoBombarral**: 33
- **Comadre**: 33
- **thethyroidmadness**: 33
- **Xooginme**: 33
- **HomebrewTactical**: 33
- **kosmic**: 33
- **GameGeeks**: 33
- **TESHeroes**: 66
- **SchoolMotivation**: 66
- **dataforenergyskills**: 33
- **VeganJunkFood**: 66
- **TheRealArkhamWorld**: 33
- **ExpressiveArtsTherapy**: 33
- **TojinoMiko**: 33
- **Suicide_Talk**: 33
- **Shaolin**: 33
- **PositiveHerpes**: 33
- **MaddiMays**: 33
- **GeneseoIllinois**: 33
- **AutoClassifieds**: 33
- **littlenatie_**: 33
- **KoolKorsunsky**: 33
- **Airpower**: 33
- **foundCust0mCraft**: 33
- **PowerUniversity**: 66
- **BirdBoxMemes**: 33
- **Sevigor**: 33
- **Wreddit**: 66
- **digitalcats**: 33
- **llamas**: 66
- **XGIMI_Beta_Testing**: 33
- **orlandofishing**: 33
- **animalsasleaders**: 33
- **MUCC**: 33
- **Icefuse**: 33
- **FunkyLizard**: 33
- **Keensmemes**: 66
- **hamood**: 66
- **Terrarya**: 33
- **indianmetamisogynist**: 99
- **CelebrityBoots**: 66
- **autokey**: 66
- **kegswitharms**: 33
- **Unexpectedllamas**: 132
- **accidentalsubreddit**: 99
- **temporarygunowners**: 33
- **djtzone**: 33
- **LetsTalkRocks**: 66
- **antisexwork**: 33
- **aquaponie**: 33
- **clandestineairsoft**: 33
- **Paget_Schroetter**: 33
- **civcast**: 66
- **GHSE**: 33
- **massnorthshorefmlokin**: 33
- **HandmadeLure**: 33
- **PantslessPajamas**: 33
- **CrocodileHunters**: 33
- **lumberjackcidents**: 33
- **ShintoReligion**: 33
- **Moondrop_Sundrop**: 33
- **NarutoNinjaStorm**: 33
- **TrevTutor**: 66
- **ZichWedding**: 33
- **ParadigmFoundation**: 33
- **unsentLoveLetters1st**: 33
- **PaidInternships**: 33
- **serfposting**: 33
- **thatsthesexnumber**: 33
- **OutlanderPHEV**: 33
- **SUITrump**: 33
- **redwolf344**: 33
- **ratemyfridge**: 33
- **Starry_Flowers**: 33
- **f1online**: 66
- **hodlandshill**: 33
- **bigchungusarmy**: 33
- **OddportAcademy**: 33
- **MCxYuri**: 66
- **Islamic_History**: 99
- **FMF**: 33
- **JacindaArdern**: 33
- **bristoldrill**: 33
- **Halloweenlove**: 33
- **iPhone14Pro**: 99
- **Mountaineering**: 66
- **PuyEnVelay**: 66
- **GameGearMicroMods**: 33
- **ThriftStoreHaulsValue**: 66
- **Hoisan**: 33
- **artcovers**: 33
- **BrawlStarsGiftShop**: 33
- **GrumpyCoinCommunity**: 33
- **foundAwesomeness_999**: 33
- **imaginaryelectionscj**: 33
- **akbbyy**: 66
- **kleptomanicsupport**: 33
- **CivScarcity**: 66
- **DesignforFDM**: 66
- **cursedminecraftimages**: 66
- **simplethoughts**: 33
- **Nat1pub**: 66
- **hawaiiboonie**: 33
- **investigators**: 33
- **YabukiNako**: 33
- **hiphoptoday**: 99
- **maximalistfashion**: 33
- **accurateguides**: 66
- **DevExpress**: 33
- **WonderlandGameSeries**: 66
- **ErgoMechKeyboards**: 99
- **Urgot**: 99
- **PopulationMaps**: 66
- **CatsNamedToothless**: 33
- **MinecartDriverServer**: 33
- **GwapaGangNFT**: 66
- **Midnightthoughts**: 66
- **comicstriphistory**: 33
- **unibo_studenti**: 99
- **IronGiantMemes**: 33
- **homelessmind**: 33
- **MajesticCats**: 33
- **preprocessing**: 66
- **Ripsquad**: 99
- **Flowerfellfuture**: 33
- **badradio**: 33
- **SavingPrivateNoOne**: 33
- **ToothpasteEnema**: 33
- **karinerino**: 99
- **albertanudist**: 33
- **LINCHWAR**: 33
- **vanifestos**: 33
- **FakemonDesignContests**: 33
- **schmitty**: 33
- **gensoukyou**: 33
- **Warside**: 33
- **Apex_NC**: 33
- **CreamyCami8407**: 33
- **smallengines**: 33
- **IABrasil**: 33
- **FeiProtocol**: 66
- **BugandSeek**: 33
- **Finntroll**: 33
- **SneakersNL**: 33
- **ImpracticalFoods**: 33
- **arabhealth**: 66
- **okbuddynicotine**: 33
- **stoicfemina**: 99
- **mathsucks**: 33
- **freezepop**: 33
- **AuthoritarianNewsHub**: 33
- **corpsepaint**: 66
- **FXMasterRace**: 33
- **RetroConsoleModding**: 33
- **e17**: 66
- **UruguaySoftCombat**: 33
- **cupcakes_please**: 33
- **OfMonstersAndMen**: 33
- **HingeStories**: 33
- **sashannarcy**: 33
- **slapbass**: 66
- **fakepng**: 33
- **hibid**: 33
- **obletsvet**: 33
- **MaxarTechnologies**: 66
- **HopeKelesis3**: 33
- **wrestlingbelts**: 99
- **indian_desi**: 33
- **failedshinypokemon**: 99
- **ChutneyMusic**: 33
- **kleinerfanclub**: 33
- **TheeCrew**: 33
- **OctopathTraveler2**: 33
- **slidan**: 33
- **Worldbuilders**: 33
- **AussieTherians**: 66
- **PaganRockCollectors**: 33
- **SixDegreesOfCoulson**: 66
- **rubik**: 99
- **slk**: 33
- **Paper4Paper**: 33
- **YoutubeSings**: 33
- **KingdomHearts**: 33
- **reallycoolrobots**: 99
- **RedmiNote9pro**: 33
- **NarutoRP**: 99
- **basketballanalytics**: 33
- **russianfemboy**: 33
- **IndyOnlyfans**: 33
- **Teentalks**: 33
- **UKISP**: 33
- **VisualNovelCryptids**: 33
- **StudyCatalogPublish**: 33
- **Pawar2018**: 33
- **CanadaMonopolyProtest**: 33
- **chainsawLife**: 66
- **AltPolitic**: 33
- **yizhuoning**: 99
- **calculus**: 66
- **deservedapmaan**: 66
- **proofoftalk**: 33
- **joinelevatehealth**: 33
- **readbeforeeat**: 33
- **DramaticReading**: 33
- **worst_maps**: 33
- **anitwtitter**: 33
- **dramatic_kibbe**: 66
- **lookingforgroup**: 33
- **HISHE**: 66
- **laserweapons**: 66
- **CrazyPassengers**: 33
- **KSPMildlyInteresting**: 33
- **stpetebeach**: 99
- **BeingTube**: 66
- **JaydenBartels**: 99
- **Ticos_TI**: 66
- **MuseumOfArtifacts**: 33
- **GastroHealth**: 33
- **Gailbatescncdream**: 66
- **CoalitionAgainstEvil**: 99
- **TotalDramaPolls**: 66
- **TOPTIERSESSIONS**: 66
- **BossfightBestiary**: 66
- **playlost**: 33
- **DurangoCJAYM**: 33
- **Plunderstorm**: 33
- **Solitary**: 33
- **SproutValleyGame**: 33
- **LesbicasPortuguesas**: 33
- **blip**: 33
- **DeerfieldHighSchool**: 99
- **norwalkct**: 66
- **Dumbconspiracytheorys**: 33
- **FBEPC**: 33
- **bestdealsPH**: 33
- **careerguidance**: 33
- **izombiecw**: 33
- **tongueknots**: 33
- **MarketingCareers**: 66
- **Sommerspiele**: 33
- **Spaceagency2138**: 66
- **monkeynft**: 33
- **foundthewaltwhitepfp**: 33
- **MillaSofiaLookbook**: 66
- **horizonmemes**: 33
- **MVAgusta**: 33
- **FlagRedesigns**: 33
- **ShesJudgingYou**: 66
- **Lisle**: 33
- **NoMansSkyOfficial**: 33
- **PeptideForum**: 66
- **cutoutdresses**: 33
- **r2northstar**: 66
- **PatrioticMurica**: 33
- **WildernessBackpacking**: 66
- **herpes_dating**: 99
- **Unsatisfying**: 33
- **naming_animals**: 33
- **AskPhotography**: 66
- **jakeneutrality**: 66
- **tacticalops**: 33
- **BackyardBoxing**: 33
- **yujin**: 33
- **DammitZappa**: 33
- **AITL**: 33
- **jianca14**: 33
- **CuteAndFunnyAnime**: 33
- **MyHotwife_**: 33
- **Razorlight**: 33
- **yunli**: 33
- **wholesomebutstrange**: 66
- **jaywalking**: 66
- **marmello**: 33
- **turkayak1**: 66
- **reddithydrogen**: 99
- **Szormok_AVE**: 66
- **OpenMatte**: 33
- **XboxStudios**: 66
- **ScottishMeetUps**: 33
- **gsh**: 33
- **DasBootMaimais**: 33
- **phf**: 66
- **TopCustomPaper**: 66
- **TheJournal4Project**: 33
- **agfy**: 99
- **GOPwatch**: 33
- **meepo**: 66
- **LondonLadies**: 33
- **ukrainethelatest**: 99
- **PoliceDog**: 33
- **ActinicFolliculitis**: 33
- **BlockedAndReported**: 33
- **DPSdubai**: 66
- **skku**: 99
- **formulaminecraft**: 33
- **aremystitchestwisted**: 33
- **JESSIFOXXinflates**: 33
- **DungeonBuilder**: 33
- **myronkoops_**: 33
- **GeorginaCampbell**: 66
- **MeticulousEspresso**: 33
- **RoadRash64**: 33
- **HamiltonEuchre**: 33
- **JusticeforKarenRead_2**: 33
- **Lystfiskeri**: 33
- **dogsofukraine**: 33
- **CubaVexillology**: 33
- **Manga_Germany**: 33
- **MadLed**: 33
- **SuddenlyGayCommunism**: 66
- **TransFem18**: 33
- **Stickwars**: 66
- **minecraft_youtube**: 33
- **penguin**: 33
- **Charliethething**: 66
- **2imperialeurope4u**: 33
- **imsorrymumbo**: 33
- **guitarproblems**: 33
- **actuallyItsCMF**: 66
- **Surfari**: 66
- **WorldwideKahoot**: 66
- **BillBrasky**: 33
- **Shoeler**: 33
- **DishaPataniFans**: 33
- **tunnelsnakes**: 33
- **TangleAndWhisper**: 33
- **Onshape**: 33
- **LGV60**: 33
- **weddinghashtags**: 66
- **Travelbuddy**: 33
- **RedGreenShow**: 33
- **tankionlineint**: 33
- **mermaid**: 66
- **FansofMissAnnaGigi**: 66
- **AtheistCheeseCake**: 66
- **systemml**: 33
- **TransPuns**: 33
- **deepnutrition**: 33
- **cindernft**: 33
- **Farmasi_drama_snark**: 33
- **pollotarian**: 33
- **CityOfLiverpoolFC**: 33
- **femboy_kay**: 33
- **inmigracionhaitiana**: 33
- **OuterSpaceTravel**: 66
- **duckliftingnews**: 33
- **Tasha_sexy_21**: 33
- **SMK25**: 66
- **EnochianMagick**: 33
- **FtMFashionAdvice**: 33
- **twosentencevindicated**: 33
- **GameStopCanada**: 33
- **southbaldwincountyfwb**: 99
- **RealTruthSeeking**: 33
- **CarDIY**: 33
- **brookiecookiesnark**: 33
- **beardmeetsfood**: 33
- **shiphappens**: 33
- **zuhanygondolatok**: 33
- **jojoly**: 66
- **Love101Netflix**: 33
- **GoodTask**: 66
- **F95zonee**: 66
- **cutetextfaces**: 33
- **boardgamingmy**: 66
- **carbobike**: 33
- **FlowerGirlsTamagotchi**: 99
- **Gemology**: 33
- **northrupphoto**: 33
- **rdms**: 33
- **ErikaHerceg**: 66
- **aua**: 66
- **blockethaveri**: 33
- **routeros**: 33
- **IndieWorldOrder**: 33
- **EliteOuendan**: 99
- **ChristianHistory**: 33
- **EastAfricaNews**: 33
- **FatFrankie**: 33
- **HDDcoinNetwork**: 33
- **americanman**: 33
- **callthephotoshoppers**: 33
- **Pisstearmanseries**: 33
- **shaderDev**: 33
- **RollWithAdvantage**: 33
- **thenightmarecollector**: 33
- **SeadooSpark**: 33
- **JadeAzevedo**: 66
- **CHSMichelleSnark**: 33
- **unpresidented**: 33
- **SimLab**: 99
- **Cybers_public_school**: 33
- **monte_video**: 33
- **chaoticswifties**: 66
- **ImmersiveDaydreaming**: 33
- **MattChampion**: 33
- **RLCarShow**: 33
- **KtuKerala**: 33
- **TaylorSwiftAnkles**: 66
- **ryze**: 33
- **ratemysinging**: 99
- **HMHSBritannic**: 33
- **WhoSampled**: 33
- **DiscussChrist**: 33
- **JavelinTalk**: 33
- **titaniumbikes**: 33
- **AnimeAndEtc**: 66
- **KSPToMarslanderteam**: 66
- **Catchirps**: 33
- **virtualwarfighter**: 33
- **coopgames**: 66
- **YouthHockeyHighlights**: 33
- **buyaplantsellaplant**: 33
- **BolsaChile**: 33
- **Dikpipiler**: 33
- **EryingMotherboard**: 33
- **RUFUSDUSOL**: 66
- **sanantoniobeer**: 33
- **techpolitics**: 66
- **RockPosters**: 33
- **dorajar**: 33
- **suddenlyvicodin**: 66
- **nadide**: 33
- **BigStickEnergy**: 66
- **zoomergen**: 99
- **drivingUK**: 66
- **MSPA**: 66
- **TheATOShow**: 33
- **TheMegatronDon**: 33
- **EnemiesBeingBros**: 33
- **ratmemes**: 33
- **MirMGlobal**: 33
- **AppleAndOnion**: 33
- **B638**: 66
- **LGBT_iel**: 66
- **TrainingEverywhere**: 99
- **biggestbroent**: 66
- **WWYDif**: 33
- **DSTAlumnaeChapter**: 33
- **WWE_Eva_Marie**: 33
- **guatemala_gay**: 66
- **CringyAntiVegans**: 33
- **NinaDobrev_**: 33
- **makemoneyforexreddit**: 33
- **iJustPublishedMyGame**: 66
- **lordtuts**: 33
- **brennanjones**: 33
- **SubmarineTitans**: 33
- **ClonazepamHealth**: 33
- **okbuddyrhodok**: 66
- **FinancialMeme**: 33
- **FiverFilth**: 33
- **Theni**: 33
- **VZfit**: 132
- **FunctionalFill**: 33
- **SACWATR**: 33
- **GiuliaBenitee**: 33
- **Reality_Quest**: 33
- **SheGotFrameWork**: 33
- **abertayhackers**: 99
- **ThePower**: 66
- **IceandFirePowers**: 33
- **datasatanism**: 66
- **badshirtanatomy**: 33
- **Necrontyr**: 33
- **signs**: 33
- **vrbohosts**: 66
- **erannorthchronicles**: 66
- **Brainysweddit**: 66
- **Ooergonewild**: 66
- **EB2_NIW_Cybersecurity**: 33
- **Djlynch2009**: 33
- **foundtheAngela**: 66
- **digitaltabletop**: 33
- **Idvitalia**: 33
- **surebet**: 33
- **subscriptionsharing**: 33
- **Exodus90**: 33
- **northfork**: 33
- **Emsland**: 33
- **CoilCommunity**: 66
- **shameless**: 33
- **ManitobaGardening**: 33
- **Telekinetic**: 99
- **TheNew52**: 33
- **TeamSky**: 33
- **HypixelSlovenija**: 33
- **Stabbingthemap**: 33
- **mysteriousmetallicorb**: 33
- **DeerVirus**: 66
- **EquinoxEv**: 33
- **titration**: 33
- **NoSenseMemes**: 33
- **HellenicMemes**: 66
- **FoundTheGiantSquid**: 33
- **MetroidDread**: 33
- **Chiskate**: 33
- **cannadips**: 66
- **WeHateYT**: 33
- **askdisqus**: 33
- **marniexvictor**: 66
- **RootandDigby**: 33
- **SuperheroLeague**: 33
- **CipherMains_HSR**: 33
- **obviouslyincorrect**: 33
- **InfinityNado**: 33
- **desporto**: 33
- **Dallas_commons**: 33
- **go_ask_your_crush_out**: 132
- **heelonly**: 33
- **Porsgrunn**: 33
- **truetexan**: 66
- **rushzilla**: 33
- **banime**: 33
- **Nalbinding**: 33
- **oliviadejarnet**: 66
- **preparedguitar**: 66
- **NielsonShow**: 66
- **transsexmalesurgeries**: 99
- **cleanpuppers**: 33
- **InesperadoPau**: 33
- **louderthanlife**: 33
- **ZamasuDidNothingWrong**: 99
- **OdeyaRush**: 33
- **CyberSecurityAdvice**: 33
- **dutchposer**: 33
- **WK2**: 33
- **minecraftinfographics**: 33
- **applycoldwatertoburn**: 66
- **fearofheights**: 33
- **themeaneyoflife**: 33
- **mildlyaccessible**: 33
- **BLACKED_AL**: 33
- **bluegrassguitar**: 66
- **azuretips**: 66
- **fitbod**: 33
- **somaligay**: 33
- **TheWellsTable**: 33
- **Poudlard**: 33
- **10000ShipsHBO**: 66
- **TookOurJerbs**: 66
- **FreeGameKeys**: 33
- **CloudAssessments**: 33
- **OTRebels**: 66
- **berlinfoodies**: 66
- **DisneyParkHistory**: 33
- **infjhome**: 33
- **jotform**: 33
- **danielledominguez_**: 66
- **ryzencpu**: 33
- **bandirma**: 33
- **cdldriver**: 99
- **CarpetsforAirports**: 66
- **Superchunk**: 66
- **artisansasylum**: 33
- **ClubeDoZanzigau**: 99
- **HowToMen**: 33
- **accidentaldiamondclub**: 33
- **CelebrityOutside**: 33
- **RobertoArguelles**: 33
- **ConcourseCityTransit**: 33
- **VisitingNorway**: 33
- **Sims4Build**: 33
- **TNOmod**: 66
- **WorshipTheColander**: 33
- **CowboyIPTV**: 66
- **owsisajoke**: 33
- **DACA_Medicine**: 33
- **shironeko**: 33
- **anomic**: 33
- **GardeningIndia2**: 33
- **TeenDevsIndia**: 66
- **Ssupdhq**: 33
- **JCVAP**: 33
- **DecorativePlumbing**: 99
- **TwoSentenceFeels**: 33
- **CodeMystery**: 66
- **htlhollabrunn**: 66
- **AlternativeMedia**: 33
- **guildgrumps**: 33
- **kiwibrowser**: 33
- **BabyCheeks**: 33
- **Milo_Baughman_Design**: 66
- **PadresVent**: 33
- **Nyelveszet**: 33
- **AltFantasySports**: 33
- **TOTC_UFOs**: 33
- **VIU**: 66
- **FistBros**: 99
- **catTunes**: 33
- **okbuddyinfantry**: 33
- **We_are_doing_this**: 33
- **asianamericanytsnark**: 33
- **lssmod_526**: 33
- **zora**: 33
- **Beginners_Photography**: 99
- **Anarcho_Frontierism**: 33
- **DiwaariHeads**: 33
- **hmtforsale**: 33
- **LosingItTogether**: 99
- **StormHawksTv**: 33
- **theCRAMPSstaysick**: 33
- **nokeric**: 33
- **GRITS**: 66
- **KamersNijmegen**: 33
- **UkraineLongRead**: 99
- **MyInternetCrushes**: 66
- **LawandOrder_OC**: 33
- **rachaelharris**: 33
- **In_the_name_of_Beers**: 99
- **godscomputer**: 33
- **PokemonPlaza**: 33
- **theUKSpaceNews**: 33
- **CivRealms**: 33
- **expectedundertale**: 99
- **BigGangsterMod**: 66
- **minecraftdupers**: 66
- **BipocWitchesandPagans**: 99
- **GSMFerroglobe**: 66
- **SingleIncomeNoKid**: 66
- **FansOfBAL**: 33
- **TwinRiverPoker**: 66
- **Redditquandaryroyale**: 33
- **chiptunes**: 33
- **notdisappointed**: 66
- **DionisoES**: 33
- **SweetBabyRays**: 33
- **khargone**: 33
- **KennedySpaceCenter**: 33
- **jmc2obj**: 66
- **Aerospace_Engineering**: 33
- **xb70**: 66
- **SheSaidSheWas12**: 66
- **LockportTownshipHS**: 33
- **F5Robin**: 33
- **BroilKingKeg**: 66
- **highdesert**: 66
- **AdvancedEntityScience**: 33
- **CATAGRUM**: 33
- **tiffin**: 66
- **Wealthsimple_Invest**: 99
- **xplane12**: 99
- **silverbackgorillas**: 33
- **boysarescary**: 33
- **dendaris**: 66
- **toyexchange**: 99
- **Brightline**: 33
- **LordoftheLost**: 33
- **YardHypeSpot**: 66
- **CalebHarris**: 33
- **livefromhere**: 33
- **Revita**: 33
- **sgcgrading**: 33
- **HecarimMains**: 33
- **TheMetGala**: 132
- **ForzaModding**: 33
- **Farbenfuchs_Facharzt**: 33
- **SteamWorldHeist**: 33
- **MeerkatMillionaires**: 66
- **Foodrecalls**: 66
- **CodeFellows**: 66
- **OffGridCrappers**: 66
- **ChurchOfBlueAlienChef**: 33
- **VeeInfuso**: 66
- **GameScience**: 132
- **wtfshippingsupplies**: 33
- **Gamecocks**: 66
- **WeloBk**: 33
- **SneakersEgypt**: 33
- **bon_appetit**: 33
- **RemoteHRJobs**: 33
- **feathery**: 33
- **CuriousExpedition**: 33
- **Cebu**: 33
- **SmallBusinessSellers**: 33
- **germanchubbywife**: 33
- **ImaginaryPirates**: 66
- **TheDeprogramEurope**: 33
- **CycleKarts**: 66
- **hornystan**: 33
- **schleeey**: 33
- **JetEngines**: 33
- **QUITOO**: 66
- **findayoutube**: 33
- **A_Separate_Peace**: 33
- **kittykeep**: 33
- **EiyuuouBuwoKiwameru**: 66
- **YECirclehugs**: 33
- **hallenhollylove**: 66
- **PhotoEnts**: 66
- **Network_Automation**: 33
- **GymOwnerNetwork**: 66
- **Bunkd**: 99
- **kota**: 33
- **canplfantasy**: 33
- **FortunateCum**: 66
- **waukeshaparade**: 33
- **Mario3DWorldOnline**: 33
- **Gandhinagar_**: 66
- **Arazhul**: 33
- **kfee**: 33
- **ShinyPokeCommunity**: 33
- **ShortStoryPrompts**: 33
- **thrashcub**: 66
- **Cuteaggression**: 33
- **CritCrab**: 33
- **Alipay**: 33
- **agrochemistry**: 99
- **ThatsLegal**: 33
- **EarthFinalConflict**: 33
- **Tetr_College**: 33
- **FoxestheSinger**: 66
- **PFFO**: 33
- **whyperart**: 33
- **heilzamememes**: 66
- **TamilCinephiles**: 33
- **trumpsolana**: 33
- **nuance**: 33
- **RealPDKC**: 66
- **fukuyamameme**: 33
- **GreenwichVillage**: 33
- **Tramlines**: 66

## Languages
Unique languages and their frequencies:
- **French**: 450
- **Italian**: 450
- **German**: 450
- **English (American)**: 450
- **Korean**: 450
- **Spanish**: 450
- **Chinese (traditional)**: 450
- **Chinese (simplified)**: 450
- **Japanese**: 450
- **Russian**: 450
- **Czech**: 450
- **Danish**: 450
- **Dutch**: 450
- **Arabic**: 450
- **Bulgarian**: 450
- **English (British)**: 450
- **Estonian**: 450
- **Hungarian**: 450
- **Indonesian**: 450
- **Norwegian (bokmål)**: 450
- **Portuguese**: 450
- **Greek**: 450
- **Lithuanian**: 450
- **Finnish**: 450
- **Latvian**: 450
- **Portuguese (Brazilian)**: 450
- **Polish**: 450
- **Swedish**: 450
- **Slovak**: 450
- **Slovenian**: 450
- **Romanian**: 450
- **Turkish**: 450
- **Ukrainian**: 450

## Models
Unique models and their frequencies:
- **Granite-3.1-8B-Instruct**: 2871
- **UnieInfra2.aqua-mini**: 3069
- **aqua-mini-fast**: 6303
- **phi4**: 2607

## Article Statistics
- **Average article length (characters):** 1275.34
- **Minimum article length (characters):** 2
- **Maximum article length (characters):** 15407
- **Average word count:** 184.62
- **Minimum word count:** 1
- **Maximum word count:** 1134
",Medium,2.0
Summarization,YuRiVeRTi/V1Q,3.0,510.0,2025-03-11 05:39:24+00:00,apache-2.0,0.0,161 MB,168820736.0,161 MB,168820736.0,185799,none,none,"---
license: apache-2.0
task_categories:
- text-classification
- token-classification
- table-question-answering
- question-answering
- zero-shot-classification
- summarization
- translation
- feature-extraction
- text-generation
- text2text-generation
- sentence-similarity
- fill-mask
- text-to-speech
- text-to-audio
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
- image-to-text
- image-to-image
- image-to-video
- unconditional-image-generation
- video-classification
- reinforcement-learning
- tabular-classification
- robotics
- tabular-regression
- tabular-to-text
- table-to-text
- multiple-choice
- text-retrieval
- time-series-forecasting
- text-to-video
- visual-question-answering
- zero-shot-image-classification
- graph-ml
- mask-generation
- zero-shot-object-detection
- text-to-3d
- image-to-3d
- image-feature-extraction
- video-text-to-text
language:
- en
- aa
- ab
- ae
- af
- ak
- am
- an
- ar
- as
- av
- ay
- az
- ba
- be
- bg
- bh
- bi
- bm
- bn
- bo
- br
- bs
- ca
- ce
- ch
- co
- cr
- cs
- cu
- cv
- cy
- da
- de
- dv
- dz
- ee
- el
- eo
- es
- et
- eu
- fa
- ff
- fi
- fj
- fo
- fr
- fy
- ga
- gd
- gl
- gn
- gu
- gv
- ha
- he
- hi
- ho
- hr
- ht
- hu
- hy
- hz
- ia
- id
- ie
- ig
- ii
- ik
- io
- is
- it
- iu
- ja
- jv
- ka
- kg
- ki
- kj
- kk
- kl
- km
- kn
- ko
- kr
- ks
- ku
- kv
- kw
- ky
- la
- lb
- lg
- li
- ln
- lo
- lt
- lu
- lv
- mg
- mh
- mi
- mk
- ml
- mn
- mr
- ms
- 'no'
- my
- na
- nb
- nd
- ne
- mt
- ng
- nl
- nn
- nr
- nv
- ny
- oc
- oj
- om
- or
- os
- pa
- pi
- pl
- ps
- pt
- qu
- rm
- rn
- ro
- ru
- sm
- rw
- sc
- sd
- se
- sg
- si
- sk
- sl
- sn
- so
- sq
- sr
- ss
- st
- su
- sv
- sw
- ta
- te
- tg
- th
- ti
- tk
- tl
- tn
- to
- tr
- ts
- tt
- sa
- tw
- ty
- ug
- uk
- ur
- uz
- ve
- vi
- vo
- wa
- wo
- xh
- yi
- yo
- za
- zh
- zu
tags:
- code
- chemistry
- synthetic
size_categories:
- n>1T
pretty_name: VQ1
---
from datasets import load_dataset

ds = load_dataset(""b3x0m/Chinese-H-Novels"")
import sagemaker
import boto3
from sagemaker.huggingface import HuggingFaceModel

try:
	role = sagemaker.get_execution_role()
except ValueError:
	iam = boto3.client('iam')
	role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']

# Hub Model configuration. https://huggingface.co/models
hub = {
	'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',
	'HF_TASK':'any-to-any'
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
	transformers_version='4.37.0',
	pytorch_version='2.1.0',
	py_version='py310',
	env=hub,
	role=role, 
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
	initial_instance_count=1, # number of instances
	instance_type='ml.m5.xlarge' # ec2 instance type
)",Low,1.0
Summarization,aliahabeeb/usul-alkafi,0.0,8.0,2025-02-28 17:25:51+00:00,mit,0.0,15.1 MB,15833497.6,4.26 MB,4466933.76,4816,none,none,"---
license: mit
task_categories:
  - text-classification
  - text-generation
  - text-retrieval
  - summarization
language:
- ar
tags:
- shia
size_categories:
- 1K<n<10K
---

# Usul Al-Kafi Dataset

## Overview

This dataset contains the full text of *الأصول من الكافي*, a foundational Islamic book authored by **ثقة الإسلام أبي جعفر محمد بن يعقوب بن إسحاق الكليني الرازي**. Each row in the dataset represents a single page from the book, preserving the original Arabic text structure. The book is divided into eight parts and includes beneficial annotations derived from various commentaries.

## Dataset Details

- **Version:** 1.0  
- **Source:** Digitized from *Dar Al-Kutub Al-Islamiyyah, Tehran - Bazaar Sultani, Third Edition (1388 AH).*  
- **Language:** Arabic  
- **Format:** CSV  
- **License:** MIT  

## Structure

The dataset is structured as follows:  

- Each row represents a single page of the book.  
- The dataset is divided into 8 parts corresponding to the book’s structure.  

### Columns

- **page**: Integer representing the page number.  
- **part**: Integer indicating the part of the book (1 to 8).  
- **content**: The full Arabic text of the corresponding page.  


",Medium,3.0
Summarization,lenoxaugo254/LenoxAI,1.0,4.0,2025-03-04 04:48:43+00:00,none,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
task_categories:
- text-classification
- table-question-answering
- question-answering
- zero-shot-classification
- translation
- summarization
- feature-extraction
- text-generation
- text2text-generation
- fill-mask
- sentence-similarity
- text-to-speech
- text-to-audio
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
language:
- aa
- ab
- ae
- af
- ak
- am
- an
- ar
- as
- av
- ay
- az
- ba
- be
- bg
- bh
- bi
- bm
- bn
- bo
- br
- bs
- ca
- ce
- ch
- co
- cr
- cs
- cu
- cv
- cy
- da
- de
- dv
- dz
- ee
- el
- en
- eo
- es
- et
- eu
- fa
- ff
- fi
- fj
- fo
- fr
- fy
- ga
- gd
- gl
- gn
- gu
- gv
- ha
- he
- hi
- ho
- hr
- ht
- hu
- hy
- hz
- ia
- id
- ie
- ig
- ii
- ik
- io
- is
- it
- iu
- ja
- jv
- ka
- kg
- ki
- kj
- kk
- kl
- km
- lb
- kr
- ku
- ko
- kn
- ks
- kv
- kw
- ky
- la
- lg
- li
- ln
- lo
- lt
- lu
- lv
- mg
- mh
- mi
- mk
- ml
- mn
tags:
- chemistry
- biology
- finance
- legal
- music
- art
- code
- climate
- medical
- not-for-all-audiences
- synthetic
pretty_name: Lenox Augo
size_categories:
- 1M<n<10M
---",Low,0.0
Summarization,EMINES/summarized-darija-msa-wiki-data,1.0,19.0,2025-03-16 15:02:14+00:00,apache-2.0,0.0,20.7 MB,21705523.2,20.7 MB,21705523.2,4800,none,none,"---
language:
- ar

license: apache-2.0
task_categories:
- summarization
pretty_name: MSA-Darija Summarization Dataset
size_categories:
- 1K<n<10K
tags:
- text
- Moroccan Darija
- arabic
- summarization
- low-resource
- north-africa
---

# MSA-Darija Summarization Dataset

## Overview

This is the EMINES organization-hosted version of the [MSA-Darija Summarization Dataset](https://huggingface.co/datasets/abir-hr196/summarized-darija-msa-wiki-data), synchronized with the original dataset. It contains 4800 rows of Moroccan and Arabic texts with Arabic summarization, designed for developing summarization models.

### Quick Start

```python
from datasets import load_dataset

# Load the dataset
dataset = load_dataset(""EMINES/summarized-darija-msa-wiki-data"")

# Example usage
for example in dataset[""train""]:
    text = example[""text""]
    summary = example[""summary""]
    category = example[""category""]
```


## Dataset Description
The dataset contains text segments in Modern Standard Arabic (MSA) and Moroccan Darija paired with their summaries. It serves as a foundation for:
* Text summarization models for Arabic and Darija
* Cross-dialect language processing
* Low-resource language research

### Dataset Statistics
* **Total Examples:** ~4800 text segments
* **Categories:** Wikipedia, Darija, MSA
* **Format:** Plain text

### Data Fields

```python
{
    'text': str,       # Original text content in MSA or Darija
    'summary': str,    # Summary of the text
    'category': str    # Category label (Wikipedia, Darija, or MSA)
}
```

## Data Collection Methodology  

The dataset was constructed by combining multiple sources of Modern Standard Arabic (MSA) and Moroccan Darija text to support summarization tasks. The collection process involved selecting high-quality datasets, filtering inappropriate content, and ensuring a balanced representation of both formal and informal Arabic text.  

### Moroccan Darija (20%)  
This subset was initially sourced from open datasets such as *No-Arabic-Dialect-Left-Behind*, *Darija_Dataset*, and *DarijaStory*. However, some datasets were ultimately excluded due to inaccessibility or explicit content. The final selection aimed to capture diverse dialectal expressions used in casual communication.  

### Arabic Web Content (60%)  
Given that web-based text is a crucial component of modern summarization tasks, we incorporated the *FineWeb2-multilingual* dataset, specifically the [*fineweb-2-arb_Arab*](https://huggingface.co/datasets/alielfilali01/fineweb-2-arb_Arab/viewer?row=1) subset. This dataset was chosen for its pre-filtered, well-curated Arabic content, which streamlined the data preparation process and ensured quality.  

### Arabic Educational Content (20%)  
To enhance the dataset’s coverage of formal, structured Arabic, we included text from [*Arabic Wikipedia*](https://huggingface.co/datasets/SaiedAlshahrani/Arabic_Wikipedia_20230101_bots/viewer/default/train?p=1), leveraging an existing dataset curated by *Saied Alshahrani*. Wikipedia articles were selected for their factual accuracy and consistency in linguistic style.  

The collected text was then processed, annotated using an Alpaca-style prompt, and summarized. Long documents were chunked into 1,700-token segments to optimize performance for a small language model with a 2,048-token context window. This chunking approach helped mitigate performance degradation due to model quantization constraints.  



## Community & Development
This dataset is based on the work of [abir-hr196](https://huggingface.co/datasets/abir-hr196/mixed-darija-msa-summarization).

### Citation  

To cite this dataset in your work, please use the following reference:  

```bibtex
@dataset{msa_darija_summarization,
  author    = {Abir Harrasse},
  title     = {MSA-Darija Summarization Dataset},
  year      = {2025},
  url       = {https://huggingface.co/datasets/abir-hr196/mixed-darija-msa-summarization},
  note      = {Dataset for summarization tasks in Modern Standard Arabic (MSA) and Moroccan Darija.}
}
```
We encourage researchers to cite this dataset when using it in publications or projects to acknowledge the efforts involved in data collection, cleaning, and curation.  

",High,6.0
Summarization,ademchaoua/darja-en-translation,1.0,36.0,2025-04-08 02:08:21+00:00,mit,0.0,3.25 MB,3407872.0,1.61 MB,1688207.36,30464,none,none,"---
license: mit
task_categories:
- translation
- text-classification
- summarization
- text-generation
language:
- ar
- en
tags:
- translation
- multilingual
- algerian-darja
- english
- natural-language-processing
pretty_name: Algerian Darja to English Translations
size_categories:
- 10K<n<100K
---
# Darja-English Translation Dataset

This dataset contains translations from Algerian Darja (Arabic dialect) to English. The dataset includes sentences in Darja along with their corresponding English translations.

## Languages

- Darja (Algerian Arabic dialect)
- English

## Dataset Structure

The dataset consists of two fields:

- `input`: Sentence in Darja
- `translation`: Corresponding translation in English

## License

This dataset is licensed under the MIT License. Please refer to the LICENSE file for more details.",Low,1.0
Summarization,Groovy-123/deep-think,1.0,58.0,2025-05-22 06:06:36+00:00,other,1.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: other
license_name: nexgen
license_link: LICENSE
task_categories:
- text-classification
- token-classification
- table-question-answering
- question-answering
- zero-shot-classification
- translation
- summarization
- feature-extraction
- text-generation
- text2text-generation
- fill-mask
- sentence-similarity
- text-to-speech
- text-to-audio
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
- image-to-text
- image-to-image
- image-to-video
- unconditional-image-generation
- video-classification
- reinforcement-learning
- robotics
- tabular-classification
- tabular-regression
- tabular-to-text
- table-to-text
- multiple-choice
- text-ranking
- text-retrieval
- time-series-forecasting
- text-to-video
- visual-question-answering
- zero-shot-image-classification
- graph-ml
- mask-generation
- zero-shot-object-detection
- image-to-3d
- text-to-3d
- image-feature-extraction
- video-text-to-text
language:
- ak
- en
- ar
- ha
- ee
size_categories:
- n>1T
---",Low,1.0
Summarization,karimraouf/Arabic-Summarization-Dataset-AsDs,2.0,134.0,2025-06-16 06:05:27+00:00,mit,0.0,2.14 MB,2243952.64,2.14 MB,2243952.64,1229,none,none,"---
license: mit
task_categories:
- summarization
language:
- ar
size_categories:
- 1K<n<10K
---
## Arabic Summarization Dataset
***Dataset Description***

This dataset was created to address the significant gap in high-quality Arabic text summarization resources. After extensive research, we found that existing Arabic summarization datasets often suffer from poor summary quality, inconsistent formatting, or limited domain coverage. To overcome these limitations, this dataset was meticulously crafted using Google's Gemini AI model to generate high-quality, coherent summaries for Arabic texts.

*Dataset Summary*

- Language: Arabic (العربية)
- Task: Abstractive Text Summarization
- Size: 1250 samples
- Quality: High-quality summaries generated using Gemini AI
- Format: CSV with text-summary pairs
- License: MIT

## Dataset Details
***Problem Statement***
The Arabic NLP community faces a critical shortage of high-quality summarization datasets. Existing resources often contain:

- Low-quality summaries that lack coherence
- Inconsistent summary lengths and styles
- Limited domain diversity
- Poor preprocessing and formatting
- Inadequate coverage of Modern Standard Arabic (MSA)

***Solution Approach***
To address these challenges, we leveraged Google's Gemini AI model to create a comprehensive Arabic summarization dataset with the following advantages:

- Consistent Quality: All summaries maintain high linguistic quality and coherence
- Standardized Format: Uniform structure and formatting across all samples
- Diverse Content: Coverage of multiple domains and topics
- Cultural Relevance: Summaries that respect Arabic linguistic and cultural nuances

## Dataset Creation Process

1 - Source Text Collection: Carefully curated Arabic texts from various domains
2 - Quality Filtering: Rigorous filtering to ensure source text quality
3 - Summarization: Generated summaries using Gemini AI with optimized prompts
4 - Post-processing: Manual review and automated quality checks
5 - Validation: Linguistic review by native Arabic speakers
6 - Final Curation: Final dataset compilation and formatting

## Dataset Structure
***Data Fields***

- text: Original Arabic text to be summarized
- summary: High-quality Arabic summary generated by Gemini
- topic: Content domain/category (news, literature, science, etc.)
- title: Title of the text field article

***Data Statistics***
- Total Samples: 1250
- Average Text Length: 1400 words
- Average Summary Length: 460 words
- Average Compression Ratio: 3:1
- Domains Covered: 8

***Data Splits***
- Training Set: 82% (1007 samples)
- Validation Set: 18% (222 samples)

## Quality Assurance
***Summary Quality Criteria***

- Coherence: Summaries maintain logical flow and readability
- Completeness: Key information from source text is preserved
- Conciseness: Appropriate length reduction while maintaining meaning
- Accuracy: Factual information is correctly represented
- Fluency: Natural Arabic language expression

***Validation Process***

- Automated quality checks using linguistic metrics
- Manual review by Arabic language experts
- Consistency verification across samples
- Domain-specific accuracy validation

## Usage

***Recommended Use Cases***

- Training Arabic text summarization models
- Evaluating Arabic NLP summarization systems
- Research in Arabic computational linguistics
- Educational purposes for Arabic NLP
- Benchmarking cross-lingual summarization models

***Loading the Dataset***

<pre lang=""python""><code>
!pip install datasets
</pre></code>
<pre lang=""python""><code> 
from datasets import load_dataset

repo_name = ""karimraouf/Arabic-Summarization-Dataset-AsDs""
dataset = load_dataset(repo_name)

# You can now access the splits
train_split = dataset['train']
validation_split = dataset['validation']

# Print the first example from the training split
print(train_split[0])
</pre></code>


## Limitations and Considerations
***Known Limitations***

Summaries are generated by AI and may contain occasional inconsistencies
Domain coverage may not be perfectly balanced
Some specialized terminology might be simplified in summaries
Cultural and regional Arabic variations may not be fully represented

***Ethical Considerations***

Source texts were carefully selected to avoid harmful or biased content
Summaries maintain cultural sensitivity and respect for Arabic linguistic norms
No personal or private information is included in the dataset
Generated content follows ethical AI guidelines



## Contact Information

For questions, issues, or collaboration opportunities:

Email: karimraoufm@gmail.com

GitHub: karim-raouf

Hugging Face: karimraouf


## Version History

v1.0: Initial release with 1250 samples
Future versions: Planned expansions and improvements


Last updated: 6/16/2025

Dataset version: 1.0",High,4.0
Translation,mohamed-khalil/ATHAR,9.0,60.0,2024-08-04 11:49:52+00:00,cc-by-sa-4.0,0.0,14.7 MB,15414067.2,14.7 MB,15414067.2,66043,https://arxiv.org/abs/2407.19835,none,"---
dataset_info:
  features:
  - name: arabic
    dtype: string
  - name: english
    dtype: string
  splits:
  - name: train
    num_bytes: 27878710
    num_examples: 65043
  - name: test
    num_bytes: 430500
    num_examples: 1000
  download_size: 14722818
  dataset_size: 28309210
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: test
    path: data/test-*
task_categories:
- translation
language:
- ar
- en
pretty_name: ATHAR
size_categories:
- 10K<n<100K
license: cc-by-sa-4.0
---


<!-- 
# Dataset Card for ""ATHAR""

# ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation

> Welcome to ATHAR Dataset

<p align=""center"">
<img src=""https://cdn-uploads.huggingface.co/production/uploads/64af7c627ab7586520ed8688/SwkKk3Z6kT5VZ3Oj-bVMC.jpeg"" width = ""150px""/>
  <p align=""center""> [ <a href=""https://arxiv.org/abs/2407.19835"">Paper</a> ]<!-- - <a href=""https://github.com/ARBML/CIDAR"">GitHub</a> ]</p>

</p>


![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/64af7c627ab7586520ed8688/SwkKk3Z6kT5VZ3Oj-bVMC.jpeg) 

<div align=""center"">
    <picture>
        <source 
        srcset=""https://cdn-uploads.huggingface.co/production/uploads/64af7c627ab7586520ed8688/SwkKk3Z6kT5VZ3Oj-bVMC.jpeg""
        media=""(prefers-color-scheme: dark)""
        />
        <source
        srcset=""https://cdn-uploads.huggingface.co/production/uploads/64af7c627ab7586520ed8688/SwkKk3Z6kT5VZ3Oj-bVMC.jpeg""
        media=""(prefers-color-scheme: light), (prefers-color-scheme: no-preference)""
        />
        <img src=""https://cdn-uploads.huggingface.co/production/uploads/64af7c627ab7586520ed8688/SwkKk3Z6kT5VZ3Oj-bVMC.jpeg"" width=""100%"" height=""350px"" />
    </picture>
</div>


## Datasets Summary

The ATHAR dataset is divided into two main subsets: test data and training data.

   | Name             | Description                                                                                                             |
   |------------------|-------------------------------------------------------------------------------------------------------------------------|
   | `Training Data`  | Contains 65,043 rows of classical Arabic texts and their English translations, used for training translation models.    |
   | `Test Data`      | Contains 1,000 rows of classical Arabic texts and their English translations, reserved for evaluating model performance.|





## Dataset Structure

Each entry in the dataset is represented by a row with the following fields:

   | Field     | Description                                                                                                 |
   |-----------|-------------------------------------------------------------------------------------------------------------|
   | `Arabic(str)`  | The original text in classical Arabic.                                                                 |
   | `English(str)` | The English translation of the classical Arabic text.                                                  |




## Loading The Dataset
You can download the dataset directly from HuggingFace or use the following code:

```python
from datasets import load_dataset

athar = load_dataset(""mohamed-khalil/ATHAR"")
```


## Sample From The Dataset:

Below is a sample from the ATHAR dataset, showcasing a classical Arabic text and its English translation.

 **`Arabic`**: فَلم يزل الْفلس يعبد حَتَّى ظهرت دَعْوَة النَّبِي عَلَيْهِ السَّلَام فَبعث إِلَيْهِ على ابْن أَبِي طَالِبٍ فَهَدَمَهُ وَأَخَذَ سَيْفَيْنِ كَانَ الْحَارِثُ بن أبي شمرٍ الغساني, ملك غَسَّان قَلَّدَهُ إِيَّاهُمَا, يُقَالُ لَهُمَا مِخْذَمٌ وَرَسُوبٌ(وَهُمَا السَّيْفَانِ اللَّذَانِ ذَكَرَهُمَا عَلْقَمَةُ بْنُ عَبْدَةَ فِي شِعْرِهِ). فَقَدِمَ بِهِمَا عَلِيُّ بْنُ أَبِي طَالِبٍ عَلَى النَّبِيِّ صَلَّى اللَّهُ عَلَيْهِ وَسَلَّمَ فَتَقَلَّدَ أَحَدَهُمَا ثُمَّ دَفَعَهُ إِلَى عَلِيِّ بْنِ أَبِي طَالِبٍ, فَهُوَ سَيْفُهُ الَّذِي كَانَ يَتَقَلَّدُهُ

 **`English`**: Al-Fals continued to be worshipped until the advent of the Prophet, at which time ‘Ali ibn-abi-Talib was dispatched to destroy it. ‘Ali destroyed the idol and carried away therefrom two swords called Mikhdham and Rasub (the same two swords which ‘Alqamah ibn-’Abadah had mentioned in his poetry), which al-Harith ibn-abi-Shamir, king of Ghassan, had presented al-Fals. ‘Ali brought them to the Prophet who wore one of them and gave it back to him. It was the sword which ‘Ali was always wont to wear

 -->



# Dataset card

## Description

### ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation

> Welcome to ATHAR Dataset

The ATHAR dataset is a comprehensive collection of classical Arabic texts translated into English. This dataset contains approximately 66,000 rows of translated texts, including the original Classical Arabic texts and their English translations.

<p align=""center"">
<!-- <img src=""https://cdn-uploads.huggingface.co/production/uploads/64af7c627ab7586520ed8688/SwkKk3Z6kT5VZ3Oj-bVMC.jpeg"" width = ""150px""/> -->
  <p align=""center""> [ <a href=""https://arxiv.org/abs/2407.19835"">Paper</a> ]<!-- - <a href=""https://github.com/ARBML/CIDAR"">GitHub</a> ]</p> -->

</p>



## License

The ATHAR dataset and weight diffs are licensed under **CC BY NC 4.0**
[Creative Commons NonCommercial (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/deed.en).



## Attribution

```bibtex
@misc{khalil2024atharhighqualitydiversedataset,
      title={ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation}, 
      author={Mohammed Khalil and Mohammed Sabry},
      year={2024},
      eprint={2407.19835},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.19835}, 
}
```

## Language codes

### Arabic 

* ISO 639-3: eng
* ISO 15924: Arab
* Glottocode: arab1395	 <!-- Classical Arabic Code: clas1259 -->

### English

* ISO 639-3: ara
* ISO 15924: Latn
* Glottocode: stan1293



## Additional language information

<!-- Any relevant additional information on the language, such as:
* A list of reference publications and software (dictionaries, grammars, spellcheckers).
* If applicable, any additional information about dialectal variation that is not captured by the Glottocode.
* If relevant, the orthography used in your contribution. --> 

### Dictionaries
 **Lisan al-Arab (لسان العرب)**
   - Author: Ibn Manzur
   - Description: One of the most comprehensive and authoritative dictionaries of the Arabic language.
   <!-- - Link: [Lisan al-Arab](https://archive.org/details/LisanalArab) -->

 **Al-Qamus al-Muhit (القاموس المحيط)**
   - Author: Al-Firuzabadi
   - Description: A highly respected and widely used Arabic dictionary, focusing on classical Arabic lexicon.
   <!-- - Link: [Al-Qamus al-Muhit](https://archive.org/details/FP15723) -->

### Grammars
 **Al-Kitab (الكتاب)**
   - Author: Sibawayh
   - Description: The foundational text of Arabic grammar, written by the Persian grammarian Sibawayh.
   <!-- - Link: [Al-Kitab](https://archive.org/details/AlKitabSibawayh) -->






## Workflow

The ATHAR dataset comprises 66K sentences extracted from seminal Classical Arabic texts, expertly translated into English. These texts are cornerstone works spanning diverse genres and historical periods, providing profound insights into Islamic and world history, philosophy, science, medicine, and culture. This rich literary heritage is showcased in our dataset, sourced from the Rasaif website at [rasaif.com](https://rasaif.com/) At the time of data collection and publication of this work, there were no restrictions on scraping and using resources from this website.


## Additional guidelines

**No Additional Guidelines Were Used**
<!-- Were any additional guidelines agreed upon? 
Examples might include style guidelines, 
the use of particular grammatical forms or sentence structures, 
specific spelling or punctuation rules to be followed, etc. -->


## Datasets Summary

The ATHAR dataset is divided into two main subsets: test data and training data.

   | Name             | Description                                                                                                             |
   |------------------|-------------------------------------------------------------------------------------------------------------------------|
   | `Training Data`  | Contains 65,043 rows of classical Arabic texts and their English translations, used for training translation models.    |
   | `Test Data`      | Contains 1,000 rows of classical Arabic texts and their English translations, reserved for evaluating model performance.|





## Dataset Structure

Each entry in the dataset is represented by a row with the following fields:

   | Field     | Description                                                                                                 |
   |-----------|-------------------------------------------------------------------------------------------------------------|
   | `Arabic(str)`  | The original text in classical Arabic.                                                                 |
   | `English(str)` | The English translation of the classical Arabic text.                                                  |




## Loading The Dataset
You can download the dataset directly from HuggingFace or use the following code:

```python
from datasets import load_dataset

athar = load_dataset(""mohamed-khalil/ATHAR"")
```


## Sample From The Dataset:

Below is a sample from the ATHAR dataset, showcasing a classical Arabic text and its English translation.

 **`Arabic`**: فَلم يزل الْفلس يعبد حَتَّى ظهرت دَعْوَة النَّبِي عَلَيْهِ السَّلَام فَبعث إِلَيْهِ على ابْن أَبِي طَالِبٍ فَهَدَمَهُ وَأَخَذَ سَيْفَيْنِ كَانَ الْحَارِثُ بن أبي شمرٍ الغساني, ملك غَسَّان قَلَّدَهُ إِيَّاهُمَا, يُقَالُ لَهُمَا مِخْذَمٌ وَرَسُوبٌ(وَهُمَا السَّيْفَانِ اللَّذَانِ ذَكَرَهُمَا عَلْقَمَةُ بْنُ عَبْدَةَ فِي شِعْرِهِ). فَقَدِمَ بِهِمَا عَلِيُّ بْنُ أَبِي طَالِبٍ عَلَى النَّبِيِّ صَلَّى اللَّهُ عَلَيْهِ وَسَلَّمَ فَتَقَلَّدَ أَحَدَهُمَا ثُمَّ دَفَعَهُ إِلَى عَلِيِّ بْنِ أَبِي طَالِبٍ, فَهُوَ سَيْفُهُ الَّذِي كَانَ يَتَقَلَّدُهُ

 **`English`**: Al-Fals continued to be worshipped until the advent of the Prophet, at which time ‘Ali ibn-abi-Talib was dispatched to destroy it. ‘Ali destroyed the idol and carried away therefrom two swords called Mikhdham and Rasub (the same two swords which ‘Alqamah ibn-’Abadah had mentioned in his poetry), which al-Harith ibn-abi-Shamir, king of Ghassan, had presented al-Fals. ‘Ali brought them to the Prophet who wore one of them and gave it back to him. It was the sword which ‘Ali was always wont to wear

",High,4.0
Translation,nazimali/quran,11.0,771.0,2024-09-08 18:58:17+00:00,cc-by-3.0,0.0,130 MB,136314880.0,130 MB,136314880.0,6236,none,none,"---
dataset_info:
  features:
  - name: surah
    dtype: int64
  - name: ayah
    dtype: int64
  - name: surah-name
    dtype: string
  - name: surah-total-ayas
    dtype: int64
  - name: surah-name-transliteration
    dtype: string
  - name: surah-name-en
    dtype: string
  - name: surah-type
    dtype: string
  - name: surah-order-revealed
    dtype: int64
  - name: surah-rukus
    dtype: int64
  - name: arabic-text-simple
    dtype: string
  - name: arabic-text-simple-min
    dtype: string
  - name: arabic-text-simple-plain
    dtype: string
  - name: arabic-text-simple-clean
    dtype: string
  - name: arabic-text-uthmani
    dtype: string
  - name: translation-am-sadiq
    dtype: string
  - name: translation-ar-jalalayn
    dtype: string
  - name: translation-ar-muyassar
    dtype: string
  - name: translation-az-mammadaliyev
    dtype: string
  - name: translation-az-musayev
    dtype: string
  - name: translation-ber-mensur
    dtype: string
  - name: translation-bg-theophanov
    dtype: string
  - name: translation-bn-bengali
    dtype: string
  - name: translation-bn-hoque
    dtype: string
  - name: translation-bs-korkut
    dtype: string
  - name: translation-bs-mlivo
    dtype: string
  - name: translation-cs-hrbek
    dtype: string
  - name: translation-cs-nykl
    dtype: string
  - name: translation-de-aburida
    dtype: string
  - name: translation-de-bubenheim
    dtype: string
  - name: translation-de-khoury
    dtype: string
  - name: translation-de-zaidan
    dtype: string
  - name: translation-dv-divehi
    dtype: string
  - name: translation-en-ahmedali
    dtype: string
  - name: translation-en-ahmedraza
    dtype: string
  - name: translation-en-arberry
    dtype: string
  - name: translation-en-hilali
    dtype: string
  - name: translation-en-itani
    dtype: string
  - name: translation-en-maududi
    dtype: string
  - name: translation-en-mubarakpuri
    dtype: string
  - name: translation-en-pickthall
    dtype: string
  - name: translation-en-qarai
    dtype: string
  - name: translation-en-qaribullah
    dtype: string
  - name: translation-en-sahih
    dtype: string
  - name: translation-en-sarwar
    dtype: string
  - name: translation-en-shakir
    dtype: string
  - name: translation-en-transliteration
    dtype: string
  - name: translation-en-wahiduddin
    dtype: string
  - name: translation-en-yusufali
    dtype: string
  - name: translation-es-bornez
    dtype: string
  - name: translation-es-cortes
    dtype: string
  - name: translation-es-garcia
    dtype: string
  - name: translation-fa-ansarian
    dtype: string
  - name: translation-fa-ayati
    dtype: string
  - name: translation-fa-bahrampour
    dtype: string
  - name: translation-fa-fooladvand
    dtype: string
  - name: translation-fa-gharaati
    dtype: string
  - name: translation-fa-ghomshei
    dtype: string
  - name: translation-fa-khorramdel
    dtype: string
  - name: translation-fa-khorramshahi
    dtype: string
  - name: translation-fa-makarem
    dtype: string
  - name: translation-fa-moezzi
    dtype: string
  - name: translation-fa-mojtabavi
    dtype: string
  - name: translation-fa-sadeqi
    dtype: string
  - name: translation-fa-safavi
    dtype: string
  - name: translation-fr-hamidullah
    dtype: string
  - name: translation-ha-gumi
    dtype: string
  - name: translation-hi-farooq
    dtype: string
  - name: translation-hi-hindi
    dtype: string
  - name: translation-id-indonesian
    dtype: string
  - name: translation-id-jalalayn
    dtype: string
  - name: translation-id-muntakhab
    dtype: string
  - name: translation-it-piccardo
    dtype: string
  - name: translation-ja-japanese
    dtype: string
  - name: translation-ko-korean
    dtype: string
  - name: translation-ku-asan
    dtype: string
  - name: translation-ml-abdulhameed
    dtype: string
  - name: translation-ml-karakunnu
    dtype: string
  - name: translation-ms-basmeih
    dtype: string
  - name: translation-nl-keyzer
    dtype: string
  - name: translation-nl-leemhuis
    dtype: string
  - name: translation-nl-siregar
    dtype: string
  - name: translation-no-berg
    dtype: string
  - name: translation-pl-bielawskiego
    dtype: string
  - name: translation-ps-abdulwali
    dtype: string
  - name: translation-pt-elhayek
    dtype: string
  - name: translation-ro-grigore
    dtype: string
  - name: translation-ru-abuadel
    dtype: string
  - name: translation-ru-kalam
    dtype: string
  - name: translation-ru-krachkovsky
    dtype: string
  - name: translation-ru-kuliev-alsaadi
    dtype: string
  - name: translation-ru-kuliev
    dtype: string
  - name: translation-ru-muntahab
    dtype: string
  - name: translation-ru-osmanov
    dtype: string
  - name: translation-ru-porokhova
    dtype: string
  - name: translation-ru-sablukov
    dtype: string
  - name: translation-sd-amroti
    dtype: string
  - name: translation-so-abduh
    dtype: string
  - name: translation-sq-ahmeti
    dtype: string
  - name: translation-sq-mehdiu
    dtype: string
  - name: translation-sq-nahi
    dtype: string
  - name: translation-sv-bernstrom
    dtype: string
  - name: translation-sw-barwani
    dtype: string
  - name: translation-ta-tamil
    dtype: string
  - name: translation-tg-ayati
    dtype: string
  - name: translation-th-thai
    dtype: string
  - name: translation-tr-ates
    dtype: string
  - name: translation-tr-bulac
    dtype: string
  - name: translation-tr-diyanet
    dtype: string
  - name: translation-tr-golpinarli
    dtype: string
  - name: translation-tr-ozturk
    dtype: string
  - name: translation-tr-transliteration
    dtype: string
  - name: translation-tr-vakfi
    dtype: string
  - name: translation-tr-yazir
    dtype: string
  - name: translation-tr-yildirim
    dtype: string
  - name: translation-tr-yuksel
    dtype: string
  - name: translation-tt-nugman
    dtype: string
  - name: translation-ug-saleh
    dtype: string
  - name: translation-ur-ahmedali
    dtype: string
  - name: translation-ur-jalandhry
    dtype: string
  - name: translation-ur-jawadi
    dtype: string
  - name: translation-ur-junagarhi
    dtype: string
  - name: translation-ur-kanzuliman
    dtype: string
  - name: translation-ur-maududi
    dtype: string
  - name: translation-ur-najafi
    dtype: string
  - name: translation-ur-qadri
    dtype: string
  - name: translation-uz-sodik
    dtype: string
  - name: translation-zh-jian
    dtype: string
  - name: translation-zh-majian
    dtype: string
  splits:
  - name: train
    num_bytes: 171759080
    num_examples: 6236
  download_size: 129834597
  dataset_size: 171759080
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: cc-by-3.0
task_categories:
- text-classification
- token-classification
- translation
- feature-extraction
- text-generation
tags:
- islam
- quran
- translations
pretty_name: Quran
multilinguality:
- monolingual
- multilingual
language:
- sq
- ber
- ar
- am
- az
- bn
- bs
- bg
- zh
- cs
- dv
- nl
- en
- fr
- de
- ha
- hi
- id
- it
- ja
- ko
- ku
- ms
- ml
- no
- ps
- fa
- pl
- pt
- ro
- ru
- sd
- so
- es
- sw
- sv
- tg
- ta
- tt
- th
- tr
- ur
- ug
- uz
size_categories:
- 1K<n<10K
---

# Dataset Card for the Quran

## Summary

The Quran with metadata, translations, and multiple Arabic text (can use specific types for embeddings, search, classification, and display). There are 126+ columns containing 43+ languages.

## TODO

1. [ ]  Add Tafsirs  
2. [ ]  Add topics/ontology  

## Usage

```python
from datasets import load_dataset

ds = load_dataset(""nazimali/quran"", split=""train"")
ds
```

Output:

```python
Dataset({
    features: ['surah', 'ayah', 'surah-name', 'surah-total-ayas', 'surah-name-transliteration', 'surah-name-en', 'surah-type', 'surah-order-revealed', 'surah-rukus', 'arabic-text-simple', 'arabic-text-simple-min', 'arabic-text-simple-plain', 'arabic-text-simple-clean', 'arabic-text-uthmani', 'translation-am-sadiq', 'translation-ar-jalalayn', 'translation-ar-muyassar', 'translation-az-mammadaliyev', 'translation-az-musayev', 'translation-ber-mensur', 'translation-bg-theophanov', 'translation-bn-bengali', 'translation-bn-hoque', 'translation-bs-korkut', 'translation-bs-mlivo', 'translation-cs-hrbek', 'translation-cs-nykl', 'translation-de-aburida', 'translation-de-bubenheim', 'translation-de-khoury', 'translation-de-zaidan', 'translation-dv-divehi', 'translation-en-ahmedali', 'translation-en-ahmedraza', 'translation-en-arberry', 'translation-en-hilali', 'translation-en-itani', 'translation-en-maududi', 'translation-en-mubarakpuri', 'translation-en-pickthall', 'translation-en-qarai', 'translation-en-qaribullah', 'translation-en-sahih', 'translation-en-sarwar', 'translation-en-shakir', 'translation-en-transliteration', 'translation-en-wahiduddin', 'translation-en-yusufali', 'translation-es-bornez', 'translation-es-cortes', 'translation-es-garcia', 'translation-fa-ansarian', 'translation-fa-ayati', 'translation-fa-bahrampour', 'translation-fa-fooladvand', 'translation-fa-gharaati', 'translation-fa-ghomshei', 'translation-fa-khorramdel', 'translation-fa-khorramshahi', 'translation-fa-makarem', 'translation-fa-moezzi', 'translation-fa-mojtabavi', 'translation-fa-sadeqi', 'translation-fa-safavi', 'translation-fr-hamidullah', 'translation-ha-gumi', 'translation-hi-farooq', 'translation-hi-hindi', 'translation-id-indonesian', 'translation-id-jalalayn', 'translation-id-muntakhab', 'translation-it-piccardo', 'translation-ja-japanese', 'translation-ko-korean', 'translation-ku-asan', 'translation-ml-abdulhameed', 'translation-ml-karakunnu', 'translation-ms-basmeih', 'translation-nl-keyzer', 'translation-nl-leemhuis', 'translation-nl-siregar', 'translation-no-berg', 'translation-pl-bielawskiego', 'translation-ps-abdulwali', 'translation-pt-elhayek', 'translation-ro-grigore', 'translation-ru-abuadel', 'translation-ru-kalam', 'translation-ru-krachkovsky', 'translation-ru-kuliev-alsaadi', 'translation-ru-kuliev', 'translation-ru-muntahab', 'translation-ru-osmanov', 'translation-ru-porokhova', 'translation-ru-sablukov', 'translation-sd-amroti', 'translation-so-abduh', 'translation-sq-ahmeti', 'translation-sq-mehdiu', 'translation-sq-nahi', 'translation-sv-bernstrom', 'translation-sw-barwani', 'translation-ta-tamil', 'translation-tg-ayati', 'translation-th-thai', 'translation-tr-ates', 'translation-tr-bulac', 'translation-tr-diyanet', 'translation-tr-golpinarli', 'translation-tr-ozturk', 'translation-tr-transliteration', 'translation-tr-vakfi', 'translation-tr-yazir', 'translation-tr-yildirim', 'translation-tr-yuksel', 'translation-tt-nugman', 'translation-ug-saleh', 'translation-ur-ahmedali', 'translation-ur-jalandhry', 'translation-ur-jawadi', 'translation-ur-junagarhi', 'translation-ur-kanzuliman', 'translation-ur-maududi', 'translation-ur-najafi', 'translation-ur-qadri', 'translation-uz-sodik', 'translation-zh-jian', 'translation-zh-majian'],
    num_rows: 6236
})
```

## Data Cleaning

Removed extra spaces, old/unused unicode characters, and incorrect pipe seperators.

## Columns Info

### Metadata

General metadata for the Quran.

```json
{
  ""surah"": Number of the surah,
  ""ayah"": Number of the ayah,
  ""surah-name"": Surah name in Arabic,
  ""surah-total-ayas"": Total number of ayat in the surah,
  ""surah-name-transliteration"": Surah transliteration,
  ""surah-name-en"": Surah name in English,
  ""surah-type"": `Meccan` or `Medinan` surah,
  ""surah-order-revealed"": Order of when the surah was revealed,
  ""surah-rukus"": Number of rukus
}
```

### Arabic

Different Arabic text can be used for better results depending on the task. See [Quran text types](https://tanzil.net/docs/Quran_Text_Types) for more detail.

```json
{
  ""arabic-text-simple"": Quran text in Imlaei script,
  ""arabic-text-simple-min"": Minimal number of diacritics and symbols. Suitable for embedding in other texts,
  ""arabic-text-simple-plain"": Without special demonstration of Ikhfas and Idghams,
  ""arabic-text-simple-clean"": Without any diacritics or symbols. Suitable for search,
  ""arabic-text-uthmani"": Script used by the third Caliph, Uthman (RA), to produce the first standard Quran manuscript,
}
```

### Translations

Each language can have multiple translations. The translation columns use the naming pattern: `translation-{lang_iso_code}-{translator_surname}`.


| Language    | Name                       | Translator                                                | Column                         |
|:------------|:---------------------------|:----------------------------------------------------------|:-------------------------------|
| Albanian    | Efendi Nahi                | Hasan Efendi Nahi                                         | translation-sq-nahi            |
| Albanian    | Feti Mehdiu                | Feti Mehdiu                                               | translation-sq-mehdiu          |
| Albanian    | Sherif Ahmeti              | Sherif Ahmeti                                             | translation-sq-ahmeti          |
| Amazigh     | At Mensur                  | Ramdane At Mansour                                        | translation-be-.mensur         |
| Arabic      | تفسير الجلالين                 | Jalal ad-Din al-Mahalli and Jalal ad-Din as-Suyuti        | translation-ar-jalalayn        |
| Arabic      | تفسير المیسر                 | King Fahad Quran Complex                                  | translation-ar-muyassar        |
| Amharic     | ሳዲቅ & ሳኒ ሐቢብ              | Muhammed Sadiq and Muhammed Sani Habib                    | translation-am-sadiq           |
| Azerbaijani | Məmmədəliyev & Bünyadov    | Vasim Mammadaliyev and Ziya Bunyadov                      | translation-az-mammadaliyev    |
| Azerbaijani | Musayev                    | Alikhan Musayev                                           | translation-az-musayev         |
| Bengali     | জহুরুল হক                   | Zohurul Hoque                                             | translation-bn-hoque           |
| Bengali     | মুহিউদ্দীন খান                 | Muhiuddin Khan                                            | translation-bn-bengali         |
| Bosnian     | Korkut                     | Besim Korkut                                              | translation-bs-korkut          |
| Bosnian     | Mlivo                      | Mustafa Mlivo                                             | translation-bs-mlivo           |
| Bulgarian   | Теофанов                   | Tzvetan Theophanov                                        | translation-bg-theophanov      |
| Chinese     | Ma Jian                    | Ma Jian                                                   | translation-zh-jian            |
| Chinese     | Ma Jian (Traditional)      | Ma Jian                                                   | translation-zh-majian          |
| Czech       | Hrbek                      | Preklad I. Hrbek                                          | translation-cs-hrbek           |
| Czech       | Nykl                       | A. R. Nykl                                                | translation-cs-nykl            |
| Divehi      | ދިވެހި                         | Office of the President of Maldives                       | translation-dv-divehi          |
| Dutch       | Keyzer                     | Salomo Keyzer                                             | translation-nl-keyzer          |
| Dutch       | Leemhuis                   | Fred Leemhuis                                             | translation-nl-leemhuis        |
| Dutch       | Siregar                    | Sofian S. Siregar                                         | translation-nl-siregar         |
| English     | Ahmed Ali                  | Ahmed Ali                                                 | translation-en-ahmedali        |
| English     | Ahmed Raza Khan            | Ahmed Raza Khan                                           | translation-en-ahmedraza       |
| English     | Arberry                    | A. J. Arberry                                             | translation-en-arberry         |
| English     | Daryabadi                  | Abdul Majid Daryabadi                                     | translation-en-daryabadi       |
| English     | Hilali & Khan              | Muhammad Taqi-ud-Din al-Hilali and Muhammad Muhsin Khan   | translation-en-hilali          |
| English     | Itani                      | Talal Itani                                               | translation-en-itani           |
| English     | Maududi                    | Abul Ala Maududi                                          | translation-en-maududi         |
| English     | Mubarakpuri                | Safi-ur-Rahman al-Mubarakpuri                             | translation-en-mubarakpuri     |
| English     | Pickthall                  | Mohammed Marmaduke William Pickthall                      | translation-en-pickthall       |
| English     | Qarai                      | Ali Quli Qarai                                            | translation-en-qarai           |
| English     | Qaribullah & Darwish       | Hasan al-Fatih Qaribullah and Ahmad Darwish               | translation-en-qaribullah      |
| English     | Saheeh International       | Saheeh International                                      | translation-en-sahih           |
| English     | Sarwar                     | Muhammad Sarwar                                           | translation-en-sarwar          |
| English     | Shakir                     | Mohammad Habib Shakir                                     | translation-en-shakir          |
| English     | Transliteration            | English Transliteration                                   | translation-en-transliteration |
| English     | Wahiduddin Khan            | Wahiduddin Khan                                           | translation-en-wahiduddin      |
| English     | Yusuf Ali                  | Abdullah Yusuf Ali                                        | translation-en-yusufali        |
| French      | Hamidullah                 | Muhammad Hamidullah                                       | translation-fr-hamidullah      |
| German      | Abu Rida                   | Abu Rida Muhammad ibn Ahmad ibn Rassoul                   | translation-de-aburida         |
| German      | Bubenheim & Elyas          | A. S. F. Bubenheim and N. Elyas                           | translation-de-bubenheim       |
| German      | Khoury                     | Adel Theodor Khoury                                       | translation-de-khoury          |
| German      | Zaidan                     | Amir Zaidan                                               | translation-de-zaidan          |
| Hausa       | Gumi                       | Abubakar Mahmoud Gumi                                     | translation-ha-gumi            |
| Hindi       | फ़ारूक़ ख़ान & अहमद            | Muhammad Farooq Khan and Muhammad Ahmed                   | translation-hi-farooq          |
| Hindi       | फ़ारूक़ ख़ान & नदवी            | Suhel Farooq Khan and Saifur Rahman Nadwi                 | translation-hi-hindi           |
| Indonesian  | Bahasa Indonesia           | Indonesian Ministry of Religious Affairs                  | translation-id-indonesian      |
| Indonesian  | Quraish Shihab             | Muhammad Quraish Shihab et al.                            | translation-id-muntakhab       |
| Indonesian  | Tafsir Jalalayn            | Jalal ad-Din al-Mahalli and Jalal ad-Din as-Suyuti        | translation-id-jalalayn        |
| Italian     | Piccardo                   | Hamza Roberto Piccardo                                    | translation-it-piccardo        |
| Japanese    | Japanese                   | Unknown                                                   | translation-ja-japanese        |
| Korean      | Korean                     | Unknown                                                   | translation-ko-korean          |
| Kurdish     | تهفسیری ئاسان                | Burhan Muhammad-Amin                                      | translation-ku-asan            |
| Malay       | Basmeih                    | Abdullah Muhammad Basmeih                                 | translation-ms-basmeih         |
| Malayalam   | അബ്ദുല്‍ ഹമീദ് & പറപ്പൂര്‍           | Cheriyamundam Abdul Hameed and Kunhi Mohammed Parappoor   | translation-ml-abdulhameed     |
| Malayalam   | കാരകുന്ന് & എളയാവൂര്           | Muhammad Karakunnu and Vanidas Elayavoor                  | translation-ml-karakunnu       |
| Norwegian   | Einar Berg                 | Einar Berg                                                | translation-no-berg            |
| Pashto      | عبدالولي                    | Abdulwali Khan                                            | translation-ps-abdulwali       |
| Persian     | انصاریان                   | Hussain Ansarian                                          | translation-fa-ansarian        |
| Persian     | آیتی                       | AbdolMohammad Ayati                                       | translation-fa-ayati           |
| Persian     | بهرامپور                    | Abolfazl Bahrampour                                       | translation-fa-bahrampour      |
| Persian     | قرائتی                     | Mohsen Gharaati                                           | translation-fa-gharaati        |
| Persian     | الهی قمشهای                 | Mahdi Elahi Ghomshei                                      | translation-fa-ghomshei        |
| Persian     | خرمدل                      | Mostafa Khorramdel                                        | translation-fa-khorramdel      |
| Persian     | خرمشاهی                    | Baha'oddin Khorramshahi                                   | translation-fa-khorramshahi    |
| Persian     | صادقی تهرانی                | Mohammad Sadeqi Tehrani                                   | translation-fa-sadeqi          |
| Persian     | صفوی                       | Sayyed Mohammad Reza Safavi                               | translation-fa-safavi          |
| Persian     | فولادوند                    | Mohammad Mahdi Fooladvand                                 | translation-fa-fooladvand      |
| Persian     | مجتبوی                      | Sayyed Jalaloddin Mojtabavi                               | translation-fa-mojtabavi       |
| Persian     | معزی                       | Mohammad Kazem Moezzi                                     | translation-fa-moezzi          |
| Persian     | مکارم شیرازی                | Naser Makarem Shirazi                                     | translation-fa-makarem         |
| Polish      | Bielawskiego               | Józefa Bielawskiego                                       | translation-pl-bielawskiego    |
| Portuguese  | El-Hayek                   | Samir El-Hayek                                            | translation-pt-elhayek         |
| Romanian    | Grigore                    | George Grigore                                            | translation-ro-grigore         |
| Russian     | Абу Адель                  | Abu Adel                                                  | translation-ru-abuadel         |
| Russian     | Абу Адель                  | Abu Adel                                                  | translation-ru-abuadel         |
| Russian     | Аль-Мунтахаб               | Ministry of Awqaf, Egypt                                  | translation-ru-muntahab        |
| Russian     | Калям Шариф                | Muslim Religious Board of the Republiс of Tatarstan       | translation-ru-kalam           |
| Russian     | Крачковский                | Ignaty Yulianovich Krachkovsky                            | translation-ru-krachkovsky     |
| Russian     | Кулиев                     | Elmir Kuliev                                              | translation-ru-kuliev          |
| Russian     | Кулиев + ас-Саади          | Elmir Kuliev (with Abd ar-Rahman as-Saadi's commentaries) | translation-ru-kuliev-alsaadi  |
| Russian     | Османов                    | Magomed-Nuri Osmanovich Osmanov                           | translation-ru-osmanov         |
| Russian     | Порохова                   | V. Porokhova                                              | translation-ru-porokhova       |
| Russian     | Саблуков                   | Gordy Semyonovich Sablukov                                | translation-ru-sablukov        |
| Sindhi      | امروٽي                     | Taj Mehmood Amroti                                        | translation-sd-amroti          |
| Somali      | Abduh                      | Mahmud Muhammad Abduh                                     | translation-so-abduh           |
| Spanish     | Bornez                     | Raúl González Bórnez                                      | translation-es-bornez          |
| Spanish     | Cortes                     | Julio Cortes                                              | translation-es-cortes          |
| Spanish     | Garcia                     | Muhammad Isa García                                       | translation-es-garcia          |
| Swahili     | Al-Barwani                 | Ali Muhsin Al-Barwani                                     | translation-sw-barwani         |
| Swedish     | Bernström                  | Knut Bernström                                            | translation-sv-bernstrom       |
| Tajik       | Оятӣ                       | AbdolMohammad Ayati                                       | translation-tg-ayati           |
| Tamil       | ஜான் டிரஸ்ட்                 | Jan Turst Foundation                                      | translation-ta-tamil           |
| Tatar       | Yakub Ibn Nugman           | Yakub Ibn Nugman                                          | translation-tt-nugman          |
| Thai        | ภาษาไทย                    | King Fahad Quran Complex                                  | translation-th-thai            |
| Turkish     | Abdulbakî Gölpınarlı       | Abdulbaki Golpinarli                                      | translation-tr-golpinarli      |
| Turkish     | Alİ Bulaç                  | Alİ Bulaç                                                 | translation-tr-bulac           |
| Turkish     | Çeviriyazı                 | Muhammet Abay                                             | translation-tr-transliteration |
| Turkish     | Diyanet İşleri             | Diyanet Isleri                                            | translation-tr-diyanet         |
| Turkish     | Diyanet Vakfı              | Diyanet Vakfi                                             | translation-tr-vakfi           |
| Turkish     | Edip Yüksel                | Edip Yüksel                                               | translation-tr-yuksel          |
| Turkish     | Elmalılı Hamdi Yazır       | Elmalili Hamdi Yazir                                      | translation-tr-yazir           |
| Turkish     | Öztürk                     | Yasar Nuri Ozturk                                         | translation-tr-ozturk          |
| Turkish     | Suat Yıldırım              | Suat Yildirim                                             | translation-tr-yildirim        |
| Turkish     | Süleyman Ateş              | Suleyman Ates                                             | translation-tr-ates            |
| Urdu        | ابوالاعلی مودودی            | Abul A'ala Maududi                                        | translation-ur-maududi         |
| Urdu        | احمد رضا خان                | Ahmed Raza Khan                                           | translation-ur-kanzuliman      |
| Urdu        | احمد علی                    | Ahmed Ali                                                 | translation-ur-ahmedali        |
| Urdu        | جالندہری                    | Fateh Muhammad Jalandhry                                  | translation-ur-jalandhry       |
| Urdu        | طاہر القادری                | Tahir ul Qadri                                            | translation-ur-qadri           |
| Urdu        | علامہ جوادی                  | Syed Zeeshan Haider Jawadi                                | translation-ur-jawadi          |
| Urdu        | محمد جوناگڑھی               | Muhammad Junagarhi                                        | translation-ur-junagarhi       |
| Urdu        | محمد حسین نجفی                | Muhammad Hussain Najafi                                   | translation-ur-najafi          |
| Uyghur      | محمد صالح                    | Muhammad Saleh                                            | translation-ug-saleh           |
| Uzbek       | Мухаммад Содик             | Muhammad Sodik Muhammad Yusuf                             | translation-uz-sodik           |

## Initial Data Collection

The first version is sourced from **[Tanzil](https://tanzil.net/trans/)**

## Licensing Information

Tanzil [license](https://tanzil.net/docs/Text_License): **Creative Commons Attribution 3.0**",Medium,3.0
Translation,google/smol,59.0,2225.0,2025-03-03 18:49:38+00:00,cc-by-4.0,1.0,229 MB,240123904.0,68.7 MB,72037171.2,810660,https://arxiv.org/abs/2502.12301,none,"---
license: cc-by-4.0
task_categories:
- translation
pretty_name: Smol
size_categories:
- 10K<n<100K
language:
- aa
- ab
- ace
- ach
- ady
- aeb
- af
- ahr
- aii
- ak
- alz
- am
- apc
- apd
- ar
- arn
- arz
- as
- av
- awa
- ay
- ayl
- ba
- bal
- ban
- bbc
- bci
- bem
- ber
- bew
- bfq
- bfy
- bgq
- bho
- bik
- bjn
- bm
- bns
- bo
- br
- bra
- brx
- bts
- btx
- bua
- bug
- ccp
- ce
- cgg
- ch
- chk
- chm
- ckb
- cnh
- crh
- crs
- ctg
- cv
- dhd
- din
- doi
- dov
- dv
- dyu
- dz
- ee
- efi
- en
- es
- fa
- ff
- fj
- fo
- fon
- fr
- fur
- gaa
- gn
- gom
- grt
- ha
- hi
- hil
- hne
- hoc
- hrx
- iba
- ig
- ilo
- iso
- iu
- jam
- kaa
- kac
- kbd
- kek
- kfy
- kg
- kha
- ki
- kl
- kr
- kri
- kru
- ks
- ktu
- kv
- lep
- lg
- li
- lif
- lij
- lmo
- ln
- ltg
- lu
- lua
- luo
- lus
- mad
- mag
- mai
- mak
- mam
- meo
- mfe
- mg
- mh
- min
- mjl
- mni
- mos
- ms
- mtr
- mwr
- nd
- ndc
- ne
- new
- nhe
- noe
- nr
- nso
- nus
- nv
- ny
- oc
- om
- os
- pa
- pag
- pam
- pap
- pcm
- pt
- qu
- quc
- rhg
- rn
- rom
- rw
- sa
- sah
- sat
- scl
- scn
- sd
- se
- sg
- sgj
- shn
- sjp
- skr
- sn
- so
- spv
- ss
- st
- sus
- sw
- syl
- szl
- tcy
- tet
- ti
- tiv
- tn
- to
- tpi
- trp
- ts
- tum
- ty
- tyv
- udm
- unr
- ve
- vec
- war
- wbr
- wo
- xh
- xnr
- xsr
- yo
- yua
- yue
- zap
- zh
- zu
- zza
configs:
  - config_name: smolsent__en_es
    data_files:
      - split: train
        path: smolsent/en_es.jsonl
  - config_name: smolsent__en_qu
    data_files:
      - split: train
        path: smolsent/en_qu.jsonl
  - config_name: smolsent__en_ay
    data_files:
      - split: train
        path: smolsent/en_ay.jsonl
  - config_name: smolsent__en_gn
    data_files:
      - split: train
        path: smolsent/en_gn.jsonl
  - config_name: smolsent__en_bo
    data_files:
      - split: train
        path: smolsent/en_bo.jsonl
  - config_name: smolsent__en_brx
    data_files:
      - split: train
        path: smolsent/en_brx.jsonl
  - config_name: smolsent__en_pa-Arab
    data_files:
      - split: train
        path: smolsent/en_pa-Arab.jsonl
  - config_name: smolsent__en_sat-Latn
    data_files:
      - split: train
        path: smolsent/en_sat-Latn.jsonl
  - config_name: smolsent__en_sat
    data_files:
      - split: train
        path: smolsent/en_sat.jsonl
  - config_name: smolsent__en_sa
    data_files:
      - split: train
        path: smolsent/en_sa.jsonl
  - config_name: smolsent__en_ks
    data_files:
      - split: train
        path: smolsent/en_ks.jsonl
  - config_name: smolsent__en_ks-Deva
    data_files:
      - split: train
        path: smolsent/en_ks-Deva.jsonl
  - config_name: smolsent__en_yue
    data_files:
      - split: train
        path: smolsent/en_yue.jsonl
  - config_name: smolsent__en_lus
    data_files:
      - split: train
        path: smolsent/en_lus.jsonl
  - config_name: smolsent__en_ff
    data_files:
      - split: train
        path: smolsent/en_ff.jsonl
  - config_name: smolsent__en_kl
    data_files:
      - split: train
        path: smolsent/en_kl.jsonl
  - config_name: smolsent__en_mni-Mtei
    data_files:
      - split: train
        path: smolsent/en_mni-Mtei.jsonl
  - config_name: smolsent__en_af
    data_files:
      - split: train
        path: smolsent/en_af.jsonl
  - config_name: smolsent__en_am
    data_files:
      - split: train
        path: smolsent/en_am.jsonl
  - config_name: smolsent__en_ny
    data_files:
      - split: train
        path: smolsent/en_ny.jsonl
  - config_name: smolsent__en_ha
    data_files:
      - split: train
        path: smolsent/en_ha.jsonl
  - config_name: smolsent__en_ig
    data_files:
      - split: train
        path: smolsent/en_ig.jsonl
  - config_name: smolsent__en_rw
    data_files:
      - split: train
        path: smolsent/en_rw.jsonl
  - config_name: smolsent__en_ln
    data_files:
      - split: train
        path: smolsent/en_ln.jsonl
  - config_name: smolsent__en_luo
    data_files:
      - split: train
        path: smolsent/en_luo.jsonl
  - config_name: smolsent__en_pcm
    data_files:
      - split: train
        path: smolsent/en_pcm.jsonl
  - config_name: smolsent__en_om
    data_files:
      - split: train
        path: smolsent/en_om.jsonl
  - config_name: smolsent__en_nso
    data_files:
      - split: train
        path: smolsent/en_nso.jsonl
  - config_name: smolsent__en_st
    data_files:
      - split: train
        path: smolsent/en_st.jsonl
  - config_name: smolsent__en_sn
    data_files:
      - split: train
        path: smolsent/en_sn.jsonl
  - config_name: smolsent__en_sw
    data_files:
      - split: train
        path: smolsent/en_sw.jsonl
  - config_name: smolsent__en_ber-Latn
    data_files:
      - split: train
        path: smolsent/en_ber-Latn.jsonl
  - config_name: smolsent__en_ak
    data_files:
      - split: train
        path: smolsent/en_ak.jsonl
  - config_name: smolsent__en_zu
    data_files:
      - split: train
        path: smolsent/en_zu.jsonl
  - config_name: smolsent__en_wo
    data_files:
      - split: train
        path: smolsent/en_wo.jsonl
  - config_name: smolsent__en_so
    data_files:
      - split: train
        path: smolsent/en_so.jsonl
  - config_name: smolsent__en_nd
    data_files:
      - split: train
        path: smolsent/en_nd.jsonl
  - config_name: smolsent__en_mg
    data_files:
      - split: train
        path: smolsent/en_mg.jsonl
  - config_name: smolsent__en_din
    data_files:
      - split: train
        path: smolsent/en_din.jsonl
  - config_name: smolsent__en_gaa
    data_files:
      - split: train
        path: smolsent/en_gaa.jsonl
  - config_name: smolsent__en_xh
    data_files:
      - split: train
        path: smolsent/en_xh.jsonl
  - config_name: smolsent__en_ts
    data_files:
      - split: train
        path: smolsent/en_ts.jsonl
  - config_name: smolsent__en_lg
    data_files:
      - split: train
        path: smolsent/en_lg.jsonl
  - config_name: smolsent__en_bm
    data_files:
      - split: train
        path: smolsent/en_bm.jsonl
  - config_name: smolsent__en_arz
    data_files:
      - split: train
        path: smolsent/en_arz.jsonl
  - config_name: smolsent__en_ve
    data_files:
      - split: train
        path: smolsent/en_ve.jsonl
  - config_name: smolsent__en_efi
    data_files:
      - split: train
        path: smolsent/en_efi.jsonl
  - config_name: smolsent__en_dyu
    data_files:
      - split: train
        path: smolsent/en_dyu.jsonl
  - config_name: smolsent__en_tn
    data_files:
      - split: train
        path: smolsent/en_tn.jsonl
  - config_name: smolsent__en_rn
    data_files:
      - split: train
        path: smolsent/en_rn.jsonl
  - config_name: smolsent__en_tiv
    data_files:
      - split: train
        path: smolsent/en_tiv.jsonl
  - config_name: smolsent__en_mfe
    data_files:
      - split: train
        path: smolsent/en_mfe.jsonl
  - config_name: smolsent__en_ti
    data_files:
      - split: train
        path: smolsent/en_ti.jsonl
  - config_name: smolsent__en_sus
    data_files:
      - split: train
        path: smolsent/en_sus.jsonl
  - config_name: smolsent__en_ndc
    data_files:
      - split: train
        path: smolsent/en_ndc.jsonl
  - config_name: smolsent__en_ki
    data_files:
      - split: train
        path: smolsent/en_ki.jsonl
  - config_name: smolsent__en_cgg
    data_files:
      - split: train
        path: smolsent/en_cgg.jsonl
  - config_name: smolsent__en_bem
    data_files:
      - split: train
        path: smolsent/en_bem.jsonl
  - config_name: smolsent__en_tum
    data_files:
      - split: train
        path: smolsent/en_tum.jsonl
  - config_name: smolsent__en_nr
    data_files:
      - split: train
        path: smolsent/en_nr.jsonl
  - config_name: smolsent__en_kr
    data_files:
      - split: train
        path: smolsent/en_kr.jsonl
  - config_name: smolsent__en_fon
    data_files:
      - split: train
        path: smolsent/en_fon.jsonl
  - config_name: smolsent__en_yo
    data_files:
      - split: train
        path: smolsent/en_yo.jsonl
  - config_name: smolsent__en_ee
    data_files:
      - split: train
        path: smolsent/en_ee.jsonl
  - config_name: smolsent__en_ar-MA
    data_files:
      - split: train
        path: smolsent/en_ar-MA.jsonl
  - config_name: smolsent__en_apd
    data_files:
      - split: train
        path: smolsent/en_apd.jsonl
  - config_name: smolsent__en_ss
    data_files:
      - split: train
        path: smolsent/en_ss.jsonl
  - config_name: smolsent__en_ayl
    data_files:
      - split: train
        path: smolsent/en_ayl.jsonl
  - config_name: smolsent__en_ktu
    data_files:
      - split: train
        path: smolsent/en_ktu.jsonl
  - config_name: smolsent__en_aeb
    data_files:
      - split: train
        path: smolsent/en_aeb.jsonl
  - config_name: smolsent__en_aa
    data_files:
      - split: train
        path: smolsent/en_aa.jsonl
  - config_name: smolsent__en_lu
    data_files:
      - split: train
        path: smolsent/en_lu.jsonl
  - config_name: smolsent__en_ach
    data_files:
      - split: train
        path: smolsent/en_ach.jsonl
  - config_name: smolsent__en_alz
    data_files:
      - split: train
        path: smolsent/en_alz.jsonl
  - config_name: smolsent__en_bci
    data_files:
      - split: train
        path: smolsent/en_bci.jsonl
  - config_name: smolsent__en_dov
    data_files:
      - split: train
        path: smolsent/en_dov.jsonl
  - config_name: smolsent__en_kg
    data_files:
      - split: train
        path: smolsent/en_kg.jsonl
  - config_name: smolsent__en_mos
    data_files:
      - split: train
        path: smolsent/en_mos.jsonl
  - config_name: smolsent__en_nqo
    data_files:
      - split: train
        path: smolsent/en_nqo.jsonl
  - config_name: smolsent__en_ber
    data_files:
      - split: train
        path: smolsent/en_ber.jsonl
  - config_name: smolsent__en_kri
    data_files:
      - split: train
        path: smolsent/en_kri.jsonl
  - config_name: smoldoc__en_ki
    data_files:
      - split: train
        path: smoldoc/en_ki.jsonl
  - config_name: smoldoc__en_ach
    data_files:
      - split: train
        path: smoldoc/en_ach.jsonl
  - config_name: smoldoc__en_aa
    data_files:
      - split: train
        path: smoldoc/en_aa.jsonl
  - config_name: smoldoc__en_bem
    data_files:
      - split: train
        path: smoldoc/en_bem.jsonl
  - config_name: smoldoc__en_cgg
    data_files:
      - split: train
        path: smoldoc/en_cgg.jsonl
  - config_name: smoldoc__en_din
    data_files:
      - split: train
        path: smoldoc/en_din.jsonl
  - config_name: smoldoc__en_fon
    data_files:
      - split: train
        path: smoldoc/en_fon.jsonl
  - config_name: smoldoc__en_gaa
    data_files:
      - split: train
        path: smoldoc/en_gaa.jsonl
  - config_name: smoldoc__en_kr
    data_files:
      - split: train
        path: smoldoc/en_kr.jsonl
  - config_name: smoldoc__en_lu
    data_files:
      - split: train
        path: smoldoc/en_lu.jsonl
  - config_name: smoldoc__en_mfe
    data_files:
      - split: train
        path: smoldoc/en_mfe.jsonl
  - config_name: smoldoc__en_rn
    data_files:
      - split: train
        path: smoldoc/en_rn.jsonl
  - config_name: smoldoc__en_nr
    data_files:
      - split: train
        path: smoldoc/en_nr.jsonl
  - config_name: smoldoc__en_ts
    data_files:
      - split: train
        path: smoldoc/en_ts.jsonl
  - config_name: smoldoc__en_tn
    data_files:
      - split: train
        path: smoldoc/en_tn.jsonl
  - config_name: smoldoc__en_aeb
    data_files:
      - split: train
        path: smoldoc/en_aeb.jsonl
  - config_name: smoldoc__en_ve
    data_files:
      - split: train
        path: smoldoc/en_ve.jsonl
  - config_name: smoldoc__en_alz
    data_files:
      - split: train
        path: smoldoc/en_alz.jsonl
  - config_name: smoldoc__en_dov
    data_files:
      - split: train
        path: smoldoc/en_dov.jsonl
  - config_name: smoldoc__en_dyu
    data_files:
      - split: train
        path: smoldoc/en_dyu.jsonl
  - config_name: smoldoc__en_ayl
    data_files:
      - split: train
        path: smoldoc/en_ayl.jsonl
  - config_name: smoldoc__en_ndc
    data_files:
      - split: train
        path: smoldoc/en_ndc.jsonl
  - config_name: smoldoc__en_nd
    data_files:
      - split: train
        path: smoldoc/en_nd.jsonl
  - config_name: smoldoc__en_tum
    data_files:
      - split: train
        path: smoldoc/en_tum.jsonl
  - config_name: smoldoc__en_bci
    data_files:
      - split: train
        path: smoldoc/en_bci.jsonl
  - config_name: smoldoc__en_ktu
    data_files:
      - split: train
        path: smoldoc/en_ktu.jsonl
  - config_name: smoldoc__en_kg
    data_files:
      - split: train
        path: smoldoc/en_kg.jsonl
  - config_name: smoldoc__en_apd
    data_files:
      - split: train
        path: smoldoc/en_apd.jsonl
  - config_name: smoldoc__en_ss
    data_files:
      - split: train
        path: smoldoc/en_ss.jsonl
  - config_name: smoldoc__en_efi
    data_files:
      - split: train
        path: smoldoc/en_efi.jsonl
  - config_name: smoldoc__en_tiv
    data_files:
      - split: train
        path: smoldoc/en_tiv.jsonl
  - config_name: smoldoc__en_mos
    data_files:
      - split: train
        path: smoldoc/en_mos.jsonl
  - config_name: smoldoc__en_sus
    data_files:
      - split: train
        path: smoldoc/en_sus.jsonl
  - config_name: smoldoc__en_nqo
    data_files:
      - split: train
        path: smoldoc/en_nqo.jsonl
  - config_name: smoldoc__en_am
    data_files:
      - split: train
        path: smoldoc/en_am.jsonl
  - config_name: smoldoc__en_sw
    data_files:
      - split: train
        path: smoldoc/en_sw.jsonl
  - config_name: smoldoc__en_yo
    data_files:
      - split: train
        path: smoldoc/en_yo.jsonl
  - config_name: smoldoc__en_ha
    data_files:
      - split: train
        path: smoldoc/en_ha.jsonl
  - config_name: smoldoc__en_yue
    data_files:
      - split: train
        path: smoldoc/en_yue.jsonl
  - config_name: smoldoc__en_ig
    data_files:
      - split: train
        path: smoldoc/en_ig.jsonl
  - config_name: smoldoc__en_mg
    data_files:
      - split: train
        path: smoldoc/en_mg.jsonl
  - config_name: smoldoc__en_om
    data_files:
      - split: train
        path: smoldoc/en_om.jsonl
  - config_name: smoldoc__en_xh
    data_files:
      - split: train
        path: smoldoc/en_xh.jsonl
  - config_name: smoldoc__en_ny
    data_files:
      - split: train
        path: smoldoc/en_ny.jsonl
  - config_name: smoldoc__en_ln
    data_files:
      - split: train
        path: smoldoc/en_ln.jsonl
  - config_name: smoldoc__en_wo
    data_files:
      - split: train
        path: smoldoc/en_wo.jsonl
  - config_name: smoldoc__en_sn
    data_files:
      - split: train
        path: smoldoc/en_sn.jsonl
  - config_name: smoldoc__en_rw
    data_files:
      - split: train
        path: smoldoc/en_rw.jsonl
  - config_name: smoldoc__en_st
    data_files:
      - split: train
        path: smoldoc/en_st.jsonl
  - config_name: smoldoc__en_ff
    data_files:
      - split: train
        path: smoldoc/en_ff.jsonl
  - config_name: smoldoc__en_lg
    data_files:
      - split: train
        path: smoldoc/en_lg.jsonl
  - config_name: smoldoc__en_luo
    data_files:
      - split: train
        path: smoldoc/en_luo.jsonl
  - config_name: smoldoc__en_scn
    data_files:
      - split: train
        path: smoldoc/en_scn.jsonl
  - config_name: smoldoc__en_ar-MA
    data_files:
      - split: train
        path: smoldoc/en_ar-MA.jsonl
  - config_name: smoldoc__en_ti
    data_files:
      - split: train
        path: smoldoc/en_ti.jsonl
  - config_name: smoldoc__en_zu
    data_files:
      - split: train
        path: smoldoc/en_zu.jsonl
  - config_name: smoldoc__en_ak
    data_files:
      - split: train
        path: smoldoc/en_ak.jsonl
  - config_name: smoldoc__en_arz
    data_files:
      - split: train
        path: smoldoc/en_arz.jsonl
  - config_name: smoldoc__en_bm
    data_files:
      - split: train
        path: smoldoc/en_bm.jsonl
  - config_name: smoldoc__en_so
    data_files:
      - split: train
        path: smoldoc/en_so.jsonl
  - config_name: smoldoc__en_pcm
    data_files:
      - split: train
        path: smoldoc/en_pcm.jsonl
  - config_name: smoldoc__en_nso
    data_files:
      - split: train
        path: smoldoc/en_nso.jsonl
  - config_name: smoldoc__en_af
    data_files:
      - split: train
        path: smoldoc/en_af.jsonl
  - config_name: smoldoc__en_ee
    data_files:
      - split: train
        path: smoldoc/en_ee.jsonl
  - config_name: smoldoc__en_kri
    data_files:
      - split: train
        path: smoldoc/en_kri.jsonl
  - config_name: smoldoc__en_ber-Latn
    data_files:
      - split: train
        path: smoldoc/en_ber-Latn.jsonl
  - config_name: smoldoc__en_ber
    data_files:
      - split: train
        path: smoldoc/en_ber.jsonl
  - config_name: smoldoc__en_kru
    data_files:
      - split: train
        path: smoldoc/en_kru.jsonl
  - config_name: smoldoc__en_hoc-Wara
    data_files:
      - split: train
        path: smoldoc/en_hoc-Wara.jsonl
  - config_name: smoldoc__en_bra
    data_files:
      - split: train
        path: smoldoc/en_bra.jsonl
  - config_name: smoldoc__en_bgq
    data_files:
      - split: train
        path: smoldoc/en_bgq.jsonl
  - config_name: smoldoc__en_trp
    data_files:
      - split: train
        path: smoldoc/en_trp.jsonl
  - config_name: smoldoc__en_xsr-Tibt
    data_files:
      - split: train
        path: smoldoc/en_xsr-Tibt.jsonl
  - config_name: smoldoc__en_grt-Latn
    data_files:
      - split: train
        path: smoldoc/en_grt-Latn.jsonl
  - config_name: smoldoc__en_bfq
    data_files:
      - split: train
        path: smoldoc/en_bfq.jsonl
  - config_name: smoldoc__en_ahr
    data_files:
      - split: train
        path: smoldoc/en_ahr.jsonl
  - config_name: smoldoc__en_ccp-Latn
    data_files:
      - split: train
        path: smoldoc/en_ccp-Latn.jsonl
  - config_name: smoldoc__en_xnr
    data_files:
      - split: train
        path: smoldoc/en_xnr.jsonl
  - config_name: smoldoc__en_lep
    data_files:
      - split: train
        path: smoldoc/en_lep.jsonl
  - config_name: smoldoc__en_kfy
    data_files:
      - split: train
        path: smoldoc/en_kfy.jsonl
  - config_name: smoldoc__en_lif-Limb
    data_files:
      - split: train
        path: smoldoc/en_lif-Limb.jsonl
  - config_name: smoldoc__en_mjl
    data_files:
      - split: train
        path: smoldoc/en_mjl.jsonl
  - config_name: smoldoc__en_scl
    data_files:
      - split: train
        path: smoldoc/en_scl.jsonl
  - config_name: smoldoc__en_unr-Deva
    data_files:
      - split: train
        path: smoldoc/en_unr-Deva.jsonl
  - config_name: smoldoc__en_tcy
    data_files:
      - split: train
        path: smoldoc/en_tcy.jsonl
  - config_name: smoldoc__en_sd-Deva
    data_files:
      - split: train
        path: smoldoc/en_sd-Deva.jsonl
  - config_name: smoldoc__en_bns
    data_files:
      - split: train
        path: smoldoc/en_bns.jsonl
  - config_name: smoldoc__en_wbr
    data_files:
      - split: train
        path: smoldoc/en_wbr.jsonl
  - config_name: smoldoc__en_mtr
    data_files:
      - split: train
        path: smoldoc/en_mtr.jsonl
  - config_name: smoldoc__en_sjp
    data_files:
      - split: train
        path: smoldoc/en_sjp.jsonl
  - config_name: smoldoc__en_spv
    data_files:
      - split: train
        path: smoldoc/en_spv.jsonl
  - config_name: smoldoc__en_ne
    data_files:
      - split: train
        path: smoldoc/en_ne.jsonl
  - config_name: smoldoc__en_mag
    data_files:
      - split: train
        path: smoldoc/en_mag.jsonl
  - config_name: smoldoc__en_sgj
    data_files:
      - split: train
        path: smoldoc/en_sgj.jsonl
  - config_name: smoldoc__en_noe
    data_files:
      - split: train
        path: smoldoc/en_noe.jsonl
  - config_name: smoldoc__en_doi
    data_files:
      - split: train
        path: smoldoc/en_doi.jsonl
  - config_name: smoldoc__en_dhd
    data_files:
      - split: train
        path: smoldoc/en_dhd.jsonl
  - config_name: smoldoc__en_bfy
    data_files:
      - split: train
        path: smoldoc/en_bfy.jsonl
  - config_name: smoldoc__zh_sw
    data_files:
      - split: train
        path: smoldoc/zh_sw.jsonl
  - config_name: smoldoc__ar_sw
    data_files:
      - split: train
        path: smoldoc/ar_sw.jsonl
  - config_name: smoldoc__zh_am
    data_files:
      - split: train
        path: smoldoc/zh_am.jsonl
  - config_name: smoldoc__ar_am
    data_files:
      - split: train
        path: smoldoc/ar_am.jsonl
  - config_name: gatitos__en_fr-CA
    data_files:
      - split: train
        path: gatitos/en_fr-CA.jsonl
  - config_name: gatitos__en_iu-Latn
    data_files:
      - split: train
        path: gatitos/en_iu-Latn.jsonl
  - config_name: gatitos__en_pt-PT
    data_files:
      - split: train
        path: gatitos/en_pt-PT.jsonl
  - config_name: gatitos__en_nhe
    data_files:
      - split: train
        path: gatitos/en_nhe.jsonl
  - config_name: gatitos__en_aii
    data_files:
      - split: train
        path: gatitos/en_aii.jsonl
  - config_name: gatitos__en_bjn
    data_files:
      - split: train
        path: gatitos/en_bjn.jsonl
  - config_name: gatitos__en_bjn-Arab
    data_files:
      - split: train
        path: gatitos/en_bjn-Arab.jsonl
  - config_name: gatitos__en_bci
    data_files:
      - split: train
        path: gatitos/en_bci.jsonl
  - config_name: gatitos__en_bem
    data_files:
      - split: train
        path: gatitos/en_bem.jsonl
  - config_name: gatitos__en_bua
    data_files:
      - split: train
        path: gatitos/en_bua.jsonl
  - config_name: gatitos__en_dov
    data_files:
      - split: train
        path: gatitos/en_dov.jsonl
  - config_name: gatitos__en_fur
    data_files:
      - split: train
        path: gatitos/en_fur.jsonl
  - config_name: gatitos__en_hrx
    data_files:
      - split: train
        path: gatitos/en_hrx.jsonl
  - config_name: gatitos__en_kr
    data_files:
      - split: train
        path: gatitos/en_kr.jsonl
  - config_name: gatitos__en_alz
    data_files:
      - split: train
        path: gatitos/en_alz.jsonl
  - config_name: gatitos__en_luo
    data_files:
      - split: train
        path: gatitos/en_luo.jsonl
  - config_name: gatitos__en_pap
    data_files:
      - split: train
        path: gatitos/en_pap.jsonl
  - config_name: gatitos__en_sg
    data_files:
      - split: train
        path: gatitos/en_sg.jsonl
  - config_name: gatitos__en_sus
    data_files:
      - split: train
        path: gatitos/en_sus.jsonl
  - config_name: gatitos__en_ty
    data_files:
      - split: train
        path: gatitos/en_ty.jsonl
  - config_name: gatitos__en_ab
    data_files:
      - split: train
        path: gatitos/en_ab.jsonl
  - config_name: gatitos__en_nqo
    data_files:
      - split: train
        path: gatitos/en_nqo.jsonl
  - config_name: gatitos__en_hne
    data_files:
      - split: train
        path: gatitos/en_hne.jsonl
  - config_name: gatitos__en_cgg
    data_files:
      - split: train
        path: gatitos/en_cgg.jsonl
  - config_name: gatitos__en_kv
    data_files:
      - split: train
        path: gatitos/en_kv.jsonl
  - config_name: gatitos__en_kg
    data_files:
      - split: train
        path: gatitos/en_kg.jsonl
  - config_name: gatitos__en_lij
    data_files:
      - split: train
        path: gatitos/en_lij.jsonl
  - config_name: gatitos__en_li
    data_files:
      - split: train
        path: gatitos/en_li.jsonl
  - config_name: gatitos__en_lmo
    data_files:
      - split: train
        path: gatitos/en_lmo.jsonl
  - config_name: gatitos__en_mos
    data_files:
      - split: train
        path: gatitos/en_mos.jsonl
  - config_name: gatitos__en_szl
    data_files:
      - split: train
        path: gatitos/en_szl.jsonl
  - config_name: gatitos__en_ss
    data_files:
      - split: train
        path: gatitos/en_ss.jsonl
  - config_name: gatitos__en_tiv
    data_files:
      - split: train
        path: gatitos/en_tiv.jsonl
  - config_name: gatitos__en_btx
    data_files:
      - split: train
        path: gatitos/en_btx.jsonl
  - config_name: gatitos__en_fj
    data_files:
      - split: train
        path: gatitos/en_fj.jsonl
  - config_name: gatitos__en_iso
    data_files:
      - split: train
        path: gatitos/en_iso.jsonl
  - config_name: gatitos__en_ltg
    data_files:
      - split: train
        path: gatitos/en_ltg.jsonl
  - config_name: gatitos__en_lua
    data_files:
      - split: train
        path: gatitos/en_lua.jsonl
  - config_name: gatitos__en_pag
    data_files:
      - split: train
        path: gatitos/en_pag.jsonl
  - config_name: gatitos__en_rn
    data_files:
      - split: train
        path: gatitos/en_rn.jsonl
  - config_name: gatitos__en_sat-Latn
    data_files:
      - split: train
        path: gatitos/en_sat-Latn.jsonl
  - config_name: gatitos__en_sat
    data_files:
      - split: train
        path: gatitos/en_sat.jsonl
  - config_name: gatitos__en_ch
    data_files:
      - split: train
        path: gatitos/en_ch.jsonl
  - config_name: gatitos__en_crh
    data_files:
      - split: train
        path: gatitos/en_crh.jsonl
  - config_name: gatitos__en_efi
    data_files:
      - split: train
        path: gatitos/en_efi.jsonl
  - config_name: gatitos__en_ki
    data_files:
      - split: train
        path: gatitos/en_ki.jsonl
  - config_name: gatitos__en_trp
    data_files:
      - split: train
        path: gatitos/en_trp.jsonl
  - config_name: gatitos__en_mam
    data_files:
      - split: train
        path: gatitos/en_mam.jsonl
  - config_name: gatitos__en_tyv
    data_files:
      - split: train
        path: gatitos/en_tyv.jsonl
  - config_name: gatitos__en_tum
    data_files:
      - split: train
        path: gatitos/en_tum.jsonl
  - config_name: gatitos__en_din
    data_files:
      - split: train
        path: gatitos/en_din.jsonl
  - config_name: gatitos__en_ks
    data_files:
      - split: train
        path: gatitos/en_ks.jsonl
  - config_name: gatitos__en_ndc
    data_files:
      - split: train
        path: gatitos/en_ndc.jsonl
  - config_name: gatitos__en_ban
    data_files:
      - split: train
        path: gatitos/en_ban.jsonl
  - config_name: gatitos__en_bal
    data_files:
      - split: train
        path: gatitos/en_bal.jsonl
  - config_name: gatitos__en_ba
    data_files:
      - split: train
        path: gatitos/en_ba.jsonl
  - config_name: gatitos__en_bts
    data_files:
      - split: train
        path: gatitos/en_bts.jsonl
  - config_name: gatitos__en_bbc
    data_files:
      - split: train
        path: gatitos/en_bbc.jsonl
  - config_name: gatitos__en_bew
    data_files:
      - split: train
        path: gatitos/en_bew.jsonl
  - config_name: gatitos__en_br
    data_files:
      - split: train
        path: gatitos/en_br.jsonl
  - config_name: gatitos__en_bik
    data_files:
      - split: train
        path: gatitos/en_bik.jsonl
  - config_name: gatitos__en_ctg
    data_files:
      - split: train
        path: gatitos/en_ctg.jsonl
  - config_name: gatitos__en_chk
    data_files:
      - split: train
        path: gatitos/en_chk.jsonl
  - config_name: gatitos__en_fo
    data_files:
      - split: train
        path: gatitos/en_fo.jsonl
  - config_name: gatitos__en_dz
    data_files:
      - split: train
        path: gatitos/en_dz.jsonl
  - config_name: gatitos__en_gaa
    data_files:
      - split: train
        path: gatitos/en_gaa.jsonl
  - config_name: gatitos__en_quc
    data_files:
      - split: train
        path: gatitos/en_quc.jsonl
  - config_name: gatitos__en_kbd
    data_files:
      - split: train
        path: gatitos/en_kbd.jsonl
  - config_name: gatitos__en_kaa
    data_files:
      - split: train
        path: gatitos/en_kaa.jsonl
  - config_name: gatitos__en_meo
    data_files:
      - split: train
        path: gatitos/en_meo.jsonl
  - config_name: gatitos__en_ktu
    data_files:
      - split: train
        path: gatitos/en_ktu.jsonl
  - config_name: gatitos__en_chm
    data_files:
      - split: train
        path: gatitos/en_chm.jsonl
  - config_name: gatitos__en_mfe
    data_files:
      - split: train
        path: gatitos/en_mfe.jsonl
  - config_name: gatitos__en_pcm
    data_files:
      - split: train
        path: gatitos/en_pcm.jsonl
  - config_name: gatitos__en_nd
    data_files:
      - split: train
        path: gatitos/en_nd.jsonl
  - config_name: gatitos__en_se
    data_files:
      - split: train
        path: gatitos/en_se.jsonl
  - config_name: gatitos__en_oc
    data_files:
      - split: train
        path: gatitos/en_oc.jsonl
  - config_name: gatitos__en_os
    data_files:
      - split: train
        path: gatitos/en_os.jsonl
  - config_name: gatitos__en_rhg-Latn
    data_files:
      - split: train
        path: gatitos/en_rhg-Latn.jsonl
  - config_name: gatitos__en_rom
    data_files:
      - split: train
        path: gatitos/en_rom.jsonl
  - config_name: gatitos__en_skr
    data_files:
      - split: train
        path: gatitos/en_skr.jsonl
  - config_name: gatitos__en_shn
    data_files:
      - split: train
        path: gatitos/en_shn.jsonl
  - config_name: gatitos__en_ber-Latn
    data_files:
      - split: train
        path: gatitos/en_ber-Latn.jsonl
  - config_name: gatitos__en_ber
    data_files:
      - split: train
        path: gatitos/en_ber.jsonl
  - config_name: gatitos__en_tet
    data_files:
      - split: train
        path: gatitos/en_tet.jsonl
  - config_name: gatitos__en_bo
    data_files:
      - split: train
        path: gatitos/en_bo.jsonl
  - config_name: gatitos__en_tpi
    data_files:
      - split: train
        path: gatitos/en_tpi.jsonl
  - config_name: gatitos__en_to
    data_files:
      - split: train
        path: gatitos/en_to.jsonl
  - config_name: gatitos__en_tn
    data_files:
      - split: train
        path: gatitos/en_tn.jsonl
  - config_name: gatitos__en_aeb
    data_files:
      - split: train
        path: gatitos/en_aeb.jsonl
  - config_name: gatitos__en_udm
    data_files:
      - split: train
        path: gatitos/en_udm.jsonl
  - config_name: gatitos__en_ve
    data_files:
      - split: train
        path: gatitos/en_ve.jsonl
  - config_name: gatitos__en_vec
    data_files:
      - split: train
        path: gatitos/en_vec.jsonl
  - config_name: gatitos__en_sah
    data_files:
      - split: train
        path: gatitos/en_sah.jsonl
  - config_name: gatitos__en_yua
    data_files:
      - split: train
        path: gatitos/en_yua.jsonl
  - config_name: gatitos__en_zza
    data_files:
      - split: train
        path: gatitos/en_zza.jsonl
  - config_name: gatitos__en_ace
    data_files:
      - split: train
        path: gatitos/en_ace.jsonl
  - config_name: gatitos__en_ach
    data_files:
      - split: train
        path: gatitos/en_ach.jsonl
  - config_name: gatitos__en_ady
    data_files:
      - split: train
        path: gatitos/en_ady.jsonl
  - config_name: gatitos__en_aa
    data_files:
      - split: train
        path: gatitos/en_aa.jsonl
  - config_name: gatitos__en_awa
    data_files:
      - split: train
        path: gatitos/en_awa.jsonl
  - config_name: gatitos__en_bug
    data_files:
      - split: train
        path: gatitos/en_bug.jsonl
  - config_name: gatitos__en_ce
    data_files:
      - split: train
        path: gatitos/en_ce.jsonl
  - config_name: gatitos__en_cv
    data_files:
      - split: train
        path: gatitos/en_cv.jsonl
  - config_name: gatitos__en_fa-AF
    data_files:
      - split: train
        path: gatitos/en_fa-AF.jsonl
  - config_name: gatitos__en_dyu
    data_files:
      - split: train
        path: gatitos/en_dyu.jsonl
  - config_name: gatitos__yue_zh-Hant
    data_files:
      - split: train
        path: gatitos/yue_zh-Hant.jsonl
  - config_name: gatitos__zh-Hant_yue
    data_files:
      - split: train
        path: gatitos/zh-Hant_yue.jsonl
  - config_name: gatitos__yue_zh
    data_files:
      - split: train
        path: gatitos/yue_zh.jsonl
  - config_name: gatitos__zh_yue
    data_files:
      - split: train
        path: gatitos/zh_yue.jsonl
  - config_name: gatitos__en_yue
    data_files:
      - split: train
        path: gatitos/en_yue.jsonl
  - config_name: gatitos__en_arz
    data_files:
      - split: train
        path: gatitos/en_arz.jsonl
  - config_name: gatitos__en_cnh
    data_files:
      - split: train
        path: gatitos/en_cnh.jsonl
  - config_name: gatitos__en_hil
    data_files:
      - split: train
        path: gatitos/en_hil.jsonl
  - config_name: gatitos__en_iba
    data_files:
      - split: train
        path: gatitos/en_iba.jsonl
  - config_name",High,5.0
Translation,Helsinki-NLP/bible_para,19.0,215.0,2024-01-18 11:01:58+00:00,cc0-1.0,11.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- acu
- af
- agr
- ake
- am
- amu
- ar
- bg
- bsn
- cak
- ceb
- ch
- chq
- chr
- cjp
- cni
- cop
- crp
- cs
- da
- de
- dik
- dje
- djk
- dop
- ee
- el
- en
- eo
- es
- et
- eu
- fi
- fr
- gbi
- gd
- gu
- gv
- he
- hi
- hr
- hu
- hy
- id
- is
- it
- ja
- jak
- jiv
- kab
- kbh
- kek
- kn
- ko
- la
- lt
- lv
- mam
- mi
- ml
- mr
- my
- ne
- nhg
- nl
- 'no'
- ojb
- pck
- pes
- pl
- plt
- pot
- ppk
- pt
- quc
- quw
- ro
- rom
- ru
- shi
- sk
- sl
- sn
- so
- sq
- sr
- ss
- sv
- syr
- te
- th
- tl
- tmh
- tr
- uk
- usp
- vi
- wal
- wo
- xh
- zh
- zu
license:
- cc0-1.0
multilinguality:
- multilingual
size_categories:
- 10K<n<100K
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: null
pretty_name: BiblePara
dataset_info:
- config_name: de-en
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - en
  splits:
  - name: train
    num_bytes: 17262178
    num_examples: 62195
  download_size: 5440713
  dataset_size: 17262178
- config_name: en-fr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fr
  splits:
  - name: train
    num_bytes: 17536445
    num_examples: 62195
  download_size: 5470044
  dataset_size: 17536445
- config_name: en-es
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - es
  splits:
  - name: train
    num_bytes: 17105724
    num_examples: 62191
  download_size: 5418998
  dataset_size: 17105724
- config_name: en-fi
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fi
  splits:
  - name: train
    num_bytes: 17486055
    num_examples: 62026
  download_size: 5506407
  dataset_size: 17486055
- config_name: en-no
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - 'no'
  splits:
  - name: train
    num_bytes: 16681323
    num_examples: 62107
  download_size: 5293164
  dataset_size: 16681323
- config_name: en-hi
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - hi
  splits:
  - name: train
    num_bytes: 27849361
    num_examples: 62073
  download_size: 6224765
  dataset_size: 27849361
---

# Dataset Card for BiblePara

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** http://opus.nlpl.eu/bible-uedin.php
- **Repository:** None
- **Paper:** https://link.springer.com/article/10.1007/s10579-014-9287-y
- **Leaderboard:** [More Information Needed]
- **Point of Contact:** [More Information Needed]

### Dataset Summary

To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.
You can find the valid pairs in Homepage section of Dataset Description: http://opus.nlpl.eu/bible-uedin.php
E.g.

`dataset = load_dataset(""bible_para"", lang1=""fi"", lang2=""hi"")`


### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

[More Information Needed]

## Dataset Structure

### Data Instances

Here are some examples of questions and facts:


### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

[More Information Needed]

### Contributions

Thanks to [@abhishekkrthakur](https://github.com/abhishekkrthakur) for adding this dataset.",High,4.0
Translation,IWSLT/iwslt2017,35.0,6862.0,2023-04-05 10:07:51+00:00,cc-by-nc-nd-4.0,2.0,57.08 MB,unknown,UNKNOWN,unknown,463426,none,"['https://aclanthology.org/2017.iwslt-1.1/)', 'https://aclanthology.org/2017.iwslt-1.1"",']","---
annotations_creators:
- crowdsourced
language:
- ar
- de
- en
- fr
- it
- ja
- ko
- nl
- ro
- zh
language_creators:
- expert-generated
license:
- cc-by-nc-nd-4.0
multilinguality:
- translation
pretty_name: IWSLT 2017
size_categories:
- 1M<n<10M
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: iwslt-2017
dataset_info:
- config_name: iwslt2017-en-it
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - it
  splits:
  - name: train
    num_bytes: 46647925
    num_examples: 231619
  - name: test
    num_bytes: 305246
    num_examples: 1566
  - name: validation
    num_bytes: 200023
    num_examples: 929
  download_size: 329391132
  dataset_size: 47153194
- config_name: iwslt2017-en-nl
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - nl
  splits:
  - name: train
    num_bytes: 42843933
    num_examples: 237240
  - name: test
    num_bytes: 311646
    num_examples: 1777
  - name: validation
    num_bytes: 197814
    num_examples: 1003
  download_size: 329391132
  dataset_size: 43353393
- config_name: iwslt2017-en-ro
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ro
  splits:
  - name: train
    num_bytes: 44129950
    num_examples: 220538
  - name: test
    num_bytes: 316790
    num_examples: 1678
  - name: validation
    num_bytes: 205028
    num_examples: 914
  download_size: 329391132
  dataset_size: 44651768
- config_name: iwslt2017-it-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - it
        - en
  splits:
  - name: train
    num_bytes: 46647925
    num_examples: 231619
  - name: test
    num_bytes: 305246
    num_examples: 1566
  - name: validation
    num_bytes: 200023
    num_examples: 929
  download_size: 329391132
  dataset_size: 47153194
- config_name: iwslt2017-it-nl
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - it
        - nl
  splits:
  - name: train
    num_bytes: 43033168
    num_examples: 233415
  - name: test
    num_bytes: 309725
    num_examples: 1669
  - name: validation
    num_bytes: 197774
    num_examples: 1001
  download_size: 329391132
  dataset_size: 43540667
- config_name: iwslt2017-it-ro
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - it
        - ro
  splits:
  - name: train
    num_bytes: 44485169
    num_examples: 217551
  - name: test
    num_bytes: 314974
    num_examples: 1643
  - name: validation
    num_bytes: 204989
    num_examples: 914
  download_size: 329391132
  dataset_size: 45005132
- config_name: iwslt2017-nl-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - en
  splits:
  - name: train
    num_bytes: 42843933
    num_examples: 237240
  - name: test
    num_bytes: 311646
    num_examples: 1777
  - name: validation
    num_bytes: 197814
    num_examples: 1003
  download_size: 329391132
  dataset_size: 43353393
- config_name: iwslt2017-nl-it
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - it
  splits:
  - name: train
    num_bytes: 43033168
    num_examples: 233415
  - name: test
    num_bytes: 309725
    num_examples: 1669
  - name: validation
    num_bytes: 197774
    num_examples: 1001
  download_size: 329391132
  dataset_size: 43540667
- config_name: iwslt2017-nl-ro
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - ro
  splits:
  - name: train
    num_bytes: 41338738
    num_examples: 206920
  - name: test
    num_bytes: 320952
    num_examples: 1680
  - name: validation
    num_bytes: 202380
    num_examples: 913
  download_size: 329391132
  dataset_size: 41862070
- config_name: iwslt2017-ro-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ro
        - en
  splits:
  - name: train
    num_bytes: 44129950
    num_examples: 220538
  - name: test
    num_bytes: 316790
    num_examples: 1678
  - name: validation
    num_bytes: 205028
    num_examples: 914
  download_size: 329391132
  dataset_size: 44651768
- config_name: iwslt2017-ro-it
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ro
        - it
  splits:
  - name: train
    num_bytes: 44485169
    num_examples: 217551
  - name: test
    num_bytes: 314974
    num_examples: 1643
  - name: validation
    num_bytes: 204989
    num_examples: 914
  download_size: 329391132
  dataset_size: 45005132
- config_name: iwslt2017-ro-nl
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ro
        - nl
  splits:
  - name: train
    num_bytes: 41338738
    num_examples: 206920
  - name: test
    num_bytes: 320952
    num_examples: 1680
  - name: validation
    num_bytes: 202380
    num_examples: 913
  download_size: 329391132
  dataset_size: 41862070
- config_name: iwslt2017-ar-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - en
  splits:
  - name: train
    num_bytes: 56481059
    num_examples: 231713
  - name: test
    num_bytes: 2014296
    num_examples: 8583
  - name: validation
    num_bytes: 241206
    num_examples: 888
  download_size: 27748780
  dataset_size: 58736561
- config_name: iwslt2017-de-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - en
  splits:
  - name: train
    num_bytes: 42608380
    num_examples: 206112
  - name: test
    num_bytes: 1608474
    num_examples: 8079
  - name: validation
    num_bytes: 210975
    num_examples: 888
  download_size: 16758320
  dataset_size: 44427829
- config_name: iwslt2017-en-ar
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ar
  splits:
  - name: train
    num_bytes: 56481059
    num_examples: 231713
  - name: test
    num_bytes: 2014296
    num_examples: 8583
  - name: validation
    num_bytes: 241206
    num_examples: 888
  download_size: 29333173
  dataset_size: 58736561
- config_name: iwslt2017-en-de
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - de
  splits:
  - name: train
    num_bytes: 42608380
    num_examples: 206112
  - name: test
    num_bytes: 1608474
    num_examples: 8079
  - name: validation
    num_bytes: 210975
    num_examples: 888
  download_size: 16758334
  dataset_size: 44427829
- config_name: iwslt2017-en-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fr
  splits:
  - name: train
    num_bytes: 49273286
    num_examples: 232825
  - name: test
    num_bytes: 1767465
    num_examples: 8597
  - name: validation
    num_bytes: 207579
    num_examples: 890
  download_size: 27699724
  dataset_size: 51248330
- config_name: iwslt2017-en-ja
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ja
  splits:
  - name: train
    num_bytes: 48204987
    num_examples: 223108
  - name: test
    num_bytes: 1809007
    num_examples: 8469
  - name: validation
    num_bytes: 208124
    num_examples: 871
  download_size: 26983602
  dataset_size: 50222118
- config_name: iwslt2017-en-ko
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ko
  splits:
  - name: train
    num_bytes: 51678043
    num_examples: 230240
  - name: test
    num_bytes: 1869793
    num_examples: 8514
  - name: validation
    num_bytes: 219295
    num_examples: 879
  download_size: 19364776
  dataset_size: 53767131
- config_name: iwslt2017-en-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - zh
  splits:
  - name: train
    num_bytes: 44271004
    num_examples: 231266
  - name: test
    num_bytes: 1605527
    num_examples: 8549
  - name: validation
    num_bytes: 202537
    num_examples: 879
  download_size: 27597071
  dataset_size: 46079068
- config_name: iwslt2017-fr-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - en
  splits:
  - name: train
    num_bytes: 49273286
    num_examples: 232825
  - name: test
    num_bytes: 1767465
    num_examples: 8597
  - name: validation
    num_bytes: 207579
    num_examples: 890
  download_size: 26880731
  dataset_size: 51248330
- config_name: iwslt2017-ja-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ja
        - en
  splits:
  - name: train
    num_bytes: 48204987
    num_examples: 223108
  - name: test
    num_bytes: 1809007
    num_examples: 8469
  - name: validation
    num_bytes: 208124
    num_examples: 871
  download_size: 26190859
  dataset_size: 50222118
- config_name: iwslt2017-ko-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ko
        - en
  splits:
  - name: train
    num_bytes: 51678043
    num_examples: 230240
  - name: test
    num_bytes: 1869793
    num_examples: 8514
  - name: validation
    num_bytes: 219295
    num_examples: 879
  download_size: 19364733
  dataset_size: 53767131
- config_name: iwslt2017-zh-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - zh
        - en
  splits:
  - name: train
    num_bytes: 44271004
    num_examples: 231266
  - name: test
    num_bytes: 1605527
    num_examples: 8549
  - name: validation
    num_bytes: 202537
    num_examples: 879
  download_size: 26849290
  dataset_size: 46079068
---

# Dataset Card for IWSLT 2017

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [https://sites.google.com/site/iwsltevaluation2017/TED-tasks](https://sites.google.com/site/iwsltevaluation2017/TED-tasks)
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [Overview of the IWSLT 2017 Evaluation Campaign](https://aclanthology.org/2017.iwslt-1.1/)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Size of downloaded dataset files:** 4.24 GB
- **Size of the generated dataset:** 1.14 GB
- **Total amount of disk used:** 5.38 GB

### Dataset Summary

The IWSLT 2017 Multilingual Task addresses text translation, including zero-shot translation, with a single MT system
across all directions including English, German, Dutch, Italian and Romanian. As unofficial task, conventional
bilingual text translation is offered between English and Arabic, French, Japanese, Chinese, German and Korean.

### Supported Tasks and Leaderboards

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Languages

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Dataset Structure

### Data Instances

#### iwslt2017-ar-en

- **Size of downloaded dataset files:** 27.75 MB
- **Size of the generated dataset:** 58.74 MB
- **Total amount of disk used:** 86.49 MB

An example of 'train' looks as follows.
```
This example was too long and was cropped:

{
    ""translation"": ""{\""ar\"": \""لقد طرت في \\\""القوات الجوية \\\"" لمدة ثمان سنوات. والآن أجد نفسي مضطرا لخلع حذائي قبل صعود الطائرة!\"", \""en\"": \""I flew on Air ...""
}
```

#### iwslt2017-de-en

- **Size of downloaded dataset files:** 16.76 MB
- **Size of the generated dataset:** 44.43 MB
- **Total amount of disk used:** 61.18 MB

An example of 'train' looks as follows.
```
{
    ""translation"": {
        ""de"": ""Es ist mir wirklich eine Ehre, zweimal auf dieser Bühne stehen zu dürfen. Tausend Dank dafür."",
        ""en"": ""And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.""
    }
}
```

#### iwslt2017-en-ar

- **Size of downloaded dataset files:** 29.33 MB
- **Size of the generated dataset:** 58.74 MB
- **Total amount of disk used:** 88.07 MB

An example of 'train' looks as follows.
```
This example was too long and was cropped:

{
    ""translation"": ""{\""ar\"": \""لقد طرت في \\\""القوات الجوية \\\"" لمدة ثمان سنوات. والآن أجد نفسي مضطرا لخلع حذائي قبل صعود الطائرة!\"", \""en\"": \""I flew on Air ...""
}
```

#### iwslt2017-en-de

- **Size of downloaded dataset files:** 16.76 MB
- **Size of the generated dataset:** 44.43 MB
- **Total amount of disk used:** 61.18 MB

An example of 'validation' looks as follows.
```
{
    ""translation"": {
        ""de"": ""Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist."",
        ""en"": ""The next slide I show you will be  a rapid fast-forward of what's happened over the last 25 years.""
    }
}
```

#### iwslt2017-en-fr

- **Size of downloaded dataset files:** 27.69 MB
- **Size of the generated dataset:** 51.24 MB
- **Total amount of disk used:** 78.94 MB

An example of 'validation' looks as follows.
```
{
    ""translation"": {
        ""en"": ""But this understates the seriousness of this particular problem  because it doesn't show the thickness of the ice."",
        ""fr"": ""Mais ceci tend à amoindrir le problème parce qu'on ne voit pas l'épaisseur de la glace.""
    }
}
```

### Data Fields

The data fields are the same among all splits.

#### iwslt2017-ar-en
- `translation`: a multilingual `string` variable, with possible languages including `ar`, `en`.

#### iwslt2017-de-en
- `translation`: a multilingual `string` variable, with possible languages including `de`, `en`.

#### iwslt2017-en-ar
- `translation`: a multilingual `string` variable, with possible languages including `en`, `ar`.

#### iwslt2017-en-de
- `translation`: a multilingual `string` variable, with possible languages including `en`, `de`.

#### iwslt2017-en-fr
- `translation`: a multilingual `string` variable, with possible languages including `en`, `fr`.

### Data Splits

|     name      |train |validation|test|
|---------------|-----:|---------:|---:|
|iwslt2017-ar-en|231713|       888|8583|
|iwslt2017-de-en|206112|       888|8079|
|iwslt2017-en-ar|231713|       888|8583|
|iwslt2017-en-de|206112|       888|8079|
|iwslt2017-en-fr|232825|       890|8597|

## Dataset Creation

### Curation Rationale

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the source language producers?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Annotations

#### Annotation process

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

#### Who are the annotators?

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Personal and Sensitive Information

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Discussion of Biases

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Other Known Limitations

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

## Additional Information

### Dataset Curators

[More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Licensing Information

Creative Commons BY-NC-ND

See the (TED Talks Usage Policy)[https://www.ted.com/about/our-organization/our-policies-terms/ted-talks-usage-policy].

### Citation Information

```
@inproceedings{cettolo-etal-2017-overview,
    title = ""Overview of the {IWSLT} 2017 Evaluation Campaign"",
    author = {Cettolo, Mauro  and
      Federico, Marcello  and
      Bentivogli, Luisa  and
      Niehues, Jan  and
      St{\""u}ker, Sebastian  and
      Sudoh, Katsuhito  and
      Yoshino, Koichiro  and
      Federmann, Christian},
    booktitle = ""Proceedings of the 14th International Conference on Spoken Language Translation"",
    month = dec # "" 14-15"",
    year = ""2017"",
    address = ""Tokyo, Japan"",
    publisher = ""International Workshop on Spoken Language Translation"",
    url = ""https://aclanthology.org/2017.iwslt-1.1"",
    pages = ""2--14"",
}
```

### Contributions

Thanks to [@thomwolf](https://github.com/thomwolf), [@Narsil](https://github.com/Narsil) for adding this dataset.",High,6.0
Translation,Helsinki-NLP/kde4,24.0,776.0,2024-01-18 11:07:20+00:00,unknown,3.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- af
- ar
- as
- ast
- be
- bg
- bn
- br
- ca
- crh
- cs
- csb
- cy
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fr
- fy
- ga
- gl
- gu
- ha
- he
- hi
- hne
- hr
- hsb
- hu
- hy
- id
- is
- it
- ja
- ka
- kk
- km
- kn
- ko
- ku
- lb
- lt
- lv
- mai
- mk
- ml
- mr
- ms
- mt
- nb
- nds
- ne
- nl
- nn
- nso
- oc
- or
- pa
- pl
- ps
- pt
- ro
- ru
- rw
- se
- si
- sk
- sl
- sr
- sv
- ta
- te
- tg
- th
- tr
- uk
- uz
- vi
- wa
- xh
- zh
language_bcp47:
- bn-IN
- en-GB
- pt-BR
- zh-CN
- zh-HK
- zh-TW
license:
- unknown
multilinguality:
- multilingual
size_categories:
- 100K<n<1M
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: null
pretty_name: KDE4
dataset_info:
- config_name: fi-nl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - fi
        - nl
  splits:
  - name: train
    num_bytes: 8845933
    num_examples: 101593
  download_size: 2471355
  dataset_size: 8845933
- config_name: it-ro
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - it
        - ro
  splits:
  - name: train
    num_bytes: 8827049
    num_examples: 109003
  download_size: 2389051
  dataset_size: 8827049
- config_name: nl-sv
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - sv
  splits:
  - name: train
    num_bytes: 22294586
    num_examples: 188454
  download_size: 6203460
  dataset_size: 22294586
- config_name: en-it
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - it
  splits:
  - name: train
    num_bytes: 27132585
    num_examples: 220566
  download_size: 7622662
  dataset_size: 27132585
- config_name: en-fr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fr
  splits:
  - name: train
    num_bytes: 25650409
    num_examples: 210173
  download_size: 7049364
  dataset_size: 25650409
---
# Dataset Card for KDE4

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** http://opus.nlpl.eu/KDE4.php
- **Repository:** None
- **Paper:** http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf
- **Leaderboard:** [More Information Needed]
- **Point of Contact:** [More Information Needed]

### Dataset Summary

To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.
You can find the valid pairs in Homepage section of Dataset Description: http://opus.nlpl.eu/KDE4.php
E.g.

`dataset = load_dataset(""kde4"", lang1=""en"", lang2=""nl"")`

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

[More Information Needed]

## Dataset Structure

### Data Instances

[More Information Needed]

### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

[More Information Needed]

### Contributions

Thanks to [@abhishekkrthakur](https://github.com/abhishekkrthakur) for adding this dataset.",High,4.0
Translation,microsoft/ms_terms,5.0,140.0,2024-01-18 11:09:25+00:00,ms-pl,0.0,UNKNOWN,unknown,UNKNOWN,unknown,33738,none,none,"---
annotations_creators:
- expert-generated
language_creators:
- expert-generated
language:
- af
- am
- ar
- as
- az
- be
- bg
- bn
- bs
- ca
- chr
- cs
- cy
- da
- de
- el
- en
- es
- et
- eu
- fa
- fi
- fil
- fr
- ga
- gd
- gl
- gu
- guc
- ha
- he
- hi
- hr
- hu
- hy
- id
- ig
- is
- it
- iu
- ja
- ka
- kk
- km
- kn
- knn
- ko
- ku
- ky
- lb
- lo
- lt
- lv
- mi
- mk
- ml
- mn
- mr
- ms
- mt
- nb
- ne
- nl
- nn
- ory
- pa
- pl
- prs
- pst
- pt
- qu
- quc
- ro
- ru
- rw
- sd
- si
- sk
- sl
- sq
- sr
- st
- sv
- swh
- ta
- te
- tg
- th
- ti
- tk
- tn
- tr
- tt
- ug
- uk
- ur
- uz
- vi
- wo
- xh
- yo
- zh
- zu
language_bcp47:
- bn-IN
- bs-Latn
- es-MX
- fr-CA
- ms-BN
- pt-BR
- sr-BH
- sr-Latn
- zh-Hant-HK
- zh-Hant-TW
license:
- ms-pl
multilinguality:
- multilingual
- translation
size_categories:
- 10K<n<100K
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: null
pretty_name: MsTerms
dataset_info:
  features:
  - name: entry_id
    dtype: string
  - name: term_source
    dtype: string
  - name: pos
    dtype: string
  - name: definition
    dtype: string
  - name: term_target
    dtype: string
  splits:
  - name: train
    num_bytes: 6995497
    num_examples: 33738
  download_size: 0
  dataset_size: 6995497
---

# Dataset Card for [ms_terms]

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:**

[Microsoft Terminology Collection](https://www.microsoft.com/en-us/language/terminology)

- **Repository:**
- **Paper:**
- **Leaderboard:**
- **Point of Contact:**

### Dataset Summary

The Microsoft Terminology Collection can be used to develop localized versions of applications that integrate with Microsoft products. It can also be used to integrate Microsoft terminology into other terminology collections or serve as a base IT glossary for language development in the nearly 100 languages available. Terminology is provided in .tbx format, an industry standard for terminology exchange.

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

Nearly 100 Languages.

## Dataset Structure

### Data Instances

[More Information Needed]

### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

[More Information Needed]

### Contributions

Thanks to [@leoxzhao](https://github.com/leoxzhao), [@lhoestq](https://github.com/lhoestq) for adding this dataset.",High,4.0
Translation,Helsinki-NLP/news_commentary,35.0,3354.0,2024-02-29 15:28:06+00:00,unknown,0.0,1.2 GB,1288490188.8,1.2 GB,1288490188.8,4232524,none,['https://aclanthology.org/L12-1246/'],"---
annotations_creators:
- found
language_creators:
- found
language:
- ar
- cs
- de
- en
- es
- fr
- it
- ja
- nl
- pt
- ru
- zh
license:
- unknown
multilinguality:
- multilingual
size_categories:
- 10K<n<100K
source_datasets:
- original
task_categories:
- translation
task_ids: []
pretty_name: News-Commentary
dataset_info:
- config_name: ar-cs
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - cs
  splits:
  - name: train
    num_bytes: 51546388
    num_examples: 52128
  download_size: 28342257
  dataset_size: 51546388
- config_name: ar-de
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - de
  splits:
  - name: train
    num_bytes: 69681335
    num_examples: 68916
  download_size: 37202855
  dataset_size: 69681335
- config_name: ar-en
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - en
  splits:
  - name: train
    num_bytes: 80655165
    num_examples: 83187
  download_size: 42807620
  dataset_size: 80655165
- config_name: ar-es
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - es
  splits:
  - name: train
    num_bytes: 79255889
    num_examples: 78074
  download_size: 42005622
  dataset_size: 79255889
- config_name: ar-fr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - fr
  splits:
  - name: train
    num_bytes: 71034977
    num_examples: 69157
  download_size: 37543169
  dataset_size: 71034977
- config_name: ar-it
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - it
  splits:
  - name: train
    num_bytes: 17413426
    num_examples: 17227
  download_size: 9186088
  dataset_size: 17413426
- config_name: ar-ja
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - ja
  splits:
  - name: train
    num_bytes: 661980
    num_examples: 569
  download_size: 354690
  dataset_size: 661980
- config_name: ar-nl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - nl
  splits:
  - name: train
    num_bytes: 9054122
    num_examples: 9047
  download_size: 4808380
  dataset_size: 9054122
- config_name: ar-pt
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - pt
  splits:
  - name: train
    num_bytes: 11340050
    num_examples: 11433
  download_size: 6098489
  dataset_size: 11340050
- config_name: ar-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - ru
  splits:
  - name: train
    num_bytes: 105804195
    num_examples: 84455
  download_size: 52467607
  dataset_size: 105804195
- config_name: ar-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - zh
  splits:
  - name: train
    num_bytes: 65483120
    num_examples: 66021
  download_size: 36527030
  dataset_size: 65483120
- config_name: cs-de
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - de
  splits:
  - name: train
    num_bytes: 57470583
    num_examples: 172706
  download_size: 37013107
  dataset_size: 57470583
- config_name: cs-en
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - en
  splits:
  - name: train
    num_bytes: 54487658
    num_examples: 177278
  download_size: 35385370
  dataset_size: 54487658
- config_name: cs-es
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - es
  splits:
  - name: train
    num_bytes: 56794609
    num_examples: 170489
  download_size: 36325813
  dataset_size: 56794609
- config_name: cs-fr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - fr
  splits:
  - name: train
    num_bytes: 50364657
    num_examples: 148578
  download_size: 31970167
  dataset_size: 50364657
- config_name: cs-it
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - it
  splits:
  - name: train
    num_bytes: 10441797
    num_examples: 30547
  download_size: 6651753
  dataset_size: 10441797
- config_name: cs-ja
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - ja
  splits:
  - name: train
    num_bytes: 487890
    num_examples: 622
  download_size: 304917
  dataset_size: 487890
- config_name: cs-nl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - nl
  splits:
  - name: train
    num_bytes: 5860952
    num_examples: 17358
  download_size: 3727739
  dataset_size: 5860952
- config_name: cs-pt
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - pt
  splits:
  - name: train
    num_bytes: 6183701
    num_examples: 18356
  download_size: 3984228
  dataset_size: 6183701
- config_name: cs-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - ru
  splits:
  - name: train
    num_bytes: 71185491
    num_examples: 161133
  download_size: 40217853
  dataset_size: 71185491
- config_name: cs-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - zh
  splits:
  - name: train
    num_bytes: 29971132
    num_examples: 45424
  download_size: 20270691
  dataset_size: 29971132
- config_name: de-en
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - en
  splits:
  - name: train
    num_bytes: 73085175
    num_examples: 223153
  download_size: 45240694
  dataset_size: 73085175
- config_name: de-es
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - es
  splits:
  - name: train
    num_bytes: 74708488
    num_examples: 209839
  download_size: 45574007
  dataset_size: 74708488
- config_name: de-fr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - fr
  splits:
  - name: train
    num_bytes: 67083671
    num_examples: 185442
  download_size: 40685965
  dataset_size: 67083671
- config_name: de-it
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - it
  splits:
  - name: train
    num_bytes: 13993406
    num_examples: 38961
  download_size: 8509324
  dataset_size: 13993406
- config_name: de-ja
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - ja
  splits:
  - name: train
    num_bytes: 465563
    num_examples: 582
  download_size: 281101
  dataset_size: 465563
- config_name: de-nl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - nl
  splits:
  - name: train
    num_bytes: 7645529
    num_examples: 21439
  download_size: 4664824
  dataset_size: 7645529
- config_name: de-pt
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - pt
  splits:
  - name: train
    num_bytes: 7699047
    num_examples: 21884
  download_size: 4755247
  dataset_size: 7699047
- config_name: de-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - ru
  splits:
  - name: train
    num_bytes: 81811798
    num_examples: 175905
  download_size: 44732705
  dataset_size: 81811798
- config_name: de-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - zh
  splits:
  - name: train
    num_bytes: 39044632
    num_examples: 59020
  download_size: 25362199
  dataset_size: 39044632
- config_name: en-es
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - es
  splits:
  - name: train
    num_bytes: 78600501
    num_examples: 238872
  download_size: 48099801
  dataset_size: 78600501
- config_name: en-fr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fr
  splits:
  - name: train
    num_bytes: 70339762
    num_examples: 209479
  download_size: 42791798
  dataset_size: 70339762
- config_name: en-it
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - it
  splits:
  - name: train
    num_bytes: 14213912
    num_examples: 40009
  download_size: 8519809
  dataset_size: 14213912
- config_name: en-ja
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ja
  splits:
  - name: train
    num_bytes: 485472
    num_examples: 637
  download_size: 292084
  dataset_size: 485472
- config_name: en-nl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - nl
  splits:
  - name: train
    num_bytes: 7316575
    num_examples: 19399
  download_size: 4313377
  dataset_size: 7316575
- config_name: en-pt
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - pt
  splits:
  - name: train
    num_bytes: 9238783
    num_examples: 25929
  download_size: 5612678
  dataset_size: 9238783
- config_name: en-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ru
  splits:
  - name: train
    num_bytes: 83282240
    num_examples: 190104
  download_size: 45349681
  dataset_size: 83282240
- config_name: en-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - zh
  splits:
  - name: train
    num_bytes: 44596003
    num_examples: 69206
  download_size: 28997427
  dataset_size: 44596003
- config_name: es-fr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - fr
  splits:
  - name: train
    num_bytes: 71025693
    num_examples: 195241
  download_size: 42650193
  dataset_size: 71025693
- config_name: es-it
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - it
  splits:
  - name: train
    num_bytes: 15139576
    num_examples: 41497
  download_size: 9097532
  dataset_size: 15139576
- config_name: es-ja
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - ja
  splits:
  - name: train
    num_bytes: 484451
    num_examples: 602
  download_size: 289298
  dataset_size: 484451
- config_name: es-nl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - nl
  splits:
  - name: train
    num_bytes: 7560087
    num_examples: 21012
  download_size: 4572049
  dataset_size: 7560087
- config_name: es-pt
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - pt
  splits:
  - name: train
    num_bytes: 9195649
    num_examples: 25551
  download_size: 5633226
  dataset_size: 9195649
- config_name: es-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - ru
  splits:
  - name: train
    num_bytes: 84345622
    num_examples: 180217
  download_size: 45710609
  dataset_size: 84345622
- config_name: es-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - zh
  splits:
  - name: train
    num_bytes: 43939929
    num_examples: 65424
  download_size: 28264415
  dataset_size: 43939929
- config_name: fr-it
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - it
  splits:
  - name: train
    num_bytes: 14216031
    num_examples: 38485
  download_size: 8499047
  dataset_size: 14216031
- config_name: fr-ja
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - ja
  splits:
  - name: train
    num_bytes: 418176
    num_examples: 519
  download_size: 251240
  dataset_size: 418176
- config_name: fr-nl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - nl
  splits:
  - name: train
    num_bytes: 7603467
    num_examples: 20898
  download_size: 4553502
  dataset_size: 7603467
- config_name: fr-pt
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - pt
  splits:
  - name: train
    num_bytes: 9261133
    num_examples: 25642
  download_size: 5614816
  dataset_size: 9261133
- config_name: fr-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - ru
  splits:
  - name: train
    num_bytes: 75967049
    num_examples: 160740
  download_size: 41078195
  dataset_size: 75967049
- config_name: fr-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - zh
  splits:
  - name: train
    num_bytes: 40143999
    num_examples: 59060
  download_size: 25753128
  dataset_size: 40143999
- config_name: it-nl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - it
        - nl
  splits:
  - name: train
    num_bytes: 5380888
    num_examples: 15428
  download_size: 3279009
  dataset_size: 5380888
- config_name: it-pt
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - it
        - pt
  splits:
  - name: train
    num_bytes: 3988546
    num_examples: 11407
  download_size: 2432377
  dataset_size: 3988546
- config_name: it-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - it
        - ru
  splits:
  - name: train
    num_bytes: 12915037
    num_examples: 27267
  download_size: 7009784
  dataset_size: 12915037
- config_name: it-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - it
        - zh
  splits:
  - name: train
    num_bytes: 9676732
    num_examples: 14652
  download_size: 6219158
  dataset_size: 9676732
- config_name: ja-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ja
        - ru
  splits:
  - name: train
    num_bytes: 596154
    num_examples: 586
  download_size: 324916
  dataset_size: 596154
- config_name: ja-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ja
        - zh
  splits:
  - name: train
    num_bytes: 462673
    num_examples: 570
  download_size: 290801
  dataset_size: 462673
- config_name: nl-pt
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - pt
  splits:
  - name: train
    num_bytes: 3612315
    num_examples: 10598
  download_size: 2204974
  dataset_size: 3612315
- config_name: nl-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - ru
  splits:
  - name: train
    num_bytes: 8933781
    num_examples: 19112
  download_size: 4857132
  dataset_size: 8933781
- config_name: nl-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - zh
  splits:
  - name: train
    num_bytes: 5509058
    num_examples: 8433
  download_size: 3573395
  dataset_size: 5509058
- config_name: pt-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - pt
        - ru
  splits:
  - name: train
    num_bytes: 8645451
    num_examples: 18458
  download_size: 4739066
  dataset_size: 8645451
- config_name: pt-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - pt
        - zh
  splits:
  - name: train
    num_bytes: 7152750
    num_examples: 10873
  download_size: 4668616
  dataset_size: 7152750
- config_name: ru-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ru
        - zh
  splits:
  - name: train
    num_bytes: 43112764
    num_examples: 47687
  download_size: 24587160
  dataset_size: 43112764
configs:
- config_name: ar-cs
  data_files:
  - split: train
    path: ar-cs/train-*
- config_name: ar-de
  data_files:
  - split: train
    path: ar-de/train-*
- config_name: ar-en
  data_files:
  - split: train
    path: ar-en/train-*
- config_name: ar-es
  data_files:
  - split: train
    path: ar-es/train-*
- config_name: ar-fr
  data_files:
  - split: train
    path: ar-fr/train-*
- config_name: ar-it
  data_files:
  - split: train
    path: ar-it/train-*
- config_name: ar-ja
  data_files:
  - split: train
    path: ar-ja/train-*
- config_name: ar-nl
  data_files:
  - split: train
    path: ar-nl/train-*
- config_name: ar-pt
  data_files:
  - split: train
    path: ar-pt/train-*
- config_name: ar-ru
  data_files:
  - split: train
    path: ar-ru/train-*
- config_name: ar-zh
  data_files:
  - split: train
    path: ar-zh/train-*
- config_name: cs-de
  data_files:
  - split: train
    path: cs-de/train-*
- config_name: cs-en
  data_files:
  - split: train
    path: cs-en/train-*
- config_name: cs-es
  data_files:
  - split: train
    path: cs-es/train-*
- config_name: cs-fr
  data_files:
  - split: train
    path: cs-fr/train-*
- config_name: cs-it
  data_files:
  - split: train
    path: cs-it/train-*
- config_name: cs-ja
  data_files:
  - split: train
    path: cs-ja/train-*
- config_name: cs-nl
  data_files:
  - split: train
    path: cs-nl/train-*
- config_name: cs-pt
  data_files:
  - split: train
    path: cs-pt/train-*
- config_name: cs-ru
  data_files:
  - split: train
    path: cs-ru/train-*
- config_name: cs-zh
  data_files:
  - split: train
    path: cs-zh/train-*
- config_name: de-en
  data_files:
  - split: train
    path: de-en/train-*
- config_name: de-es
  data_files:
  - split: train
    path: de-es/train-*
- config_name: de-fr
  data_files:
  - split: train
    path: de-fr/train-*
- config_name: de-it
  data_files:
  - split: train
    path: de-it/train-*
- config_name: de-ja
  data_files:
  - split: train
    path: de-ja/train-*
- config_name: de-nl
  data_files:
  - split: train
    path: de-nl/train-*
- config_name: de-pt
  data_files:
  - split: train
    path: de-pt/train-*
- config_name: de-ru
  data_files:
  - split: train
    path: de-ru/train-*
- config_name: de-zh
  data_files:
  - split: train
    path: de-zh/train-*
- config_name: en-es
  data_files:
  - split: train
    path: en-es/train-*
- config_name: en-fr
  data_files:
  - split: train
    path: en-fr/train-*
- config_name: en-it
  data_files:
  - split: train
    path: en-it/train-*
- config_name: en-ja
  data_files:
  - split: train
    path: en-ja/train-*
- config_name: en-nl
  data_files:
  - split: train
    path: en-nl/train-*
- config_name: en-pt
  data_files:
  - split: train
    path: en-pt/train-*
- config_name: en-ru
  data_files:
  - split: train
    path: en-ru/train-*
- config_name: en-zh
  data_files:
  - split: train
    path: en-zh/train-*
- config_name: es-fr
  data_files:
  - split: train
    path: es-fr/train-*
- config_name: es-it
  data_files:
  - split: train
    path: es-it/train-*
- config_name: es-ja
  data_files:
  - split: train
    path: es-ja/train-*
- config_name: es-nl
  data_files:
  - split: train
    path: es-nl/train-*
- config_name: es-pt
  data_files:
  - split: train
    path: es-pt/train-*
- config_name: es-ru
  data_files:
  - split: train
    path: es-ru/train-*
- config_name: es-zh
  data_files:
  - split: train
    path: es-zh/train-*
- config_name: fr-it
  data_files:
  - split: train
    path: fr-it/train-*
- config_name: fr-ja
  data_files:
  - split: train
    path: fr-ja/train-*
- config_name: fr-nl
  data_files:
  - split: train
    path: fr-nl/train-*
- config_name: fr-pt
  data_files:
  - split: train
    path: fr-pt/train-*
- config_name: fr-ru
  data_files:
  - split: train
    path: fr-ru/train-*
- config_name: fr-zh
  data_files:
  - split: train
    path: fr-zh/train-*
- config_name: it-nl
  data_files:
  - split: train
    path: it-nl/train-*
- config_name: it-pt
  data_files:
  - split: train
    path: it-pt/train-*
- config_name: it-ru
  data_files:
  - split: train
    path: it-ru/train-*
- config_name: it-zh
  data_files:
  - split: train
    path: it-zh/train-*
- config_name: ja-ru
  data_files:
  - split: train
    path: ja-ru/train-*
- config_name: ja-zh
  data_files:
  - split: train
    path: ja-zh/train-*
- config_name: nl-pt
  data_files:
  - split: train
    path: nl-pt/train-*
- config_name: nl-ru
  data_files:
  - split: train
    path: nl-ru/train-*
- config_name: nl-zh
  data_files:
  - split: train
    path: nl-zh/train-*
- config_name: pt-ru
  data_files:
  - split: train
    path: pt-ru/train-*
- config_name: pt-zh
  data_files:
  - split: train
    path: pt-zh/train-*
- config_name: ru-zh
  data_files:
  - split: train
    path: ru-zh/train-*
---

# Dataset Card for OPUS News-Commentary

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://opus.nlpl.eu/News-Commentary/corpus/version/News-Commentary
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** https://aclanthology.org/L12-1246/
- **Leaderboard:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Dataset Summary

[More Information Needed]

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

[More Information Needed]

## Dataset Structure

### Data Instances

[More Information Needed]

### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

Please cite the following article if you use any part of the OPUS corpus in your own work:
```bibtex
@inproceedings{tiedemann-2012-parallel,
    title = ""Parallel Data, Tools and Interfaces in {OPUS}"",
    author = {Tiedemann, J{\""o}rg},
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf"",
    pages = ""2214--2218"",
}
```

### Contributions

Thanks to [@abhishekkrthakur](https://github.com/abhishekkrthakur) for adding this dataset.",High,5.0
Translation,Helsinki-NLP/open_subtitles,68.0,1008.0,2024-01-18 11:11:17+00:00,unknown,1.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- af
- ar
- bg
- bn
- br
- bs
- ca
- cs
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fr
- gl
- he
- hi
- hr
- hu
- hy
- id
- is
- it
- ja
- ka
- kk
- ko
- lt
- lv
- mk
- ml
- ms
- nl
- 'no'
- pl
- pt
- ro
- ru
- si
- sk
- sl
- sq
- sr
- sv
- ta
- te
- th
- tl
- tr
- uk
- ur
- vi
- zh
language_bcp47:
- pt-BR
- ze-EN
- ze-ZH
- zh-CN
- zh-TW
license:
- unknown
multilinguality:
- multilingual
size_categories:
- 10K<n<100K
- 1M<n<10M
- n<1K
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: opensubtitles
pretty_name: OpenSubtitles
dataset_info:
- config_name: bs-eo
  features:
  - name: id
    dtype: string
  - name: meta
    struct:
    - name: year
      dtype: uint32
    - name: imdbId
      dtype: uint32
    - name: subtitleId
      struct:
      - name: bs
        dtype: uint32
      - name: eo
        dtype: uint32
    - name: sentenceIds
      struct:
      - name: bs
        sequence: uint32
      - name: eo
        sequence: uint32
  - name: translation
    dtype:
      translation:
        languages:
        - bs
        - eo
  splits:
  - name: train
    num_bytes: 1204266
    num_examples: 10989
  download_size: 333050
  dataset_size: 1204266
- config_name: fr-hy
  features:
  - name: id
    dtype: string
  - name: meta
    struct:
    - name: year
      dtype: uint32
    - name: imdbId
      dtype: uint32
    - name: subtitleId
      struct:
      - name: fr
        dtype: uint32
      - name: hy
        dtype: uint32
    - name: sentenceIds
      struct:
      - name: fr
        sequence: uint32
      - name: hy
        sequence: uint32
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - hy
  splits:
  - name: train
    num_bytes: 132450
    num_examples: 668
  download_size: 41861
  dataset_size: 132450
- config_name: da-ru
  features:
  - name: id
    dtype: string
  - name: meta
    struct:
    - name: year
      dtype: uint32
    - name: imdbId
      dtype: uint32
    - name: subtitleId
      struct:
      - name: da
        dtype: uint32
      - name: ru
        dtype: uint32
    - name: sentenceIds
      struct:
      - name: da
        sequence: uint32
      - name: ru
        sequence: uint32
  - name: translation
    dtype:
      translation:
        languages:
        - da
        - ru
  splits:
  - name: train
    num_bytes: 1082649105
    num_examples: 7543012
  download_size: 267995167
  dataset_size: 1082649105
- config_name: en-hi
  features:
  - name: id
    dtype: string
  - name: meta
    struct:
    - name: year
      dtype: uint32
    - name: imdbId
      dtype: uint32
    - name: subtitleId
      struct:
      - name: en
        dtype: uint32
      - name: hi
        dtype: uint32
    - name: sentenceIds
      struct:
      - name: en
        sequence: uint32
      - name: hi
        sequence: uint32
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - hi
  splits:
  - name: train
    num_bytes: 13845544
    num_examples: 93016
  download_size: 2967295
  dataset_size: 13845544
- config_name: bn-is
  features:
  - name: id
    dtype: string
  - name: meta
    struct:
    - name: year
      dtype: uint32
    - name: imdbId
      dtype: uint32
    - name: subtitleId
      struct:
      - name: bn
        dtype: uint32
      - name: is
        dtype: uint32
    - name: sentenceIds
      struct:
      - name: bn
        sequence: uint32
      - name: is
        sequence: uint32
  - name: translation
    dtype:
      translation:
        languages:
        - bn
        - is
  splits:
  - name: train
    num_bytes: 6371251
    num_examples: 38272
  download_size: 1411625
  dataset_size: 6371251
config_names:
- bn-is
- bs-eo
- da-ru
- en-hi
- fr-hy
---

# Dataset Card for OpenSubtitles

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** http://opus.nlpl.eu/OpenSubtitles.php
- **Repository:** None
- **Paper:** http://www.lrec-conf.org/proceedings/lrec2016/pdf/62_Paper.pdf
- **Leaderboard:** [More Information Needed]
- **Point of Contact:** [More Information Needed]

### Dataset Summary

To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.
You can find the valid pairs in Homepage section of Dataset Description: http://opus.nlpl.eu/OpenSubtitles.php
E.g.

`dataset = load_dataset(""open_subtitles"", lang1=""fi"", lang2=""hi"")`


### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

The languages in the dataset are:
- af
- ar
- bg
- bn
- br
- bs
- ca
- cs
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fr
- gl
- he
- hi
- hr
- hu
- hy
- id
- is
- it
- ja
- ka
- kk
- ko
- lt
- lv
- mk
- ml
- ms
- nl
- no
- pl
- pt
- pt_br: Portuguese (Brazil) (pt-BR)
- ro
- ru
- si
- sk
- sl
- sq
- sr
- sv
- ta
- te
- th
- tl
- tr
- uk
- ur
- vi
- ze_en: English constituent of Bilingual Chinese-English (subtitles displaying two languages at once, one per line)
- ze_zh: Chinese constituent of Bilingual Chinese-English (subtitles displaying two languages at once, one per line)
- zh_cn: Simplified Chinese (zh-CN, `zh-Hans`)
- zh_tw: Traditional Chinese (zh-TW, `zh-Hant`)

## Dataset Structure

### Data Instances

Here are some examples of questions and facts:


### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

[More Information Needed]

### Contributions

Thanks to [@abhishekkrthakur](https://github.com/abhishekkrthakur) for adding this dataset.",High,4.0
Translation,Helsinki-NLP/opus-100,197.0,11191.0,2024-02-28 09:17:34+00:00,unknown,61.0,4.77 GB,5121748500.48,4.77 GB,5121748500.48,55057504,https://arxiv.org/abs/2004.11867,"['https://aclanthology.org/L10-1473/', 'https://aclanthology.org/2020.acl-main.148"",']","---
annotations_creators:
- no-annotation
language_creators:
- found
language:
- af
- am
- an
- ar
- as
- az
- be
- bg
- bn
- br
- bs
- ca
- cs
- cy
- da
- de
- dz
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fr
- fy
- ga
- gd
- gl
- gu
- ha
- he
- hi
- hr
- hu
- hy
- id
- ig
- is
- it
- ja
- ka
- kk
- km
- kn
- ko
- ku
- ky
- li
- lt
- lv
- mg
- mk
- ml
- mn
- mr
- ms
- mt
- my
- nb
- ne
- nl
- nn
- 'no'
- oc
- or
- pa
- pl
- ps
- pt
- ro
- ru
- rw
- se
- sh
- si
- sk
- sl
- sq
- sr
- sv
- ta
- te
- tg
- th
- tk
- tr
- tt
- ug
- uk
- ur
- uz
- vi
- wa
- xh
- yi
- yo
- zh
- zu
license:
- unknown
multilinguality:
- translation
size_categories:
- 100K<n<1M
- 10K<n<100K
- 1K<n<10K
- 1M<n<10M
- n<1K
source_datasets:
- extended
task_categories:
- translation
task_ids: []
paperswithcode_id: opus-100
pretty_name: OPUS-100
config_names:
- af-en
- am-en
- an-en
- ar-de
- ar-en
- ar-fr
- ar-nl
- ar-ru
- ar-zh
- as-en
- az-en
- be-en
- bg-en
- bn-en
- br-en
- bs-en
- ca-en
- cs-en
- cy-en
- da-en
- de-en
- de-fr
- de-nl
- de-ru
- de-zh
- dz-en
- el-en
- en-eo
- en-es
- en-et
- en-eu
- en-fa
- en-fi
- en-fr
- en-fy
- en-ga
- en-gd
- en-gl
- en-gu
- en-ha
- en-he
- en-hi
- en-hr
- en-hu
- en-hy
- en-id
- en-ig
- en-is
- en-it
- en-ja
- en-ka
- en-kk
- en-km
- en-kn
- en-ko
- en-ku
- en-ky
- en-li
- en-lt
- en-lv
- en-mg
- en-mk
- en-ml
- en-mn
- en-mr
- en-ms
- en-mt
- en-my
- en-nb
- en-ne
- en-nl
- en-nn
- en-no
- en-oc
- en-or
- en-pa
- en-pl
- en-ps
- en-pt
- en-ro
- en-ru
- en-rw
- en-se
- en-sh
- en-si
- en-sk
- en-sl
- en-sq
- en-sr
- en-sv
- en-ta
- en-te
- en-tg
- en-th
- en-tk
- en-tr
- en-tt
- en-ug
- en-uk
- en-ur
- en-uz
- en-vi
- en-wa
- en-xh
- en-yi
- en-yo
- en-zh
- en-zu
- fr-nl
- fr-ru
- fr-zh
- nl-ru
- nl-zh
- ru-zh
dataset_info:
- config_name: af-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - af
        - en
  splits:
  - name: test
    num_bytes: 135908
    num_examples: 2000
  - name: train
    num_bytes: 18726247
    num_examples: 275512
  - name: validation
    num_bytes: 132769
    num_examples: 2000
  download_size: 14852797
  dataset_size: 18994924
- config_name: am-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - am
        - en
  splits:
  - name: test
    num_bytes: 588021
    num_examples: 2000
  - name: train
    num_bytes: 21950572
    num_examples: 89027
  - name: validation
    num_bytes: 566069
    num_examples: 2000
  download_size: 12630031
  dataset_size: 23104662
- config_name: an-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - an
        - en
  splits:
  - name: train
    num_bytes: 438324
    num_examples: 6961
  download_size: 232976
  dataset_size: 438324
- config_name: ar-de
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - de
  splits:
  - name: test
    num_bytes: 238591
    num_examples: 2000
  download_size: 161557
  dataset_size: 238591
- config_name: ar-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - en
  splits:
  - name: test
    num_bytes: 331640
    num_examples: 2000
  - name: train
    num_bytes: 152765684
    num_examples: 1000000
  - name: validation
    num_bytes: 2272098
    num_examples: 2000
  download_size: 100486814
  dataset_size: 155369422
- config_name: ar-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - fr
  splits:
  - name: test
    num_bytes: 547374
    num_examples: 2000
  download_size: 334226
  dataset_size: 547374
- config_name: ar-nl
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - nl
  splits:
  - name: test
    num_bytes: 212928
    num_examples: 2000
  download_size: 144863
  dataset_size: 212928
- config_name: ar-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - ru
  splits:
  - name: test
    num_bytes: 808262
    num_examples: 2000
  download_size: 441536
  dataset_size: 808262
- config_name: ar-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - zh
  splits:
  - name: test
    num_bytes: 713404
    num_examples: 2000
  download_size: 438598
  dataset_size: 713404
- config_name: as-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - as
        - en
  splits:
  - name: test
    num_bytes: 261458
    num_examples: 2000
  - name: train
    num_bytes: 15634536
    num_examples: 138479
  - name: validation
    num_bytes: 248131
    num_examples: 2000
  download_size: 8794616
  dataset_size: 16144125
- config_name: az-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - az
        - en
  splits:
  - name: test
    num_bytes: 393101
    num_examples: 2000
  - name: train
    num_bytes: 56431043
    num_examples: 262089
  - name: validation
    num_bytes: 407101
    num_examples: 2000
  download_size: 34988859
  dataset_size: 57231245
- config_name: be-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - be
        - en
  splits:
  - name: test
    num_bytes: 166850
    num_examples: 2000
  - name: train
    num_bytes: 5298444
    num_examples: 67312
  - name: validation
    num_bytes: 175197
    num_examples: 2000
  download_size: 3807669
  dataset_size: 5640491
- config_name: bg-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - bg
        - en
  splits:
  - name: test
    num_bytes: 243743
    num_examples: 2000
  - name: train
    num_bytes: 108929547
    num_examples: 1000000
  - name: validation
    num_bytes: 234840
    num_examples: 2000
  download_size: 71575310
  dataset_size: 109408130
- config_name: bn-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - bn
        - en
  splits:
  - name: test
    num_bytes: 510093
    num_examples: 2000
  - name: train
    num_bytes: 249906046
    num_examples: 1000000
  - name: validation
    num_bytes: 498406
    num_examples: 2000
  download_size: 134076596
  dataset_size: 250914545
- config_name: br-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - br
        - en
  splits:
  - name: test
    num_bytes: 127917
    num_examples: 2000
  - name: train
    num_bytes: 8538878
    num_examples: 153447
  - name: validation
    num_bytes: 133764
    num_examples: 2000
  download_size: 6881865
  dataset_size: 8800559
- config_name: bs-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - bs
        - en
  splits:
  - name: test
    num_bytes: 168614
    num_examples: 2000
  - name: train
    num_bytes: 75082148
    num_examples: 1000000
  - name: validation
    num_bytes: 172473
    num_examples: 2000
  download_size: 59514403
  dataset_size: 75423235
- config_name: ca-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ca
        - en
  splits:
  - name: test
    num_bytes: 205658
    num_examples: 2000
  - name: train
    num_bytes: 88404710
    num_examples: 1000000
  - name: validation
    num_bytes: 212629
    num_examples: 2000
  download_size: 68438385
  dataset_size: 88822997
- config_name: cs-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - en
  splits:
  - name: test
    num_bytes: 205266
    num_examples: 2000
  - name: train
    num_bytes: 91896919
    num_examples: 1000000
  - name: validation
    num_bytes: 219076
    num_examples: 2000
  download_size: 73028514
  dataset_size: 92321261
- config_name: cy-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - cy
        - en
  splits:
  - name: test
    num_bytes: 124281
    num_examples: 2000
  - name: train
    num_bytes: 17244748
    num_examples: 289521
  - name: validation
    num_bytes: 118848
    num_examples: 2000
  download_size: 13398765
  dataset_size: 17487877
- config_name: da-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - da
        - en
  splits:
  - name: test
    num_bytes: 298115
    num_examples: 2000
  - name: train
    num_bytes: 126424474
    num_examples: 1000000
  - name: validation
    num_bytes: 300616
    num_examples: 2000
  download_size: 91005252
  dataset_size: 127023205
- config_name: de-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - en
  splits:
  - name: test
    num_bytes: 330951
    num_examples: 2000
  - name: train
    num_bytes: 152245956
    num_examples: 1000000
  - name: validation
    num_bytes: 332342
    num_examples: 2000
  download_size: 116680890
  dataset_size: 152909249
- config_name: de-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - fr
  splits:
  - name: test
    num_bytes: 458738
    num_examples: 2000
  download_size: 311929
  dataset_size: 458738
- config_name: de-nl
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - nl
  splits:
  - name: test
    num_bytes: 403878
    num_examples: 2000
  download_size: 281548
  dataset_size: 403878
- config_name: de-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - ru
  splits:
  - name: test
    num_bytes: 315771
    num_examples: 2000
  download_size: 203225
  dataset_size: 315771
- config_name: de-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - zh
  splits:
  - name: test
    num_bytes: 280389
    num_examples: 2000
  download_size: 215301
  dataset_size: 280389
- config_name: dz-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - dz
        - en
  splits:
  - name: train
    num_bytes: 81154
    num_examples: 624
  download_size: 37361
  dataset_size: 81154
- config_name: el-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - el
        - en
  splits:
  - name: test
    num_bytes: 302385
    num_examples: 2000
  - name: train
    num_bytes: 127963903
    num_examples: 1000000
  - name: validation
    num_bytes: 291226
    num_examples: 2000
  download_size: 84137722
  dataset_size: 128557514
- config_name: en-eo
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - eo
  splits:
  - name: test
    num_bytes: 167378
    num_examples: 2000
  - name: train
    num_bytes: 24431681
    num_examples: 337106
  - name: validation
    num_bytes: 168830
    num_examples: 2000
  download_size: 19545461
  dataset_size: 24767889
- config_name: en-es
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - es
  splits:
  - name: test
    num_bytes: 326262
    num_examples: 2000
  - name: train
    num_bytes: 136643104
    num_examples: 1000000
  - name: validation
    num_bytes: 326727
    num_examples: 2000
  download_size: 100103907
  dataset_size: 137296093
- config_name: en-et
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - et
  splits:
  - name: test
    num_bytes: 272163
    num_examples: 2000
  - name: train
    num_bytes: 112298253
    num_examples: 1000000
  - name: validation
    num_bytes: 276954
    num_examples: 2000
  download_size: 83690450
  dataset_size: 112847370
- config_name: en-eu
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - eu
  splits:
  - name: test
    num_bytes: 280877
    num_examples: 2000
  - name: train
    num_bytes: 112329285
    num_examples: 1000000
  - name: validation
    num_bytes: 281495
    num_examples: 2000
  download_size: 84805467
  dataset_size: 112891657
- config_name: en-fa
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fa
  splits:
  - name: test
    num_bytes: 296548
    num_examples: 2000
  - name: train
    num_bytes: 125400535
    num_examples: 1000000
  - name: validation
    num_bytes: 291121
    num_examples: 2000
  download_size: 82783248
  dataset_size: 125988204
- config_name: en-fi
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fi
  splits:
  - name: test
    num_bytes: 245814
    num_examples: 2000
  - name: train
    num_bytes: 106024990
    num_examples: 1000000
  - name: validation
    num_bytes: 247219
    num_examples: 2000
  download_size: 79320220
  dataset_size: 106518023
- config_name: en-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fr
  splits:
  - name: test
    num_bytes: 469723
    num_examples: 2000
  - name: train
    num_bytes: 201440450
    num_examples: 1000000
  - name: validation
    num_bytes: 481476
    num_examples: 2000
  download_size: 142251860
  dataset_size: 202391649
- config_name: en-fy
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fy
  splits:
  - name: test
    num_bytes: 101238
    num_examples: 2000
  - name: train
    num_bytes: 3895640
    num_examples: 54342
  - name: validation
    num_bytes: 100121
    num_examples: 2000
  download_size: 2984283
  dataset_size: 4096999
- config_name: en-ga
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ga
  splits:
  - name: test
    num_bytes: 503309
    num_examples: 2000
  - name: train
    num_bytes: 42132510
    num_examples: 289524
  - name: validation
    num_bytes: 503209
    num_examples: 2000
  download_size: 27937448
  dataset_size: 43139028
- config_name: en-gd
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - gd
  splits:
  - name: test
    num_bytes: 218354
    num_examples: 1606
  - name: train
    num_bytes: 1254779
    num_examples: 16316
  - name: validation
    num_bytes: 203877
    num_examples: 1605
  download_size: 1124506
  dataset_size: 1677010
- config_name: en-gl
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - gl
  splits:
  - name: test
    num_bytes: 190691
    num_examples: 2000
  - name: train
    num_bytes: 43327028
    num_examples: 515344
  - name: validation
    num_bytes: 193598
    num_examples: 2000
  download_size: 34084028
  dataset_size: 43711317
- config_name: en-gu
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - gu
  splits:
  - name: test
    num_bytes: 199725
    num_examples: 2000
  - name: train
    num_bytes: 33641719
    num_examples: 318306
  - name: validation
    num_bytes: 205542
    num_examples: 2000
  download_size: 19235779
  dataset_size: 34046986
- config_name: en-ha
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ha
  splits:
  - name: test
    num_bytes: 407344
    num_examples: 2000
  - name: train
    num_bytes: 20391884
    num_examples: 97983
  - name: validation
    num_bytes: 411518
    num_examples: 2000
  download_size: 12686187
  dataset_size: 21210746
- config_name: en-he
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - he
  splits:
  - name: test
    num_bytes: 208467
    num_examples: 2000
  - name: train
    num_bytes: 91159631
    num_examples: 1000000
  - name: validation
    num_bytes: 209438
    num_examples: 2000
  download_size: 61144758
  dataset_size: 91577536
- config_name: en-hi
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - hi
  splits:
  - name: test
    num_bytes: 496570
    num_examples: 2000
  - name: train
    num_bytes: 124923545
    num_examples: 534319
  - name: validation
    num_bytes: 474079
    num_examples: 2000
  download_size: 65725886
  dataset_size: 125894194
- config_name: en-hr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - hr
  splits:
  - name: test
    num_bytes: 179636
    num_examples: 2000
  - name: train
    num_bytes: 75309516
    num_examples: 1000000
  - name: validation
    num_bytes: 179615
    num_examples: 2000
  download_size: 59468892
  dataset_size: 75668767
- config_name: en-hu
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - hu
  splits:
  - name: test
    num_bytes: 206039
    num_examples: 2000
  - name: train
    num_bytes: 87483462
    num_examples: 1000000
  - name: validation
    num_bytes: 208307
    num_examples: 2000
  download_size: 67971116
  dataset_size: 87897808
- config_name: en-hy
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - hy
  splits:
  - name: train
    num_bytes: 652623
    num_examples: 7059
  download_size: 422847
  dataset_size: 652623
- config_name: en-id
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - id
  splits:
  - name: test
    num_bytes: 177685
    num_examples: 2000
  - name: train
    num_bytes: 78698973
    num_examples: 1000000
  - name: validation
    num_bytes: 180024
    num_examples: 2000
  download_size: 57693678
  dataset_size: 79056682
- config_name: en-ig
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ig
  splits:
  - name: test
    num_bytes: 137324
    num_examples: 1843
  - name: train
    num_bytes: 1612523
    num_examples: 18415
  - name: validation
    num_bytes: 135987
    num_examples: 1843
  download_size: 859440
  dataset_size: 1885834
- config_name: en-is
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - is
  splits:
  - name: test
    num_bytes: 170879
    num_examples: 2000
  - name: train
    num_bytes: 73964115
    num_examples: 1000000
  - name: validation
    num_bytes: 170632
    num_examples: 2000
  download_size: 56242149
  dataset_size: 74305626
- config_name: en-it
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - it
  splits:
  - name: test
    num_bytes: 299029
    num_examples: 2000
  - name: train
    num_bytes: 123654286
    num_examples: 1000000
  - name: validation
    num_bytes: 294354
    num_examples: 2000
  download_size: 92133897
  dataset_size: 124247669
- config_name: en-ja
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ja
  splits:
  - name: test
    num_bytes: 190991
    num_examples: 2000
  - name: train
    num_bytes: 88348569
    num_examples: 1000000
  - name: validation
    num_bytes: 191411
    num_examples: 2000
  download_size: 64817108
  dataset_size: 88730971
- config_name: en-ka
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ka
  splits:
  - name: test
    num_bytes: 256219
    num_examples: 2000
  - name: train
    num_bytes: 42465402
    num_examples: 377306
  - name: validation
    num_bytes: 260408
    num_examples: 2000
  download_size: 24394633
  dataset_size: 42982029
- config_name: en-kk
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - kk
  splits:
  - name: test
    num_bytes: 137656
    num_examples: 2000
  - name: train
    num_bytes: 7124314
    num_examples: 79927
  - name: validation
    num_bytes: 139657
    num_examples: 2000
  download_size: 4808360
  dataset_size: 7401627
- config_name: en-km
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - km
  splits:
  - name: test
    num_bytes: 289019
    num_examples: 2000
  - name: train
    num_bytes: 19680515
    num_examples: 111483
  - name: validation
    num_bytes: 302519
    num_examples: 2000
  download_size: 10022919
  dataset_size: 20272053
- config_name: en-kn
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - kn
  splits:
  - name: test
    num_bytes: 77197
    num_examples: 918
  - name: train
    num_bytes: 1833318
    num_examples: 14537
  - name: validation
    num_bytes: 77599
    num_examples: 917
  download_size: 1062554
  dataset_size: 1988114
- config_name: en-ko
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ko
  splits:
  - name: test
    num_bytes: 190688
    num_examples: 2000
  - name: train
    num_bytes: 93664532
    num_examples: 1000000
  - name: validation
    num_bytes: 189360
    num_examples: 2000
  download_size: 70383271
  dataset_size: 94044580
- config_name: en-ku
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ku
  splits:
  - name: test
    num_bytes: 247839
    num_examples: 2000
  - name: train
    num_bytes: 49107744
    num_examples: 144844
  - name: validation
    num_bytes: 239317
    num_examples: 2000
  download_size: 25358389
  dataset_size: 49594900
- config_name: en-ky
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ky
  splits:
  - name: test
    num_bytes: 142522
    num_examples: 2000
  - name: train
    num_bytes: 1879274
    num_examples: 27215
  - name: validation
    num_bytes: 138479
    num_examples: 2000
  download_size: 1338686
  dataset_size: 2160275
- config_name: en-li
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - li
  splits:
  - name: test
    num_bytes: 93342
    num_examples: 2000
  - name: train
    num_bytes: 1628577
    num_examples: 25535
  - name: validation
    num_bytes: 92898
    num_examples: 2000
  download_size: 1040760
  dataset_size: 1814817
- config_name: en-lt
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - lt
  splits:
  - name: test
    num_bytes: 482607
    num_examples: 2000
  - name: train
    num_bytes: 177060244
    num_examples: 1000000
  - name: validation
    num_bytes: 469109
    num_examples: 2000
  download_size: 124444053
  dataset_size: 178011960
- config_name: en-lv
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - lv
  splits:
  - name: test
    num_bytes: 536568
    num_examples: 2000
  - name: train
    num_bytes: 206051049
    num_examples: 1000000
  - name: validation
    num_bytes: 522064
    num_examples: 2000
  download_size: 140538527
  dataset_size: 207109681
- config_name: en-mg
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - mg
  splits:
  - name: test
    num_bytes: 525059
    num_examples: 2000
  - name: train
    num_bytes: 130865169
    num_examples: 590771
  - name: validation
    num_bytes: 511163
    num_examples: 2000
  download_size: 91102165
  dataset_size: 131901391
- config_name: en-mk
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - mk
  splits:
  - name: test
    num_bytes: 308926
    num_examples: 2000
  - name: train
    num_bytes: 117068689
    num_examples: 1000000
  - name: validation
    num_bytes: 305490
    num_examples: 2000
  download_size: 76810811
  dataset_size: 117683105
- config_name: en-ml
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ml
  splits:
  - name: test
    num_bytes: 340618
    num_examples: 2000
  - name: train
    num_bytes: 199971079
    num_examples: 822746
  - name: validation
    num_bytes: 334451
    num_examples: 2000
  download_size: 95497482
  dataset_size: 200646148
- config_name: en-mn
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - mn
  splits:
  - name: train
    num_bytes: 250770
    num_examples: 4294
  download_size: 85037
  dataset_size: 250770
- config_name: en-mr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - mr
  splits:
  - name: test
    num_bytes: 238604
    num_examples: 2000
  - name: train
    num_bytes: 2724107
    num_examples: 27007
  - name: validation
    num_bytes: 235532
    num_examples: 2000
  download_size: 1838618
  dataset_size: 3198243
- config_name: en-ms
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ms
  splits:
  - name: test
    num_bytes: 179697
    num_examples: 2000
  - name: train
    num_bytes: 76828845
    num_examples: 1000000
  - name: validation
    num_bytes: 180175
    num_examples: 2000
  download_size: 57412836
  dataset_size: 77188717
- config_name: en-mt
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - mt
  splits:
  - name: test
    num_bytes: 566126
    num_examples: 2000
  - name: train
    num_bytes: 222221596
    num_examples: 1000000
  - name: validation
    num_bytes: 594378
    num_examples: 2000
  download_size: 147836637
  dataset_size: 223382100
- config_name: en-my
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - my
  splits:
  - name: test
    num_bytes: 337343
    num_examples: 2000
  - name: train
    num_bytes: 3673477
    num_examples: 24594
  - name: validation
    num_bytes: 336147
    num_examples: 2000
  download_size: 1952573
  dataset_size: 4346967
- config_name: en-nb
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - nb
  splits:
  - name: test
    num_bytes: 334109
    num_examples: 2000
  - name: train
    num_bytes: 13611589
    num_examples: 142906
  - name: validation
    num_bytes: 324392
    num_examples: 2000
  download_size: 10630769
  dataset_size: 14270090
- config_name: en-ne
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ne
  splits:
  - name: test
    num_bytes: 186519
    num_examples: 2000
  - name: train
    num_bytes: 44135952
    num_examples: 406381
  - name: validation
    num_bytes: 204912
    num_examples: 2000
  download_size: 24107523
  dataset_size: 44527383
- config_name: en-nl
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - nl
  splits:
  - name: test
    num_bytes: 282747
    num_examples: 2000
  - name: train
    num_bytes: 112326273
    num_examples: 1000000
  - name: validation
    num_bytes: 270932
    num_examples: 2000
  download_size: 82923916
  dataset_size: 112879952
- config_name: en-nn
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - nn
  splits:
  - name: test
    num_bytes: 178999
    num_examples: 2000
  - name: train
    num_bytes: 32924429
    num_examples: 486055
  - name: validation
    num_bytes: 187642
    num_examples: 2000
  download_size: 25184676
  dataset_size: 33291070
- config_name: en-no
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - 'no'
  splits:
  - name: test
    num_bytes: 173320
    num_examples: 2000
  - name: train
    num_bytes: 74105483
    num_examples: 1000000
  - name: validation
    num_bytes: 178005
    num_examples: 2000
  download_size: 56277000
  dataset_size: 74456808
- config_name: en-oc
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - oc
  splits:
  - name: test
    num_bytes: 82342
    num_examples: 2000
  - name: train
    num_bytes: 1627174
    num_examples: 35791
  - name: validation
    num_bytes: 81642
    num_examples: 2000
  download_size: 1308338
  dataset_size: 1791158
- config_name: en-or
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - or
  splits:
  - name: test
    num_bytes: 163939
    num_examples: 1318
  - name: train
    num_bytes: 1500733
    num_examples: 14273
  - name: validation
    num_bytes: 155323
    num_examples: 1317
  download_size: 1019971
  dataset_size: 1819995
- config_name: en-pa
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - pa
  splits:
  - name: test
    num_bytes: 133901
    num_examples: 2000
  - name: train
    num_bytes: 8509140
    num_examples: 107296
  - name: validation
    num_bytes: 136188
    num_examples: 2000
  download_size: 5315298
  dataset_size: 8779229
- config_name: en-pl
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - pl
  splits:
  - name: test
    num_bytes: 212495
    num_examples: 2000
  - name: train
    num_bytes: 95247723
    num_examples: 1000000
  - name: validation
    num_bytes: 218208
    num_examples: 2000
  download_size: 73574044
  dataset_size: 95678426
- config_name: en-ps
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ps
  splits:
  - name: test
    num_bytes: 92995
    num_examples: 2000
  - name: train
    num_bytes: 4436512
    num_examples: 79127
  - name: validation
    num_bytes: 95156
    num_examples: 2000
  download_size: 2851899
  dataset_size: 4624663
- config_name: en-pt
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - pt
  splits:
  - name: test
    num_bytes: 296114
    num_examples: 2000
  - name: train
    num_bytes: 118242849
    num_examples: 1000000
  - name: validation
    num_bytes: 292074
    num_examples: 2000
  download_size: 87661907
  dataset_size: 118831037
- config_name: en-ro
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ro
  splits:
  - name: test
    num_bytes: 198639
    num_examples: 2000
  - name: train
    num_bytes: 85249051
    num_examples: 1000000
  - name: validation
    num_bytes: 199164
    num_examples: 2000
  download_size: 66294317
  dataset_size: 85646854
- config_name: en-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ru
  splits:
  - name: test
    num_bytes: 490976
    num_examples: 2000
  - name: train
    num_bytes: 195100937
    num_examples: 1000000
  - name: validation
    num_bytes: 490238
    num_examples: 2000
  download_size: 124460816
  dataset_size: 196082151
- config_name: en-rw
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - rw
  splits:
  - name: test
    num_bytes: 136189
    num_examples: 2000
  - name: train
    num_bytes: 15286159
    num_examples: 173823
  - name: validation
    num_bytes: 134957
    num_examples: 2000
  download_size: 10093708
  dataset_size: 15557305
- config_name: en-se
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - se
  splits:
  - name: test
    num_bytes: 85697
    num_examples: 2000
  - name: train
    num_bytes: 2047380
    num_examples: 35907
  - name: validation
    num_bytes: 83664
    num_examples: 2000
  download_size: 1662845
  dataset_size: 2216741
- config_name: en-sh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - sh
  splits:
  - name: test
    num_bytes: 569479
    num_examples: 2000
  - name: train
    num_bytes: 60900023
    num_examples: 267211
  - name: validation
    num_bytes: 555594
    num_examples: 2000
  download_size: 39988454
  dataset_size: 62025096
- config_name: en-si
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - si
  splits:
  - name: test
    num_bytes: 271735
    num_examples: 2000
  - name: train
    num_bytes: 114950891
    num_examples: 979109
  - name: validation
    num_bytes: 271236
    num_examples: 2000
  download_size: 66124160
  dataset_size: 115493862
- config_name: en-sk
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - sk
  splits:
  - name: test
    num_bytes: 258034
    num_examples: 2000
  - name: train
    num_bytes: 111743068
    num_examples: 1000000
  - na",High,5.0
Translation,Helsinki-NLP/opus_gnome,1.0,186.0,2024-02-22 15:04:29+00:00,unknown,0.0,1.98 MB,unknown,1.98 MB,unknown,59485,none,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- af
- am
- an
- ang
- ar
- as
- ast
- az
- bal
- be
- bem
- bg
- bn
- bo
- br
- brx
- bs
- ca
- crh
- cs
- csb
- cy
- da
- de
- dv
- dz
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fo
- fr
- fur
- fy
- ga
- gd
- gl
- gn
- gu
- gv
- ha
- he
- hi
- hr
- hu
- hy
- ia
- id
- ig
- io
- is
- it
- ja
- jbo
- ka
- kg
- kk
- km
- kn
- ko
- kr
- ks
- ku
- ky
- la
- lg
- li
- lo
- lt
- lv
- mai
- mg
- mi
- mk
- ml
- mn
- mr
- ms
- mt
- mus
- my
- nb
- nds
- ne
- nhn
- nl
- nn
- 'no'
- nqo
- nr
- nso
- oc
- or
- os
- pa
- pl
- ps
- pt
- quz
- ro
- ru
- rw
- si
- sk
- sl
- so
- sq
- sr
- st
- sv
- sw
- szl
- ta
- te
- tg
- th
- tk
- tl
- tr
- ts
- tt
- tyj
- ug
- uk
- ur
- uz
- vi
- wa
- xh
- yi
- yo
- zh
- zu
license:
- unknown
multilinguality:
- multilingual
size_categories:
- 10K<n<100K
- 1K<n<10K
- n<1K
source_datasets:
- original
task_categories:
- translation
task_ids: []
pretty_name: OpusGnome
config_names:
- ar-bal
- bg-csb
- ca-en_GB
- cs-eo
- cs-tk
- da-vi
- de-ha
- de-tt
- el-sk
- en_GB-my
language_bcp47:
- ar-TN
- az-IR
- bg-BG
- bn-IN
- da-DK
- de-CH
- en-AU
- en-CA
- en-GB
- en-NZ
- en-US
- en-ZA
- es-AR
- es-CL
- es-CO
- es-CR
- es-DO
- es-EC
- es-ES
- es-GT
- es-HN
- es-MX
- es-NI
- es-PA
- es-PE
- es-PR
- es-SV
- es-UY
- es-VE
- fa-IR
- hi-IN
- it-IT
- ms-MY
- nb-NO
- nn-NO
- no-NB
- pt-BR
- pt-PT
- sr-ME
- tg-TJ
- tl-PH
- tr-TR
- ur-PK
- vi-VN
- zh-CN
- zh-HK
- zh-TW
dataset_info:
- config_name: ar-bal
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - bal
  splits:
  - name: train
    num_bytes: 5138
    num_examples: 60
  download_size: 3695
  dataset_size: 5138
- config_name: bg-csb
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - bg
        - csb
  splits:
  - name: train
    num_bytes: 172533
    num_examples: 1768
  download_size: 48120
  dataset_size: 172533
- config_name: ca-en_GB
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ca
        - en_GB
  splits:
  - name: train
    num_bytes: 1007476
    num_examples: 7982
  download_size: 422733
  dataset_size: 1007476
- config_name: cs-eo
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - eo
  splits:
  - name: train
    num_bytes: 2883
    num_examples: 73
  download_size: 4183
  dataset_size: 2883
- config_name: cs-tk
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - cs
        - tk
  splits:
  - name: train
    num_bytes: 1197707
    num_examples: 18686
  download_size: 387467
  dataset_size: 1197707
- config_name: da-vi
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - da
        - vi
  splits:
  - name: train
    num_bytes: 9360
    num_examples: 149
  download_size: 8243
  dataset_size: 9360
- config_name: de-ha
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - ha
  splits:
  - name: train
    num_bytes: 22887
    num_examples: 216
  download_size: 8846
  dataset_size: 22887
- config_name: de-tt
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - tt
  splits:
  - name: train
    num_bytes: 134966
    num_examples: 2169
  download_size: 38009
  dataset_size: 134966
- config_name: el-sk
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - el
        - sk
  splits:
  - name: train
    num_bytes: 12109
    num_examples: 150
  download_size: 9379
  dataset_size: 12109
- config_name: en_GB-my
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en_GB
        - my
  splits:
  - name: train
    num_bytes: 3298038
    num_examples: 28232
  download_size: 1045971
  dataset_size: 3298038
configs:
- config_name: ar-bal
  data_files:
  - split: train
    path: ar-bal/train-*
- config_name: bg-csb
  data_files:
  - split: train
    path: bg-csb/train-*
- config_name: ca-en_GB
  data_files:
  - split: train
    path: ca-en_GB/train-*
- config_name: cs-eo
  data_files:
  - split: train
    path: cs-eo/train-*
- config_name: cs-tk
  data_files:
  - split: train
    path: cs-tk/train-*
- config_name: da-vi
  data_files:
  - split: train
    path: da-vi/train-*
- config_name: de-ha
  data_files:
  - split: train
    path: de-ha/train-*
- config_name: de-tt
  data_files:
  - split: train
    path: de-tt/train-*
- config_name: el-sk
  data_files:
  - split: train
    path: el-sk/train-*
- config_name: en_GB-my
  data_files:
  - split: train
    path: en_GB-my/train-*
---

# Dataset Card for Opus Gnome

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** http://opus.nlpl.eu/GNOME.php
- **Repository:** None
- **Paper:** http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf
- **Leaderboard:** [More Information Needed]
- **Point of Contact:** [More Information Needed]

### Dataset Summary


To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.
You can find the valid pairs in Homepage section of Dataset Description: http://opus.nlpl.eu/GNOME.php
E.g.

`dataset = load_dataset(""opus_gnome"", lang1=""it"", lang2=""pl"")`


### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

[More Information Needed]

## Dataset Structure

### Data Instances  
```
{
  'id': '0', 
  'translation': {
    'ar': 'إعداد سياسة القفل',
    'bal': 'تنظیم کتن سیاست کبل'
  }
}
```
### Data Fields

Each instance has two fields:
- **id**: the id of the example
- **translation**: a dictionary containing translated texts in two languages.

### Data Splits

Each subset simply consists in a train set. We provide the number of examples for certain language pairs:

|          |   train |
|:---------|--------:|
| ar-bal   |      60 |
| bg-csb   |      10 |
| ca-en_GB |    7982 |
| cs-eo    |      73 |
| de-ha    |     216 |
| cs-tk    |   18686 |
| da-vi    |     149 |
| en_GB-my |   28232 |
| el-sk    |     150 |
| de-tt    |    2169 |

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

@InProceedings{TIEDEMANN12.463,
  author = {J{\""o}rg Tiedemann},
  title = {Parallel Data, Tools and Interfaces in OPUS},
  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},
  year = {2012},
  month = {may},
  date = {23-25},
  address = {Istanbul, Turkey},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {978-2-9517408-7-7},
  language = {english}
 }

### Contributions

Thanks to [@rkc007](https://github.com/rkc007) for adding this dataset.",High,5.0
Translation,Helsinki-NLP/opus_infopankki,4.0,1711.0,2024-02-22 15:10:55+00:00,cc-by-4.0,0.0,153 MB,160432128.0,153 MB,160432128.0,2934399,none,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- ar
- en
- es
- et
- fa
- fi
- fr
- ru
- so
- sv
- tr
- zh
license: cc-by-4.0
multilinguality:
- multilingual
size_categories:
- 10K<n<100K
source_datasets:
- original
task_categories:
- translation
task_ids: []
pretty_name: OpusInfopankki
config_names:
- ar-en
- ar-es
- ar-et
- ar-fa
- ar-fi
- ar-fr
- ar-ru
- ar-so
- ar-sv
- ar-tr
- ar-zh
- en-es
- en-et
- en-fa
- en-fi
- en-fr
- en-ru
- en-so
- en-sv
- en-tr
- en-zh
- es-et
- es-fa
- es-fi
- es-fr
- es-ru
- es-so
- es-sv
- es-tr
- es-zh
- et-fa
- et-fi
- et-fr
- et-ru
- et-so
- et-sv
- et-tr
- et-zh
- fa-fi
- fa-fr
- fa-ru
- fa-so
- fa-sv
- fa-tr
- fa-zh
- fi-fr
- fi-ru
- fi-so
- fi-sv
- fi-tr
- fi-zh
- fr-ru
- fr-so
- fr-sv
- fr-tr
- fr-zh
- ru-so
- ru-sv
- ru-tr
- ru-zh
- so-sv
- so-tr
- so-zh
- sv-tr
- sv-zh
- tr-zh
dataset_info:
- config_name: ar-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - en
  splits:
  - name: train
    num_bytes: 10133337
    num_examples: 50769
  download_size: 2775475
  dataset_size: 10133337
- config_name: ar-es
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - es
  splits:
  - name: train
    num_bytes: 8665355
    num_examples: 40514
  download_size: 2366264
  dataset_size: 8665355
- config_name: ar-et
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - et
  splits:
  - name: train
    num_bytes: 9087555
    num_examples: 46573
  download_size: 2475165
  dataset_size: 9087555
- config_name: ar-fa
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - fa
  splits:
  - name: train
    num_bytes: 12220196
    num_examples: 47007
  download_size: 3017006
  dataset_size: 12220196
- config_name: ar-fi
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - fi
  splits:
  - name: train
    num_bytes: 9524265
    num_examples: 49608
  download_size: 2704144
  dataset_size: 9524265
- config_name: ar-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - fr
  splits:
  - name: train
    num_bytes: 8877629
    num_examples: 41061
  download_size: 2434048
  dataset_size: 8877629
- config_name: ar-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - ru
  splits:
  - name: train
    num_bytes: 13648194
    num_examples: 50286
  download_size: 3393441
  dataset_size: 13648194
- config_name: ar-so
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - so
  splits:
  - name: train
    num_bytes: 9555548
    num_examples: 44736
  download_size: 2614055
  dataset_size: 9555548
- config_name: ar-sv
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - sv
  splits:
  - name: train
    num_bytes: 8585135
    num_examples: 43085
  download_size: 2312217
  dataset_size: 8585135
- config_name: ar-tr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - tr
  splits:
  - name: train
    num_bytes: 8691077
    num_examples: 41710
  download_size: 2417172
  dataset_size: 8691077
- config_name: ar-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - zh
  splits:
  - name: train
    num_bytes: 5973634
    num_examples: 29943
  download_size: 1523722
  dataset_size: 5973634
- config_name: en-es
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - es
  splits:
  - name: train
    num_bytes: 6933983
    num_examples: 42657
  download_size: 2108422
  dataset_size: 6933983
- config_name: en-et
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - et
  splits:
  - name: train
    num_bytes: 8211562
    num_examples: 58410
  download_size: 2473732
  dataset_size: 8211562
- config_name: en-fa
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fa
  splits:
  - name: train
    num_bytes: 10166305
    num_examples: 48277
  download_size: 2696051
  dataset_size: 10166305
- config_name: en-fi
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fi
  splits:
  - name: train
    num_bytes: 10913601
    num_examples: 84645
  download_size: 3183398
  dataset_size: 10913601
- config_name: en-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fr
  splits:
  - name: train
    num_bytes: 8903183
    num_examples: 56120
  download_size: 2522185
  dataset_size: 8903183
- config_name: en-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ru
  splits:
  - name: train
    num_bytes: 15918195
    num_examples: 75305
  download_size: 3834067
  dataset_size: 15918195
- config_name: en-so
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - so
  splits:
  - name: train
    num_bytes: 7602290
    num_examples: 47220
  download_size: 2317274
  dataset_size: 7602290
- config_name: en-sv
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - sv
  splits:
  - name: train
    num_bytes: 7410975
    num_examples: 51749
  download_size: 2214196
  dataset_size: 7410975
- config_name: en-tr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - tr
  splits:
  - name: train
    num_bytes: 6929154
    num_examples: 44030
  download_size: 2158897
  dataset_size: 6929154
- config_name: en-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - zh
  splits:
  - name: train
    num_bytes: 4666963
    num_examples: 29907
  download_size: 1313255
  dataset_size: 4666963
- config_name: es-et
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - et
  splits:
  - name: train
    num_bytes: 6611956
    num_examples: 42342
  download_size: 2109076
  dataset_size: 6611956
- config_name: es-fa
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - fa
  splits:
  - name: train
    num_bytes: 9338210
    num_examples: 41218
  download_size: 2535729
  dataset_size: 9338210
- config_name: es-fi
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - fi
  splits:
  - name: train
    num_bytes: 6436298
    num_examples: 41479
  download_size: 2052254
  dataset_size: 6436298
- config_name: es-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - fr
  splits:
  - name: train
    num_bytes: 7368724
    num_examples: 41940
  download_size: 2234633
  dataset_size: 7368724
- config_name: es-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - ru
  splits:
  - name: train
    num_bytes: 9844937
    num_examples: 41061
  download_size: 2638368
  dataset_size: 9844937
- config_name: es-so
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - so
  splits:
  - name: train
    num_bytes: 7257038
    num_examples: 41752
  download_size: 2261851
  dataset_size: 7257038
- config_name: es-sv
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - sv
  splits:
  - name: train
    num_bytes: 6650652
    num_examples: 41256
  download_size: 2027874
  dataset_size: 6650652
- config_name: es-tr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - tr
  splits:
  - name: train
    num_bytes: 7144065
    num_examples: 42191
  download_size: 2206245
  dataset_size: 7144065
- config_name: es-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - zh
  splits:
  - name: train
    num_bytes: 4358751
    num_examples: 26004
  download_size: 1176333
  dataset_size: 4358751
- config_name: et-fa
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - et
        - fa
  splits:
  - name: train
    num_bytes: 9795996
    num_examples: 47633
  download_size: 2680445
  dataset_size: 9795996
- config_name: et-fi
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - et
        - fi
  splits:
  - name: train
    num_bytes: 7656989
    num_examples: 57353
  download_size: 2419554
  dataset_size: 7656989
- config_name: et-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - et
        - fr
  splits:
  - name: train
    num_bytes: 7012430
    num_examples: 44753
  download_size: 2193006
  dataset_size: 7012430
- config_name: et-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - et
        - ru
  splits:
  - name: train
    num_bytes: 12001391
    num_examples: 55901
  download_size: 3160673
  dataset_size: 12001391
- config_name: et-so
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - et
        - so
  splits:
  - name: train
    num_bytes: 7260797
    num_examples: 46933
  download_size: 2319211
  dataset_size: 7260797
- config_name: et-sv
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - et
        - sv
  splits:
  - name: train
    num_bytes: 6523041
    num_examples: 46775
  download_size: 2074448
  dataset_size: 6523041
- config_name: et-tr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - et
        - tr
  splits:
  - name: train
    num_bytes: 6621665
    num_examples: 43729
  download_size: 2123880
  dataset_size: 6621665
- config_name: et-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - et
        - zh
  splits:
  - name: train
    num_bytes: 4305273
    num_examples: 27826
  download_size: 1201275
  dataset_size: 4305273
- config_name: fa-fi
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fa
        - fi
  splits:
  - name: train
    num_bytes: 9579257
    num_examples: 46924
  download_size: 2618699
  dataset_size: 9579257
- config_name: fa-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fa
        - fr
  splits:
  - name: train
    num_bytes: 9574254
    num_examples: 41975
  download_size: 2588917
  dataset_size: 9574254
- config_name: fa-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fa
        - ru
  splits:
  - name: train
    num_bytes: 13544451
    num_examples: 47814
  download_size: 3351553
  dataset_size: 13544451
- config_name: fa-so
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fa
        - so
  splits:
  - name: train
    num_bytes: 10254723
    num_examples: 45571
  download_size: 2813443
  dataset_size: 10254723
- config_name: fa-sv
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fa
        - sv
  splits:
  - name: train
    num_bytes: 9153752
    num_examples: 43510
  download_size: 2512908
  dataset_size: 9153752
- config_name: fa-tr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fa
        - tr
  splits:
  - name: train
    num_bytes: 9393209
    num_examples: 42708
  download_size: 2599794
  dataset_size: 9393209
- config_name: fa-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fa
        - zh
  splits:
  - name: train
    num_bytes: 5792439
    num_examples: 27748
  download_size: 1413779
  dataset_size: 5792439
- config_name: fi-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fi
        - fr
  splits:
  - name: train
    num_bytes: 8310851
    num_examples: 55087
  download_size: 2455971
  dataset_size: 8310851
- config_name: fi-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fi
        - ru
  splits:
  - name: train
    num_bytes: 15188168
    num_examples: 74699
  download_size: 3842831
  dataset_size: 15188168
- config_name: fi-so
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fi
        - so
  splits:
  - name: train
    num_bytes: 7076221
    num_examples: 46032
  download_size: 2219872
  dataset_size: 7076221
- config_name: fi-sv
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fi
        - sv
  splits:
  - name: train
    num_bytes: 6947224
    num_examples: 51506
  download_size: 2137629
  dataset_size: 6947224
- config_name: fi-tr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fi
        - tr
  splits:
  - name: train
    num_bytes: 6438716
    num_examples: 42781
  download_size: 2081615
  dataset_size: 6438716
- config_name: fi-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fi
        - zh
  splits:
  - name: train
    num_bytes: 4434168
    num_examples: 29503
  download_size: 1312557
  dataset_size: 4434168
- config_name: fr-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - ru
  splits:
  - name: train
    num_bytes: 12564196
    num_examples: 54213
  download_size: 3159587
  dataset_size: 12564196
- config_name: fr-so
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - so
  splits:
  - name: train
    num_bytes: 7473559
    num_examples: 42652
  download_size: 2344399
  dataset_size: 7473559
- config_name: fr-sv
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - sv
  splits:
  - name: train
    num_bytes: 7027563
    num_examples: 43524
  download_size: 2107653
  dataset_size: 7027563
- config_name: fr-tr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - tr
  splits:
  - name: train
    num_bytes: 7341078
    num_examples: 43036
  download_size: 2279611
  dataset_size: 7341078
- config_name: fr-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - zh
  splits:
  - name: train
    num_bytes: 4525109
    num_examples: 26654
  download_size: 1211652
  dataset_size: 4525109
- config_name: ru-so
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ru
        - so
  splits:
  - name: train
    num_bytes: 10809193
    num_examples: 45430
  download_size: 2932790
  dataset_size: 10809193
- config_name: ru-sv
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ru
        - sv
  splits:
  - name: train
    num_bytes: 10517433
    num_examples: 47672
  download_size: 2724280
  dataset_size: 10517433
- config_name: ru-tr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ru
        - tr
  splits:
  - name: train
    num_bytes: 9930592
    num_examples: 42587
  download_size: 2727600
  dataset_size: 9930592
- config_name: ru-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ru
        - zh
  splits:
  - name: train
    num_bytes: 6417808
    num_examples: 29523
  download_size: 1582749
  dataset_size: 6417808
- config_name: so-sv
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - so
        - sv
  splits:
  - name: train
    num_bytes: 6763754
    num_examples: 42384
  download_size: 2098877
  dataset_size: 6763754
- config_name: so-tr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - so
        - tr
  splits:
  - name: train
    num_bytes: 7272349
    num_examples: 43242
  download_size: 2279999
  dataset_size: 7272349
- config_name: so-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - so
        - zh
  splits:
  - name: train
    num_bytes: 4535955
    num_examples: 27090
  download_size: 1267321
  dataset_size: 4535955
- config_name: sv-tr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - sv
        - tr
  splits:
  - name: train
    num_bytes: 6637744
    num_examples: 42555
  download_size: 2045078
  dataset_size: 6637744
- config_name: sv-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - sv
        - zh
  splits:
  - name: train
    num_bytes: 4216405
    num_examples: 26898
  download_size: 1149609
  dataset_size: 4216405
- config_name: tr-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - tr
        - zh
  splits:
  - name: train
    num_bytes: 4494071
    num_examples: 27323
  download_size: 1221951
  dataset_size: 4494071
configs:
- config_name: ar-en
  data_files:
  - split: train
    path: ar-en/train-*
- config_name: ar-es
  data_files:
  - split: train
    path: ar-es/train-*
- config_name: ar-et
  data_files:
  - split: train
    path: ar-et/train-*
- config_name: ar-fa
  data_files:
  - split: train
    path: ar-fa/train-*
- config_name: ar-fi
  data_files:
  - split: train
    path: ar-fi/train-*
- config_name: ar-fr
  data_files:
  - split: train
    path: ar-fr/train-*
- config_name: ar-ru
  data_files:
  - split: train
    path: ar-ru/train-*
- config_name: ar-so
  data_files:
  - split: train
    path: ar-so/train-*
- config_name: ar-sv
  data_files:
  - split: train
    path: ar-sv/train-*
- config_name: ar-tr
  data_files:
  - split: train
    path: ar-tr/train-*
- config_name: ar-zh
  data_files:
  - split: train
    path: ar-zh/train-*
- config_name: en-es
  data_files:
  - split: train
    path: en-es/train-*
- config_name: en-et
  data_files:
  - split: train
    path: en-et/train-*
- config_name: en-fa
  data_files:
  - split: train
    path: en-fa/train-*
- config_name: en-fi
  data_files:
  - split: train
    path: en-fi/train-*
- config_name: en-fr
  data_files:
  - split: train
    path: en-fr/train-*
- config_name: en-ru
  data_files:
  - split: train
    path: en-ru/train-*
- config_name: en-so
  data_files:
  - split: train
    path: en-so/train-*
- config_name: en-sv
  data_files:
  - split: train
    path: en-sv/train-*
- config_name: en-tr
  data_files:
  - split: train
    path: en-tr/train-*
- config_name: en-zh
  data_files:
  - split: train
    path: en-zh/train-*
- config_name: es-et
  data_files:
  - split: train
    path: es-et/train-*
- config_name: es-fa
  data_files:
  - split: train
    path: es-fa/train-*
- config_name: es-fi
  data_files:
  - split: train
    path: es-fi/train-*
- config_name: es-fr
  data_files:
  - split: train
    path: es-fr/train-*
- config_name: es-ru
  data_files:
  - split: train
    path: es-ru/train-*
- config_name: es-so
  data_files:
  - split: train
    path: es-so/train-*
- config_name: es-sv
  data_files:
  - split: train
    path: es-sv/train-*
- config_name: es-tr
  data_files:
  - split: train
    path: es-tr/train-*
- config_name: es-zh
  data_files:
  - split: train
    path: es-zh/train-*
- config_name: et-fa
  data_files:
  - split: train
    path: et-fa/train-*
- config_name: et-fi
  data_files:
  - split: train
    path: et-fi/train-*
- config_name: et-fr
  data_files:
  - split: train
    path: et-fr/train-*
- config_name: et-ru
  data_files:
  - split: train
    path: et-ru/train-*
- config_name: et-so
  data_files:
  - split: train
    path: et-so/train-*
- config_name: et-sv
  data_files:
  - split: train
    path: et-sv/train-*
- config_name: et-tr
  data_files:
  - split: train
    path: et-tr/train-*
- config_name: et-zh
  data_files:
  - split: train
    path: et-zh/train-*
- config_name: fa-fi
  data_files:
  - split: train
    path: fa-fi/train-*
- config_name: fa-fr
  data_files:
  - split: train
    path: fa-fr/train-*
- config_name: fa-ru
  data_files:
  - split: train
    path: fa-ru/train-*
- config_name: fa-so
  data_files:
  - split: train
    path: fa-so/train-*
- config_name: fa-sv
  data_files:
  - split: train
    path: fa-sv/train-*
- config_name: fa-tr
  data_files:
  - split: train
    path: fa-tr/train-*
- config_name: fa-zh
  data_files:
  - split: train
    path: fa-zh/train-*
- config_name: fi-fr
  data_files:
  - split: train
    path: fi-fr/train-*
- config_name: fi-ru
  data_files:
  - split: train
    path: fi-ru/train-*
- config_name: fi-so
  data_files:
  - split: train
    path: fi-so/train-*
- config_name: fi-sv
  data_files:
  - split: train
    path: fi-sv/train-*
- config_name: fi-tr
  data_files:
  - split: train
    path: fi-tr/train-*
- config_name: fi-zh
  data_files:
  - split: train
    path: fi-zh/train-*
- config_name: fr-ru
  data_files:
  - split: train
    path: fr-ru/train-*
- config_name: fr-so
  data_files:
  - split: train
    path: fr-so/train-*
- config_name: fr-sv
  data_files:
  - split: train
    path: fr-sv/train-*
- config_name: fr-tr
  data_files:
  - split: train
    path: fr-tr/train-*
- config_name: fr-zh
  data_files:
  - split: train
    path: fr-zh/train-*
- config_name: ru-so
  data_files:
  - split: train
    path: ru-so/train-*
- config_name: ru-sv
  data_files:
  - split: train
    path: ru-sv/train-*
- config_name: ru-tr
  data_files:
  - split: train
    path: ru-tr/train-*
- config_name: ru-zh
  data_files:
  - split: train
    path: ru-zh/train-*
- config_name: so-sv
  data_files:
  - split: train
    path: so-sv/train-*
- config_name: so-tr
  data_files:
  - split: train
    path: so-tr/train-*
- config_name: so-zh
  data_files:
  - split: train
    path: so-zh/train-*
- config_name: sv-tr
  data_files:
  - split: train
    path: sv-tr/train-*
- config_name: sv-zh
  data_files:
  - split: train
    path: sv-zh/train-*
- config_name: tr-zh
  data_files:
  - split: train
    path: tr-zh/train-*
---

# Dataset Card for infopankki

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://opus.nlpl.eu/infopankki/corpus/version/infopankki
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Leaderboard:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Dataset Summary

A parallel corpus of 12 languages, 66 bitexts.

### Supported Tasks and Leaderboards

The underlying task is machine translation.

### Languages

[More Information Needed]

## Dataset Structure

### Data Instances

[More Information Needed]

### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

#### Initial Data Collection and Normalization

Source: http://www.infopankki.fi via the Open Data API

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

Licensed under [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).

### Citation Information

If you use any part of the corpus in your own work, please cite the following article:
```
@inproceedings{tiedemann-2012-parallel,
    title = ""Parallel Data, Tools and Interfaces in {OPUS}"",
    author = {Tiedemann, J{\""o}rg},
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf"",
    pages = ""2214--2218"",
    abstract = ""This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project."",
}
```

### Contributions

Thanks to [@patil-suraj](https://github.com/patil-suraj) for adding this dataset.",High,5.0
Translation,Helsinki-NLP/opus_ubuntu,4.0,198.0,2024-02-22 15:45:27+00:00,bsd-3-clause,0.0,1.61 MB,unknown,1.61 MB,unknown,37448,none,none,"---
annotations_creators:
- crowdsourced
- expert-generated
language_creators:
- found
language:
- ace
- af
- ak
- am
- an
- ang
- ar
- ary
- as
- ast
- az
- ba
- bal
- be
- bem
- ber
- bg
- bho
- bn
- bo
- br
- brx
- bs
- bua
- byn
- ca
- ce
- ceb
- chr
- ckb
- co
- crh
- cs
- csb
- cv
- cy
- da
- de
- dsb
- dv
- dz
- el
- en
- eo
- es
- et
- eu
- fa
- ff
- fi
- fil
- fo
- fr
- frm
- frp
- fur
- fy
- ga
- gd
- gl
- gn
- grc
- gu
- guc
- gv
- ha
- haw
- he
- hi
- hil
- hne
- hr
- hsb
- ht
- hu
- hy
- ia
- id
- ig
- io
- is
- it
- iu
- ja
- jbo
- jv
- ka
- kab
- kg
- kk
- kl
- km
- kn
- ko
- kok
- ks
- ksh
- ku
- kw
- ky
- la
- lb
- lg
- li
- lij
- lld
- ln
- lo
- lt
- ltg
- lv
- mai
- mg
- mh
- mhr
- mi
- miq
- mk
- ml
- mn
- mr
- ms
- mt
- mus
- my
- nan
- nap
- nb
- nds
- ne
- nhn
- nl
- nn
- 'no'
- nso
- ny
- oc
- om
- or
- os
- pa
- pam
- pap
- pl
- pms
- pmy
- ps
- pt
- qu
- rm
- ro
- rom
- ru
- rw
- sa
- sc
- sco
- sd
- se
- shn
- shs
- si
- sk
- sl
- sm
- sml
- sn
- so
- son
- sq
- sr
- st
- sv
- sw
- syr
- szl
- ta
- te
- tet
- tg
- th
- ti
- tk
- tl
- tlh
- tr
- trv
- ts
- tt
- ug
- uk
- ur
- uz
- ve
- vec
- vi
- wa
- wae
- wo
- xal
- xh
- yi
- yo
- zh
- zu
- zza
license:
- bsd-3-clause
multilinguality:
- multilingual
size_categories:
- 10K<n<100K
- 1K<n<10K
- n<1K
source_datasets:
- original
task_categories:
- translation
task_ids: []
pretty_name: Opus Ubuntu
config_names:
- as-bs
- az-cs
- bg-de
- bn-ga
- br-es_PR
- br-hi
- br-la
- br-uz
- br-yi
- bs-szl
language_bcp47:
- ar-SY
- bn-IN
- de-AT
- de-DE
- en-AU
- en-CA
- en-GB
- en-NZ
- en-US
- es-AR
- es-CL
- es-CO
- es-CR
- es-DO
- es-EC
- es-ES
- es-GT
- es-HN
- es-MX
- es-NI
- es-PA
- es-PE
- es-PR
- es-SV
- es-UY
- es-VE
- fa-AF
- fr-CA
- fr-FR
- nl-NL
- pt-BR
- pt-PT
- ta-LK
- zh-CN
- zh-HK
- zh-TW
dataset_info:
- config_name: as-bs
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - as
        - bs
  splits:
  - name: train
    num_bytes: 1037799
    num_examples: 8583
  download_size: 470874
  dataset_size: 1037799
- config_name: az-cs
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - az
        - cs
  splits:
  - name: train
    num_bytes: 17809
    num_examples: 293
  download_size: 14637
  dataset_size: 17809
- config_name: bg-de
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - bg
        - de
  splits:
  - name: train
    num_bytes: 27615
    num_examples: 184
  download_size: 16278
  dataset_size: 27615
- config_name: bn-ga
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - bn
        - ga
  splits:
  - name: train
    num_bytes: 584617
    num_examples: 7324
  download_size: 272247
  dataset_size: 584617
- config_name: br-es_PR
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - br
        - es_PR
  splits:
  - name: train
    num_bytes: 8863
    num_examples: 125
  download_size: 8194
  dataset_size: 8863
- config_name: br-hi
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - br
        - hi
  splits:
  - name: train
    num_bytes: 1300057
    num_examples: 15551
  download_size: 641803
  dataset_size: 1300057
- config_name: br-la
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - br
        - la
  splits:
  - name: train
    num_bytes: 29329
    num_examples: 527
  download_size: 17723
  dataset_size: 29329
- config_name: br-uz
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - br
        - uz
  splits:
  - name: train
    num_bytes: 110266
    num_examples: 1416
  download_size: 62660
  dataset_size: 110266
- config_name: br-yi
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - br
        - yi
  splits:
  - name: train
    num_bytes: 172834
    num_examples: 2799
  download_size: 77870
  dataset_size: 172834
- config_name: bs-szl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - bs
        - szl
  splits:
  - name: train
    num_bytes: 41104
    num_examples: 646
  download_size: 30035
  dataset_size: 41104
configs:
- config_name: as-bs
  data_files:
  - split: train
    path: as-bs/train-*
- config_name: az-cs
  data_files:
  - split: train
    path: az-cs/train-*
- config_name: bg-de
  data_files:
  - split: train
    path: bg-de/train-*
- config_name: bn-ga
  data_files:
  - split: train
    path: bn-ga/train-*
- config_name: br-es_PR
  data_files:
  - split: train
    path: br-es_PR/train-*
- config_name: br-hi
  data_files:
  - split: train
    path: br-hi/train-*
- config_name: br-la
  data_files:
  - split: train
    path: br-la/train-*
- config_name: br-uz
  data_files:
  - split: train
    path: br-uz/train-*
- config_name: br-yi
  data_files:
  - split: train
    path: br-yi/train-*
- config_name: bs-szl
  data_files:
  - split: train
    path: bs-szl/train-*
---

# Dataset Card for Opus Ubuntu

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** http://opus.nlpl.eu/Ubuntu.php
- **Repository:** None
- **Paper:** http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf
- **Leaderboard:** [More Information Needed]
- **Point of Contact:** [More Information Needed]

### Dataset Summary

These are translations of the Ubuntu software package messages, donated by the Ubuntu community.

To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.
You can find the valid pairs in Homepage section of Dataset Description: http://opus.nlpl.eu/Ubuntu.php
E.g.

`dataset = load_dataset(""opus_ubuntu"", lang1=""it"", lang2=""pl"")`


### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

[More Information Needed]

## Dataset Structure

### Data Instances

Example instance:

```
{
  'id': '0', 
  'translation': {
    'it': 'Comprende Gmail, Google Docs, Google+, YouTube e Picasa',
    'pl': 'Zawiera Gmail, Google Docs, Google+, YouTube oraz Picasa'
  }
}
```

### Data Fields

Each instance has two fields:
- **id**: the id of the example
- **translation**: a dictionary containing translated texts in two languages.

### Data Splits

Each subset simply consists in a train set. We provide the number of examples for certain language pairs:

|          |   train |
|:---------|--------:|
| as-bs    |    8583 |
| az-cs    |     293 |
| bg-de    |     184 |
| br-es_PR |     125 |
| bn-ga    |    7324 |
| br-hi    |   15551 |
| br-la    |     527 |
| bs-szl   |     646 |
| br-uz    |    1416 |
| br-yi    |    2799 |

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

BSD ""Revised"" license (see (https://help.launchpad.net/Legal#Translations_copyright)[https://help.launchpad.net/Legal#Translations_copyright])

### Citation Information

```bibtex
@InProceedings{TIEDEMANN12.463,
  author = {J{\""o}rg Tiedemann},
  title = {Parallel Data, Tools and Interfaces in OPUS},
  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},
  year = {2012},
  month = {may},
  date = {23-25},
  address = {Istanbul, Turkey},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {978-2-9517408-7-7},
  language = {english}
 }
```

### Contributions

Thanks to [@rkc007](https://github.com/rkc007) for adding this dataset.",High,5.0
Translation,Helsinki-NLP/opus_wikipedia,9.0,189.0,2024-02-22 15:50:00+00:00,unknown,0.0,322 MB,unknown,322 MB,unknown,1745808,none,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- ar
- bg
- cs
- de
- el
- en
- es
- fa
- fr
- he
- hu
- it
- nl
- pl
- pt
- ro
- ru
- sl
- tr
- vi
license:
- unknown
multilinguality:
- multilingual
size_categories:
- 100K<n<1M
- 10K<n<100K
source_datasets:
- original
task_categories:
- translation
task_ids: []
pretty_name: OpusWikipedia
config_names:
- ar-en
- ar-pl
- en-ru
- en-sl
- en-vi
dataset_info:
- config_name: ar-en
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - en
  splits:
  - name: train
    num_bytes: 45207523
    num_examples: 151136
  download_size: 26617751
  dataset_size: 45207523
- config_name: ar-pl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - pl
  splits:
  - name: train
    num_bytes: 304850680
    num_examples: 823715
  download_size: 175806051
  dataset_size: 304850680
- config_name: en-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ru
  splits:
  - name: train
    num_bytes: 167648361
    num_examples: 572717
  download_size: 97008376
  dataset_size: 167648361
- config_name: en-sl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - sl
  splits:
  - name: train
    num_bytes: 30479559
    num_examples: 140124
  download_size: 18557819
  dataset_size: 30479559
- config_name: en-vi
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - vi
  splits:
  - name: train
    num_bytes: 7571526
    num_examples: 58116
  download_size: 3969559
  dataset_size: 7571526
configs:
- config_name: ar-en
  data_files:
  - split: train
    path: ar-en/train-*
- config_name: ar-pl
  data_files:
  - split: train
    path: ar-pl/train-*
- config_name: en-ru
  data_files:
  - split: train
    path: en-ru/train-*
- config_name: en-sl
  data_files:
  - split: train
    path: en-sl/train-*
- config_name: en-vi
  data_files:
  - split: train
    path: en-vi/train-*
---

# Dataset Card for OpusWikipedia

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** http://opus.nlpl.eu/Wikipedia.php
- **Repository:** None
- **Paper:** http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf
- **Leaderboard:** [More Information Needed]
- **Point of Contact:** [More Information Needed]

### Dataset Summary

This is a corpus of parallel sentences extracted from Wikipedia by Krzysztof Wołk and Krzysztof Marasek.

Tha dataset contains 20 languages and 36 bitexts.

To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs,
e.g.

```python
dataset = load_dataset(""opus_wikipedia"", lang1=""it"", lang2=""pl"")
```

You can find the valid pairs in Homepage section of Dataset Description: http://opus.nlpl.eu/Wikipedia.php


### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

The languages in the dataset are:
- ar
- bg
- cs
- de
- el
- en
- es
- fa
- fr
- he
- hu
- it
- nl
- pl
- pt
- ro
- ru
- sl
- tr
- vi

## Dataset Structure

### Data Instances

```
{
  'id': '0', 
  'translation': {
    ""ar"": ""* Encyclopaedia of Mathematics online encyclopaedia from Springer, Graduate-level reference work with over 8,000 entries, illuminating nearly 50,000 notions in mathematics."",
    ""en"": ""*Encyclopaedia of Mathematics online encyclopaedia from Springer, Graduate-level reference work with over 8,000 entries, illuminating nearly 50,000 notions in mathematics.""
  } 
}
```

### Data Fields

- `id` (`str`): Unique identifier of the parallel sentence for the pair of languages.
- `translation` (`dict`): Parallel sentences for the pair of languages.

### Data Splits

The dataset contains a single `train` split.

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

```bibtex
@article{WOLK2014126,
title = {Building Subject-aligned Comparable Corpora and Mining it for Truly Parallel Sentence Pairs},
journal = {Procedia Technology},
volume = {18},
pages = {126-132},
year = {2014},
note = {International workshop on Innovations in Information and Communication Science and Technology, IICST 2014, 3-5 September 2014, Warsaw, Poland},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314005453},
author = {Krzysztof Wołk and Krzysztof Marasek},
keywords = {Comparable corpora, machine translation, NLP},
}
```

```bibtex
@InProceedings{TIEDEMANN12.463,
  author = {J{\""o}rg Tiedemann},
  title = {Parallel Data, Tools and Interfaces in OPUS},
  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},
  year = {2012},
  month = {may},
  date = {23-25},
  address = {Istanbul, Turkey},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {978-2-9517408-7-7},
  language = {english}
}
```

### Contributions

Thanks to [@rkc007](https://github.com/rkc007) for adding this dataset.",High,5.0
Translation,Helsinki-NLP/qed_amara,5.0,125.0,2024-01-18 11:14:04+00:00,unknown,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- aa
- ab
- ae
- aeb
- af
- ak
- am
- an
- ar
- arq
- arz
- as
- ase
- ast
- av
- ay
- az
- ba
- be
- ber
- bg
- bh
- bi
- bm
- bn
- bnt
- bo
- br
- bs
- bug
- ca
- ce
- ceb
- ch
- cho
- cku
- cnh
- co
- cr
- cs
- cu
- cv
- cy
- da
- de
- dv
- dz
- ee
- efi
- el
- en
- eo
- es
- et
- eu
- fa
- ff
- fi
- fil
- fj
- fo
- fr
- ga
- gd
- gl
- gn
- gu
- ha
- hai
- haw
- haz
- hch
- he
- hi
- ho
- hr
- ht
- hu
- hup
- hus
- hy
- hz
- ia
- id
- ie
- ig
- ik
- inh
- io
- iro
- is
- it
- iu
- ja
- jv
- ka
- kar
- ki
- kj
- kk
- kl
- km
- kn
- ko
- kr
- ksh
- ku
- kv
- kw
- ky
- la
- lb
- lg
- li
- lkt
- lld
- ln
- lo
- lt
- ltg
- lu
- luo
- luy
- lv
- mad
- mfe
- mg
- mi
- mk
- ml
- mn
- mni
- moh
- mos
- mr
- ms
- mt
- mus
- my
- nb
- nci
- nd
- ne
- nl
- nn
- nso
- nv
- ny
- oc
- om
- or
- pa
- pam
- pap
- pi
- pl
- pnb
- prs
- ps
- pt
- qu
- rm
- rn
- ro
- ru
- rup
- rw
- sa
- sc
- scn
- sco
- sd
- sg
- sgn
- sh
- si
- sk
- sl
- sm
- sn
- so
- sq
- sr
- st
- sv
- sw
- szl
- ta
- te
- tet
- tg
- th
- ti
- tk
- tl
- tlh
- to
- tr
- ts
- tt
- tw
- ug
- uk
- umb
- ur
- uz
- ve
- vi
- vls
- vo
- wa
- wo
- xh
- yaq
- yi
- yo
- za
- zam
- zh
- zu
license:
- unknown
multilinguality:
- multilingual
size_categories:
- 100K<n<1M
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: null
pretty_name: QedAmara
dataset_info:
- config_name: ar-ko
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - ko
  splits:
  - name: train
    num_bytes: 79605277
    num_examples: 592589
  download_size: 23410393
  dataset_size: 79605277
- config_name: de-fr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - fr
  splits:
  - name: train
    num_bytes: 75861416
    num_examples: 407224
  download_size: 26579871
  dataset_size: 75861416
- config_name: es-it
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - it
  splits:
  - name: train
    num_bytes: 80650321
    num_examples: 447369
  download_size: 28344317
  dataset_size: 80650321
- config_name: en-ja
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ja
  splits:
  - name: train
    num_bytes: 86731218
    num_examples: 497531
  download_size: 29836171
  dataset_size: 86731218
- config_name: he-nl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - he
        - nl
  splits:
  - name: train
    num_bytes: 51448732
    num_examples: 273165
  download_size: 16642865
  dataset_size: 51448732
---

# Dataset Card for QedAmara

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** http://opus.nlpl.eu/QED.php
- **Repository:** None
- **Paper:** https://www.aclweb.org/anthology/L14-1675/
- **Leaderboard:** [More Information Needed]
- **Point of Contact:** [More Information Needed]

### Dataset Summary


To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.
You can find the valid pairs in Homepage section of Dataset Description: http://opus.nlpl.eu/QED.php
E.g.

`dataset = load_dataset(""qed_amara"", lang1=""cs"", lang2=""nb"")`


### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

The languages in the dataset are:
- aa
- ab
- ae
- aeb
- af
- aka: `ak`
- amh: `am`
- an
- ar
- arq
- arz
- as
- ase
- ast
- av
- ay
- az
- ba
- bam: `bm`
- be
- ber
- bg
- bh
- bi
- bn
- bnt
- bo
- br
- bs
- bug
- ca
- ce
- ceb
- ch
- cho
- cku
- cnh
- co
- cr
- cs
- cu
- cv
- cy
- da
- de
- dv
- dz
- ee
- efi
- el
- en
- eo
- es
- et
- eu
- fa
- ff
- fi
- fil
- fj
- fo
- fr
- ful: `ff`
- ga
- gd
- gl
- gn
- gu
- hai
- hau: `ha`
- haw
- haz
- hb: ?
- hch
- he
- hi
- ho
- hr
- ht
- hu
- hup
- hus
- hy
- hz
- ia
- ibo: `ig`
- id
- ie
- ik
- inh
- io
- iro
- is
- it
- iu
- ja
- jv
- ka
- kar
- kau: `kr`
- kik: `ki`
- kin: `rw`
- kj
- kk
- kl
- km
- kn
- ko
- ksh
- ku
- kv
- kw
- ky
- la
- lb
- lg
- li
- lin: `ln`
- lkt
- lld
- lo
- lt
- ltg
- lu
- luo
- luy
- lv
- mad
- mfe
- mi
- mk
- ml
- mlg: `mg`
- mn
- mni
- mo: Moldavian (deprecated tag; preferred value: Romanian; Moldavian; Moldovan (`ro`))
- moh
- mos
- mr
- ms
- mt
- mus
- my
- nb
- nci
- nd
- ne
- nl
- nn
- nso
- nv
- nya: `ny`
- oc
- or
- orm: `om`
- pam
- pan: `pa`
- pap
- pi
- pl
- pnb
- prs
- ps
- pt
- que: `qu`
- rm
- ro
- ru
- run: `rn`
- rup
- ry: ?
- sa
- sc
- scn
- sco
- sd
- sg
- sgn
- sh
- si
- sk
- sl
- sm
- sna: `sn`
- som: `so`
- sot: `st`
- sq
- sr
- srp: `sr`
- sv
- swa: `sw`
- szl
- ta
- te
- tet
- tg
- th
- tir: `ti`
- tk
- tl
- tlh
- to
- tr
- ts
- tt
- tw
- ug
- uk
- umb
- ur
- uz
- ve
- vi
- vls
- vo
- wa
- wol: `wo`
- xh
- yaq
- yi
- yor: `yo`
- za
- zam
- zh
- zul: `zu`

## Dataset Structure

### Data Instances

Here are some examples of questions and facts:


### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

[More Information Needed]

### Contributions

Thanks to [@abhishekkrthakur](https://github.com/abhishekkrthakur) for adding this dataset.",High,4.0
Translation,Helsinki-NLP/tanzil,4.0,119.0,2024-01-18 11:16:42+00:00,unknown,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- am
- ar
- az
- bg
- bn
- bs
- cs
- de
- dv
- en
- es
- fa
- fr
- ha
- hi
- id
- it
- ja
- ko
- ku
- ml
- ms
- nl
- 'no'
- pl
- pt
- ro
- ru
- sd
- so
- sq
- sv
- sw
- ta
- tg
- th
- tr
- tt
- ug
- ur
- uz
- zh
license:
- unknown
multilinguality:
- multilingual
size_categories:
- 100K<n<1M
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: null
pretty_name: tanzil
dataset_info:
- config_name: bg-en
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - bg
        - en
  splits:
  - name: train
    num_bytes: 34473016
    num_examples: 135477
  download_size: 9305292
  dataset_size: 34473016
- config_name: bn-hi
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - bn
        - hi
  splits:
  - name: train
    num_bytes: 18869103
    num_examples: 24942
  download_size: 3542740
  dataset_size: 18869103
- config_name: fa-sv
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - fa
        - sv
  splits:
  - name: train
    num_bytes: 29281634
    num_examples: 68601
  download_size: 8550826
  dataset_size: 29281634
- config_name: ru-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ru
        - zh
  splits:
  - name: train
    num_bytes: 59736143
    num_examples: 99779
  download_size: 16214659
  dataset_size: 59736143
- config_name: en-tr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - tr
  splits:
  - name: train
    num_bytes: 255891913
    num_examples: 1189967
  download_size: 82954694
  dataset_size: 255891913
---

# Dataset Card for tanzil

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** http://opus.nlpl.eu/Tanzil.php
- **Repository:** None
- **Paper:** http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf
- **Leaderboard:** [More Information Needed]
- **Point of Contact:** [More Information Needed]

### Dataset Summary


To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.
You can find the valid pairs in Homepage section of Dataset Description: http://opus.nlpl.eu/Tanzil.php
E.g.

`dataset = load_dataset(""tanzil"", lang1=""en"", lang2=""ru"")`


### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

[More Information Needed]

## Dataset Structure

### Data Instances

Here are some examples of questions and facts:


### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

[More Information Needed]

### Contributions

Thanks to [@abhishekkrthakur](https://github.com/abhishekkrthakur) for adding this dataset.",High,4.0
Translation,community-datasets/tapaco,46.0,3733.0,2024-06-26 07:42:27+00:00,cc-by-2.0,1.0,131 MB,137363456.0,131 MB,137363456.0,3852384,none,none,"---
annotations_creators:
- machine-generated
language_creators:
- crowdsourced
language:
- af
- ar
- az
- be
- ber
- bg
- bn
- br
- ca
- cbk
- cmn
- cs
- da
- de
- el
- en
- eo
- es
- et
- eu
- fi
- fr
- gl
- gos
- he
- hi
- hr
- hu
- hy
- ia
- id
- ie
- io
- is
- it
- ja
- jbo
- kab
- ko
- kw
- la
- lfn
- lt
- mk
- mr
- nb
- nds
- nl
- orv
- ota
- pes
- pl
- pt
- rn
- ro
- ru
- sl
- sr
- sv
- tk
- tl
- tlh
- tok
- tr
- tt
- ug
- uk
- ur
- vi
- vo
- war
- wuu
- yue
license:
- cc-by-2.0
multilinguality:
- multilingual
size_categories:
- 100K<n<1M
- 10K<n<100K
- 1K<n<10K
- 1M<n<10M
- n<1K
source_datasets:
- extended|other-tatoeba
task_categories:
- text2text-generation
- translation
- text-classification
task_ids:
- semantic-similarity-classification
paperswithcode_id: tapaco
pretty_name: TaPaCo Corpus
config_names:
- af
- all_languages
- ar
- az
- be
- ber
- bg
- bn
- br
- ca
- cbk
- cmn
- cs
- da
- de
- el
- en
- eo
- es
- et
- eu
- fi
- fr
- gl
- gos
- he
- hi
- hr
- hu
- hy
- ia
- id
- ie
- io
- is
- it
- ja
- jbo
- kab
- ko
- kw
- la
- lfn
- lt
- mk
- mr
- nb
- nds
- nl
- orv
- ota
- pes
- pl
- pt
- rn
- ro
- ru
- sl
- sr
- sv
- tk
- tl
- tlh
- tok
- tr
- tt
- ug
- uk
- ur
- vi
- vo
- war
- wuu
- yue
tags:
- paraphrase-generation
dataset_info:
- config_name: af
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 21187
    num_examples: 307
  download_size: 11781
  dataset_size: 21187
- config_name: all_languages
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 162796380
    num_examples: 1926192
  download_size: 65568453
  dataset_size: 162796380
- config_name: ar
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 546168
    num_examples: 6446
  download_size: 220221
  dataset_size: 546168
- config_name: az
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 44429
    num_examples: 624
  download_size: 20418
  dataset_size: 44429
- config_name: be
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 140344
    num_examples: 1512
  download_size: 58566
  dataset_size: 140344
- config_name: ber
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 5118396
    num_examples: 67484
  download_size: 1813469
  dataset_size: 5118396
- config_name: bg
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 590503
    num_examples: 6324
  download_size: 219682
  dataset_size: 590503
- config_name: bn
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 146622
    num_examples: 1440
  download_size: 46005
  dataset_size: 146622
- config_name: br
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 177887
    num_examples: 2536
  download_size: 65451
  dataset_size: 177887
- config_name: ca
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 39372
    num_examples: 518
  download_size: 20977
  dataset_size: 39372
- config_name: cbk
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 19372
    num_examples: 262
  download_size: 11222
  dataset_size: 19372
- config_name: cmn
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 964450
    num_examples: 12549
  download_size: 440938
  dataset_size: 964450
- config_name: cs
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 482260
    num_examples: 6659
  download_size: 213014
  dataset_size: 482260
- config_name: da
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 848822
    num_examples: 11220
  download_size: 362445
  dataset_size: 848822
- config_name: de
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 10592961
    num_examples: 125091
  download_size: 4618002
  dataset_size: 10592961
- config_name: el
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 925990
    num_examples: 10072
  download_size: 332949
  dataset_size: 925990
- config_name: en
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 15069837
    num_examples: 158053
  download_size: 5633416
  dataset_size: 15069837
- config_name: eo
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 16810293
    num_examples: 207105
  download_size: 7386804
  dataset_size: 16810293
- config_name: es
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 6850847
    num_examples: 85064
  download_size: 3037352
  dataset_size: 6850847
- config_name: et
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 17095
    num_examples: 241
  download_size: 10628
  dataset_size: 17095
- config_name: eu
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 42670
    num_examples: 573
  download_size: 21506
  dataset_size: 42670
- config_name: fi
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 2520039
    num_examples: 31753
  download_size: 1066381
  dataset_size: 2520039
- config_name: fr
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 9481042
    num_examples: 116733
  download_size: 4010846
  dataset_size: 9481042
- config_name: gl
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 26519
    num_examples: 351
  download_size: 15428
  dataset_size: 26519
- config_name: gos
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 18410
    num_examples: 279
  download_size: 10252
  dataset_size: 18410
- config_name: he
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 6024121
    num_examples: 68350
  download_size: 2352588
  dataset_size: 6024121
- config_name: hi
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 209350
    num_examples: 1913
  download_size: 71529
  dataset_size: 209350
- config_name: hr
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 36606
    num_examples: 505
  download_size: 19165
  dataset_size: 36606
- config_name: hu
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 5289386
    num_examples: 67964
  download_size: 2387466
  dataset_size: 5289386
- config_name: hy
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 49198
    num_examples: 603
  download_size: 19715
  dataset_size: 49198
- config_name: ia
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 194003
    num_examples: 2548
  download_size: 85581
  dataset_size: 194003
- config_name: id
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 124536
    num_examples: 1602
  download_size: 54675
  dataset_size: 124536
- config_name: ie
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 31924
    num_examples: 488
  download_size: 15718
  dataset_size: 31924
- config_name: io
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 33860
    num_examples: 480
  download_size: 17288
  dataset_size: 33860
- config_name: is
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 132030
    num_examples: 1641
  download_size: 58439
  dataset_size: 132030
- config_name: it
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 15073110
    num_examples: 198919
  download_size: 5419790
  dataset_size: 15073110
- config_name: ja
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 4314263
    num_examples: 44267
  download_size: 1736954
  dataset_size: 4314263
- config_name: jbo
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 201532
    num_examples: 2704
  download_size: 90713
  dataset_size: 201532
- config_name: kab
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 1210987
    num_examples: 15944
  download_size: 471650
  dataset_size: 1210987
- config_name: ko
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 40426
    num_examples: 503
  download_size: 20200
  dataset_size: 40426
- config_name: kw
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 88545
    num_examples: 1328
  download_size: 35630
  dataset_size: 88545
- config_name: la
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 485717
    num_examples: 6889
  download_size: 201849
  dataset_size: 485717
- config_name: lfn
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 203351
    num_examples: 2313
  download_size: 91808
  dataset_size: 203351
- config_name: lt
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 599134
    num_examples: 8042
  download_size: 263103
  dataset_size: 599134
- config_name: mk
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 1240121
    num_examples: 14678
  download_size: 466182
  dataset_size: 1240121
- config_name: mr
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 1838857
    num_examples: 16413
  download_size: 590051
  dataset_size: 1838857
- config_name: nb
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 85339
    num_examples: 1094
  download_size: 39494
  dataset_size: 85339
- config_name: nds
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 194989
    num_examples: 2633
  download_size: 85174
  dataset_size: 194989
- config_name: nl
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 1790879
    num_examples: 23561
  download_size: 779446
  dataset_size: 1790879
- config_name: orv
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 40452
    num_examples: 471
  download_size: 20411
  dataset_size: 40452
- config_name: ota
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 44964
    num_examples: 486
  download_size: 23354
  dataset_size: 44964
- config_name: pes
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 433374
    num_examples: 4285
  download_size: 180609
  dataset_size: 433374
- config_name: pl
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 1722092
    num_examples: 22391
  download_size: 782825
  dataset_size: 1722092
- config_name: pt
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 6140922
    num_examples: 78430
  download_size: 2618549
  dataset_size: 6140922
- config_name: rn
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 47355
    num_examples: 648
  download_size: 20827
  dataset_size: 47355
- config_name: ro
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 162923
    num_examples: 2092
  download_size: 76736
  dataset_size: 162923
- config_name: ru
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 24539835
    num_examples: 251263
  download_size: 9240710
  dataset_size: 24539835
- config_name: sl
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 49578
    num_examples: 706
  download_size: 24925
  dataset_size: 49578
- config_name: sr
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 667276
    num_examples: 8175
  download_size: 305343
  dataset_size: 667276
- config_name: sv
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 509852
    num_examples: 7005
  download_size: 213626
  dataset_size: 509852
- config_name: tk
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 95015
    num_examples: 1165
  download_size: 43014
  dataset_size: 95015
- config_name: tl
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 76027
    num_examples: 1017
  download_size: 36676
  dataset_size: 76027
- config_name: tlh
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 185277
    num_examples: 2804
  download_size: 77664
  dataset_size: 185277
- config_name: toki
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 310832
    num_examples: 3738
  download_size: 120515
  dataset_size: 310832
- config_name: tr
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 11270678
    num_examples: 142088
  download_size: 4811347
  dataset_size: 11270678
- config_name: tt
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 277237
    num_examples: 2398
  download_size: 133537
  dataset_size: 277237
- config_name: ug
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 118442
    num_examples: 1183
  download_size: 45577
  dataset_size: 118442
- config_name: uk
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 4885485
    num_examples: 54431
  download_size: 1814842
  dataset_size: 4885485
- config_name: ur
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 24043
    num_examples: 252
  download_size: 12712
  dataset_size: 24043
- config_name: vi
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 84741
    num_examples: 962
  download_size: 38362
  dataset_size: 84741
- config_name: vo
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 22132
    num_examples: 328
  download_size: 12011
  dataset_size: 22132
- config_name: war
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 25727
    num_examples: 327
  download_size: 14217
  dataset_size: 25727
- config_name: wuu
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 31608
    num_examples: 408
  download_size: 17221
  dataset_size: 31608
- config_name: yue
  features:
  - name: paraphrase_set_id
    dtype: string
  - name: sentence_id
    dtype: string
  - name: paraphrase
    dtype: string
  - name: lists
    sequence: string
  - name: tags
    sequence: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 42734
    num_examples: 561
  download_size: 21047
  dataset_size: 42734
configs:
- config_name: af
  data_files:
  - split: train
    path: af/train-*
- config_name: all_languages
  data_files:
  - split: train
    path: all_languages/train-*
  default: true
- config_name: ar
  data_files:
  - split: train
    path: ar/train-*
- config_name: az
  data_files:
  - split: train
    path: az/train-*
- config_name: be
  data_files:
  - split: train
    path: be/train-*
- config_name: ber
  data_files:
  - split: train
    path: ber/train-*
- config_name: bg
  data_files:
  - split: train
    path: bg/train-*
- config_name: bn
  data_files:
  - split: train
    path: bn/train-*
- config_name: br
  data_files:
  - split: train
    path: br/train-*
- config_name: ca
  data_files:
  - split: train
    path: ca/train-*
- config_name: cbk
  data_files:
  - split: train
    path: cbk/train-*
- config_name: cmn
  data_files:
  - split: train
    path: cmn/train-*
- config_name: cs
  data_files:
  - split: train
    path: cs/train-*
- config_name: da
  data_files:
  - split: train
    path: da/train-*
- config_name: de
  data_files:
  - split: train
    path: de/train-*
- config_name: el
  data_files:
  - split: train
    path: el/train-*
- config_name: en
  data_files:
  - split: train
    path: en/train-*
- config_name: eo
  data_files:
  - split: train
    path: eo/train-*
- config_name: es
  data_files:
  - split: train
    path: es/train-*
- config_name: et
  data_files:
  - split: train
    path: et/train-*
- config_name: eu
  data_files:
  - split: train
    path: eu/train-*
- config_name: fi
  data_files:
  - split: train
    path: fi/train-*
- config_name: fr
  data_files:
  - split: train
    path: fr/train-*
- config_name: gl
  data_files:
  - split: train
    path: gl/train-*
- config_name: gos
  data_files:
  - split: train
    path: gos/train-*
- config_name: he
  data_files:
  - split: train
    path: he/train-*
- config_name: hi
  data_files:
  - split: train
    path: hi/train-*
- config_name: hr
  data_files:
  - split: train
    path: hr/train-*
- config_name: hu
  data_files:
  - split: train
    path: hu/train-*
- config_name: hy
  data_files:
  - split: train
    path: hy/train-*
- config_name: ia
  data_files:
  - split: train
    path: ia/train-*
- config_name: id
  data_files:
  - split: train
    path: id/train-*
- config_name: ie
  data_files:
  - split: train
    path: ie/train-*
- config_name: io
  data_files:
  - split: train
    path: io/train-*
- config_name: is
  data_files:
  - split: train
    path: is/train-*
- config_name: it
  data_files:
  - split: train
    path: it/train-*
- config_name: ja
  data_files:
  - split: train
    path: ja/train-*
- config_name: jbo
  data_files:
  - split: train
    path: jbo/train-*
- config_name: kab
  data_files:
  - split: train
    path: kab/train-*
- config_name: ko
  data_files:
  - split: train
    path: ko/train-*
- config_name: kw
  data_files:
  - split: train
    path: kw/train-*
- config_name: la
  data_files:
  - split: train
    path: la/train-*
- config_name: lfn
  data_files:
  - split: train
    path: lfn/train-*
- config_name: lt
  data_files:
  - split: train
    path: lt/train",High,5.0
Translation,Helsinki-NLP/tatoeba,48.0,1830.0,2024-01-18 11:16:48+00:00,cc-by-2.0,4.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- ab
- acm
- ady
- af
- afb
- afh
- aii
- ain
- ajp
- akl
- aln
- am
- an
- ang
- aoz
- apc
- ar
- arq
- ary
- arz
- as
- ast
- avk
- awa
- ayl
- az
- ba
- bal
- bar
- be
- ber
- bg
- bho
- bjn
- bm
- bn
- bo
- br
- brx
- bs
- bua
- bvy
- bzt
- ca
- cay
- cbk
- ce
- ceb
- ch
- chg
- chn
- cho
- chr
- cjy
- ckb
- ckt
- cmn
- co
- code
- cpi
- crh
- crk
- cs
- csb
- cv
- cy
- da
- de
- dng
- drt
- dsb
- dtp
- dv
- dws
- ee
- egl
- el
- emx
- en
- enm
- eo
- es
- et
- eu
- ext
- fi
- fj
- fkv
- fo
- fr
- frm
- fro
- frr
- fuc
- fur
- fuv
- fy
- ga
- gag
- gan
- gbm
- gcf
- gd
- gil
- gl
- gn
- gom
- gos
- got
- grc
- gsw
- gu
- gv
- ha
- hak
- haw
- hbo
- he
- hi
- hif
- hil
- hnj
- hoc
- hr
- hrx
- hsb
- hsn
- ht
- hu
- hy
- ia
- iba
- id
- ie
- ig
- ii
- ike
- ilo
- io
- is
- it
- izh
- ja
- jam
- jbo
- jdt
- jpa
- jv
- ka
- kaa
- kab
- kam
- kek
- kha
- kjh
- kk
- kl
- km
- kmr
- kn
- ko
- koi
- kpv
- krc
- krl
- ksh
- ku
- kum
- kw
- kxi
- ky
- la
- laa
- lad
- lb
- ldn
- lfn
- lg
- lij
- liv
- lkt
- lld
- lmo
- ln
- lo
- lt
- ltg
- lut
- lv
- lzh
- lzz
- mad
- mai
- max
- mdf
- mfe
- mg
- mgm
- mh
- mhr
- mi
- mic
- min
- mk
- ml
- mn
- mni
- mnw
- moh
- mr
- mt
- mvv
- mwl
- mww
- my
- myv
- na
- nah
- nan
- nb
- nch
- nds
- ngt
- ngu
- niu
- nl
- nlv
- nn
- nog
- non
- nov
- npi
- nst
- nus
- nv
- ny
- nys
- oar
- oc
- ofs
- ood
- or
- orv
- os
- osp
- ota
- otk
- pa
- pag
- pal
- pam
- pap
- pau
- pcd
- pdc
- pes
- phn
- pi
- pl
- pms
- pnb
- ppl
- prg
- ps
- pt
- qu
- quc
- qya
- rap
- rif
- rm
- rn
- ro
- rom
- ru
- rue
- rw
- sa
- sah
- sc
- scn
- sco
- sd
- sdh
- se
- sg
- sgs
- shs
- shy
- si
- sjn
- sl
- sm
- sma
- sn
- so
- sq
- sr
- stq
- su
- sux
- sv
- swg
- swh
- syc
- ta
- te
- tet
- tg
- th
- thv
- ti
- tig
- tk
- tl
- tlh
- tly
- tmr
- tmw
- tn
- to
- toi
- tok
- tpi
- tpw
- tr
- ts
- tt
- tts
- tvl
- ty
- tyv
- tzl
- udm
- ug
- uk
- umb
- ur
- uz
- vec
- vep
- vi
- vo
- vro
- wa
- war
- wo
- wuu
- xal
- xh
- xqa
- yi
- yo
- yue
- zlm
- zsm
- zu
- zza
license:
- cc-by-2.0
multilinguality:
- multilingual
size_categories:
- 10K<n<100K
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: tatoeba
pretty_name: Tatoeba
dataset_info:
- config_name: en-mr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - mr
  splits:
  - name: train
    num_bytes: 6190484
    num_examples: 53462
  download_size: 1436200
  dataset_size: 6190484
- config_name: eo-nl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - eo
        - nl
  splits:
  - name: train
    num_bytes: 8150048
    num_examples: 93650
  download_size: 3020382
  dataset_size: 8150048
- config_name: es-pt
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - pt
  splits:
  - name: train
    num_bytes: 6180464
    num_examples: 67782
  download_size: 2340361
  dataset_size: 6180464
- config_name: fr-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - ru
  splits:
  - name: train
    num_bytes: 19775390
    num_examples: 195161
  download_size: 5509784
  dataset_size: 19775390
- config_name: es-gl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - gl
  splits:
  - name: train
    num_bytes: 287683
    num_examples: 3135
  download_size: 128506
  dataset_size: 287683
---
# Dataset Card for Tatoeba

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** http://opus.nlpl.eu/Tatoeba.php
- **Repository:** None
- **Paper:** http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf
- **Leaderboard:** [More Information Needed]
- **Point of Contact:** [More Information Needed]

### Dataset Summary

Tatoeba is a collection of sentences and translations.

To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.
You can find the valid pairs in Homepage section of Dataset Description: http://opus.nlpl.eu/Tatoeba.php
E.g.

`dataset = load_dataset(""tatoeba"", lang1=""en"", lang2=""he"")`

The default date is v2021-07-22, but you can also change the date with

`dataset = load_dataset(""tatoeba"", lang1=""en"", lang2=""he"", date=""v2020-11-09"")`


### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

The languages in the dataset are:
- ab
- acm
- ady
- af
- afb
- afh
- aii
- ain
- ajp
- akl
- aln
- am
- an
- ang
- aoz
- apc
- ar
- arq
- ary
- arz
- as
- ast
- avk
- awa
- ayl
- az
- ba
- bal
- bar
- be
- ber
- bg
- bho
- bjn
- bm
- bn
- bo
- br
- brx
- bs
- bua
- bvy
- bzt
- ca
- cay
- cbk
- ce
- ceb
- ch
- chg
- chn
- cho
- chr
- cjy
- ckb
- ckt
- cmn
- co
- code
- cpi
- crh
- crk
- cs
- csb
- cv
- cy
- da
- de
- dng
- drt
- dsb
- dtp
- dv
- dws
- ee
- egl
- el
- emx
- en
- enm
- eo
- es
- et
- eu
- ext
- fi
- fj
- fkv
- fo
- fr
- frm
- fro
- frr
- fuc
- fur
- fuv
- fy
- ga
- gag
- gan
- gbm
- gcf
- gd
- gil
- gl
- gn
- gom
- gos
- got
- grc
- gsw
- gu
- gv
- ha
- hak
- haw
- hbo
- he
- hi
- hif
- hil
- hnj
- hoc
- hr
- hrx
- hsb
- hsn
- ht
- hu
- hy
- ia
- iba
- id
- ie
- ig
- ii
- ike
- ilo
- io
- is
- it
- izh
- ja
- jam
- jbo
- jdt
- jpa
- jv
- ka
- kaa
- kab
- kam
- kek
- kha
- kjh
- kk
- kl
- km
- kmr
- kn
- ko
- koi
- kpv
- krc
- krl
- ksh
- ku
- kum
- kw
- kxi
- ky
- kzj: Coastal Kadazan (deprecated tag; preferred value: Kadazan Dusun; Central Dusun (`dtp`))
- la
- laa
- lad
- lb
- ldn
- lfn
- lg
- lij
- liv
- lkt
- lld
- lmo
- ln
- lo
- lt
- ltg
- lut
- lv
- lzh
- lzz
- mad
- mai
- max
- mdf
- mfe
- mg
- mgm
- mh
- mhr
- mi
- mic
- min
- mk
- ml
- mn
- mni
- mnw
- moh
- mr
- mt
- mvv
- mwl
- mww
- my
- myv
- na
- nah
- nan
- nb
- nch
- nds
- ngt
- ngu
- niu
- nl
- nlv
- nn
- nog
- non
- nov
- npi
- nst
- nus
- nv
- ny
- nys
- oar
- oc
- ofs
- ood
- or
- orv
- os
- osp
- ota
- otk
- pa
- pag
- pal
- pam
- pap
- pau
- pcd
- pdc
- pes
- phn
- pi
- pl
- pms
- pnb
- ppl
- prg
- ps
- pt
- qu
- quc
- qya
- rap
- rif
- rm
- rn
- ro
- rom
- ru
- rue
- rw
- sa
- sah
- sc
- scn
- sco
- sd
- sdh
- se
- sg
- sgs
- shs
- shy
- si
- sjn
- sl
- sm
- sma
- sn
- so
- sq
- sr
- stq
- su
- sux
- sv
- swg
- swh
- syc
- ta
- te
- tet
- tg
- th
- thv
- ti
- tig
- tk
- tl
- tlh
- tly
- tmr
- tmw
- tn
- to
- toi
- tok
- tpi
- tpw
- tr
- ts
- tt
- tts
- tvl
- ty
- tyv
- tzl
- udm
- ug
- uk
- umb
- ur
- uz
- vec
- vep
- vi
- vo
- vro
- wa
- war
- wo
- wuu
- xal
- xh
- xqa
- yi
- yo
- yue
- zlm
- zsm
- zu
- zza

## Dataset Structure

### Data Instances

[More Information Needed]

### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

[More Information Needed]

### Contributions

Thanks to [@abhishekkrthakur](https://github.com/abhishekkrthakur) for adding this dataset.",High,4.0
Translation,Helsinki-NLP/ted_iwlst2013,2.0,135.0,2024-01-18 11:16:53+00:00,unknown,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- ar
- de
- en
- es
- fa
- fr
- it
- nl
- pl
- pt
- ro
- ru
- sl
- tr
- zh
license:
- unknown
multilinguality:
- multilingual
size_categories:
- 100K<n<1M
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: null
pretty_name: TedIwlst2013
dataset_info:
- config_name: ar-en
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - en
  splits:
  - name: train
    num_bytes: 37413446
    num_examples: 152838
  download_size: 12065234
  dataset_size: 37413446
- config_name: de-en
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - en
  splits:
  - name: train
    num_bytes: 30295518
    num_examples: 143836
  download_size: 10931406
  dataset_size: 30295518
- config_name: en-es
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - es
  splits:
  - name: train
    num_bytes: 32522545
    num_examples: 157895
  download_size: 11642092
  dataset_size: 32522545
- config_name: en-fa
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fa
  splits:
  - name: train
    num_bytes: 22228781
    num_examples: 80510
  download_size: 6579696
  dataset_size: 22228781
- config_name: en-fr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fr
  splits:
  - name: train
    num_bytes: 34355481
    num_examples: 160420
  download_size: 12061420
  dataset_size: 34355481
- config_name: en-it
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - it
  splits:
  - name: train
    num_bytes: 32916537
    num_examples: 159391
  download_size: 11774644
  dataset_size: 32916537
- config_name: en-nl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - nl
  splits:
  - name: train
    num_bytes: 29679822
    num_examples: 145951
  download_size: 10712032
  dataset_size: 29679822
- config_name: en-pl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - pl
  splits:
  - name: train
    num_bytes: 29776339
    num_examples: 149120
  download_size: 10999482
  dataset_size: 29776339
- config_name: en-pt
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - pt
  splits:
  - name: train
    num_bytes: 32179607
    num_examples: 155995
  download_size: 11493053
  dataset_size: 32179607
- config_name: en-ro
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ro
  splits:
  - name: train
    num_bytes: 32958421
    num_examples: 158483
  download_size: 11936172
  dataset_size: 32958421
- config_name: en-ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ru
  splits:
  - name: train
    num_bytes: 36529465
    num_examples: 133660
  download_size: 11167700
  dataset_size: 36529465
- config_name: en-sl
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - sl
  splits:
  - name: train
    num_bytes: 2831344
    num_examples: 14960
  download_size: 1060712
  dataset_size: 2831344
- config_name: en-tr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - tr
  splits:
  - name: train
    num_bytes: 28016103
    num_examples: 137028
  download_size: 10038531
  dataset_size: 28016103
- config_name: en-zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - zh
  splits:
  - name: train
    num_bytes: 30205477
    num_examples: 154579
  download_size: 11714497
  dataset_size: 30205477
config_names:
- ar-en
- de-en
- en-es
- en-fa
- en-fr
- en-it
- en-nl
- en-pl
- en-pt
- en-ro
- en-ru
- en-sl
- en-tr
- en-zh
---

# Dataset Card for TedIwlst2013

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** http://opus.nlpl.eu/TED2013.php
- **Repository:** None
- **Paper:** hhttp://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf
- **Leaderboard:** None
- **Point of Contact:** [More Information Needed]

### Dataset Summary

[More Information Needed]

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

[More Information Needed]

## Dataset Structure

### Data Instances

[More Information Needed]

### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

[More Information Needed]

### Contributions

Thanks to [@abhishekkrthakur](https://github.com/abhishekkrthakur) for adding this dataset.",High,4.0
Translation,IWSLT/ted_talks_iwslt,19.0,545.0,2024-01-18 11:16:58+00:00,cc-by-nc-nd-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
annotations_creators:
- expert-generated
language_creators:
- crowdsourced
- expert-generated
language:
- af
- am
- ar
- arq
- art
- as
- ast
- az
- be
- bg
- bi
- bn
- bo
- bs
- ca
- ceb
- cnh
- cs
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fil
- fr
- ga
- gl
- gu
- ha
- he
- hi
- hr
- ht
- hu
- hup
- hy
- id
- ig
- inh
- is
- it
- ja
- ka
- kk
- km
- kn
- ko
- ku
- ky
- la
- lb
- lo
- lt
- ltg
- lv
- mg
- mk
- ml
- mn
- mr
- ms
- mt
- my
- nb
- ne
- nl
- nn
- oc
- pa
- pl
- ps
- pt
- ro
- ru
- rup
- sh
- si
- sk
- sl
- so
- sq
- sr
- sv
- sw
- szl
- ta
- te
- tg
- th
- tl
- tlh
- tr
- tt
- ug
- uk
- ur
- uz
- vi
- zh
language_bcp47:
- art-x-bork
- fr-CA
- pt-BR
- zh-CN
- zh-TW
license:
- cc-by-nc-nd-4.0
multilinguality:
- translation
size_categories:
- 1K<n<10K
- n<1K
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: null
pretty_name: Web Inventory of Transcribed & Translated (WIT) Ted Talks
dataset_info:
- config_name: eu_ca_2014
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - eu
        - ca
  splits:
  - name: train
    num_bytes: 15192
    num_examples: 44
  download_size: 1666674366
  dataset_size: 15192
- config_name: eu_ca_2015
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - eu
        - ca
  splits:
  - name: train
    num_bytes: 18768
    num_examples: 52
  download_size: 1666674366
  dataset_size: 18768
- config_name: eu_ca_2016
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - eu
        - ca
  splits:
  - name: train
    num_bytes: 19506
    num_examples: 54
  download_size: 1666674366
  dataset_size: 19506
- config_name: nl_en_2014
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - en
  splits:
  - name: train
    num_bytes: 1035545
    num_examples: 2966
  download_size: 1666674366
  dataset_size: 1035545
- config_name: nl_en_2015
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - en
  splits:
  - name: train
    num_bytes: 1292610
    num_examples: 3550
  download_size: 1666674366
  dataset_size: 1292610
- config_name: nl_en_2016
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - en
  splits:
  - name: train
    num_bytes: 1434207
    num_examples: 3852
  download_size: 1666674366
  dataset_size: 1434207
- config_name: nl_hi_2014
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - hi
  splits:
  - name: train
    num_bytes: 214870
    num_examples: 367
  download_size: 1666674366
  dataset_size: 214870
- config_name: nl_hi_2015
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - hi
  splits:
  - name: train
    num_bytes: 252192
    num_examples: 421
  download_size: 1666674366
  dataset_size: 252192
- config_name: nl_hi_2016
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - hi
  splits:
  - name: train
    num_bytes: 310922
    num_examples: 496
  download_size: 1666674366
  dataset_size: 310922
- config_name: de_ja_2014
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - ja
  splits:
  - name: train
    num_bytes: 1074403
    num_examples: 2536
  download_size: 1666674366
  dataset_size: 1074403
- config_name: de_ja_2015
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - ja
  splits:
  - name: train
    num_bytes: 1442047
    num_examples: 3247
  download_size: 1666674366
  dataset_size: 1442047
- config_name: de_ja_2016
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - ja
  splits:
  - name: train
    num_bytes: 1630729
    num_examples: 3590
  download_size: 1666674366
  dataset_size: 1630729
- config_name: fr-ca_hi_2014
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr-ca
        - hi
  splits:
  - name: train
    num_bytes: 74472
    num_examples: 127
  download_size: 1666674366
  dataset_size: 74472
- config_name: fr-ca_hi_2015
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr-ca
        - hi
  splits:
  - name: train
    num_bytes: 82448
    num_examples: 141
  download_size: 1666674366
  dataset_size: 82448
- config_name: fr-ca_hi_2016
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr-ca
        - hi
  splits:
  - name: train
    num_bytes: 93425
    num_examples: 156
  download_size: 1666674366
  dataset_size: 93425
config_names:
- de_ja_2014
- de_ja_2015
- de_ja_2016
- eu_ca_2014
- eu_ca_2015
- eu_ca_2016
- fr-ca_hi_2014
- fr-ca_hi_2015
- fr-ca_hi_2016
- nl_en_2014
- nl_en_2015
- nl_en_2016
- nl_hi_2014
- nl_hi_2015
- nl_hi_2016
---

# Dataset Card for Web Inventory of Transcribed & Translated(WIT) Ted Talks

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://wit3.fbk.eu/home
- **Repository:** https://drive.google.com/file/d/1Cz1Un9p8Xn9IpEMMrg2kXSDt0dnjxc4z/view?usp=sharing
- **Paper:** https://www.aclweb.org/anthology/2012.eamt-1.60.pdf
- **Leaderboard:** [Needs More Information]
- **Point of Contact:** [Mauro Cettolo](mailto:cettolo@fbk.eu)
[Roldano Cattoni](mailto:cattoni@fbk.eu)


### Dataset Summary

The Web Inventory Talk is a collection of the original Ted talks and their translated version. The translations are available in more than 109+ languages, though the distribution is not uniform. 

To load a language pair which isn't part of the config, all you need to do is specify the language code as pairs.
E.g.

`dataset = load_dataset(""ted_talks_iwslt"", language_pair=(""it"", ""pl""), year=""2014"")`

The full list of languages is: 'af', 'am', 'ar', 'arq', 'art-x-bork', 'as', 'ast', 'az', 'be', 'bg', 'bi', 'bn', 'bo', 'bs', 'ca', 'ceb', 'cnh', 'cs', 'da', 'de', 'el', 'en', 'eo', 'es', 'et', 'eu', 'fa', 'fi', 'fil', 'fr', 'fr-ca', 'ga', 'gl', 'gu', 'ha', 'he', 'hi', 'hr', 'ht', 'hu', 'hup', 'hy', 'id', 'ig', 'inh', 'is', 'it', 'ja', 'ka', 'kk', 'km', 'kn', 'ko', 'ku', 'ky', 'la', 'lb', 'lo', 'lt', 'ltg', 'lv', 'mg', 'mk', 'ml', 'mn', 'mr', 'ms', 'mt', 'my', 'nb', 'ne', 'nl', 'nn', 'oc', 'pa', 'pl', 'ps', 'pt', 'pt-br', 'ro', 'ru', 'rup', 'sh', 'si', 'sk', 'sl', 'so', 'sq', 'sr', 'srp', 'sv', 'sw', 'szl', 'ta', 'te', 'tg', 'th', 'tl', 'tlh', 'tr', 'tt', 'ug', 'uk', 'ur', 'uz', 'vi', 'zh', 'zh-cn', 'zh-tw'.

The full list of years is: '2014', '2015', '2016'.

### Supported Tasks and Leaderboards

machine learning task, language modeling and generation

### Languages

Ted talks are mostly held in English (`en`). Almost all of the talks have been translated, by volunteers, into Arabic, Bulgarian, Chinese (simplified), French, Italian, Korean, Portuguese (Brazil) and Spanish. For about 70 other languages, the number of translated talks ranges from several hundreds (e.g. such as other Dutch, German, Hebrew, Romanian) to one (e.g. Hausa, Hupa, Bislama, Ingush, Maltese).

The languages in the dataset are:
- af
- am
- ar
- arq
- art
- as
- ast
- az
- be
- bg
- bi
- bn
- bo
- bs
- ca
- ceb
- cnh
- cs
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fil
- fr
- ga
- gl
- gu
- ha
- he
- hi
- hr
- ht
- hu
- hup
- hy
- id
- ig
- inh
- is
- it
- ja
- ka
- kk
- km
- kn
- ko
- ku
- ky
- la
- lb
- lo
- lt
- ltg
- lv
- mg
- mk
- ml
- mn
- mr
- ms
- mt
- my
- nb
- ne
- nl
- nn
- oc
- pa
- pl
- ps
- pt
- ro
- ru
- rup
- sh
- si
- sk
- sl
- so
- sq
- sr
- srp: Serbian (`sr`)
- sv
- sw
- szl
- ta
- te
- tg
- th
- tl
- tlh
- tr
- tt
- ug
- uk
- ur
- uz
- vi
- zh

## Dataset Structure

### Data Instances

One example from the dataset is:

```
{'translation': {'hi': 'जब मार्च २०१४ में इबोला का प्रकोप छाया, पर्डिस सबेटी और उनकी टीम को वाइरस के जीनोम का अनुक्रमण करना था, सीखना था कि यह कैसे परवतिर्त होते हैं और फैलते हैं। सबेटी ने तुरंत ही अपने अनुसंधान को वेब में जारी किया, ताकि दुनिया भर के वाइरस ट्रैकर्स और वैज्ञानिक इस तत्काल लड़ाई में शामिल हो सकें। इस बातचीत में, वह दिखाती हैं कि सबका सहयोग ही कुंजी है वाइरस को रोकने के लिए--और लड़ने के लिए आगे आने वाले हमलों से। सबेटी ने कहा,""हमने खुले तौर पर काम किया, साझा किया और साथ काम किया""। ""हमे दुनिया को एक वाइरस के विनाश से नहीं, पर अरबों दिलों और दिमागों की एकता से परिभाषित करना है""।',
  'nl': 'Toen Ebola in maart 2014 uitbrak, zijn Pardis Sabeti en haar team aan het werk gegaan om het genoom in kaart te brengen. Zo ontdekten ze hoe het virus zich verspreidde en muteerde. Sabeti zette direct haar onderzoek op het internet, zodat wereldwijd virus-jagers en wetenschappers mee konden werken aan de strijd. In deze talk laat ze zien hoe die openheid geholpen heeft bij het stoppen van het virus en hoe het kan helpen bij de strijd tegen het volgende virus. ""We moesten transparant werken, delen en samenwerken"". Sabeti zegt:""Laat de wereld niet ten onder gaan aan een virus, maar verlicht worden door miljoenen harten en geesten die samenwerken.""'}}
```

The original XML files are formatted like this example:


```
<file id=""1"">
  <head>
    <url>http://www.ted.com/talks/ryan_holladay_to_hear_this_music_you_have_to_be_there_literally.html</url>
    <pagesize>66634</pagesize>
    <dtime>Sun Jan 12 15:17:32 CET 2014</dtime>
    <content-type>text/html; charset=utf-8</content-type>
    <encoding>utf-8</encoding>
    <videourl>http://download.ted.com/talks/RyanHolladay_2013S.mp4</videourl>
    <videopath>talks/RyanHolladay_2013S.mp4</videopath>
    <transcription>
      <seekvideo id=""2939"">(Music)</seekvideo>
      <seekvideo id=""7555"">For any of you who have visited or lived in New York City,</seekvideo>
      <seekvideo id=""11221"">these shots might start to look familiar.</seekvideo>
      <seekvideo id=""16116"">This is Central Park,</seekvideo>
      .
      .
      .
      <seekvideo id=""361992"">for people to interact with</seekvideo>
      <seekvideo id=""363709"">and experience music.</seekvideo>
      <seekvideo id=""365451"">Thank you.</seekvideo>
      <seekvideo id=""367495"">(Applause)</seekvideo>
    </transcription>
    <talkid>1903</talkid>
    <title>Ryan Holladay: To hear this music you have to be there. Literally</title>
    <description>The music industry ......segments of sounds that only play when a listener is physically nearby. (Filmed at TED@BCG.)</description>
    <keywords>entertainment,music,technology</keywords>
    <image>http://images.ted.com/images/ted/d98c17773da6f84e9f915895c270c7ffd2de3778_389x292.jpg</image>
    <date>2014/01/12</date>
    <wordnum>885</wordnum>
    <charnum>5051</charnum>
  </head>
  <content>(Music) For any of you who have visited or lived in New York City, these shots might start to look familiar. This is Central Park, ............new ways for people to interact with and experience music. Thank you. (Applause)</content>
</file>
```

### Data Fields
The fields of the dataset are:
- translation:
  - <lang1>: text in <lang1>
  - <lang2>L translated text in <lang2> 

Information about the original data files:

For each language, a single XML file is generated which includes all talks subtitled in 
that language. Each talk is enclosed in tags `<file id=""int"">` and `</file>` and includes, among other tags: 

| Tags | Description |
|---|:---|
| `<url>`| the address of the original HTML document of the talk |
| `<speaker>` | the name of the talk speaker |
| `<talkid>` | the numeric talk identifier |
| `<transcript>` | talk subtitles split in captions |
| `<date>` | the issue date of the talk |
| `<content>` |  talk subtitles |


### Data Splits

The paper doesn't provide any specific train-test-dev splits. However data can be split by available years (2014, 2015, 2016)


## Dataset Creation

### Curation Rationale

TED Conference, based in California, has been posting all video recordings of its talks together with subtitles in English and their translations in more than 80 languages. Aside from its cultural and social relevance, this content, which is published under the Creative Commons BYNC-ND license, also represents a precious language resource for the machine translation research community, thanks to its size, variety of topics, and covered languages.

### Source Data

#### Initial Data Collection and Normalization

The talks were collected from the [Ted Conference website](http://www.ted.com/)

#### Who are the source language producers?

[Needs More Information]

### Annotations

#### Annotation process

[Needs More Information]

#### Who are the annotators?

Translation has been contributed by volunteers

### Personal and Sensitive Information

No personal and sensitive information is provided in the dataset. All talks are publicly available

## Considerations for Using the Data

### Social Impact of Dataset

In statistical machine translation, large amount of in-domain parallel data are usually required to properly train translation and reordering models. With more than 900+ Ted talks (as of 2011) and translation in more than 90+ languages. This dataset provides a useful resource for the MT research community. 

In turn, this enables easy access to a vast treasure trove of human knowledge. 

### Discussion of Biases

[Needs More Information]

### Other Known Limitations

[Needs More Information]

## Additional Information

### Dataset Curators

The original dataset was curated by:
[Mauro Cettolo](mailto:cettolo@fbk.eu)
[Roldano Cattoni](mailto:cattoni@fbk.eu)

Author: 
Christian Girardi

For issues with the HuggingFace Dataset implementation, reach out: [Aakash Gupta](mailto:aakashg80@gmail.com)

### Licensing Information

cc-by-nc-nd-4.0

### Citation Information
```
@inproceedings{cettolo-etal-2012-wit3,
    title = ""{WIT}3: Web Inventory of Transcribed and Translated Talks"",
    author = ""Cettolo, Mauro  and
      Girardi, Christian  and
      Federico, Marcello"",
    booktitle = ""Proceedings of the 16th Annual conference of the European Association for Machine Translation"",
    month = may # "" 28{--}30"",
    year = ""2012"",
    address = ""Trento, Italy"",
    publisher = ""European Association for Machine Translation"",
    url = ""https://www.aclweb.org/anthology/2012.eamt-1.60"",
    pages = ""261--268"",
}

```

### Contributions

Thanks to [@skyprince999](https://github.com/skyprince999) for adding this dataset.",High,5.0
Translation,alt-qsri/tweets_ar_en_parallel,4.0,152.0,2024-01-18 11:17:35+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
annotations_creators:
- expert-generated
- no-annotation
language_creators:
- found
language:
- ar
- en
license:
- apache-2.0
multilinguality:
- translation
size_categories:
- 100K<n<1M
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: bilingual-corpus-of-arabic-english-parallel
pretty_name: Bilingual Corpus of Arabic-English Parallel Tweets
tags:
- tweets-translation
dataset_info:
- config_name: parallelTweets
  features:
  - name: ArabicTweetID
    dtype: int64
  - name: EnglishTweetID
    dtype: int64
  splits:
  - name: test
    num_bytes: 2667296
    num_examples: 166706
  download_size: 2937626
  dataset_size: 2667296
- config_name: accountList
  features:
  - name: account
    dtype: string
  splits:
  - name: test
    num_bytes: 20108
    num_examples: 1389
  download_size: 2937626
  dataset_size: 20108
- config_name: countryTopicAnnotation
  features:
  - name: account
    dtype: string
  - name: country
    dtype:
      class_label:
        names:
          '0': QA
          '1': BH
          '2': AE
          '3': OM
          '4': SA
          '5': PL
          '6': JO
          '7': IQ
          '8': Other
          '9': EG
          '10': KW
          '11': SY
  - name: topic
    dtype:
      class_label:
        names:
          '0': Gov
          '1': Culture
          '2': Education
          '3': Sports
          '4': Travel
          '5': Events
          '6': Business
          '7': Science
          '8': Politics
          '9': Health
          '10': Governoment
          '11': Media
  splits:
  - name: test
    num_bytes: 6036
    num_examples: 200
  download_size: 2937626
  dataset_size: 6036
---

# Dataset Card for Bilingual Corpus of Arabic-English Parallel Tweets

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [Bilingual Corpus of Arabic-English Parallel Tweets](https://alt.qcri.org/resources/bilingual_corpus_of_parallel_tweets)
- **Repository:**
- **Paper:** [Aclweb](https://www.aclweb.org/anthology/2020.bucc-1.3/)
- **Leaderboard:**
- **Point of Contact:**

### Dataset Summary

Twitter users often post parallel tweets—tweets that contain the same content but are written in different languages. Parallel tweets can be an important resource for developing machine translation (MT) systems among other natural language processing (NLP) tasks. This resource is a result of a generic method for collecting parallel tweets. Using the method, we compiled a bilingual corpus of English-Arabic parallel tweets and a list of Twitter accounts who post English-Arabic tweets regularly. Additionally, we annotate a subset of Twitter accounts with their countries of origin and topic of interest, which provides insights about the population who post parallel tweets.

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

[More Information Needed]

## Dataset Structure

### Data Instances

parallelTweets:
```
{
  ""ArabicTweetID"": 981111245209243600,
  ""EnglishTweetID"": 981111450432401400
}
```

accountList:
```
{
  'account': 'HukoomiQatar'
}
```

countryTopicAnnotation:
```
{
  'account': 'HukoomiQatar',
  'country': 'QA',
  'topic': 'Gov'
}
```

### Data Fields

parallelTweets:
- `ArabicTweetID` (int)
- `EnglishTweetID` (int)

accountList:
- `account` (str)

countryTopicAnnotation:
- `account` (str)
- `country` (class label): One of:
  - ""QA"",
  - ""BH"",
  - ""AE"",
  - ""OM"",
  - ""SA"",
  - ""PL"",
  - ""JO"",
  - ""IQ"",
  - ""Other"",
  - ""EG"",
  - ""KW"",
  - ""SY""
- `topic` (class label): One of:
  - ""Gov"",
  - ""Culture"",
  - ""Education"",
  - ""Sports"",
  - ""Travel"",
  - ""Events"",
  - ""Business"",
  - ""Science"",
  - ""Politics"",
  - ""Health"",
  - ""Governoment"",
  - ""Media"",

### Data Splits

All configuration have only one split: ""test"".

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

It is licensed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).

### Citation Information

```
@inproceedings{Mubarak2020bilingualtweets,
  title={Constructing a Bilingual Corpus of Parallel Tweets},
  author={Mubarak, Hamdy and Hassan, Sabit and Abdelali, Ahmed},
  booktitle={Proceedings of 13th Workshop on Building and Using Comparable Corpora (BUCC)},
  address={Marseille, France},
  year={2020}
}
```

[More Information Needed]

### Contributions

Thanks to [@sumanthd17](https://github.com/sumanthd17) for adding this dataset.",High,5.0
Translation,community-datasets/udhr,2.0,129.0,2024-01-18 11:17:42+00:00,unknown,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
annotations_creators:
- no-annotation
language_creators:
- found
language:
- aa
- ab
- ace
- acu
- ada
- ady
- af
- agr
- aii
- ajg
- als
- alt
- am
- amc
- ame
- ami
- amr
- ar
- arl
- arn
- ast
- auc
- ay
- az
- ban
- bax
- bba
- bci
- be
- bem
- bfa
- bg
- bho
- bi
- bik
- bin
- blt
- bm
- bn
- bo
- boa
- br
- bs
- buc
- bug
- bum
- ca
- cab
- cak
- cbi
- cbr
- cbs
- cbt
- cbu
- ccp
- ceb
- cfm
- ch
- chj
- chk
- chr
- cic
- cjk
- cjs
- cjy
- ckb
- cnh
- cni
- cnr
- co
- cof
- cot
- cpu
- crh
- cri
- crs
- cs
- csa
- csw
- ctd
- cy
- da
- dag
- ddn
- de
- dga
- dip
- duu
- dv
- dyo
- dyu
- dz
- ee
- el
- en
- eo
- es
- ese
- et
- eu
- eve
- evn
- fa
- fat
- fi
- fj
- fkv
- fo
- fon
- fr
- fuf
- fur
- fuv
- fvr
- fy
- ga
- gaa
- gag
- gan
- gd
- gjn
- gkp
- gl
- gld
- gn
- gsw
- gu
- guc
- guu
- gv
- gyr
- ha
- hak
- haw
- he
- hi
- hil
- hlt
- hmn
- hms
- hna
- hni
- hnj
- hns
- hr
- hsb
- hsn
- ht
- hu
- hus
- huu
- hy
- ia
- ibb
- id
- idu
- ig
- ii
- ijs
- ilo
- io
- is
- it
- iu
- ja
- jiv
- jv
- ka
- kaa
- kbd
- kbp
- kde
- kdh
- kea
- kek
- kg
- kha
- kjh
- kk
- kkh
- kl
- km
- kmb
- kn
- ko
- koi
- koo
- kqn
- kqs
- kr
- kri
- krl
- ktu
- ku
- kwi
- ky
- la
- lad
- lah
- lb
- lg
- lia
- lij
- lld
- ln
- lns
- lo
- lob
- lot
- loz
- lt
- lua
- lue
- lun
- lus
- lv
- mad
- mag
- mai
- mam
- man
- maz
- mcd
- mcf
- men
- mfq
- mg
- mh
- mi
- mic
- min
- miq
- mk
- ml
- mn
- mnw
- mor
- mos
- mr
- mt
- mto
- mxi
- mxv
- my
- mzi
- nan
- nb
- nba
- nds
- ne
- ng
- nhn
- nio
- niu
- niv
- njo
- nku
- nl
- nn
- not
- nr
- nso
- nv
- ny
- nym
- nyn
- nzi
- oaa
- oc
- ojb
- oki
- om
- orh
- os
- ote
- pa
- pam
- pap
- pau
- pbb
- pcd
- pcm
- pis
- piu
- pl
- pon
- pov
- ppl
- prq
- ps
- pt
- qu
- quc
- qug
- quh
- quy
- qva
- qvc
- qvh
- qvm
- qvn
- qwh
- qxn
- qxu
- rar
- rgn
- rm
- rmn
- rn
- ro
- ru
- rup
- rw
- sa
- sah
- sc
- sco
- se
- sey
- sg
- shk
- shn
- shp
- si
- sk
- skr
- sl
- slr
- sm
- sn
- snk
- snn
- so
- sr
- srr
- ss
- st
- su
- suk
- sus
- sv
- sw
- swb
- ta
- taj
- tbz
- tca
- tdt
- te
- tem
- tet
- tg
- th
- ti
- tiv
- tk
- tl
- tly
- tn
- to
- tob
- toi
- toj
- top
- tpi
- tr
- ts
- tsz
- tt
- tw
- ty
- tyv
- tzh
- tzm
- tzo
- udu
- ug
- uk
- umb
- und
- ur
- ura
- uz
- vai
- ve
- vec
- vep
- vi
- vmw
- wa
- war
- wo
- wuu
- wwa
- xh
- xsm
- yad
- yao
- yap
- yi
- ykg
- yo
- yrk
- yua
- yue
- za
- zam
- zdj
- zgh
- zh
- zlm
- zro
- ztu
- zu
language_bcp47:
- az-Cyrl
- az-Latn
- bs-Cyrl
- bs-Latn
- ckb-Latn
- de-1901
- de-1996
- el-monoton
- el-polyton
- fa-AF
- fuf-Adlm
- ha-NE
- ha-NG
- jv-Java
- kg-AO
- kkh-Lana
- mn-Cyrl
- pt-BR
- pt-PT
- rm-puter
- rm-rumgr
- rm-surmiran
- rm-sursilv
- rm-sutsilv
- rm-vallader
- sa-Gran
- sr-Cyrl
- sr-Latn
- ta-LK
- tk-Cyrl
- tk-Latn
- tw-akuapem
- tw-asante
- ug-Arab
- ug-Latn
- uz-Cyrl
- uz-Latn
- vi-Hani
- zh-Hant
- zlm-Arab
- zlm-Latn
license:
- unknown
multilinguality:
- multilingual
size_categories:
- n<1K
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: null
pretty_name: The Universal Declaration of Human Rights (UDHR)
dataset_info:
  features:
  - name: text
    dtype: string
  - name: lang_key
    dtype: string
  - name: lang_name
    dtype: string
  - name: iso639-3
    dtype: string
  - name: iso15924
    dtype: string
  - name: bcp47
    dtype: string
  splits:
  - name: train
    num_bytes: 6753383
    num_examples: 488
  download_size: 2389690
  dataset_size: 6753383
---

# Dataset Card for The Universal Declaration of Human Rights (UDHR)

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://www.ohchr.org/en/universal-declaration-of-human-rights, https://unicode.org/udhr/index.html
- **Repository:** https://github.com/unicode-org/udhr
- **Paper:**
- **Leaderboard:**
- **Point of Contact:**

### Dataset Summary

The Universal Declaration of Human Rights (UDHR) is a milestone document in the history of human rights. Drafted by
representatives with different legal and cultural backgrounds from all regions of the world, it set out, for the
first time, fundamental human rights to be universally protected. The Declaration was adopted by the UN General
Assembly in Paris on 10 December 1948 during its 183rd plenary meeting.

© 1996 – 2009 The Office of the High Commissioner for Human Rights

This plain text version prepared by the ""UDHR in Unicode"" project, https://www.unicode.org/udhr.

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

The dataset includes translations of the document in over 400 languages and dialects. The list of languages can be found
[here](https://unicode.org/udhr/translations.html).

## Dataset Structure

### Data Instances

Each instance corresponds to a different language and includes information about the language and the full document
text.

### Data Fields

- `text`: The full document text with each line of text delimited by a newline (`\n`).
- `lang_key`: The unique identifier of a given translation.
- `lang_name`: The textual description of language/dialect.
- `iso639-3`: The [iso639-3](https://iso639-3.sil.org/) language identifier.
- `iso15924`: The [iso15924](https://unicode.org/iso15924/iso15924-codes.html) language identifier.
- `bcp47`: The [BCP 47](https://www.rfc-editor.org/info/bcp47) language identifier.

### Data Splits

Only a `train` split included which includes the full document in all languages.

|                    | train |
|--------------------|------:|
| Number of examples |   488 |

## Dataset Creation

### Curation Rationale

In addition to its social significance, the document set a world record in 1999 for being the most translated
document in the world and as such can be useful for settings requiring paired text between many languages.

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

In addition to the social and political significance of the United Nations' Universal Declaration of Human Rights,
the document set a world record in 1999 for being the most translated document in the world and as such can be useful
for settings requiring paired text between many languages including those that are low resource and significantly
underrepresented in NLP research.

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

Although the document is translated into a very large number of languages, the text is very short and therefore may
have limited usefulness for most types of modeling and evaluation.

## Additional Information

### Dataset Curators

The txt/xml data files used here were compiled by The Unicode Consortium, which can be found
[here](https://unicode.org/udhr/index.html). The original texts can be found on the
[United Nations website](https://www.ohchr.org/EN/UDHR/Pages/UDHRIndex.aspx).

### Licensing Information

Source text © 1996 – 2022 The Office of the High Commissioner for Human Rights

The [Unicode license](https://www.unicode.org/license.txt) applies to these translations.


### Citation Information

United Nations. (1998). The Universal Declaration of Human Rights, 1948-1998. New York: United Nations Dept. of Public Information.

### Contributions

Thanks to [@joeddav](https://github.com/joeddav) for adding this dataset. Updated May 2022 [@leondz](https://github.com/leondz).",High,4.0
Translation,Helsinki-NLP/un_ga,3.0,947.0,2024-04-02 13:20:41+00:00,unknown,0.0,330 MB,346030080.0,330 MB,346030080.0,1111005,none,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- ar
- en
- es
- fr
- ru
- zh
license:
- unknown
multilinguality:
- translation
size_categories:
- 10K<n<100K
source_datasets:
- original
task_categories:
- translation
task_ids: []
pretty_name: UnGa
config_names:
- ar-to-en
- ar-to-es
- ar-to-fr
- ar-to-ru
- ar-to-zh
- en-to-es
- en-to-fr
- en-to-ru
- en-to-zh
- es-to-fr
- es-to-ru
- es-to-zh
- fr-to-ru
- fr-to-zh
- ru-to-zh
dataset_info:
- config_name: ar_to_en
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - en
  splits:
  - name: train
    num_bytes: 53122776
    num_examples: 74067
  download_size: 21418697
  dataset_size: 53122776
- config_name: ar_to_es
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - es
  splits:
  - name: train
    num_bytes: 55728615
    num_examples: 74067
  download_size: 22724976
  dataset_size: 55728615
- config_name: ar_to_fr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - fr
  splits:
  - name: train
    num_bytes: 55930802
    num_examples: 74067
  download_size: 23035904
  dataset_size: 55930802
- config_name: ar_to_ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - ru
  splits:
  - name: train
    num_bytes: 72657625
    num_examples: 74067
  download_size: 28279669
  dataset_size: 72657625
- config_name: ar_to_zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - zh
  splits:
  - name: train
    num_bytes: 48217579
    num_examples: 74067
  download_size: 20391116
  dataset_size: 48217579
- config_name: en_to_es
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - es
  splits:
  - name: train
    num_bytes: 45358770
    num_examples: 74067
  download_size: 19229141
  dataset_size: 45358770
- config_name: en_to_fr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fr
  splits:
  - name: train
    num_bytes: 45560957
    num_examples: 74067
  download_size: 19540063
  dataset_size: 45560957
- config_name: en_to_ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ru
  splits:
  - name: train
    num_bytes: 62287780
    num_examples: 74067
  download_size: 24783812
  dataset_size: 62287780
- config_name: en_to_zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - zh
  splits:
  - name: train
    num_bytes: 37847734
    num_examples: 74067
  download_size: 16895275
  dataset_size: 37847734
- config_name: es_to_fr
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - fr
  splits:
  - name: train
    num_bytes: 48166796
    num_examples: 74067
  download_size: 20846355
  dataset_size: 48166796
- config_name: es_to_ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - ru
  splits:
  - name: train
    num_bytes: 64893619
    num_examples: 74067
  download_size: 26090092
  dataset_size: 64893619
- config_name: es_to_zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - zh
  splits:
  - name: train
    num_bytes: 40453573
    num_examples: 74067
  download_size: 18201560
  dataset_size: 40453573
- config_name: fr_to_ru
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - ru
  splits:
  - name: train
    num_bytes: 65095806
    num_examples: 74067
  download_size: 26401015
  dataset_size: 65095806
- config_name: fr_to_zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - zh
  splits:
  - name: train
    num_bytes: 40655760
    num_examples: 74067
  download_size: 18512482
  dataset_size: 40655760
- config_name: ru_to_zh
  features:
  - name: id
    dtype: string
  - name: translation
    dtype:
      translation:
        languages:
        - ru
        - zh
  splits:
  - name: train
    num_bytes: 57382583
    num_examples: 74067
  download_size: 23756229
  dataset_size: 57382583
configs:
- config_name: ar_to_en
  data_files:
  - split: train
    path: ar_to_en/train-*
- config_name: ar_to_es
  data_files:
  - split: train
    path: ar_to_es/train-*
- config_name: ar_to_fr
  data_files:
  - split: train
    path: ar_to_fr/train-*
- config_name: ar_to_ru
  data_files:
  - split: train
    path: ar_to_ru/train-*
- config_name: ar_to_zh
  data_files:
  - split: train
    path: ar_to_zh/train-*
- config_name: en_to_es
  data_files:
  - split: train
    path: en_to_es/train-*
- config_name: en_to_fr
  data_files:
  - split: train
    path: en_to_fr/train-*
- config_name: en_to_ru
  data_files:
  - split: train
    path: en_to_ru/train-*
- config_name: en_to_zh
  data_files:
  - split: train
    path: en_to_zh/train-*
- config_name: es_to_fr
  data_files:
  - split: train
    path: es_to_fr/train-*
- config_name: es_to_ru
  data_files:
  - split: train
    path: es_to_ru/train-*
- config_name: es_to_zh
  data_files:
  - split: train
    path: es_to_zh/train-*
- config_name: fr_to_ru
  data_files:
  - split: train
    path: fr_to_ru/train-*
- config_name: fr_to_zh
  data_files:
  - split: train
    path: fr_to_zh/train-*
- config_name: ru_to_zh
  data_files:
  - split: train
    path: ru_to_zh/train-*
---

<div class=""course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400"">
  <p><b>Deprecated:</b> Dataset ""un_ga"" is deprecated due to the the unavailability of its source data. It has been superseded by the official United Nations Parallel Corpus, which is recommended for use in its place: <a href=""https://huggingface.co/datasets/un_pc"">un_pc</a></p>
</div>

# Dataset Card for [Dataset Name]

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://opus.nlpl.eu/legacy/UN.php
- **Repository:** [More Information Needed]
- **Paper:** https://www.researchgate.net/publication/228579662_United_nations_general_assembly_resolutions_A_six-language_parallel_corpus
- **Leaderboard:** [More Information Needed]
- **Point of Contact:** [More Information Needed]

### Dataset Summary
This is a collection of translated documents from the United Nations originally compiled into a translation memory by Alexandre Rafalovitch, Robert Dale (see http://uncorpora.org).

- Deprecated homepage URL: http://opus.nlpl.eu/UN.php
- Legacy homepage URL: https://opus.nlpl.eu/legacy/UN.php

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

[More Information Needed]

## Dataset Structure

### Data Instances

[More Information Needed]

### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

@inproceedings{title = ""United Nations General Assembly Resolutions: a six-language parallel corpus"",
abstract = ""In this paper we describe a six-ways parallel public-domain corpus consisting of 2100 United Nations General Assembly Resolutions with translations in the six official languages of the United Nations, with an average of around 3 million tokens per language. The corpus is available in a preprocessed, formatting-normalized TMX format with paragraphs aligned across multiple languages. We describe the background to the corpus and its content, the process of its construction, and some of its interesting properties."",
author = ""Alexandre Rafalovitch and Robert Dale"",
year = ""2009"",
language = ""English"",
booktitle = ""MT Summit XII proceedings"",
publisher = ""International Association of Machine Translation"",
}
### Contributions

Thanks to [@param087](https://github.com/param087) for adding this dataset.",High,5.0
Translation,Helsinki-NLP/multiun,12.0,3126.0,2024-02-27 16:59:52+00:00,unknown,0.0,31.8 GB,34144990003.2,31.8 GB,34144990003.2,159488597,none,['https://aclanthology.org/L10-1473/'],"---
annotations_creators:
- found
language_creators:
- found
language:
- ar
- de
- en
- es
- fr
- ru
- zh
license:
- unknown
multilinguality:
- multilingual
size_categories:
- 100K<n<1M
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: multiun
pretty_name: MultiUN (Multilingual Corpus from United Nation Documents)
config_names:
- ar-de
- ar-en
- ar-es
- ar-fr
- ar-ru
- ar-zh
- de-en
- de-es
- de-fr
- de-ru
- de-zh
- en-es
- en-fr
- en-ru
- en-zh
- es-fr
- es-ru
- es-zh
- fr-ru
- fr-zh
- ru-zh
dataset_info:
- config_name: ar-de
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - de
  splits:
  - name: train
    num_bytes: 94466261
    num_examples: 165090
  download_size: 41124373
  dataset_size: 94466261
- config_name: ar-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - en
  splits:
  - name: train
    num_bytes: 4189844561
    num_examples: 9759125
  download_size: 1926776740
  dataset_size: 4189844561
- config_name: ar-es
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - es
  splits:
  - name: train
    num_bytes: 4509667188
    num_examples: 10119379
  download_size: 2069474168
  dataset_size: 4509667188
- config_name: ar-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - fr
  splits:
  - name: train
    num_bytes: 4516842065
    num_examples: 9929567
  download_size: 2083442998
  dataset_size: 4516842065
- config_name: ar-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - ru
  splits:
  - name: train
    num_bytes: 5932858699
    num_examples: 10206243
  download_size: 2544128334
  dataset_size: 5932858699
- config_name: ar-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - zh
  splits:
  - name: train
    num_bytes: 3781650541
    num_examples: 9832293
  download_size: 1829880809
  dataset_size: 3781650541
- config_name: de-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - en
  splits:
  - name: train
    num_bytes: 76684413
    num_examples: 162981
  download_size: 35105094
  dataset_size: 76684413
- config_name: de-es
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - es
  splits:
  - name: train
    num_bytes: 80936517
    num_examples: 162078
  download_size: 37042740
  dataset_size: 80936517
- config_name: de-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - fr
  splits:
  - name: train
    num_bytes: 81888299
    num_examples: 164025
  download_size: 37827000
  dataset_size: 81888299
- config_name: de-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - ru
  splits:
  - name: train
    num_bytes: 111517798
    num_examples: 164792
  download_size: 46723695
  dataset_size: 111517798
- config_name: de-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - zh
  splits:
  - name: train
    num_bytes: 70534674
    num_examples: 176933
  download_size: 34964647
  dataset_size: 70534674
- config_name: en-es
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - es
  splits:
  - name: train
    num_bytes: 4128132575
    num_examples: 11350967
  download_size: 2030826335
  dataset_size: 4128132575
- config_name: en-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fr
  splits:
  - name: train
    num_bytes: 4678044616
    num_examples: 13172019
  download_size: 2312275443
  dataset_size: 4678044616
- config_name: en-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ru
  splits:
  - name: train
    num_bytes: 5632653511
    num_examples: 11654416
  download_size: 2523567444
  dataset_size: 5632653511
- config_name: en-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - zh
  splits:
  - name: train
    num_bytes: 2960368390
    num_examples: 9564315
  download_size: 1557547095
  dataset_size: 2960368390
- config_name: es-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - fr
  splits:
  - name: train
    num_bytes: 4454703338
    num_examples: 11441889
  download_size: 2187539838
  dataset_size: 4454703338
- config_name: es-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - ru
  splits:
  - name: train
    num_bytes: 5442647242
    num_examples: 10605056
  download_size: 2432480744
  dataset_size: 5442647242
- config_name: es-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - zh
  splits:
  - name: train
    num_bytes: 3223863318
    num_examples: 9847770
  download_size: 1676774308
  dataset_size: 3223863318
- config_name: fr-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - ru
  splits:
  - name: train
    num_bytes: 5979869673
    num_examples: 11761738
  download_size: 2690520032
  dataset_size: 5979869673
- config_name: fr-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - zh
  splits:
  - name: train
    num_bytes: 3241090573
    num_examples: 9690914
  download_size: 1693120344
  dataset_size: 3241090573
- config_name: ru-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ru
        - zh
  splits:
  - name: train
    num_bytes: 4233867889
    num_examples: 9557007
  download_size: 1984600328
  dataset_size: 4233867889
configs:
- config_name: ar-de
  data_files:
  - split: train
    path: ar-de/train-*
- config_name: ar-en
  data_files:
  - split: train
    path: ar-en/train-*
- config_name: ar-es
  data_files:
  - split: train
    path: ar-es/train-*
- config_name: ar-fr
  data_files:
  - split: train
    path: ar-fr/train-*
- config_name: ar-ru
  data_files:
  - split: train
    path: ar-ru/train-*
- config_name: ar-zh
  data_files:
  - split: train
    path: ar-zh/train-*
- config_name: de-en
  data_files:
  - split: train
    path: de-en/train-*
- config_name: de-es
  data_files:
  - split: train
    path: de-es/train-*
- config_name: de-fr
  data_files:
  - split: train
    path: de-fr/train-*
- config_name: de-ru
  data_files:
  - split: train
    path: de-ru/train-*
- config_name: de-zh
  data_files:
  - split: train
    path: de-zh/train-*
- config_name: en-es
  data_files:
  - split: train
    path: en-es/train-*
- config_name: en-fr
  data_files:
  - split: train
    path: en-fr/train-*
- config_name: en-ru
  data_files:
  - split: train
    path: en-ru/train-*
- config_name: en-zh
  data_files:
  - split: train
    path: en-zh/train-*
- config_name: es-fr
  data_files:
  - split: train
    path: es-fr/train-*
- config_name: es-ru
  data_files:
  - split: train
    path: es-ru/train-*
- config_name: es-zh
  data_files:
  - split: train
    path: es-zh/train-*
- config_name: fr-ru
  data_files:
  - split: train
    path: fr-ru/train-*
- config_name: fr-zh
  data_files:
  - split: train
    path: fr-zh/train-*
- config_name: ru-zh
  data_files:
  - split: train
    path: ru-zh/train-*
---

# Dataset Card for OPUS MultiUN

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://opus.nlpl.eu/MultiUN/corpus/version/MultiUN
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** https://aclanthology.org/L10-1473/
- **Leaderboard:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Dataset Summary

The MultiUN parallel corpus is extracted from the United Nations Website , and then cleaned and converted to XML at Language Technology Lab in DFKI GmbH (LT-DFKI), Germany. The documents were published by UN from 2000 to 2009.

This is a collection of translated documents from the United Nations originally compiled by Andreas Eisele and Yu Chen (see http://www.euromatrixplus.net/multi-un/).

This corpus is available in all 6 official languages of the UN consisting of around 300 million words per language

### Supported Tasks and Leaderboards

The underlying task is machine translation.

### Languages

Parallel texts are present in all six official languages: Arabic (`ar`), Chinese (`zh`), English (`en`), French (`fr`),
Russian (`ru`) and Spanish (`es`), with a small part of the documents available also in German (`de`).

## Dataset Structure

### Data Instances

```
{
  ""translation"": {
    ""ar"": ""قرار اتخذته الجمعية العامة"",
    ""de"": ""Resolution der Generalversammlung""
  }
}
```

### Data Fields

- `translation` (`dict`): Parallel sentences for the pair of languages.

### Data Splits

The dataset contains a single ""train"" split for each language pair.

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

#### Initial Data Collection and Normalization

Original MultiUN source data: http://www.euromatrixplus.net/multi-unp

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

If you use this corpus in your work, please cite the paper:
```
@inproceedings{eisele-chen-2010-multiun,
    title = ""{M}ulti{UN}: A Multilingual Corpus from United Nation Documents"",
    author = ""Eisele, Andreas  and
      Chen, Yu"",
    booktitle = ""Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)"",
    month = may,
    year = ""2010"",
    address = ""Valletta, Malta"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_Paper.pdf"",
    abstract = ""This paper describes the acquisition, preparation and properties of a corpus extracted from the official documents of the United Nations (UN). This corpus is available in all 6 official languages of the UN, consisting of around 300 million words per language. We describe the methods we used for crawling, document formatting, and sentence alignment. This corpus also includes a common test set for machine translation. We present the results of a French-Chinese machine translation experiment performed on this corpus."",
}
```

If you use any part of the corpus (hosted in OPUS) in your own work, please cite the following article:
```
@inproceedings{tiedemann-2012-parallel,
    title = ""Parallel Data, Tools and Interfaces in {OPUS}"",
    author = {Tiedemann, J{\""o}rg},
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf"",
    pages = ""2214--2218"",
    abstract = ""This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project."",
}
```

### Contributions

Thanks to [@patil-suraj](https://github.com/patil-suraj) for adding this dataset.",High,5.0
Translation,Helsinki-NLP/un_pc,22.0,4802.0,2024-04-03 07:35:04+00:00,other,0.0,59.1 GB,63458141798.4,59.1 GB,63458141798.4,323370684,none,['https://aclanthology.org/L16-1561/'],"---
annotations_creators:
- found
language_creators:
- found
language:
- ar
- en
- es
- fr
- ru
- zh
license: other
multilinguality:
- multilingual
size_categories:
- 10M<n<100M
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: united-nations-parallel-corpus
pretty_name: United Nations Parallel Corpus
config_names:
- ar-en
- ar-es
- ar-fr
- ar-ru
- ar-zh
- en-es
- en-fr
- en-ru
- en-zh
- es-fr
- es-ru
- es-zh
- fr-ru
- fr-zh
- ru-zh
dataset_info:
- config_name: ar-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - en
  splits:
  - name: train
    num_bytes: 8039673899
    num_examples: 20044478
  download_size: 3638378262
  dataset_size: 8039673899
- config_name: ar-es
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - es
  splits:
  - name: train
    num_bytes: 8715738416
    num_examples: 20532014
  download_size: 3938780664
  dataset_size: 8715738416
- config_name: ar-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - fr
  splits:
  - name: train
    num_bytes: 8897831806
    num_examples: 20281645
  download_size: 3976788621
  dataset_size: 8897831806
- config_name: ar-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - ru
  splits:
  - name: train
    num_bytes: 11395906619
    num_examples: 20571334
  download_size: 4836152717
  dataset_size: 11395906619
- config_name: ar-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - zh
  splits:
  - name: train
    num_bytes: 6447644160
    num_examples: 17306056
  download_size: 3050491574
  dataset_size: 6447644160
- config_name: en-es
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - es
  splits:
  - name: train
    num_bytes: 8241615138
    num_examples: 25227004
  download_size: 3986062875
  dataset_size: 8241615138
- config_name: en-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fr
  splits:
  - name: train
    num_bytes: 9718498495
    num_examples: 30340652
  download_size: 4580188433
  dataset_size: 9718498495
- config_name: en-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ru
  splits:
  - name: train
    num_bytes: 11156144547
    num_examples: 25173398
  download_size: 4899993315
  dataset_size: 11156144547
- config_name: en-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - zh
  splits:
  - name: train
    num_bytes: 4988798590
    num_examples: 17451549
  download_size: 2554362693
  dataset_size: 4988798590
- config_name: es-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - fr
  splits:
  - name: train
    num_bytes: 9230870495
    num_examples: 25887160
  download_size: 4379207947
  dataset_size: 9230870495
- config_name: es-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - ru
  splits:
  - name: train
    num_bytes: 10789762294
    num_examples: 22294106
  download_size: 4748706797
  dataset_size: 10789762294
- config_name: es-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - es
        - zh
  splits:
  - name: train
    num_bytes: 5475351906
    num_examples: 17599223
  download_size: 2774470102
  dataset_size: 5475351906
- config_name: fr-ru
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - ru
  splits:
  - name: train
    num_bytes: 12099649535
    num_examples: 25219973
  download_size: 5264326148
  dataset_size: 12099649535
- config_name: fr-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - zh
  splits:
  - name: train
    num_bytes: 5679208110
    num_examples: 17521170
  download_size: 2828146104
  dataset_size: 5679208110
- config_name: ru-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ru
        - zh
  splits:
  - name: train
    num_bytes: 7905429097
    num_examples: 17920922
  download_size: 3601589709
  dataset_size: 7905429097
configs:
- config_name: ar-en
  data_files:
  - split: train
    path: ar-en/train-*
- config_name: ar-es
  data_files:
  - split: train
    path: ar-es/train-*
- config_name: ar-fr
  data_files:
  - split: train
    path: ar-fr/train-*
- config_name: ar-ru
  data_files:
  - split: train
    path: ar-ru/train-*
- config_name: ar-zh
  data_files:
  - split: train
    path: ar-zh/train-*
- config_name: en-es
  data_files:
  - split: train
    path: en-es/train-*
- config_name: en-fr
  data_files:
  - split: train
    path: en-fr/train-*
- config_name: en-ru
  data_files:
  - split: train
    path: en-ru/train-*
- config_name: en-zh
  data_files:
  - split: train
    path: en-zh/train-*
- config_name: es-fr
  data_files:
  - split: train
    path: es-fr/train-*
- config_name: es-ru
  data_files:
  - split: train
    path: es-ru/train-*
- config_name: es-zh
  data_files:
  - split: train
    path: es-zh/train-*
- config_name: fr-ru
  data_files:
  - split: train
    path: fr-ru/train-*
- config_name: fr-zh
  data_files:
  - split: train
    path: fr-zh/train-*
- config_name: ru-zh
  data_files:
  - split: train
    path: ru-zh/train-*
---

# Dataset Card for United Nations Parallel Corpus

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://opus.nlpl.eu/UNPC/corpus/version/UNPC
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** https://aclanthology.org/L16-1561/
- **Leaderboard:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)

### Dataset Summary

The United Nations Parallel Corpus is the first parallel corpus composed from United Nations documents published by the original data creator. 
The parallel corpus consists of manually translated UN documents from the last 25 years (1990 to 2014)
for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish.
The corpus is freely available for download under a liberal license. 

### Supported Tasks and Leaderboards

The underlying task is machine translation.

### Languages

The six official UN languages: Arabic, Chinese, English, French, Russian, and Spanish.

## Dataset Structure

### Data Instances

[More Information Needed]

### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

https://conferences.unite.un.org/UNCORPUS/#disclaimer

The following disclaimer, an integral part of the United Nations Parallel Corpus, shall be respected with regard to the Corpus (no other restrictions apply):
- The United Nations Parallel Corpus is made available without warranty of any kind, explicit or implied. The United Nations specifically makes no warranties or representations as to the accuracy or completeness of the information contained in the United Nations Corpus.
- Under no circumstances shall the United Nations be liable for any loss, liability, injury or damage incurred or suffered that is claimed to have resulted from the use of the United Nations Corpus. The use of the United Nations Corpus is at the user's sole risk. The user specifically acknowledges and agrees that the United Nations is not liable for the conduct of any user. If the user is dissatisfied with any of the material provided in the United Nations Corpus, the user's sole and exclusive remedy is to discontinue using the United Nations Corpus.
- When using the United Nations Corpus, the user must acknowledge the United Nations as the source of the information. For references, please cite this reference: Ziemski, M., Junczys-Dowmunt, M., and Pouliquen, B., (2016), The United Nations Parallel Corpus, Language Resources and Evaluation (LREC’16), Portorož, Slovenia, May 2016.
- Nothing herein shall constitute or be considered to be a limitation upon or waiver, express or implied, of the privileges and immunities of the United Nations, which are specifically reserved.

### Citation Information

```
@inproceedings{ziemski-etal-2016-united,
    title = ""The {U}nited {N}ations Parallel Corpus v1.0"",
    author = ""Ziemski, Micha{\\l}  and
      Junczys-Dowmunt, Marcin  and
      Pouliquen, Bruno"",
    booktitle = ""Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)"",
    month = may,
    year = ""2016"",
    address = ""Portoro{\v{z}}, Slovenia"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""https://www.aclweb.org/anthology/L16-1561"",
    pages = ""3530--3534"",
    abstract = ""This paper describes the creation process and statistics of the official United Nations Parallel Corpus, the first parallel corpus composed from United Nations documents published by the original data creator. The parallel corpus presented consists of manually translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish. The corpus is freely available for download under a liberal license. Apart from the pairwise aligned documents, a fully aligned subcorpus for the six official UN languages is distributed. We provide baseline BLEU scores of our Moses-based SMT systems trained with the full data of language pairs involving English and for all possible translation directions of the six-way subcorpus."",
}
```
### Contributions

Thanks to [@patil-suraj](https://github.com/patil-suraj) for adding this dataset.",High,5.0
Translation,Helsinki-NLP/tatoeba_mt,59.0,2475.0,2024-10-08 18:12:10+00:00,cc-by-2.0,6.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,"['https://aclanthology.org/2020.wmt-1.139/)', 'https://aclanthology.org/2020.wmt-1.139/)', 'https://aclanthology.org/2020.wmt-1.139"",']","---
annotations_creators:
- no-annotation
language_creators:
- crowdsourced
language:
- af
- ar
- az
- be
- bg
- bn
- br
- bs
- ca
- ch
- cs
- cv
- cy
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fo
- fr
- fy
- ga
- gd
- gl
- gn
- he
- hi
- hr
- hu
- hy
- ia
- id
- ie
- io
- is
- it
- ja
- jv
- ka
- kk
- km
- ko
- ku
- kw
- la
- lb
- lt
- lv
- mi
- mk
- ml
- mn
- mr
- ms
- mt
- my
- nb
- nl
- nn
- 'no'
- oc
- pl
- pt
- qu
- rn
- ro
- ru
- sh
- sl
- sq
- sr
- sv
- sw
- ta
- te
- th
- tk
- tl
- tr
- tt
- ug
- uk
- ur
- uz
- vi
- vo
- yi
- zh
license:
- cc-by-2.0
multilinguality:
- translation
pretty_name: The Tatoeba Translation Challenge
size_categories:
- unknown
source_datasets:
- original
task_categories:
- text-generation
- translation
---

# Dataset Card for [Dataset Name]

## Table of Contents
- [Table of Contents](#table-of-contents)
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://github.com/Helsinki-NLP/Tatoeba-Challenge/
- **Repository:** https://github.com/Helsinki-NLP/Tatoeba-Challenge/
- **Paper:** [The Tatoeba Translation Challenge – Realistic Data Sets for Low Resource and Multilingual MT](https://aclanthology.org/2020.wmt-1.139/)
- **Leaderboard:**
- **Point of Contact:** [Jörg Tiedemann](mailto:jorg.tiedemann@helsinki.fi)

### Dataset Summary

The Tatoeba Translation Challenge is a multilingual data set of machine translation benchmarks derived from user-contributed translations collected by [Tatoeba.org](https://tatoeba.org/) and provided as parallel corpus from [OPUS](https://opus.nlpl.eu/). This dataset includes test and development data sorted by language pair. It includes test sets for hundreds of language pairs and is continuously updated. Please, check the version number tag to refer to the release that your are using.

### Supported Tasks and Leaderboards

The translation task is described in detail in the [Tatoeba-Challenge repository](https://github.com/Helsinki-NLP/Tatoeba-Challenge) and covers various sub-tasks with different data coverage and resources. [Training data](https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/data/README.md) is also available from the same repository and [results](https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/results/tatoeba-results-all.md) are published and collected as well. [Models](https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/results/tatoeba-models-all.md) are also released for public use and are also partially available from the [huggingface model hub](https://huggingface.co/Helsinki-NLP).


### Languages

The data set covers hundreds of languages and language pairs and are organized by ISO-639-3 languages. The current release covers the following language: Afrikaans, Arabic, Azerbaijani, Belarusian, Bulgarian, Bengali, Breton, Bosnian, Catalan, Chamorro, Czech, Chuvash, Welsh, Danish, German, Modern Greek, English, Esperanto, Spanish, Estonian, Basque, Persian, Finnish, Faroese, French, Western Frisian, Irish, Scottish Gaelic, Galician, Guarani, Hebrew, Hindi, Croatian, Hungarian, Armenian, Interlingua, Indonesian, Interlingue, Ido, Icelandic, Italian, Japanese, Javanese, Georgian, Kazakh, Khmer, Korean, Kurdish, Cornish, Latin, Luxembourgish, Lithuanian, Latvian, Maori, Macedonian, Malayalam, Mongolian, Marathi, Malay, Maltese, Burmese, Norwegian Bokmål, Dutch, Norwegian Nynorsk, Norwegian, Occitan, Polish, Portuguese, Quechua, Rundi, Romanian, Russian, Serbo-Croatian, Slovenian, Albanian, Serbian, Swedish, Swahili, Tamil, Telugu, Thai, Turkmen, Tagalog, Turkish, Tatar, Uighur, Ukrainian, Urdu, Uzbek, Vietnamese, Volapük, Yiddish, Chinese


## Dataset Structure

### Data Instances

Data instances are given as translation units in TAB-separated files with four columns: source and target language ISO-639-3 codes, source language text and target language text. Note that we do not imply a translation direction and consider the data set to be symmetric and to be used as a test set in both directions. Language-pair-specific subsets are only provided under the label of one direction using sorted ISO-639-3 language IDs.

Some subsets contain several sub-languages or language variants. They may refer to macro-languages such as Serbo-Croatian languages that are covered by the ISO code `hbs`. Language variants may also include different writing systems and in that case the ISO15924 script codes are attached to the language codes. Here are a few examples from the English to Serbo-Croation test set including examples for Bosnian, Croatian and Serbian in Cyrillic and in Latin scripts:

```
eng	bos_Latn	Children are the flowers of our lives.	Djeca su cvijeće našeg života.
eng	hrv	A bird was flying high up in the sky.	Ptica je visoko letjela nebom.
eng	srp_Cyrl	A bird in the hand is worth two in the bush.	Боље врабац у руци, него голуб на грани.
eng	srp_Latn	Canada is the motherland of ice hockey.	Kanada je zemlja-majka hokeja na ledu.
```

There are also data sets with sentence pairs in the same language. In most cases, those are variants with minor spelling differences but they also include rephrased sentences. Here are a few examples from the English test set:

```
eng     eng     All of us got into the car.     We all got in the car.
eng     eng     All of us hope that doesn't happen.     All of us hope that that doesn't happen.
eng     eng     All the seats are booked.       The seats are all sold out.
```


### Data Splits

Test and development data sets are disjoint with respect to sentence pairs but may include overlaps in individual source or target language sentences. Development data should not be used in training directly. The goal of the data splits is to create test sets of reasonable size with a large language coverage. Test sets include at most 10,000 instances. Development data do not exist for all language pairs.

To be comparable with other results, models should use the training data distributed from the [Tatoeba MT Challenge Repository](https://github.com/Helsinki-NLP/Tatoeba-Challenge/) including monolingual data sets also listed there.


## Dataset Creation

### Curation Rationale


The Tatoeba MT data set will be updated continuously and the data preparation procedures are also public and released on [github](https://github.com/Helsinki-NLP/Tatoeba-Challenge/). High language coverage is the main goal of the project and data sets are prepared to be consistent and systematic with standardized language labels and distribution formats.


### Source Data

#### Initial Data Collection and Normalization

The Tatoeba data sets are collected from user-contributed translations submitted to [Tatoeba.org](https://tatoeba.org/) and compiled into a multi-parallel corpus in [OPUS](https://opus.nlpl.eu/Tatoeba.php). The test and development sets are incrementally updated with new releases of the Tatoeba data collection at OPUS. New releases extend the existing data sets. Test sets should not overlap with any of the released development data sets.


#### Who are the source language producers?

The data sets come from [Tatoeba.org](https://tatoeba.org/), which provides a large database of sentences and their translations into a wide varity of languages. Its content is constantly growing as a result of voluntary contributions of thousands of users.
The original project was founded by Trang Ho in 2006, hosted on Sourceforge under the codename of multilangdict. 


### Annotations

#### Annotation process

Sentences are translated by volunteers and the Tatoeba database also provides additional metadata about each record including user ratings etc. However, the metadata is currently not used in any way for the compilation of the MT benchmark. Language skills of contributors naturally vary quite a bit and not all translations are done by native speakers of the target language. More information about the contributions can be found at [Tatoeba.org](https://tatoeba.org/).


#### Who are the annotators?

### Personal and Sensitive Information

For information about handling personal and sensitive information we refer to the [original provider](https://tatoeba.org/) of the data. This data set has not been processed in any way to detect or remove potentially sensitve or personal information.

## Considerations for Using the Data

### Social Impact of Dataset

The language coverage is high and with that it represents a highly valuable resource for machine translation development especially for lesser resourced languages and language pairs. The constantly growing database also represents a dynamic resource and its value wil grow further.


### Discussion of Biases

The original source lives from its contributors and there interest and background will to certain subjective and cultural biases. Language coverage and translation quality is also biased by the skills of the contributors.


### Other Known Limitations

The sentences are typically quite short and, therefore, rather easy to translate. For high-resource languages, this leads to results that will be less useful than more challenging benchmarks. For lesser resource language pairs, the limited complexity of the examples is actually a good thing to measure progress even in very challenging setups.


## Additional Information

### Dataset Curators

The data set is curated by the University of Helsinki and its [language technology research group](https://blogs.helsinki.fi/language-technology/). Data and tools used for creating and using the resource are [open source](https://github.com/Helsinki-NLP/Tatoeba-Challenge/) and will be maintained as part of the [OPUS ecosystem](https://opus.nlpl.eu/) for parallel data and machine translation research.


### Licensing Information

The data sets are distributed under the same licence agreement as the original Tatoeba database using a
[CC-BY 2.0 license](https://creativecommons.org/licenses/by/2.0/fr/). More information about the terms of use of the original data sets is listed [here](https://tatoeba.org/eng/terms_of_use).


### Citation Information

If you use the data sets then, please, cite the following paper: [The Tatoeba Translation Challenge – Realistic Data Sets for Low Resource and Multilingual MT](https://aclanthology.org/2020.wmt-1.139/)

```
@inproceedings{tiedemann-2020-tatoeba,
    title = ""The Tatoeba Translation Challenge {--} Realistic Data Sets for Low Resource and Multilingual {MT}"",
    author = {Tiedemann, J{\""o}rg},
    booktitle = ""Proceedings of the Fifth Conference on Machine Translation"",
    month = nov,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.wmt-1.139"",
    pages = ""1174--1182"",
}
```

### Contributions

Thanks to [@jorgtied](https://github.com/jorgtied) and [@Helsinki-NLP](https://github.com/Helsinki-NLP) for adding this dataset.
Thanks also to [CSC Finland](https://www.csc.fi/en/solutions-for-research) for providing computational resources and storage space for the work on OPUS and other MT projects.
",High,5.0
Translation,gsarti/flores_101,26.0,5251.0,2022-10-27 08:37:36+00:00,cc-by-sa-4.0,1.0,1.35 GB,1449551462.4,51 MB,53477376.0,206927,https://arxiv.org/abs/2106.03193,none,"---
annotations_creators:
- found
language_creators:
- expert-generated
language:
- af
- am
- ar
- hy
- as
- ast
- az
- be
- bn
- bs
- bg
- my
- ca
- ceb
- zho
- hr
- cs
- da
- nl
- en
- et
- tl
- fi
- fr
- ff
- gl
- lg
- ka
- de
- el
- gu
- ha
- he
- hi
- hu
- is
- ig
- id
- ga
- it
- ja
- jv
- kea
- kam
- kn
- kk
- km
- ko
- ky
- lo
- lv
- ln
- lt
- luo
- lb
- mk
- ms
- ml
- mt
- mi
- mr
- mn
- ne
- ns
- 'no'
- ny
- oc
- or
- om
- ps
- fa
- pl
- pt
- pa
- ro
- ru
- sr
- sn
- sd
- sk
- sl
- so
- ku
- es
- sw
- sv
- tg
- ta
- te
- th
- tr
- uk
- umb
- ur
- uz
- vi
- cy
- wo
- xh
- yo
- zu
license:
- cc-by-sa-4.0
multilinguality:
- multilingual
- translation
size_categories:
- unknown
source_datasets:
- extended|flores
task_categories:
- text-generation
- translation
task_ids: []
paperswithcode_id: flores
pretty_name: flores101
tags:
- conditional-text-generation
---

# Dataset Card for Flores 101

## Table of Contents

- [Dataset Card for Flores 101](#dataset-card-for-flores-101)
  - [Table of Contents](#table-of-contents)
  - [Dataset Description](#dataset-description)
    - [Dataset Summary](#dataset-summary)
    - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
    - [Languages](#languages)
  - [Dataset Structure](#dataset-structure)
    - [Data Instances](#data-instances)
    - [Data Fields](#data-fields)
    - [Data Splits](#data-splits)
    - [Dataset Creation](#dataset-creation)
  - [Additional Information](#additional-information)
    - [Dataset Curators](#dataset-curators)
    - [Licensing Information](#licensing-information)
    - [Citation Information](#citation-information)

## Dataset Description

- **Home:** [WMT](http://www.statmt.org/wmt21/large-scale-multilingual-translation-task.html)
- **Repository:** [Github](https://github.com/facebookresearch/flores)
- **Blogpost:** [FAIR](https://ai.facebook.com/blog/the-flores-101-data-set-helping-build-better-translation-systems-around-the-world)
- **Paper:** [Arxiv](https://arxiv.org/abs/2106.03193)
- **Point of Contact:** [flores@fb.com](mailto:flores@fb.com)
- **Leaderboard** [Dynabench](https://dynabench.org/flores/Flores%20MT%20Evaluation%20(FULL))

### Dataset Summary

FLORES is a benchmark dataset for machine translation between English and low-resource languages.

Abstract from the original paper:

> One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the FLORES evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.

**Disclaimer**: *The Flores-101 dataset is hosted by the Facebook and licensed under the [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).

### Supported Tasks and Leaderboards

#### Multilingual Machine Translation

Refer to the [Dynabench leaderboard](https://dynabench.org/flores/Flores%20MT%20Evaluation%20(FULL)) for additional details on model evaluation on FLORES-101 in the context of the WMT2021 shared task on [Large-Scale Multilingual Machine Translation](http://www.statmt.org/wmt21/large-scale-multilingual-translation-task.html).

### Languages

The dataset contains parallel sentences for 101 languages, as mentioned in the original [Github](https://github.com/facebookresearch/flores/blob/master/README.md) page for the project. Languages are identified with the ISO 639-3 code (e.g. `eng`, `fra`, `rus`) as in the original dataset.

**New:** Use the configuration `all` to access the full set of parallel sentences for all the available languages in a single command.


## Dataset Structure

### Data Instances

A sample from the `dev` split for the Russian language (`rus` config) is provided below. All configurations have the same structure, and all sentences are aligned across configurations and splits.

```python
{
	'id': 1,
	'sentence': 'В понедельник ученые из Медицинской школы Стэнфордского университета объявили об изобретении нового диагностического инструмента, который может сортировать клетки по их типу; это маленький чип, который можно напечатать, используя стандартный струйный принтер примерно за 1 цент США.',
	'URL': 'https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet',
	'domain': 'wikinews',
	'topic': 'health',
	'has_image': 0,
	'has_hyperlink': 0
}
```

The text is provided as-in the original dataset, without further preprocessing or tokenization.

### Data Fields

- `id`: Row number for the data entry, starting at 1.
- `sentence`: The full sentence in the specific language.
- `URL`: The URL for the English article from which the sentence was extracted.
- `domain`: The domain of the sentence.
- `topic`: The topic of the sentence.
- `has_image`: Whether the  original article contains an image.
- `has_hyperlink`: Whether the  sentence contains a hyperlink.

### Data Splits

|            config| `dev`| `devtest`|
|-----------------:|-----:|---------:|
|all configurations|   997|     1012:|

### Dataset Creation

Please refer to the original article [The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation](https://arxiv.org/abs/2106.03193) for additional information on dataset creation.

## Additional Information

### Dataset Curators

The original authors of FLORES-101 are the curators of the original dataset. For problems or updates on this 🤗 Datasets version, please contact [gabriele.sarti996@gmail.com](mailto:gabriele.sarti996@gmail.com).

### Licensing Information

Licensed with Creative Commons Attribution Share Alike 4.0. License available [here](https://creativecommons.org/licenses/by-sa/4.0/).

### Citation Information

Please cite the authors if you use these corpora in your work:

```bibtex
@inproceedings{flores101,
  title={The FLORES-101  Evaluation Benchmark for Low-Resource and Multilingual Machine Translation},
  author={Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc'Aurelio and Guzm\'{a}n, Francisco and Fan, Angela},
  journal={arXiv preprint arXiv:2106.03193},
  year={2021}
}
```",High,4.0
Translation,yhavinga/ccmatrix,24.0,1625.0,2024-03-14 08:43:02+00:00,unknown,9.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/1911.04944,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- af
- am
- ar
- ast
- az
- be
- bg
- bn
- br
- ca
- ceb
- cs
- cy
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fr
- fy
- ga
- gd
- gl
- ha
- he
- hi
- hr
- hu
- hy
- id
- ig
- ilo
- is
- it
- ja
- jv
- ka
- kk
- km
- ko
- la
- lb
- lg
- lt
- lv
- mg
- mk
- ml
- mr
- ms
- my
- ne
- nl
- 'no'
- oc
- om
- or
- pl
- pt
- ro
- ru
- sd
- si
- sk
- sl
- so
- sq
- sr
- su
- sv
- sw
- ta
- tl
- tr
- tt
- uk
- ur
- uz
- vi
- wo
- xh
- yi
- yo
- zh
- zu
- se
license:
- unknown
multilinguality:
- multilingual
source_datasets:
- original
task_categories:
- text2text-generation
- translation
task_ids: []
paperswithcode_id: ccmatrix
pretty_name: CCMatrixV1
tags:
- conditional-text-generation
---

# Dataset Card for CCMatrix v1

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description
- **Homepage:** https://opus.nlpl.eu/CCMatrix.php
- **Repository:** None
- **Paper:** https://arxiv.org/abs/1911.04944
### Dataset Summary

This corpus has been extracted from web crawls using the margin-based bitext mining techniques described at https://github.com/facebookresearch/LASER/tree/master/tasks/CCMatrix.

* 90 languages, 1,197 bitexts
* total number of files: 90
* total number of tokens: 112.14G
* total number of sentence fragments: 7.37G

### Supported Tasks and Leaderboards
[More Information Needed]

### Languages

Configs are generated for all language pairs in both directions.
You can find the valid pairs in Homepage section of Dataset Description: https://opus.nlpl.eu/CCMatrix.php
E.g.

```
from datasets import load_dataset
dataset = load_dataset(""yhavinga/ccmatrix"", ""en-nl"", streaming=True)
```

This will open the `en-nl` dataset in streaming mode. Without streaming, download and prepare will take tens of minutes.
You can inspect elements with:

```
print(next(iter(dataset['train'])))
{'id': 0, 'score': 1.2499677, 'translation': {'en': 'They come from all parts of Egypt, just like they will at the day of His coming.', 'nl': 'Zij kwamen uit alle delen van Egypte, evenals zij op de dag van Zijn komst zullen doen.'}}
```

## Dataset Structure
### Data Instances
For example:

```json
{
        ""id"": 1,
        ""score"": 1.2498379,
        ""translation"": {
            ""nl"": ""En we moeten elke waarheid vals noemen die niet minstens door een lach vergezeld ging.”"",
            ""en"": ""And we should call every truth false which was not accompanied by at least one laugh.”""
        }
    }
```

### Data Fields
Each example contains an integer id starting with 0, a score, and a translation dictionary with the language 1 and
language 2 texts.

### Data Splits
Only a `train` split is provided.

## Dataset Creation
### Curation Rationale
[More Information Needed]
### Source Data
[More Information Needed]
#### Initial Data Collection and Normalization
[More Information Needed]
#### Who are the source language producers?
[More Information Needed]
### Annotations
[More Information Needed]
#### Annotation process
[More Information Needed]
#### Who are the annotators?
[More Information Needed]
### Personal and Sensitive Information
[More Information Needed]
## Considerations for Using the Data
### Social Impact of Dataset
[More Information Needed]
### Discussion of Biases
[More Information Needed]
### Other Known Limitations
[More Information Needed]
## Additional Information
### Dataset Curators
[More Information Needed]
### Licensing Information
[More Information Needed]
### Citation Information

IMPORTANT: Please cite reference [2][3] if you use this data.

1. **[CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data](https://arxiv.org/abs/1911.00359)**
   by *Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Jouli
    and Edouard Grave*.
2. **[CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB](https://arxiv.org/abs/1911.04944)** by *Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave and Armand Joulin*.
3. **[Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125)** by *Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines,
    Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky,
    Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin.*

This HuggingFace CCMatrix dataset is a wrapper around the service and files prepared and hosted by OPUS:

* **[Parallel Data, Tools and Interfaces in OPUS](https://www.aclweb.org/anthology/L12-1246/)** by *Jörg Tiedemann*.

### Contributions
",High,4.0
Translation,GroNLP/divemt,2.0,201.0,2023-02-10 11:04:33+00:00,gpl-3.0,0.0,89.2 MB,93532979.2,10.7 MB,11219763.2,8100,https://arxiv.org/abs/2205.12215,"['https://aclanthology.org/2022.emnlp-main.532"",']","---
annotations_creators:
- machine-generated
- expert-generated
language_creators:
- found
language:
- en
- it
- vi
- nl
- uk
- tr
- ar
license:
- gpl-3.0
multilinguality:
- translation
pretty_name: divemt
size_categories:
- 1K<n<10K
source_datasets:
- original
task_categories:
- translation
---

# Dataset Card for DivEMT

*For more details on DivEMT, see our [EMNLP 2022 Paper](https://arxiv.org/abs/2205.12215) and our [Github repository](https://github.com/gsarti/divemt)*

## Dataset Description
- **Source:** [Github](https://github.com/gsarti/divemt)
- **Paper:** [Arxiv](https://arxiv.org/abs/2205.12215)
- **Point of Contact:** [Gabriele Sarti](mailto:g.sarti@rug.nl)

[Gabriele Sarti](https://gsarti.com) • [Arianna Bisazza](https://www.cs.rug.nl/~bisazza/) • [Ana Guerberof Arenas](https://scholar.google.com/citations?user=i6bqaTsAAAAJ) • [Antonio Toral](https://antoniotor.al/)


<img src=""https://huggingface.co/datasets/GroNLP/divemt/resolve/main/divemt.png"" alt=""DivEMT annotation pipeline"" width=""600""/>

>We introduce DivEMT, the first publicly available post-editing study of Neural Machine Translation (NMT) over a typologically diverse set of target languages. Using a strictly controlled setup, 18 professional translators were instructed to translate or post-edit the same set of English documents into Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process, their edits, keystrokes, editing times and pauses were recorded, enabling an in-depth, cross-lingual evaluation of NMT quality and post-editing effectiveness. Using this new dataset, we assess the impact of two state-of-the-art NMT systems, Google Translate and the multilingual mBART-50 model, on translation productivity. We find that post-editing is consistently faster than translation from scratch. However, the magnitude of productivity gains varies widely across systems and languages, highlighting major disparities in post-editing effectiveness for languages at different degrees of typological relatedness to English, even when controlling for system architecture and training data size. We publicly release the complete dataset including all collected behavioral data, to foster new research on the translation capabilities of NMT systems for typologically diverse languages.

### Dataset Summary

This dataset contains the processed `warmup` and `main` splits of the DivEMT dataset. A sample of documents extracted from the Flores-101 corpus were either translated from scratch or post-edited from an existing automatic translation by a total of 18 professional translators across six typologically diverse languages (Arabic, Dutch, Italian, Turkish, Ukrainian, Vietnamese). During the translation, behavioral data (keystrokes, pauses, editing times) were collected using the [PET](https://github.com/wilkeraziz/PET) platform.

We publicly release the processed dataset including all collected behavioural data, to foster new research on the ability of state-of-the-art NMT systems to generate text in typologically diverse languages.

### News 🎉

**February, 2023**: The DivEMT dataset now contains linguistic annotations (`*_annotations` fields) computed with Stanza and word-level quality estimation tags (`src_wmt22_qe`, `mt_wmt22_qe`) obtained using the same scripts adopted for the WMT22 QE Task 2.

### Languages

The language data of DivEMT is in English (BCP-47 `en`), Italian (BCP-47 `it`), Dutch (BCP-47 `nl`), Arabic (BCP-47 `ar`), Turkish (BCP-47 `tr`), Ukrainian (BCP-47 `uk`) and Vietnamese (BCP-47 `vi`)

## Dataset Structure

### Data Instances

The dataset contains two configurations: `main` and `warmup`. `main` contains the full data collected during the main task and analyzed during our experiments. `warmup` contains the data collected in the verification phase, before the main task begins.

### Data Fields

The following fields are contained in the training set:

|Field|Description|
|-----|-----------|
|`unit_id`   | The full entry identifier. Format: `flores101-{config}-{lang}-{doc_id}-{modality}-{sent_in_doc_num}` |
|`flores_id`   | Index of the sentence in the original [Flores-101](https://huggingface.co/datasets/gsarti/flores_101) dataset |
|`item_id`   | The sentence identifier. The first digits of the number represent the document containing the sentence, while the last digit of the number represents the sentence position inside the document. Documents can contain from 3 to 5 contiguous sentences each. |
|`subject_id`   | The identifier for the translator performing the translation from scratch or post-editing task. Values: `t1`, `t2` or `t3`. |
|`lang_id` | Language identifier for the sentence, using Flores-101 three-letter format (e.g. `ara`, `nld`)|
|`doc_id` | Document identifier for the sentence |
|`task_type`   | The modality of the translation task. Values: `ht` (translation from scratch), `pe1` (post-editing Google Translate translations), `pe2` (post-editing [mBART 1-to-50](https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt) translations). |
|`translation_type` | Either `ht` for from scratch or `pe` for post-editing |
|`src_len_chr`   | Length of the English source text in number of characters |
|`mt_len_chr`   | Length of the machine translation in number of characters (NaN for ht) |
|`tgt_len_chr`   | Length of the target text in number of characters |
|`src_len_wrd`   | Length of the English source text in number of words |
|`mt_len_wrd`   | Length of the machine translation in number of words (NaN for ht) |
|`tgt_len_wrd`   | Length of the target text in number of words |
|`edit_time`   | Total editing time for the translation in seconds. |
|`k_total`   | Total number of keystrokes for the translation. |
|`k_letter`   | Total number of letter keystrokes for the translation. |
|`k_digit`   | Total number of digit keystrokes for the translation. |
|`k_white`   | Total number of whitespace keystrokes for the translation. |
|`k_symbol`   | Total number of symbol (punctuation, etc.) keystrokes for the translation. |
|`k_nav`   | Total number of navigation keystrokes (left-right arrows, mouse clicks) for the translation. |
|`k_erase`   | Total number of erase keystrokes (backspace, cancel) for the translation. |
|`k_copy`   | Total number of copy (Ctrl + C) actions during the translation. |
|`k_cut`   | Total number of cut (Ctrl + X) actions during the translation. |
|`k_paste`   | Total number of paste (Ctrl + V) actions during the translation. |
|`k_do` | Total number of Enter actions during the translation. |
|`n_pause_geq_300`   | Number of pauses of 300ms or more during the translation. |
|`len_pause_geq_300`   | Total duration of pauses of 300ms or more, in milliseconds. |
|`n_pause_geq_1000`   | Number of pauses of 1s or more during the translation. |
|`len_pause_geq_1000`   | Total duration of pauses of 1000ms or more, in milliseconds. |
|`event_time` | Total time summed across all translation events, should be comparable to `edit_time` in most cases. |
|`num_annotations` | Number of times the translator focused the textbox for performing the translation of the sentence during the translation session. E.g. 1 means the translation was performed once and never revised. |
|`n_insert`   | Number of post-editing insertions (empty for modality `ht`) computed using the [tercom](https://github.com/jhclark/tercom) library. |
|`n_delete`   | Number of post-editing deletions (empty for modality `ht`) computed using the [tercom](https://github.com/jhclark/tercom) library. |
|`n_substitute`   | Number of post-editing substitutions (empty for modality `ht`) computed using the [tercom](https://github.com/jhclark/tercom) library. |
|`n_shift`   | Number of post-editing shifts (empty for modality `ht`) computed using the [tercom](https://github.com/jhclark/tercom) library. |
|`tot_shifted_words` | Total amount of shifted words from all shifts present in the sentence. |
|`tot_edits` | Total of all edit types for the sentence. |
|`hter` | Human-mediated Translation Edit Rate score computed between MT and post-edited TGT (empty for modality `ht`) using the [tercom](https://github.com/jhclark/tercom) library. |
|`cer` | Character-level HTER score computed between MT and post-edited TGT (empty for modality `ht`) using [CharacTER](https://github.com/rwth-i6/CharacTER).
|`bleu`   | Sentence-level BLEU score between MT and post-edited TGT (empty for modality `ht`) computed using the [SacreBLEU](https://github.com/mjpost/sacrebleu) library with default parameters. |
|`chrf`   | Sentence-level chrF score between MT and post-edited TGT (empty for modality `ht`) computed using the [SacreBLEU](https://github.com/mjpost/sacrebleu) library with default parameters. |
|`time_s` | Edit time expressed in seconds. |
|`time_m` | Edit time expressed in minutes. |
|`time_h` | Edit time expressed in hours. |
|`time_per_char` | Edit time per source character, expressed in seconds. |
|`time_per_word` | Edit time per source word, expressed in seconds. |
|`key_per_char` | Proportion of keys per character needed to perform the translation. |
|`words_per_hour` | Amount of source words translated or post-edited per hour. |
|`words_per_minute` | Amount of source words translated or post-edited per minute. |
|`per_subject_visit_order` | Id denoting the order in which the translator accessed documents. 1 correspond to the first accessed document. |
|`src_text`   | The original source sentence extracted from Wikinews, wikibooks or wikivoyage. |
|`mt_text`   | Missing if tasktype is `ht`. Otherwise, contains the automatically-translated sentence before post-editing. |
|`tgt_text`   | Final sentence produced by the translator (either via translation from scratch of `sl_text` or post-editing `mt_text`) |
|`aligned_edit`   | Aligned visual representation of REF (`mt_text`), HYP (`tl_text`) and edit operations (I = Insertion, D = Deletion, S = Substitution) performed on the field. Replace `\\n` with `\n` to show the three aligned rows.|
|`src_tokens` | List of tokens obtained tokenizing `src_text` with Stanza using default params. |
|`src_annotations` | List of lists (one per `src_tokens` token) containing dictionaries (one per word, >1 for mwt) with pos, ner and other info parsed by Stanza |
|`mt_tokens` | List of tokens obtained tokenizing `mt_text` with Stanza using default params. |
|`mt_annotations` | List of lists (one per `mt_tokens` token) containing dictionaries (one per word, >1 for mwt) with pos, ner and other info parsed by Stanza |
|`tgt_tokens` | List of tokens obtained tokenizing `tgt_text` with Stanza using default params. |
|`tgt_annotations` | List of lists (one per `tgt_tokens` token) containing dictionaries (one per word, >1 for mwt) with pos, ner and other info parsed by Stanza |

### Data Splits

| config | train|
|-------:|-----:|
|`main`  | 7740 (107 docs i.e. 430 sents x 18 translators) |
|`warmup`|  360 (5 docs i.e. 20 sents x 18 translators)    |

#### Train Split

The `train` split contains the totality of triplets (or pairs, when translation from scratch is performed) annotated with behavioral data produced during the translation.

The following is an example of the subject `t1` post-editing a machine translation produced by Google Translate (task_type `pe1`) taken from the `train` split for Turkish. The field `aligned_edit` is showed over three lines to provide a visual understanding of its contents.

```json
{
	'unit_id': 'flores101-main-tur-46-pe1-3',
	'flores_id': 871,
	'item_id': 'flores101-main-463',
	'subject_id': 'tur_t1',
	'task_type': 'pe1',
	'translation_type': 'pe',
	'src_len_chr': 109,
	'mt_len_chr': 129.0,
	'tgt_len_chr': 120,
	'src_len_wrd': 17,
	'mt_len_wrd': 15.0,
	'tgt_len_wrd': 13,
	'edit_time': 11.762999534606934,
	'k_total': 31,
	'k_letter': 9,
	'k_digit': 0,
	'k_white': 0,
	'k_symbol': 0,
	'k_nav': 20,
	'k_erase': 2,
	'k_copy': 0,
	'k_cut': 0,
	'k_paste': 0,
	'k_do': 0,
	'n_pause_geq_300': 2,
	'len_pause_geq_300': 4986,
	'n_pause_geq_1000': 1,
	'len_pause_geq_1000': 4490,
	'event_time': 11763,
	'num_annotations': 2,
	'last_modification_time': 1643569484,
	'n_insert': 0.0,
	'n_delete': 2.0,
	'n_substitute': 1.0,
	'n_shift': 0.0,
	'tot_shifted_words': 0.0,
	'tot_edits': 3.0,
	'hter': 20.0,
	'cer': 0.10,
	'bleu': 0.0,
	'chrf': 2.569999933242798,
	'lang_id': 'tur',
	'doc_id': 46,
	'time_s': 11.762999534606934,
	'time_m': 0.1960500031709671,
	'time_h': 0.0032675000838935375,
	'time_per_char': 0.1079174280166626,
	'time_per_word': 0.6919412016868591,
	'key_per_char': 0.2844036817550659,
	'words_per_hour': 5202.75439453125,
	'words_per_minute': 86.71257019042969,
	'per_subject_visit_order': 201,
	'src_text': 'As one example, American citizens in the Middle East might face different situations from Europeans or Arabs.',
	'mt_text': ""Bir örnek olarak, Orta Doğu'daki Amerikan vatandaşları, Avrupalılardan veya Araplardan farklı durumlarla karşı karşıya kalabilir."",
	'tgt_text': ""Örneğin, Orta Doğu'daki Amerikan vatandaşları, Avrupalılardan veya Araplardan farklı durumlarla karşı karşıya kalabilir."",
	'aligned_edit': ""REF:  bir örnek olarak,  orta doğu'daki amerikan vatandaşları, avrupalılardan veya araplardan farklı durumlarla karşı karşıya kalabilir.\\n
					 HYP:  *** ***** örneğin, orta doğu'daki amerikan vatandaşları, avrupalılardan veya araplardan farklı durumlarla karşı karşıya kalabilir.\\n
					 EVAL: D   D     S""
}
```

The text is provided as-is, without further preprocessing or tokenization.


### Dataset Creation

The dataset was parsed from PET XML files into CSV format using the scripts available in the [DivEMT Github repository](https://github.com/gsarti/divemt).

Those are adapted from the ones by [Antonio Toral](https://research.rug.nl/en/persons/antonio-toral-ruiz) found at the following link: [https://github.com/antot/postediting_novel_frontiers](https://github.com/antot/postediting_novel_frontiers).

## Additional Information

### Dataset Curators
For problems related to this 🤗 Datasets version, please contact me at [g.sarti@rug.nl](mailto:g.sarti@rug.nl).

### Citation Information

```bibtex
@inproceedings{sarti-etal-2022-divemt,
    title = ""{D}iv{EMT}: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages"",
    author = ""Sarti, Gabriele  and
      Bisazza, Arianna  and
      Guerberof-Arenas, Ana  and
      Toral, Antonio"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    address = ""Abu Dhabi, United Arab Emirates"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.emnlp-main.532"",
    pages = ""7795--7816"",
}
```",High,5.0
Translation,facebook/flores,78.0,11570.0,2024-01-18 15:05:58+00:00,cc-by-sa-4.0,6.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2207.04672,none,"---
annotations_creators:
- found
language_creators:
- expert-generated
language:
- ace
- acm
- acq
- aeb
- af
- ajp
- ak
- als
- am
- apc
- ar
- ars
- ary
- arz
- as
- ast
- awa
- ayr
- azb
- azj
- ba
- bm
- ban
- be
- bem
- bn
- bho
- bjn
- bo
- bs
- bug
- bg
- ca
- ceb
- cs
- cjk
- ckb
- crh
- cy
- da
- de
- dik
- dyu
- dz
- el
- en
- eo
- et
- eu
- ee
- fo
- fj
- fi
- fon
- fr
- fur
- fuv
- gaz
- gd
- ga
- gl
- gn
- gu
- ht
- ha
- he
- hi
- hne
- hr
- hu
- hy
- ig
- ilo
- id
- is
- it
- jv
- ja
- kab
- kac
- kam
- kn
- ks
- ka
- kk
- kbp
- kea
- khk
- km
- ki
- rw
- ky
- kmb
- kmr
- knc
- kg
- ko
- lo
- lij
- li
- ln
- lt
- lmo
- ltg
- lb
- lua
- lg
- luo
- lus
- lvs
- mag
- mai
- ml
- mar
- min
- mk
- mt
- mni
- mos
- mi
- my
- nl
- nn
- nb
- npi
- nso
- nus
- ny
- oc
- ory
- pag
- pa
- pap
- pbt
- pes
- plt
- pl
- pt
- prs
- quy
- ro
- rn
- ru
- sg
- sa
- sat
- scn
- shn
- si
- sk
- sl
- sm
- sn
- sd
- so
- st
- es
- sc
- sr
- ss
- su
- sv
- swh
- szl
- ta
- taq
- tt
- te
- tg
- tl
- th
- ti
- tpi
- tn
- ts
- tk
- tum
- tr
- tw
- tzm
- ug
- uk
- umb
- ur
- uzn
- vec
- vi
- war
- wo
- xh
- ydd
- yo
- yue
- zh
- zsm
- zu
license:
- cc-by-sa-4.0
multilinguality:
- multilingual
- translation
size_categories:
- unknown
source_datasets:
- extended|flores
task_categories:
- text2text-generation
- translation
task_ids: []
paperswithcode_id: flores
pretty_name: flores200
language_details: ace_Arab, ace_Latn, acm_Arab, acq_Arab, aeb_Arab, afr_Latn, ajp_Arab,
  aka_Latn, amh_Ethi, apc_Arab, arb_Arab, ars_Arab, ary_Arab, arz_Arab, asm_Beng,
  ast_Latn, awa_Deva, ayr_Latn, azb_Arab, azj_Latn, bak_Cyrl, bam_Latn, ban_Latn,bel_Cyrl,
  bem_Latn, ben_Beng, bho_Deva, bjn_Arab, bjn_Latn, bod_Tibt, bos_Latn, bug_Latn,
  bul_Cyrl, cat_Latn, ceb_Latn, ces_Latn, cjk_Latn, ckb_Arab, crh_Latn, cym_Latn,
  dan_Latn, deu_Latn, dik_Latn, dyu_Latn, dzo_Tibt, ell_Grek, eng_Latn, epo_Latn,
  est_Latn, eus_Latn, ewe_Latn, fao_Latn, pes_Arab, fij_Latn, fin_Latn, fon_Latn,
  fra_Latn, fur_Latn, fuv_Latn, gla_Latn, gle_Latn, glg_Latn, grn_Latn, guj_Gujr,
  hat_Latn, hau_Latn, heb_Hebr, hin_Deva, hne_Deva, hrv_Latn, hun_Latn, hye_Armn,
  ibo_Latn, ilo_Latn, ind_Latn, isl_Latn, ita_Latn, jav_Latn, jpn_Jpan, kab_Latn,
  kac_Latn, kam_Latn, kan_Knda, kas_Arab, kas_Deva, kat_Geor, knc_Arab, knc_Latn,
  kaz_Cyrl, kbp_Latn, kea_Latn, khm_Khmr, kik_Latn, kin_Latn, kir_Cyrl, kmb_Latn,
  kon_Latn, kor_Hang, kmr_Latn, lao_Laoo, lvs_Latn, lij_Latn, lim_Latn, lin_Latn,
  lit_Latn, lmo_Latn, ltg_Latn, ltz_Latn, lua_Latn, lug_Latn, luo_Latn, lus_Latn,
  mag_Deva, mai_Deva, mal_Mlym, mar_Deva, min_Latn, mkd_Cyrl, plt_Latn, mlt_Latn,
  mni_Beng, khk_Cyrl, mos_Latn, mri_Latn, zsm_Latn, mya_Mymr, nld_Latn, nno_Latn,
  nob_Latn, npi_Deva, nso_Latn, nus_Latn, nya_Latn, oci_Latn, gaz_Latn, ory_Orya,
  pag_Latn, pan_Guru, pap_Latn, pol_Latn, por_Latn, prs_Arab, pbt_Arab, quy_Latn,
  ron_Latn, run_Latn, rus_Cyrl, sag_Latn, san_Deva, sat_Beng, scn_Latn, shn_Mymr,
  sin_Sinh, slk_Latn, slv_Latn, smo_Latn, sna_Latn, snd_Arab, som_Latn, sot_Latn,
  spa_Latn, als_Latn, srd_Latn, srp_Cyrl, ssw_Latn, sun_Latn, swe_Latn, swh_Latn,
  szl_Latn, tam_Taml, tat_Cyrl, tel_Telu, tgk_Cyrl, tgl_Latn, tha_Thai, tir_Ethi,
  taq_Latn, taq_Tfng, tpi_Latn, tsn_Latn, tso_Latn, tuk_Latn, tum_Latn, tur_Latn,
  twi_Latn, tzm_Tfng, uig_Arab, ukr_Cyrl, umb_Latn, urd_Arab, uzn_Latn, vec_Latn,
  vie_Latn, war_Latn, wol_Latn, xho_Latn, ydd_Hebr, yor_Latn, yue_Hant, zho_Hans,
  zho_Hant, zul_Latn
tags:
- conditional-text-generation
---

# Dataset Card for Flores 200

## Table of Contents

- [Dataset Card for Flores 200](#dataset-card-for-flores-200)
  - [Table of Contents](#table-of-contents)
  - [Dataset Description](#dataset-description)
    - [Dataset Summary](#dataset-summary)
    - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
    - [Languages](#languages)
  - [Dataset Structure](#dataset-structure)
    - [Data Instances](#data-instances)
    - [Data Fields](#data-fields)
    - [Data Splits](#data-splits)
    - [Dataset Creation](#dataset-creation)
  - [Additional Information](#additional-information)
    - [Dataset Curators](#dataset-curators)
    - [Licensing Information](#licensing-information)
    - [Citation Information](#citation-information)

## Dataset Description

- **Home:** [Flores](https://github.com/facebookresearch/flores)
- **Repository:** [Github](https://github.com/facebookresearch/flores)

### Dataset Summary

FLORES is a benchmark dataset for machine translation between English and low-resource languages.

>The creation of FLORES-200 doubles the existing language coverage of FLORES-101. 
Given the nature of the new languages, which have less standardization and require 
more specialized professional translations, the verification process became more complex. 
This required modifications to the translation workflow. FLORES-200 has several languages 
which were not translated from English. Specifically, several languages were translated 
from Spanish, French, Russian and Modern Standard Arabic. Moreover, FLORES-200 also 
includes two script alternatives for four languages. FLORES-200 consists of translations 
from 842 distinct web articles, totaling 3001 sentences. These sentences are divided 
into three splits: dev, devtest, and test (hidden). On average, sentences are approximately 
21 words long.

**Disclaimer**: *The Flores-200 dataset is hosted by the Facebook and licensed under the [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).
### Supported Tasks and Leaderboards
#### Multilingual Machine Translation
Refer to the [Dynabench leaderboard](https://dynabench.org/flores/Flores%20MT%20Evaluation%20(FULL)) for additional details on model evaluation on FLORES-101 in the context of the WMT2021 shared task on [Large-Scale Multilingual Machine Translation](http://www.statmt.org/wmt21/large-scale-multilingual-translation-task.html). Flores 200 is an extention of this.

### Languages
The dataset contains parallel sentences for 200 languages, as mentioned in the original [Github](https://github.com/facebookresearch/flores/blob/master/README.md) page for the project. Languages are identified with the ISO 639-3 code (e.g. `eng`, `fra`, `rus`) plus an additional code describing the script (e.g., ""eng_Latn"", ""ukr_Cyrl""). See [the webpage for code descriptions](https://github.com/facebookresearch/flores/blob/main/flores200/README.md).

Use the configuration `all` to access the full set of parallel sentences for all the available languages in a single command. 

Use a hyphenated pairing to get two langauges in one datapoint (e.g., ""eng_Latn-ukr_Cyrl"" will provide sentences in the format below).

## Dataset Structure
### Data Instances
A sample from the `dev` split for the Ukrainian language (`ukr_Cyrl` config) is provided below. All configurations have the same structure, and all sentences are aligned across configurations and splits.
```python
{
	'id': 1,
	'sentence': 'У понеділок, науковці зі Школи медицини Стенфордського університету оголосили про винайдення нового діагностичного інструменту, що може сортувати клітини за їх видами: це малесенький друкований чіп, який можна виготовити за допомогою стандартних променевих принтерів десь по одному центу США за штуку.',
	'URL': 'https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet',
	'domain': 'wikinews',
	'topic': 'health',
	'has_image': 0,
	'has_hyperlink': 0
}
```
When using a hyphenated pairing or using the `all` function, data will be presented as follows:

```python
{
    'id': 1, 
    'URL': 'https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet', 
    'domain': 'wikinews', 
    'topic': 'health', 
    'has_image': 0, 
    'has_hyperlink': 0, 
    'sentence_eng_Latn': 'On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each.', 
    'sentence_ukr_Cyrl': 'У понеділок, науковці зі Школи медицини Стенфордського університету оголосили про винайдення нового діагностичного інструменту, що може сортувати клітини за їх видами: це малесенький друкований чіп, який можна виготовити за допомогою стандартних променевих принтерів десь по одному центу США за штуку.'
}
```


The text is provided as-in the original dataset, without further preprocessing or tokenization.
### Data Fields
- `id`: Row number for the data entry, starting at 1.
- `sentence`: The full sentence in the specific language (may have _lang for pairings)
- `URL`: The URL for the English article from which the sentence was extracted.
- `domain`: The domain of the sentence.
- `topic`: The topic of the sentence.
- `has_image`: Whether the  original article contains an image.
- `has_hyperlink`: Whether the  sentence contains a hyperlink.
### Data Splits
|            config| `dev`| `devtest`|
|-----------------:|-----:|---------:|
|all configurations|   997|     1012:|
### Dataset Creation
Please refer to the original article [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) for additional information on dataset creation.
## Additional Information
### Dataset Curators
See paper for details.
### Licensing Information
Licensed with Creative Commons Attribution Share Alike 4.0. License available [here](https://creativecommons.org/licenses/by-sa/4.0/).
### Citation Information
Please cite the authors if you use these corpora in your work:
```bibtex
@article{nllb2022,
  author    = {NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi,  Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang},
  title     = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  year      = {2022}
}
```

Please also cite prior work that this dataset builds on:

```bibtex
@inproceedings{,
  title={The FLORES-101  Evaluation Benchmark for Low-Resource and Multilingual Machine Translation},
  author={Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc'Aurelio and Guzm\'{a}n, Francisco and Fan, Angela},
  year={2021}
}
```

```bibtex
@inproceedings{,
  title={Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English},
  author={Guzm\'{a}n, Francisco and Chen, Peng-Jen and Ott, Myle and Pino, Juan and Lample, Guillaume and Koehn, Philipp and Chaudhary, Vishrav and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1902.01382},
  year={2019}
}
```",High,4.0
Translation,gsarti/mt_geneval,7.0,247.0,2022-11-21 14:52:09+00:00,cc-by-sa-3.0,0.0,23.6 MB,24746393.6,16.2 MB,16986931.2,25196,https://arxiv.org/abs/2211.01355,none,"---
annotations_creators:
- expert-generated
language:
- en
- it
- fr
- ar
- de
- hi
- pt
- ru
- es
language_creators:
- expert-generated
license:
- cc-by-sa-3.0
multilinguality:
- translation
pretty_name: mt_geneval
size_categories:
- 1K<n<10K
source_datasets:
- original
tags:
- gender
- constrained mt
task_categories:
- translation
task_ids: []
---

# Dataset Card for MT-GenEval

## Table of Contents

- [Dataset Card for MT-GenEval](#dataset-card-for-mt-geneval)
  - [Table of Contents](#table-of-contents)
  - [Dataset Description](#dataset-description)
    - [Dataset Summary](#dataset-summary)
    - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
      - [Machine Translation](#machine-translation)
    - [Languages](#languages)
  - [Dataset Structure](#dataset-structure)
    - [Data Instances](#data-instances)
    - [Data Splits](#data-splits)
    - [Dataset Creation](#dataset-creation)
  - [Additional Information](#additional-information)
    - [Dataset Curators](#dataset-curators)
    - [Licensing Information](#licensing-information)
    - [Citation Information](#citation-information)

## Dataset Description

- **Repository:** [Github](https://github.com/amazon-science/machine-translation-gender-eval)
- **Paper:** [EMNLP 2022](https://arxiv.org/abs/2211.01355)
- **Point of Contact:** [Anna Currey](mailto:ancurrey@amazon.com)

### Dataset Summary

The MT-GenEval benchmark evaluates gender translation accuracy on English -> {Arabic, French, German, Hindi, Italian, Portuguese, Russian, Spanish}. The dataset contains individual sentences with annotations on the gendered target words, and contrastive original-invertend translations with additional preceding context.

**Disclaimer**: *The MT-GenEval benchmark was released in the EMNLP 2022 paper [MT-GenEval: A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation](https://arxiv.org/abs/2211.01355) by Anna Currey, Maria Nadejde, Raghavendra Pappagari, Mia Mayer, Stanislas Lauly, Xing Niu, Benjamin Hsu, and Georgiana Dinu and is hosted through Github by the [Amazon Science](https://github.com/amazon-science?type=source) organization. The dataset is licensed under a [Creative Commons Attribution-ShareAlike 3.0 Unported License](https://creativecommons.org/licenses/by-sa/3.0/).*

### Supported Tasks and Leaderboards
#### Machine Translation
Refer to the [original paper](https://arxiv.org/abs/2211.01355) for additional details on gender accuracy evaluation with MT-GenEval.
### Languages
The dataset contains source English sentences extracted from Wikipedia translated into the following languages: Arabic (`ar`), French (`fr`), German (`de`), Hindi (`hi`), Italian (`it`), Portuguese (`pt`), Russian (`ru`), and Spanish (`es`).
## Dataset Structure
### Data Instances

The dataset contains two configuration types, `sentences` and `context`, mirroring the original repository structure, with source and target language specified in the configuration name (e.g. `sentences_en_ar`, `context_en_it`) The `sentences` configurations contains masculine and feminine versions of individual sentences with gendered word annotations. Here is an example entry of the `sentences_en_it` split (all `sentences_en_XX` splits have the same structure):

```json
{
{
  ""orig_id"": 0,
  ""source_feminine"": ""Pagratidis quickly recanted her confession, claiming she was psychologically pressured and beaten, and until the moment of her execution, she remained firm in her innocence."",
  ""reference_feminine"": ""Pagratidis subito ritrattò la sua confessione, affermando che era aveva subito pressioni psicologiche e era stata picchiata, e fino al momento della sua esecuzione, rimase ferma sulla sua innocenza."",
  ""source_masculine"": ""Pagratidis quickly recanted his confession, claiming he was psychologically pressured and beaten, and until the moment of his execution, he remained firm in his innocence."",
  ""reference_masculine"": ""Pagratidis subito ritrattò la sua confessione, affermando che era aveva subito pressioni psicologiche e era stato picchiato, e fino al momento della sua esecuzione, rimase fermo sulla sua innocenza."",
  ""source_feminine_annotated"": ""Pagratidis quickly recanted <F>her</F> confession, claiming <F>she</F> was psychologically pressured and beaten, and until the moment of <F>her</F> execution, <F>she</F> remained firm in <F>her</F> innocence."",
  ""reference_feminine_annotated"": ""Pagratidis subito ritrattò la sua confessione, affermando che era aveva subito pressioni psicologiche e era <F>stata picchiata</F>, e fino al momento della sua esecuzione, rimase <F>ferma</F> sulla sua innocenza."",
  ""source_masculine_annotated"": ""Pagratidis quickly recanted <M>his</M> confession, claiming <M>he</M> was psychologically pressured and beaten, and until the moment of <M>his</M> execution, <M>he</M> remained firm in <M>his</M> innocence."",
  ""reference_masculine_annotated"": ""Pagratidis subito ritrattò la sua confessione, affermando che era aveva subito pressioni psicologiche e era <M>stato picchiato</M>, e fino al momento della sua esecuzione, rimase <M>fermo</M> sulla sua innocenza."",
  ""source_feminine_keywords"": ""her;she;her;she;her"",
  ""reference_feminine_keywords"": ""stata picchiata;ferma"",
  ""source_masculine_keywords"": ""his;he;his;he;his"",
  ""reference_masculine_keywords"": ""stato picchiato;fermo"",
}
}
```

The `context` configuration contains instead different English sources related to stereotypical professional roles with additional preceding context and contrastive original-inverted translations. Here is an example entry of the `context_en_it` split (all `context_en_XX` splits have the same structure):

```json
{
  ""orig_id"": 0,
  ""context"": ""Pierpont told of entering and holding up the bank and then fleeing to Fort Wayne, where the loot was divided between him and three others."",
  ""source"": ""However, Pierpont stated that Skeer was the planner of the robbery."",
  ""reference_original"": ""Comunque, Pierpont disse che Skeer era il pianificatore della rapina."",
  ""reference_flipped"": ""Comunque, Pierpont disse che Skeer era la pianificatrice della rapina.""
}
```

### Data Splits

All `sentences_en_XX` configurations have 1200 examples in the `train` split and 300 in the `test` split. For the `context_en_XX` configurations, the number of example depends on the language pair:

| Configuration   | # Train    | # Test  |
| :-----------:   | :--------: | :-----: |
| `context_en_ar` |    792     |  1100   |
| `context_en_fr` |    477     |  1099   |
| `context_en_de` |    598     |  1100   |
| `context_en_hi` |    397     |  1098   |
| `context_en_it` |    465     |  1904   |
| `context_en_pt` |    574     |  1089   |
| `context_en_ru` |    583     |  1100   |
| `context_en_es` |    534     |  1096   |

### Dataset Creation

From the original paper:

>In developing MT-GenEval, our goal was to create a realistic, gender-balanced dataset that naturally incorporates a diverse range of gender phenomena. To this end, we extracted English source sentences from Wikipedia as the basis for our dataset. We automatically pre-selected relevant sentences using EN gender-referring words based on the list provided by [Zhao et al. (2018)](https://doi.org/10.18653/v1/N18-2003).

Please refer to the original article [MT-GenEval: A Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation](https://arxiv.org/abs/2211.01355) for additional information on dataset creation.

## Additional Information
### Dataset Curators

The original authors of MT-GenEval are the curators of the original dataset. For problems or updates on this 🤗 Datasets version, please contact [gabriele.sarti996@gmail.com](mailto:gabriele.sarti996@gmail.com).

### Licensing Information

The dataset is licensed under the [Creative Commons Attribution-ShareAlike 3.0 International License](https://creativecommons.org/licenses/by-sa/3.0/).

### Citation Information
Please cite the authors if you use these corpora in your work.

```bibtex
@inproceedings{currey-etal-2022-mtgeneval,
    title = ""{MT-GenEval}: {A} Counterfactual and Contextual Dataset for Evaluating Gender Accuracy in Machine Translation"",
    author = ""Currey, Anna  and
      Nadejde, Maria  and
      Pappagari, Raghavendra  and
      Mayer, Mia  and
      Lauly, Stanislas,  and
      Niu, Xing  and
      Hsu, Benjamin  and
      Dinu, Georgiana"",
    booktitle = ""Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"",
    month = dec,
    year = ""2022"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://arxiv.org/abs/2211.01355"",
}
```",High,5.0
Translation,EmnaBou/DataTranslationDT,0.0,11.0,2022-12-12 12:56:36+00:00,unknown,0.0,196 kB,unknown,56.4 kB,unknown,982,none,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- ar
license:
- unknown
multilinguality:
- multilingual
size_categories:
- 100K<n<1M
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: null
pretty_name: DataTranslationDT
dataset_info:
- config_name: disluent_fluent
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - disfluent
        - fluent
  - name: id
    dtype: string
---
# Dataset Card for DataTranslationDT

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** 
- **Repository:** None
- **Paper:** 
- **Leaderboard:** [More Information Needed]
- **Point of Contact:** [More Information Needed]

### Dataset Summary


`dataset = load_dataset(""DataTranslationDT"", lang1=""disfluent"", lang2=""fluent"")`

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

[More Information Needed]

## Dataset Structure

### Data Instances

[More Information Needed]

### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

[More Information Needed]

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

[More Information Needed]

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

[More Information Needed]

### Contributions

",Medium,3.0
Translation,inseq/divemt_attributions,0.0,99.0,2023-03-16 16:02:19+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2205.12215,none,"---
annotations_creators:
  - machine-generated
license: apache-2.0
language:
  - it
  - ar
  - nl
  - tr
  - uk
  - vi
multilinguality:
  - translation
task_categories:
  - translation
pretty_name: divemt_attributions
size_categories:
  - 1K<n<10K

---


# Dataset Card for DivEMT Attributions

*For more details on DivEMT, see our [EMNLP 2022 Paper](https://arxiv.org/abs/2205.12215) and our [Github repository](https://github.com/gsarti/divemt)*

## Dataset Description
- **DivEMT Source:** [DivEMT Github](https://github.com/gsarti/divemt)
- **Inseq Source:** [Inseq Github](https://github.com/inseq-team/inseq)
- **DivEMT Paper:** [DivEMT Arxiv](https://arxiv.org/abs/2205.12215)
- **Inseq Paper:** [Inseq Arxiv](https://arxiv.org/abs/2302.13942)
- **Point of Contact:** [Gabriele Sarti](mailto:g.sarti@rug.nl)",Medium,2.0
Translation,collectivat/amazic,10.0,133.0,2025-06-13 13:58:12+00:00,cc-by-2.0,0.0,1.42 MB,1488977.92,244 KB,249856.0,3518,none,none,"---
configs:
- config_name: Awal
  data_files: parallel/awal/raw-dumps/Contribution-24.03.14.json
  default: true
- config_name: IRCAM-clean-tifinagh
  data_files: mono/IRCAM-clean-tifinagh.txt
- config_name: tc_wajdm_v1
  data_files: mono/tc_wajdm_v1.txt
- config_name: CV-localization
  data_files: parallel/CV-localization/pontoon-CV-zgh-en.tsv
license: cc-by-2.0
language:
- zgh
- fr
- ca
- en
- es
- ary
- ar
task_categories:
- translation
- text-generation
pretty_name: Tamazight language data
size_categories:
- 100K<n<1M
---

This repository contains various Tamazight language datasets created by [Col·lectivaT](https://www.collectivat.cat) in collaboration with CIEMEN and with funding from Municipality of Barcelona and Government of Catalonia. 

Under `mono` you can find monolingual sentences. 

- `tc_wajdm_v1.txt` - Texts from language learning material [“tc wawjdm”](https://www.ciemen.cat/projectes/coneixement/som-part/amazic/)
- `IRCAM-clean-tifinagh.txt` - Tifinagh scripted sentences extracted from [IRCAM's text corpus](https://tal.ircam.ma/talam/corpus.php)

Under `parallel` you can find sentences with translations. 

- `AWAL` contains data extracted from contributions made to [AWAL](https://awaldigital.org/)
- `tatoeba-translit` contains parallel sentences from Tatoeba.org transliterated into Tifinagh using [a Python script](https://github.com/CollectivaT-dev/tifinagh_transliterator).
- `proverbs` contains Tamazight proverbs with translations in Catalan.
- `CV-localization` contains parallel English and Tamazight segments from localization of Common Voice, extracted from [Pontoon](https://pontoon.mozilla.org/zgh/common-voice/) 

",Low,1.0
Translation,vpermilp/nllb-200-distilled-600M-rust,1.0,99.0,2023-03-04 08:00:22+00:00,cc-by-nc-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
language:
- ace
- acm
- acq
- aeb
- af
- ajp
- ak
- als
- am
- apc
- ar
- ars
- ary
- arz
- as
- ast
- awa
- ayr
- azb
- azj
- ba
- bm
- ban
- be
- bem
- bn
- bho
- bjn
- bo
- bs
- bug
- bg
- ca
- ceb
- cs
- cjk
- ckb
- crh
- cy
- da
- de
- dik
- dyu
- dz
- el
- en
- eo
- et
- eu
- ee
- fo
- fj
- fi
- fon
- fr
- fur
- fuv
- gaz
- gd
- ga
- gl
- gn
- gu
- ht
- ha
- he
- hi
- hne
- hr
- hu
- hy
- ig
- ilo
- id
- is
- it
- jv
- ja
- kab
- kac
- kam
- kn
- ks
- ka
- kk
- kbp
- kea
- khk
- km
- ki
- rw
- ky
- kmb
- kmr
- knc
- kg
- ko
- lo
- lij
- li
- ln
- lt
- lmo
- ltg
- lb
- lua
- lg
- luo
- lus
- lvs
- mag
- mai
- ml
- mar
- min
- mk
- mt
- mni
- mos
- mi
- my
- nl
- nn
- nb
- npi
- nso
- nus
- ny
- oc
- ory
- pag
- pa
- pap
- pbt
- pes
- plt
- pl
- pt
- prs
- quy
- ro
- rn
- ru
- sg
- sa
- sat
- scn
- shn
- si
- sk
- sl
- sm
- sn
- sd
- so
- st
- es
- sc
- sr
- ss
- su
- sv
- swh
- szl
- ta
- taq
- tt
- te
- tg
- tl
- th
- ti
- tpi
- tn
- ts
- tk
- tum
- tr
- tw
- tzm
- ug
- uk
- umb
- ur
- uzn
- vec
- vi
- war
- wo
- xh
- ydd
- yo
- yue
- zh
- zsm
- zu
language_details: >-
  ace_Arab, ace_Latn, acm_Arab, acq_Arab, aeb_Arab, afr_Latn, ajp_Arab,
  aka_Latn, amh_Ethi, apc_Arab, arb_Arab, ars_Arab, ary_Arab, arz_Arab,
  asm_Beng, ast_Latn, awa_Deva, ayr_Latn, azb_Arab, azj_Latn, bak_Cyrl,
  bam_Latn, ban_Latn,bel_Cyrl, bem_Latn, ben_Beng, bho_Deva, bjn_Arab, bjn_Latn,
  bod_Tibt, bos_Latn, bug_Latn, bul_Cyrl, cat_Latn, ceb_Latn, ces_Latn,
  cjk_Latn, ckb_Arab, crh_Latn, cym_Latn, dan_Latn, deu_Latn, dik_Latn,
  dyu_Latn, dzo_Tibt, ell_Grek, eng_Latn, epo_Latn, est_Latn, eus_Latn,
  ewe_Latn, fao_Latn, pes_Arab, fij_Latn, fin_Latn, fon_Latn, fra_Latn,
  fur_Latn, fuv_Latn, gla_Latn, gle_Latn, glg_Latn, grn_Latn, guj_Gujr,
  hat_Latn, hau_Latn, heb_Hebr, hin_Deva, hne_Deva, hrv_Latn, hun_Latn,
  hye_Armn, ibo_Latn, ilo_Latn, ind_Latn, isl_Latn, ita_Latn, jav_Latn,
  jpn_Jpan, kab_Latn, kac_Latn, kam_Latn, kan_Knda, kas_Arab, kas_Deva,
  kat_Geor, knc_Arab, knc_Latn, kaz_Cyrl, kbp_Latn, kea_Latn, khm_Khmr,
  kik_Latn, kin_Latn, kir_Cyrl, kmb_Latn, kon_Latn, kor_Hang, kmr_Latn,
  lao_Laoo, lvs_Latn, lij_Latn, lim_Latn, lin_Latn, lit_Latn, lmo_Latn,
  ltg_Latn, ltz_Latn, lua_Latn, lug_Latn, luo_Latn, lus_Latn, mag_Deva,
  mai_Deva, mal_Mlym, mar_Deva, min_Latn, mkd_Cyrl, plt_Latn, mlt_Latn,
  mni_Beng, khk_Cyrl, mos_Latn, mri_Latn, zsm_Latn, mya_Mymr, nld_Latn,
  nno_Latn, nob_Latn, npi_Deva, nso_Latn, nus_Latn, nya_Latn, oci_Latn,
  gaz_Latn, ory_Orya, pag_Latn, pan_Guru, pap_Latn, pol_Latn, por_Latn,
  prs_Arab, pbt_Arab, quy_Latn, ron_Latn, run_Latn, rus_Cyrl, sag_Latn,
  san_Deva, sat_Beng, scn_Latn, shn_Mymr, sin_Sinh, slk_Latn, slv_Latn,
  smo_Latn, sna_Latn, snd_Arab, som_Latn, sot_Latn, spa_Latn, als_Latn,
  srd_Latn, srp_Cyrl, ssw_Latn, sun_Latn, swe_Latn, swh_Latn, szl_Latn,
  tam_Taml, tat_Cyrl, tel_Telu, tgk_Cyrl, tgl_Latn, tha_Thai, tir_Ethi,
  taq_Latn, taq_Tfng, tpi_Latn, tsn_Latn, tso_Latn, tuk_Latn, tum_Latn,
  tur_Latn, twi_Latn, tzm_Tfng, uig_Arab, ukr_Cyrl, umb_Latn, urd_Arab,
  uzn_Latn, vec_Latn, vie_Latn, war_Latn, wol_Latn, xho_Latn, ydd_Hebr,
  yor_Latn, yue_Hant, zho_Hans, zho_Hant, zul_Latn
tags:
- nllb
- translation
license: cc-by-nc-4.0
datasets:
- flores-200
metrics:
- bleu
- spbleu
- chrf++
inference: false
task_categories:
- translation
size_categories:
- 100K<n<1M
---

# NLLB-200

This is the model card of NLLB-200's distilled 600M variant.

Here are the [metrics](https://tinyurl.com/nllb200densedst600mmetrics) for that particular checkpoint.

- Information about training algorithms, parameters, fairness constraints or other applied approaches, and features. The exact training algorithm, data and the strategies to handle data imbalances for high and low resource languages that were used to train NLLB-200 is described in the paper.
- Paper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022
- License: CC-BY-NC
- Where to send questions or comments about the model: https://github.com/facebookresearch/fairseq/issues



## Intended Use
- Primary intended uses: NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data.
- Primary intended users: Primary users are researchers and machine translation research community.
- Out-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations. 

## Metrics
• Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.


## Evaluation Data
- Datasets: Flores-200 dataset is described in Section 4
- Motivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200
- Preprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The
SentencePiece model is released along with NLLB-200.

## Training Data
• We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.

## Ethical Considerations
• In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).

## Caveats and Recommendations
• Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments.

## Carbon Footprint Details
• The carbon dioxide (CO2e) estimate is reported in Section 8.8.",Medium,2.0
Translation,SaeedMLK/seq2seq_ccmatrix_ar_en,0.0,11.0,2023-03-20 07:22:43+00:00,none,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
task_categories:
- translation
language:
- ar
- en
---",Low,0.0
Translation,deliadumitrescu/disinfo22-small,2.0,27.0,2023-05-03 09:33:17+00:00,cc-by-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: cc-by-4.0
task_categories:
- feature-extraction
- token-classification
- question-answering
- translation
- summarization
language:
- en
- ar
- pt
- es
- si
- tr
- gu
- id
- ml
- uk
tags:
- not-for-all-audiences
- medical
size_categories:
- n<1K
---
Data set contains 379 images of posts identified as mis-/disinformation and 1 csv file linking the image IDs to urls. The list of urls orginates from the CoronaVirusFacts Database of the International
Fact Checking Network.",Low,1.0
Translation,gsarti/iwslt2017_context,1.0,189.0,2023-05-07 14:09:24+00:00,cc-by-nc-nd-4.0,7.0,4.24 GB,4552665333.76,713 MB,747634688.0,5548640,none,['https://aclanthology.org/2017.iwslt-1.1/)'],"---
annotations_creators:
- crowdsourced
language:
- ar
- de
- en
- fr
- it
- ja
- ko
- nl
- ro
- zh
language_creators:
- expert-generated
license:
- cc-by-nc-nd-4.0
multilinguality:
- translation
pretty_name: IWSLT 2017
size_categories:
- 1M<n<10M
source_datasets:
- original
task_categories:
- translation
task_ids: []
paperswithcode_id: iwslt-2017
dataset_info:
- config_name: iwslt2017-en-it
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - it
  splits:
  - name: train
    num_bytes: 46647925
    num_examples: 231619
  - name: test
    num_bytes: 305246
    num_examples: 1566
  - name: validation
    num_bytes: 200023
    num_examples: 929
  download_size: 329391132
  dataset_size: 47153194
- config_name: iwslt2017-en-nl
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - nl
  splits:
  - name: train
    num_bytes: 42843933
    num_examples: 237240
  - name: test
    num_bytes: 311646
    num_examples: 1777
  - name: validation
    num_bytes: 197814
    num_examples: 1003
  download_size: 329391132
  dataset_size: 43353393
- config_name: iwslt2017-en-ro
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ro
  splits:
  - name: train
    num_bytes: 44129950
    num_examples: 220538
  - name: test
    num_bytes: 316790
    num_examples: 1678
  - name: validation
    num_bytes: 205028
    num_examples: 914
  download_size: 329391132
  dataset_size: 44651768
- config_name: iwslt2017-it-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - it
        - en
  splits:
  - name: train
    num_bytes: 46647925
    num_examples: 231619
  - name: test
    num_bytes: 305246
    num_examples: 1566
  - name: validation
    num_bytes: 200023
    num_examples: 929
  download_size: 329391132
  dataset_size: 47153194
- config_name: iwslt2017-it-nl
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - it
        - nl
  splits:
  - name: train
    num_bytes: 43033168
    num_examples: 233415
  - name: test
    num_bytes: 309725
    num_examples: 1669
  - name: validation
    num_bytes: 197774
    num_examples: 1001
  download_size: 329391132
  dataset_size: 43540667
- config_name: iwslt2017-it-ro
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - it
        - ro
  splits:
  - name: train
    num_bytes: 44485169
    num_examples: 217551
  - name: test
    num_bytes: 314974
    num_examples: 1643
  - name: validation
    num_bytes: 204989
    num_examples: 914
  download_size: 329391132
  dataset_size: 45005132
- config_name: iwslt2017-nl-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - en
  splits:
  - name: train
    num_bytes: 42843933
    num_examples: 237240
  - name: test
    num_bytes: 311646
    num_examples: 1777
  - name: validation
    num_bytes: 197814
    num_examples: 1003
  download_size: 329391132
  dataset_size: 43353393
- config_name: iwslt2017-nl-it
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - it
  splits:
  - name: train
    num_bytes: 43033168
    num_examples: 233415
  - name: test
    num_bytes: 309725
    num_examples: 1669
  - name: validation
    num_bytes: 197774
    num_examples: 1001
  download_size: 329391132
  dataset_size: 43540667
- config_name: iwslt2017-nl-ro
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - nl
        - ro
  splits:
  - name: train
    num_bytes: 41338738
    num_examples: 206920
  - name: test
    num_bytes: 320952
    num_examples: 1680
  - name: validation
    num_bytes: 202380
    num_examples: 913
  download_size: 329391132
  dataset_size: 41862070
- config_name: iwslt2017-ro-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ro
        - en
  splits:
  - name: train
    num_bytes: 44129950
    num_examples: 220538
  - name: test
    num_bytes: 316790
    num_examples: 1678
  - name: validation
    num_bytes: 205028
    num_examples: 914
  download_size: 329391132
  dataset_size: 44651768
- config_name: iwslt2017-ro-it
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ro
        - it
  splits:
  - name: train
    num_bytes: 44485169
    num_examples: 217551
  - name: test
    num_bytes: 314974
    num_examples: 1643
  - name: validation
    num_bytes: 204989
    num_examples: 914
  download_size: 329391132
  dataset_size: 45005132
- config_name: iwslt2017-ro-nl
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ro
        - nl
  splits:
  - name: train
    num_bytes: 41338738
    num_examples: 206920
  - name: test
    num_bytes: 320952
    num_examples: 1680
  - name: validation
    num_bytes: 202380
    num_examples: 913
  download_size: 329391132
  dataset_size: 41862070
- config_name: iwslt2017-ar-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ar
        - en
  splits:
  - name: train
    num_bytes: 56481059
    num_examples: 231713
  - name: test
    num_bytes: 2014296
    num_examples: 8583
  - name: validation
    num_bytes: 241206
    num_examples: 888
  download_size: 27748780
  dataset_size: 58736561
- config_name: iwslt2017-de-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - de
        - en
  splits:
  - name: train
    num_bytes: 42608380
    num_examples: 206112
  - name: test
    num_bytes: 1608474
    num_examples: 8079
  - name: validation
    num_bytes: 210975
    num_examples: 888
  download_size: 16758320
  dataset_size: 44427829
- config_name: iwslt2017-en-ar
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ar
  splits:
  - name: train
    num_bytes: 56481059
    num_examples: 231713
  - name: test
    num_bytes: 2014296
    num_examples: 8583
  - name: validation
    num_bytes: 241206
    num_examples: 888
  download_size: 29333173
  dataset_size: 58736561
- config_name: iwslt2017-en-de
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - de
  splits:
  - name: train
    num_bytes: 42608380
    num_examples: 206112
  - name: test
    num_bytes: 1608474
    num_examples: 8079
  - name: validation
    num_bytes: 210975
    num_examples: 888
  download_size: 16758334
  dataset_size: 44427829
- config_name: iwslt2017-en-fr
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - fr
  splits:
  - name: train
    num_bytes: 49273286
    num_examples: 232825
  - name: test
    num_bytes: 1767465
    num_examples: 8597
  - name: validation
    num_bytes: 207579
    num_examples: 890
  download_size: 27699724
  dataset_size: 51248330
- config_name: iwslt2017-en-ja
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ja
  splits:
  - name: train
    num_bytes: 48204987
    num_examples: 223108
  - name: test
    num_bytes: 1809007
    num_examples: 8469
  - name: validation
    num_bytes: 208124
    num_examples: 871
  download_size: 26983602
  dataset_size: 50222118
- config_name: iwslt2017-en-ko
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - ko
  splits:
  - name: train
    num_bytes: 51678043
    num_examples: 230240
  - name: test
    num_bytes: 1869793
    num_examples: 8514
  - name: validation
    num_bytes: 219295
    num_examples: 879
  download_size: 19364776
  dataset_size: 53767131
- config_name: iwslt2017-en-zh
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - en
        - zh
  splits:
  - name: train
    num_bytes: 44271004
    num_examples: 231266
  - name: test
    num_bytes: 1605527
    num_examples: 8549
  - name: validation
    num_bytes: 202537
    num_examples: 879
  download_size: 27597071
  dataset_size: 46079068
- config_name: iwslt2017-fr-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - fr
        - en
  splits:
  - name: train
    num_bytes: 49273286
    num_examples: 232825
  - name: test
    num_bytes: 1767465
    num_examples: 8597
  - name: validation
    num_bytes: 207579
    num_examples: 890
  download_size: 26880731
  dataset_size: 51248330
- config_name: iwslt2017-ja-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ja
        - en
  splits:
  - name: train
    num_bytes: 48204987
    num_examples: 223108
  - name: test
    num_bytes: 1809007
    num_examples: 8469
  - name: validation
    num_bytes: 208124
    num_examples: 871
  download_size: 26190859
  dataset_size: 50222118
- config_name: iwslt2017-ko-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - ko
        - en
  splits:
  - name: train
    num_bytes: 51678043
    num_examples: 230240
  - name: test
    num_bytes: 1869793
    num_examples: 8514
  - name: validation
    num_bytes: 219295
    num_examples: 879
  download_size: 19364733
  dataset_size: 53767131
- config_name: iwslt2017-zh-en
  features:
  - name: translation
    dtype:
      translation:
        languages:
        - zh
        - en
  splits:
  - name: train
    num_bytes: 44271004
    num_examples: 231266
  - name: test
    num_bytes: 1605527
    num_examples: 8549
  - name: validation
    num_bytes: 202537
    num_examples: 879
  download_size: 26849290
  dataset_size: 46079068
---

# Dataset Card for IWSLT 2017

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [https://sites.google.com/site/iwsltevaluation2017/TED-tasks](https://sites.google.com/site/iwsltevaluation2017/TED-tasks)
- **Repository:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Paper:** [Overview of the IWSLT 2017 Evaluation Campaign](https://aclanthology.org/2017.iwslt-1.1/)
- **Point of Contact:** [More Information Needed](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)
- **Size of downloaded dataset files:** 4.24 GB
- **Size of the generated dataset:** 1.14 GB
- **Total amount of disk used:** 5.38 GB

*This repository contain a modified version of the loading script used in the official [iwslt2017](https://huggingface.co/datasets/iwslt2017) repository updated to include document and segment information for all available sentence pairs, enabling their usage for document-level and context-aware MT applications. Refer to the original repository for additional information.*

",High,5.0
Translation,fhaddad/autotrain-data-fhdd_arabic_chatbot,0.0,41.0,2023-06-14 22:00:25+00:00,none,0.0,UNKNOWN,unknown,UNKNOWN,unknown,15622,none,none,"---
language:
- en
- ar
task_categories:
- translation

---
# AutoTrain Dataset for project: fhdd_arabic_chatbot

## Dataset Description

This dataset has been automatically processed by AutoTrain for project fhdd_arabic_chatbot.

### Languages

The BCP-47 code for the dataset's language is en2ar.

## Dataset Structure

### Data Instances

A sample from this dataset looks as follows:

```json
[
  {
    ""feat_sourceLang"": ""ara"",
    ""feat_targetlang"": ""eng"",
    ""target"": ""\u064a\u0646\u0628\u063a\u064a \u0623\u0646 \u062a\u064f\u0638\u0647\u0631 \u0627\u0644\u0646\u0651\u0633\u0627\u0621 \u0648\u062c\u0648\u0647\u0647\u0646\u0651."",
    ""source"": ""Women should have their faces visible.""
  },
  {
    ""feat_sourceLang"": ""ara"",
    ""feat_targetlang"": ""eng"",
    ""target"": ""\u0623\u062a\u062f\u0631\u0633 \u0627\u0644\u0625\u0646\u062c\u0644\u064a\u0632\u064a\u0629\u061f"",
    ""source"": ""Do you study English?""
  }
]
```

### Dataset Fields

The dataset has the following fields (also called ""features""):

```json
{
  ""feat_sourceLang"": ""Value(dtype='string', id=None)"",
  ""feat_targetlang"": ""Value(dtype='string', id=None)"",
  ""target"": ""Value(dtype='string', id=None)"",
  ""source"": ""Value(dtype='string', id=None)""
}
```

### Dataset Splits

This dataset is split into a train and validation split. The split sizes are as follow:

| Split name   | Num samples         |
| ------------ | ------------------- |
| train        | 15622 |
| valid        | 3906 |
",Low,1.0
Translation,visheratin/laion-coco-nllb,43.0,2186.0,2024-04-11 16:36:31+00:00,cc-by-nc-4.0,8.0,10.4 GB,11166914969.6,10.4 GB,11166914969.6,893884,https://arxiv.org/abs/2309.01859,none,"---
language:
- ace
- acm
- acq
- aeb
- af
- ajp
- ak
- als
- am
- apc
- ar
- ars
- ary
- arz
- as
- ast
- awa
- ayr
- azb
- azj
- ba
- bm
- ban
- be
- bem
- bn
- bho
- bjn
- bo
- bs
- bug
- bg
- ca
- ceb
- cs
- cjk
- ckb
- crh
- cy
- da
- de
- dik
- dyu
- dz
- el
- en
- eo
- et
- eu
- ee
- fo
- fj
- fi
- fon
- fr
- fur
- fuv
- gaz
- gd
- ga
- gl
- gn
- gu
- ht
- ha
- he
- hi
- hne
- hr
- hu
- hy
- ig
- ilo
- id
- is
- it
- jv
- ja
- kab
- kac
- kam
- kn
- ks
- ka
- kk
- kbp
- kea
- khk
- km
- ki
- rw
- ky
- kmb
- kmr
- knc
- kg
- ko
- lo
- lij
- li
- ln
- lt
- lmo
- ltg
- lb
- lua
- lg
- luo
- lus
- lvs
- mag
- mai
- ml
- mar
- min
- mk
- mt
- mni
- mos
- mi
- my
- nl
- nn
- nb
- npi
- nso
- nus
- ny
- oc
- ory
- pag
- pa
- pap
- pbt
- pes
- plt
- pl
- pt
- prs
- quy
- ro
- rn
- ru
- sg
- sa
- sat
- scn
- shn
- si
- sk
- sl
- sm
- sn
- sd
- so
- st
- es
- sc
- sr
- ss
- su
- sv
- swh
- szl
- ta
- taq
- tt
- te
- tg
- tl
- th
- ti
- tpi
- tn
- ts
- tk
- tum
- tr
- tw
- tzm
- ug
- uk
- umb
- ur
- uzn
- vec
- vi
- war
- wo
- xh
- ydd
- yo
- yue
- zh
- zsm
- zu
license: cc-by-nc-4.0
size_categories:
- 100K<n<1M
task_categories:
- image-to-text
- translation
pretty_name: LAION-COCO translated to 200 languages
dataset_info:
  features:
  - name: id
    dtype: string
  - name: url
    dtype: string
  - name: eng_caption
    dtype: string
  - name: captions
    sequence:
      sequence: string
  - name: score
    dtype: float64
  splits:
  - name: test
    num_bytes: 271360114
    num_examples: 14906
  - name: train
    num_bytes: 15986931307
    num_examples: 878978
  download_size: 10358151216
  dataset_size: 16258291421
language_details: ace_Arab, ace_Latn, acm_Arab, acq_Arab, aeb_Arab, afr_Latn, ajp_Arab,
  aka_Latn, amh_Ethi, apc_Arab, arb_Arab, ars_Arab, ary_Arab, arz_Arab, asm_Beng,
  ast_Latn, awa_Deva, ayr_Latn, azb_Arab, azj_Latn, bak_Cyrl, bam_Latn, ban_Latn,bel_Cyrl,
  bem_Latn, ben_Beng, bho_Deva, bjn_Arab, bjn_Latn, bod_Tibt, bos_Latn, bug_Latn,
  bul_Cyrl, cat_Latn, ceb_Latn, ces_Latn, cjk_Latn, ckb_Arab, crh_Latn, cym_Latn,
  dan_Latn, deu_Latn, dik_Latn, dyu_Latn, dzo_Tibt, ell_Grek, eng_Latn, epo_Latn,
  est_Latn, eus_Latn, ewe_Latn, fao_Latn, pes_Arab, fij_Latn, fin_Latn, fon_Latn,
  fra_Latn, fur_Latn, fuv_Latn, gla_Latn, gle_Latn, glg_Latn, grn_Latn, guj_Gujr,
  hat_Latn, hau_Latn, heb_Hebr, hin_Deva, hne_Deva, hrv_Latn, hun_Latn, hye_Armn,
  ibo_Latn, ilo_Latn, ind_Latn, isl_Latn, ita_Latn, jav_Latn, jpn_Jpan, kab_Latn,
  kac_Latn, kam_Latn, kan_Knda, kas_Arab, kas_Deva, kat_Geor, knc_Arab, knc_Latn,
  kaz_Cyrl, kbp_Latn, kea_Latn, khm_Khmr, kik_Latn, kin_Latn, kir_Cyrl, kmb_Latn,
  kon_Latn, kor_Hang, kmr_Latn, lao_Laoo, lvs_Latn, lij_Latn, lim_Latn, lin_Latn,
  lit_Latn, lmo_Latn, ltg_Latn, ltz_Latn, lua_Latn, lug_Latn, luo_Latn, lus_Latn,
  mag_Deva, mai_Deva, mal_Mlym, mar_Deva, min_Latn, mkd_Cyrl, plt_Latn, mlt_Latn,
  mni_Beng, khk_Cyrl, mos_Latn, mri_Latn, zsm_Latn, mya_Mymr, nld_Latn, nno_Latn,
  nob_Latn, npi_Deva, nso_Latn, nus_Latn, nya_Latn, oci_Latn, gaz_Latn, ory_Orya,
  pag_Latn, pan_Guru, pap_Latn, pol_Latn, por_Latn, prs_Arab, pbt_Arab, quy_Latn,
  ron_Latn, run_Latn, rus_Cyrl, sag_Latn, san_Deva, sat_Beng, scn_Latn, shn_Mymr,
  sin_Sinh, slk_Latn, slv_Latn, smo_Latn, sna_Latn, snd_Arab, som_Latn, sot_Latn,
  spa_Latn, als_Latn, srd_Latn, srp_Cyrl, ssw_Latn, sun_Latn, swe_Latn, swh_Latn,
  szl_Latn, tam_Taml, tat_Cyrl, tel_Telu, tgk_Cyrl, tgl_Latn, tha_Thai, tir_Ethi,
  taq_Latn, taq_Tfng, tpi_Latn, tsn_Latn, tso_Latn, tuk_Latn, tum_Latn, tur_Latn,
  twi_Latn, tzm_Tfng, uig_Arab, ukr_Cyrl, umb_Latn, urd_Arab, uzn_Latn, vec_Latn,
  vie_Latn, war_Latn, wol_Latn, xho_Latn, ydd_Hebr, yor_Latn, yue_Hant, zho_Hans,
  zho_Hant, zul_Latn
configs:
- config_name: default
  data_files:
  - split: test
    path: data/test-*
  - split: train
    path: data/train-*
---
# LAION COCO translated into 200 languages

This dataset contains the samples of the [LAION-COCO](https://huggingface.co/datasets/laion/laion-coco) dataset translated to 200 languages using 
the largest [NLLB-200 model](https://huggingface.co/facebook/nllb-200-3.3B) (3.3B parameters).

## Fields description

1. `id` - unique ID of the image.
2. `url` - original URL of the image from the LAION-COCO dataset.
3. `eng_caption` - original English caption from the LAION-COCO dataset.
4. `captions` - a list of captions translated to the languages from the Flores 200 dataset. Every item in the list is a list where the first element is a BCP-47 language code, and the second one is a caption in this language. The list of all language codes for the Flores 200 dataset can be found [here](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200).
5. `score` - aesthetic score generated using [LAION aesthetic predictor](https://github.com/christophschuhmann/improved-aesthetic-predictor/). The images in the dataset have the score of 4.5+.

## Images

The dataset was filtered to contain only working image URLs. However, the availability may change in the future. Because of that, all images from this dataset are available at [https://nllb-data.com/](https://nllb-data.com/).
To get the image, use the following format:

```
https://nllb-data.com/{id}.jpg
```

## Paper

The dataset was used to train the models in the paper: ""[NLLB-CLIP - train performant multilingual image retrieval model on a budget](https://arxiv.org/abs/2309.01859)"".",Medium,3.0
Translation,severo/flores_101,2.0,416.0,2022-10-27 08:37:36+00:00,cc-by-sa-4.0,0.0,1.35 GB,1449551462.4,51 MB,53477376.0,206927,https://arxiv.org/abs/2106.03193,none,"---
annotations_creators:
- found
language_creators:
- expert-generated
language:
- af
- am
- ar
- hy
- as
- ast
- az
- be
- bn
- bs
- bg
- my
- ca
- ceb
- zho
- hr
- cs
- da
- nl
- en
- et
- tl
- fi
- fr
- ff
- gl
- lg
- ka
- de
- el
- gu
- ha
- he
- hi
- hu
- is
- ig
- id
- ga
- it
- ja
- jv
- kea
- kam
- kn
- kk
- km
- ko
- ky
- lo
- lv
- ln
- lt
- luo
- lb
- mk
- ms
- ml
- mt
- mi
- mr
- mn
- ne
- ns
- 'no'
- ny
- oc
- or
- om
- ps
- fa
- pl
- pt
- pa
- ro
- ru
- sr
- sn
- sd
- sk
- sl
- so
- ku
- es
- sw
- sv
- tg
- ta
- te
- th
- tr
- uk
- umb
- ur
- uz
- vi
- cy
- wo
- xh
- yo
- zu
license:
- cc-by-sa-4.0
multilinguality:
- multilingual
- translation
size_categories:
- unknown
source_datasets:
- extended|flores
task_categories:
- text-generation
- translation
task_ids: []
paperswithcode_id: flores
pretty_name: flores101
tags:
- conditional-text-generation
---

# Dataset Card for Flores 101

## Table of Contents

- [Dataset Card for Flores 101](#dataset-card-for-flores-101)
  - [Table of Contents](#table-of-contents)
  - [Dataset Description](#dataset-description)
    - [Dataset Summary](#dataset-summary)
    - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
    - [Languages](#languages)
  - [Dataset Structure](#dataset-structure)
    - [Data Instances](#data-instances)
    - [Data Fields](#data-fields)
    - [Data Splits](#data-splits)
    - [Dataset Creation](#dataset-creation)
  - [Additional Information](#additional-information)
    - [Dataset Curators](#dataset-curators)
    - [Licensing Information](#licensing-information)
    - [Citation Information](#citation-information)

## Dataset Description

- **Home:** [WMT](http://www.statmt.org/wmt21/large-scale-multilingual-translation-task.html)
- **Repository:** [Github](https://github.com/facebookresearch/flores)
- **Blogpost:** [FAIR](https://ai.facebook.com/blog/the-flores-101-data-set-helping-build-better-translation-systems-around-the-world)
- **Paper:** [Arxiv](https://arxiv.org/abs/2106.03193)
- **Point of Contact:** [flores@fb.com](mailto:flores@fb.com)
- **Leaderboard** [Dynabench](https://dynabench.org/flores/Flores%20MT%20Evaluation%20(FULL))

### Dataset Summary

FLORES is a benchmark dataset for machine translation between English and low-resource languages.

Abstract from the original paper:

> One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the FLORES evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond.

**Disclaimer**: *The Flores-101 dataset is hosted by the Facebook and licensed under the [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).

### Supported Tasks and Leaderboards

#### Multilingual Machine Translation

Refer to the [Dynabench leaderboard](https://dynabench.org/flores/Flores%20MT%20Evaluation%20(FULL)) for additional details on model evaluation on FLORES-101 in the context of the WMT2021 shared task on [Large-Scale Multilingual Machine Translation](http://www.statmt.org/wmt21/large-scale-multilingual-translation-task.html).

### Languages

The dataset contains parallel sentences for 101 languages, as mentioned in the original [Github](https://github.com/facebookresearch/flores/blob/master/README.md) page for the project. Languages are identified with the ISO 639-3 code (e.g. `eng`, `fra`, `rus`) as in the original dataset.

**New:** Use the configuration `all` to access the full set of parallel sentences for all the available languages in a single command.


## Dataset Structure

### Data Instances

A sample from the `dev` split for the Russian language (`rus` config) is provided below. All configurations have the same structure, and all sentences are aligned across configurations and splits.

```python
{
	'id': 1,
	'sentence': 'В понедельник ученые из Медицинской школы Стэнфордского университета объявили об изобретении нового диагностического инструмента, который может сортировать клетки по их типу; это маленький чип, который можно напечатать, используя стандартный струйный принтер примерно за 1 цент США.',
	'URL': 'https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet',
	'domain': 'wikinews',
	'topic': 'health',
	'has_image': 0,
	'has_hyperlink': 0
}
```

The text is provided as-in the original dataset, without further preprocessing or tokenization.

### Data Fields

- `id`: Row number for the data entry, starting at 1.
- `sentence`: The full sentence in the specific language.
- `URL`: The URL for the English article from which the sentence was extracted.
- `domain`: The domain of the sentence.
- `topic`: The topic of the sentence.
- `has_image`: Whether the  original article contains an image.
- `has_hyperlink`: Whether the  sentence contains a hyperlink.

### Data Splits

|            config| `dev`| `devtest`|
|-----------------:|-----:|---------:|
|all configurations|   997|     1012:|

### Dataset Creation

Please refer to the original article [The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation](https://arxiv.org/abs/2106.03193) for additional information on dataset creation.

## Additional Information

### Dataset Curators

The original authors of FLORES-101 are the curators of the original dataset. For problems or updates on this 🤗 Datasets version, please contact [gabriele.sarti996@gmail.com](mailto:gabriele.sarti996@gmail.com).

### Licensing Information

Licensed with Creative Commons Attribution Share Alike 4.0. License available [here](https://creativecommons.org/licenses/by-sa/4.0/).

### Citation Information

Please cite the authors if you use these corpora in your work:

```bibtex
@inproceedings{flores101,
  title={The FLORES-101  Evaluation Benchmark for Low-Resource and Multilingual Machine Translation},
  author={Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc'Aurelio and Guzm\'{a}n, Francisco and Fan, Angela},
  journal={arXiv preprint arXiv:2106.03193},
  year={2021}
}
```",High,4.0
Translation,Amani27/massive_translation_dataset,10.0,113.0,2023-07-25 14:54:44+00:00,cc-by-4.0,1.0,8.01 MB,8399093.76,4.33 MB,4540334.08,16521,none,none,"---
configs:
- config_name: default
  data_files:
  - split: train
    path: ""train.csv""
  - split: validation
    path: ""validation.csv""
  - split: test
    path: ""test.csv""
    
license: cc-by-4.0
task_categories:
- translation
language:
- en
- de
- es
- hi
- fr
- it
- ar
- nl
- ja
- pt
size_categories:
- 10K<n<100K
---
# Dataset Card for Massive Dataset for Translation


### Dataset Summary
This dataset is derived from AmazonScience/MASSIVE dataset for translation task purpose.

### Supported Tasks and Leaderboards

Translation

### Languages
1. English (en_US)
2. German (de_DE)
3. Hindi (hi_IN)
4. Spanish (es_ES)
5. French (fr_FR)
6. Italian (it_IT)
7. Arabic (ar_SA)
8. Dutch (nl_NL)
9. Japanese (ja_JP)
10. Portugese (pt_PT)

",Low,1.0
Translation,crystalai/autotrain-data-crystal_alchemist-vision,1.0,0.0,2023-08-25 01:37:45+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,none,Low,0.0
Translation,PetraAI/PetraAI,21.0,204.0,2023-09-14 21:04:52+00:00,apache-2.0,1.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- text-classification
- token-classification
- table-question-answering
- question-answering
- zero-shot-classification
- translation
- summarization
- conversational
- feature-extraction
- text-generation
- text2text-generation
- fill-mask
- sentence-similarity
- text-to-speech
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
- image-to-text
- image-to-image
- unconditional-image-generation
- video-classification
- reinforcement-learning
- robotics
- tabular-classification
- tabular-regression
- tabular-to-text
- table-to-text
- multiple-choice
- text-retrieval
- time-series-forecasting
- text-to-video
- visual-question-answering
- zero-shot-image-classification
- graph-ml
language:
- ar
- en
tags:
- chemistry
- biology
- finance
- legal
- music
- art
- code
- climate
- medical
pretty_name: PETRA
size_categories:
- 1M<n<10M
---
# PETRA 

## Overview

PETRA is a multilingual dataset for training and evaluating AI systems on a diverse range of tasks across multiple modalities. It contains data in Arabic and English for tasks including translation, summarization, question answering, and more.

## Dataset Structure

- Data is separated by language into `/ar` and `/en` directories
- Within each language directory, data is separated by task into subdirectories  
- Tasks include:
  - Translation
  - Summarization
  - Conversational
  - Feature extraction
  - Zero-shot classification
  - Text generation
  - Fill mask
  - Sentence similarity
  - Text-to-speech
  - Automatic speech recognition
  - Text classification
  - Token classification
  - Table question answering
  - Question answering
  - Text2text generation
  - Audio-to-audio
  - Audio classification
  - Voice activity detection
  - Depth estimation
  - Image classification
  - Object detection
  - Image segmentation
  - Text-to-image
  - Image-to-text
  - Image-to-image
  - Unconditional image generation
  - Reinforcement learning
  - Video classification
  - Robotics
  - Tabular classification
  - Tabular regression
  - Table-to-text
  - Multiple choice
  - Text retrieval
  - Tabular-to-text
  - Text-to-video
  - Time series forecasting
  - Visual question answering
  - Zero-shot image classification
  - Graph ML

## Dataset Tags 

- code
- art
- chemistry
- biology  
- finance
- legal
- music
- climate
- medical

## Dataset Size

1M < n < 10M samples  

## Licenses

Apache 2.0

## Citation

If you use this dataset, please cite it as:

[cite paper, arXiv, etc] 

@article{PetraAI2022PetraAI,
  title={PetraAI: A Massive Multilingual Dataset for Machine Learning}, 
  author={First Last and First Last},
  journal={arXiv},
  year={2022},
  url={https://huggingface.co/datasets/PetraAI/PetraAI}
}

## Contact

For any questions, please reach out to [shadilytn@gmail.com]


# Dataset Cards

## What are Dataset Cards?

Each dataset may be documented by the `README.md` file in the repository. This file is called a **dataset card**, and the Hugging Face Hub will render its contents on the dataset’s main page. To inform users about how to responsibly use the data, it’s a good idea to include information about any potential biases within the dataset. Generally, dataset cards help users understand the contents of the dataset and give context for how the dataset should be used. 

You can also add dataset metadata to your card. The metadata describes important information about a dataset such as its license, language, and size. It also contains tags to help users discover a dataset on the Hub. Tags are defined in a YAML metadata section at the top of the `README.md` file.

## Dataset card metadata

A dataset repo will render its README.md as a dataset card. To control how the Hub displays the card, you should create a YAML section in the README file to define some metadata. Start by adding three --- at the top, then include all of the relevant metadata, and close the section with another group of --- like the example below:


The metadata that you add to the dataset card enables certain interactions on the Hub. For example:

- Allow users to filter and discover datasets at https://huggingface.co/datasets.
  
- If you choose a license using the keywords listed in the right column of this table, the license will be displayed on the dataset page.

When creating a README.md file in a dataset repository on the Hub, use Metadata UI to fill the main metadata:

To see metadata fields, see the detailed dataset card metadata specification here.

### Dataset card creation guide

For a step-by-step guide on creating a dataset card, check out the Create a dataset card guide. 

Reading through existing dataset cards, such as the ELI5 dataset card, is a great way to familiarize yourself with the common conventions.

### Linking a Paper

If the dataset card includes a link to a paper on arXiv, the Hub will extract the arXiv ID and include it in the dataset tags with the format `arxiv:<PAPER ID>`. Clicking on the tag will let you:

- Visit the Paper page
  
- Filter for other models on the Hub that cite the same paper.

Read more about paper pages here.

https://huggingface.co/docs/hub/paper-pages",High,5.0
Translation,Sylvana/qa_en_translation,2.0,17.0,2023-08-18 07:51:14+00:00,apache-2.0,0.0,2.14 MB,unknown,1.3 MB,unknown,5500,none,none,"---
license: apache-2.0
task_categories:
- translation
language:
- ar
size_categories:
- 1K<n<10K
---

# Dataset Card for Dataset Name

## Dataset Description

- **Homepage:** 
- **Repository:** 
- **Paper:** 
- **Leaderboard:** 
- **Point of Contact:** 

### Dataset Summary

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

### Supported Tasks and Leaderboards

[More Information Needed]

### Languages

[More Information Needed]

## Dataset Structure

### Data Instances

[More Information Needed]

### Data Fields

[More Information Needed]

### Data Splits

[More Information Needed]

## Dataset Creation

### Curation Rationale

[More Information Needed]

### Source Data

#### Initial Data Collection and Normalization

[More Information Needed]

#### Who are the source language producers?

[More Information Needed]

### Annotations

#### Annotation process

[More Information Needed]

#### Who are the annotators?

[More Information Needed]

### Personal and Sensitive Information

[More Information Needed]

## Considerations for Using the Data

### Social Impact of Dataset

[More Information Needed]

### Discussion of Biases

[More Information Needed]

### Other Known Limitations

[More Information Needed]

## Additional Information

### Dataset Curators

[More Information Needed]

### Licensing Information

[More Information Needed]

### Citation Information

[More Information Needed]

### Contributions

[More Information Needed]",Medium,3.0
Translation,M-A-D/Mixed-Arabic-Datasets-Repo,34.0,1229.0,2023-10-16 21:25:35+00:00,none,0.0,28.6 GB,30709016166.4,28.6 GB,30709016166.4,208606355,none,none,"---
language:
- ar
size_categories:
- 1B<n<10B
task_categories:
- text-classification
- question-answering
- translation
- summarization
- conversational
- text-generation
- text2text-generation
- fill-mask
pretty_name: Mixed Arabic Datasets (MAD) Corpus
dataset_info:
- config_name: Ara--Ali-C137--Hindawi-Books-dataset
  features:
  - name: BookLink
    dtype: string
  - name: BookName
    dtype: string
  - name: AuthorName
    dtype: string
  - name: AboutBook
    dtype: string
  - name: ChapterLink
    dtype: string
  - name: ChapterName
    dtype: string
  - name: ChapterText
    dtype: string
  - name: AboutAuthor
    dtype: string
  splits:
  - name: train
    num_bytes: 1364854259
    num_examples: 49821
  download_size: 494678002
  dataset_size: 1364854259
- config_name: Ara--Goud--Goud-sum
  features:
  - name: article
    dtype: string
  - name: headline
    dtype: string
  - name: categories
    dtype: string
  splits:
  - name: train
    num_bytes: 288296544
    num_examples: 139288
  download_size: 147735776
  dataset_size: 288296544
- config_name: Ara--J-Mourad--MNAD.v1
  features:
  - name: Title
    dtype: string
  - name: Body
    dtype: string
  - name: Category
    dtype: string
  splits:
  - name: train
    num_bytes: 1101921980
    num_examples: 418563
  download_size: 527154122
  dataset_size: 1101921980
- config_name: Ara--JihadZa--IADD
  features:
  - name: Sentence
    dtype: string
  - name: Region
    dtype: string
  - name: DataSource
    dtype: string
  - name: Country
    dtype: string
  splits:
  - name: train
    num_bytes: 19167070
    num_examples: 135804
  download_size: 8644491
  dataset_size: 19167070
- config_name: Ara--LeMGarouani--MAC-corpus
  features:
  - name: tweets
    dtype: string
  - name: type
    dtype: string
  - name: class
    dtype: string
  splits:
  - name: train
    num_bytes: 1945646
    num_examples: 18087
  download_size: 866198
  dataset_size: 1945646
- config_name: Ara--MBZUAI--Bactrian-X
  features:
  - name: instruction
    dtype: string
  - name: input
    dtype: string
  - name: id
    dtype: string
  - name: output
    dtype: string
  splits:
  - name: train
    num_bytes: 66093524
    num_examples: 67017
  download_size: 33063779
  dataset_size: 66093524
- config_name: Ara--OpenAssistant--oasst1
  features:
  - name: message_id
    dtype: string
  - name: parent_id
    dtype: string
  - name: user_id
    dtype: string
  - name: created_date
    dtype: string
  - name: text
    dtype: string
  - name: role
    dtype: string
  - name: lang
    dtype: string
  - name: review_count
    dtype: int32
  - name: review_result
    dtype: bool
  - name: deleted
    dtype: bool
  - name: rank
    dtype: float64
  - name: synthetic
    dtype: bool
  - name: model_name
    dtype: 'null'
  - name: detoxify
    dtype: 'null'
  - name: message_tree_id
    dtype: string
  - name: tree_state
    dtype: string
  - name: emojis
    struct:
    - name: count
      sequence: int32
    - name: name
      sequence: string
  - name: labels
    struct:
    - name: count
      sequence: int32
    - name: name
      sequence: string
    - name: value
      sequence: float64
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 58168
    num_examples: 56
  download_size: 30984
  dataset_size: 58168
- config_name: Ara--Wikipedia
  features:
  - name: id
    dtype: string
  - name: url
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 3052201469
    num_examples: 1205403
  download_size: 1316212231
  dataset_size: 3052201469
- config_name: Ara--bigscience--xP3
  features:
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  splits:
  - name: train
    num_bytes: 4727881680
    num_examples: 2148955
  download_size: 2805060725
  dataset_size: 4727881680
- config_name: Ara--cardiffnlp--tweet_sentiment_multilingual
  features:
  - name: text
    dtype: string
  - name: label
    dtype:
      class_label:
        names:
          '0': negative
          '1': neutral
          '2': positive
  splits:
  - name: train
    num_bytes: 306108
    num_examples: 1839
  - name: validation
    num_bytes: 53276
    num_examples: 324
  - name: test
    num_bytes: 141536
    num_examples: 870
  download_size: 279900
  dataset_size: 500920
- config_name: Ara--miracl--miracl
  features:
  - name: query_id
    dtype: string
  - name: query
    dtype: string
  - name: positive_passages
    list:
    - name: docid
      dtype: string
    - name: text
      dtype: string
    - name: title
      dtype: string
  - name: negative_passages
    list:
    - name: docid
      dtype: string
    - name: text
      dtype: string
    - name: title
      dtype: string
  splits:
  - name: train
    num_bytes: 32012083
    num_examples: 3495
  download_size: 15798509
  dataset_size: 32012083
- config_name: Ara--mustapha--QuranExe
  features:
  - name: text
    dtype: string
  - name: resource_name
    dtype: string
  - name: verses_keys
    dtype: string
  splits:
  - name: train
    num_bytes: 133108687
    num_examples: 49888
  download_size: 58769417
  dataset_size: 133108687
- config_name: Ara--pain--Arabic-Tweets
  features:
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 41639770853
    num_examples: 202700438
  download_size: 22561651700
  dataset_size: 41639770853
- config_name: Ara--saudinewsnet
  features:
  - name: source
    dtype: string
  - name: url
    dtype: string
  - name: date_extracted
    dtype: string
  - name: title
    dtype: string
  - name: author
    dtype: string
  - name: content
    dtype: string
  splits:
  - name: train
    num_bytes: 103654009
    num_examples: 31030
  download_size: 49117164
  dataset_size: 103654009
- config_name: Ary--AbderrahmanSkiredj1--Darija-Wikipedia
  features:
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 8104410
    num_examples: 4862
  download_size: 3229966
  dataset_size: 8104410
- config_name: Ary--Ali-C137--Darija-Stories-Dataset
  features:
  - name: ChapterName
    dtype: string
  - name: ChapterLink
    dtype: string
  - name: Author
    dtype: string
  - name: Text
    dtype: string
  - name: Tags
    dtype: int64
  splits:
  - name: train
    num_bytes: 476926644
    num_examples: 6142
  download_size: 241528641
  dataset_size: 476926644
- config_name: Ary--Wikipedia
  features:
  - name: id
    dtype: string
  - name: url
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 10007364
    num_examples: 6703
  download_size: 4094377
  dataset_size: 10007364
- config_name: Arz--Wikipedia
  features:
  - name: id
    dtype: string
  - name: url
    dtype: string
  - name: title
    dtype: string
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 1364641408
    num_examples: 1617770
  download_size: 306420318
  dataset_size: 1364641408
configs:
- config_name: Ara--Ali-C137--Hindawi-Books-dataset
  data_files:
  - split: train
    path: Ara--Ali-C137--Hindawi-Books-dataset/train-*
- config_name: Ara--Goud--Goud-sum
  data_files:
  - split: train
    path: Ara--Goud--Goud-sum/train-*
- config_name: Ara--J-Mourad--MNAD.v1
  data_files:
  - split: train
    path: Ara--J-Mourad--MNAD.v1/train-*
- config_name: Ara--JihadZa--IADD
  data_files:
  - split: train
    path: Ara--JihadZa--IADD/train-*
- config_name: Ara--LeMGarouani--MAC-corpus
  data_files:
  - split: train
    path: Ara--LeMGarouani--MAC-corpus/train-*
- config_name: Ara--MBZUAI--Bactrian-X
  data_files:
  - split: train
    path: Ara--MBZUAI--Bactrian-X/train-*
- config_name: Ara--OpenAssistant--oasst1
  data_files:
  - split: train
    path: Ara--OpenAssistant--oasst1/train-*
- config_name: Ara--Wikipedia
  data_files:
  - split: train
    path: Ara--Wikipedia/train-*
- config_name: Ara--bigscience--xP3
  data_files:
  - split: train
    path: Ara--bigscience--xP3/train-*
- config_name: Ara--cardiffnlp--tweet_sentiment_multilingual
  data_files:
  - split: train
    path: Ara--cardiffnlp--tweet_sentiment_multilingual/train-*
  - split: validation
    path: Ara--cardiffnlp--tweet_sentiment_multilingual/validation-*
  - split: test
    path: Ara--cardiffnlp--tweet_sentiment_multilingual/test-*
- config_name: Ara--miracl--miracl
  data_files:
  - split: train
    path: Ara--miracl--miracl/train-*
- config_name: Ara--mustapha--QuranExe
  data_files:
  - split: train
    path: Ara--mustapha--QuranExe/train-*
- config_name: Ara--pain--Arabic-Tweets
  data_files:
  - split: train
    path: Ara--pain--Arabic-Tweets/train-*
- config_name: Ara--saudinewsnet
  data_files:
  - split: train
    path: Ara--saudinewsnet/train-*
- config_name: Ary--AbderrahmanSkiredj1--Darija-Wikipedia
  data_files:
  - split: train
    path: Ary--AbderrahmanSkiredj1--Darija-Wikipedia/train-*
- config_name: Ary--Ali-C137--Darija-Stories-Dataset
  data_files:
  - split: train
    path: Ary--Ali-C137--Darija-Stories-Dataset/train-*
- config_name: Ary--Wikipedia
  data_files:
  - split: train
    path: Ary--Wikipedia/train-*
- config_name: Arz--Wikipedia
  data_files:
  - split: train
    path: Arz--Wikipedia/train-*
---
# Dataset Card for ""Mixed Arabic Datasets (MAD) Corpus""

**The Mixed Arabic Datasets Corpus : A Community-Driven Collection of Diverse Arabic Texts**

## Dataset Description

The Mixed Arabic Datasets (MAD) presents a dynamic compilation of diverse Arabic texts sourced from various online platforms and datasets. It addresses a critical challenge faced by researchers, linguists, and language enthusiasts: the fragmentation of Arabic language datasets across the Internet. With MAD, we are trying to centralize these dispersed resources into a single, comprehensive repository.

Encompassing a wide spectrum of content, ranging from social media conversations to literary masterpieces, MAD captures the rich tapestry of Arabic communication, including both standard Arabic and regional dialects.

This corpus offers comprehensive insights into the linguistic diversity and cultural nuances of Arabic expression.

## Usage 

If you want to use this dataset you pick one among the available configs:

`Ara--MBZUAI--Bactrian-X` | `Ara--OpenAssistant--oasst1` | `Ary--AbderrahmanSkiredj1--Darija-Wikipedia`

`Ara--Wikipedia` | `Ary--Wikipedia` | `Arz--Wikipedia`

`Ary--Ali-C137--Darija-Stories-Dataset` | `Ara--Ali-C137--Hindawi-Books-dataset` | ``

Example of usage:

```python
dataset = load_dataset('M-A-D/Mixed-Arabic-Datasets-Repo', 'Ara--MBZUAI--Bactrian-X')
```

If you loaded multiple datasets and wanted to merge them together then you can simply laverage `concatenate_datasets()` from `datasets`

```pyhton
dataset3 = concatenate_datasets([dataset1['train'], dataset2['train']])
```

Note : proccess the datasets before merging in order to make sure you have a new dataset that is consistent

## Dataset Size

The Mixed Arabic Datasets (MAD) is a dynamic and evolving collection, with its size fluctuating as new datasets are added or removed. As MAD continuously expands, it becomes a living resource that adapts to the ever-changing landscape of Arabic language datasets.

**Dataset List**

MAD draws from a diverse array of sources, each contributing to its richness and breadth. While the collection is constantly evolving, some of the datasets that are poised to join MAD in the near future include:

- [✔] OpenAssistant/oasst1 (ar portion) : [Dataset Link](https://huggingface.co/datasets/OpenAssistant/oasst1)
- [✔] MBZUAI/Bactrian-X (ar portion) : [Dataset Link](https://huggingface.co/datasets/MBZUAI/Bactrian-X/viewer/ar/train)
- [✔] AbderrahmanSkiredj1/Darija-Wikipedia : [Dataset Link](https://huggingface.co/datasets/AbderrahmanSkiredj1/moroccan_darija_wikipedia_dataset)
- [✔] Arabic Wikipedia : [Dataset Link](https://huggingface.co/datasets/wikipedia)
- [✔] Moroccan Arabic Wikipedia : [Dataset Link](https://huggingface.co/datasets/wikipedia)
- [✔] Egyptian Arabic Wikipedia : [Dataset Link](https://huggingface.co/datasets/wikipedia)
- [✔] Darija Stories Dataset : [Dataset Link](https://huggingface.co/datasets/Ali-C137/Darija-Stories-Dataset)
- [✔] Hindawi Books Dataset : [Dataset Link](https://huggingface.co/datasets/Ali-C137/Hindawi-Books-dataset)
- [] uonlp/CulturaX - ar : [Dataset Link](https://huggingface.co/datasets/uonlp/CulturaX/viewer/ar/train)
- [✔] Pain/ArabicTweets : [Dataset Link](https://huggingface.co/datasets/pain/Arabic-Tweets)
- [] Abu-El-Khair Corpus : [Dataset Link](https://huggingface.co/datasets/arabic_billion_words)
- [✔] QuranExe : [Dataset Link](https://huggingface.co/datasets/mustapha/QuranExe)
- [✔] MNAD : [Dataset Link](https://huggingface.co/datasets/J-Mourad/MNAD.v1)
- [✔] IADD : [Dataset Link](https://raw.githubusercontent.com/JihadZa/IADD/main/IADD.json)
- [] OSIAN : [Dataset Link](https://wortschatz.uni-leipzig.de/en/download/Arabic#ara-tn_newscrawl-OSIAN_2018)
- [✔] MAC corpus : [Dataset Link](https://raw.githubusercontent.com/LeMGarouani/MAC/main/MAC%20corpus.csv)
- [✔] Goud.ma-Sum : [Dataset Link](https://huggingface.co/datasets/Goud/Goud-sum)
- [✔] SaudiNewsNet : [Dataset Link](https://huggingface.co/datasets/saudinewsnet)
- [✔] Miracl : [Dataset Link](https://huggingface.co/datasets/miracl/miracl)
- [✔] CardiffNLP/TweetSentimentMulti : [Dataset Link](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual)
- [] OSCAR-2301 : [Dataset Link](https://huggingface.co/datasets/oscar-corpus/OSCAR-2301/viewer/ar/train)
- [] mc4 : [Dataset Link](https://huggingface.co/datasets/mc4/viewer/ar/train)
- [✔] bigscience/xP3 : [Dataset Link](https://huggingface.co/datasets/bigscience/xP3/viewer/ar/train)
- [] Muennighoff/xP3x : [Dataset Link](https://huggingface.co/datasets/Muennighoff/xP3x)
- [] Ai_Society : [Dataset Link](https://huggingface.co/datasets/camel-ai/ai_society_translated)


## Potential Use Cases

The Mixed Arabic Datasets (MAD) holds the potential to catalyze a multitude of groundbreaking applications:

- **Linguistic Analysis:** Employ MAD to conduct in-depth linguistic studies, exploring dialectal variances, language evolution, and grammatical structures.
- **Topic Modeling:** Dive into diverse themes and subjects through the extensive collection, revealing insights into emerging trends and prevalent topics.
- **Sentiment Understanding:** Decode sentiments spanning Arabic dialects, revealing cultural nuances and emotional dynamics.
- **Sociocultural Research:** Embark on a sociolinguistic journey, unraveling the intricate connection between language, culture, and societal shifts.

## Dataset Access

MAD's access mechanism is unique: while it doesn't carry a general license itself, each constituent dataset within the corpus retains its individual license. By accessing the dataset details through the provided links in the ""Dataset List"" section above, users can understand the specific licensing terms for each dataset.

### Join Us on Discord

For discussions, contributions, and community interactions, join us on Discord! [![Discord](https://img.shields.io/discord/798499298231726101?label=Join%20us%20on%20Discord&logo=discord&logoColor=white&style=for-the-badge)](https://discord.gg/2NpJ9JGm)

### How to Contribute

Want to contribute to the Mixed Arabic Datasets project? Follow our comprehensive guide on Google Colab for step-by-step instructions: [Contribution Guide](https://colab.research.google.com/drive/1kOIRoicgCOV8TPvASAI_2uMY7rpXnqzJ?usp=sharing).

**Note**: If you'd like to test a contribution before submitting it, feel free to do so on the [MAD Test Dataset](https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Dataset-test).

## Citation

```
@dataset{ 
title = {Mixed Arabic Datasets (MAD)},
author = {MAD Community},
howpublished = {Dataset},
url = {https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Datasets-Repo},
year = {2023},
}
```",High,6.0
Translation,dot-ammar/AR-dotless-small,0.0,31.0,2023-09-11 15:35:39+00:00,none,0.0,10.5 MB,11010048.0,10.5 MB,11010048.0,103403,none,none,"---
language:
- ar
size_categories:
- 10K<n<100K
task_categories:
- translation
pretty_name: f
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
dataset_info:
  features:
  - name: clean
    dtype: string
  - name: dotless
    dtype: string
  splits:
  - name: train
    num_bytes: 18718829.46787407
    num_examples: 103403
  download_size: 10451596
  dataset_size: 18718829.46787407
---
# Dataset Card for ""AR-dotless-small""

[More Information needed](https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)",Low,1.0
Translation,Nicolas-BZRD/English_French_Songs_Lyrics_Translation_Original,13.0,47.0,2024-02-08 23:34:15+00:00,unknown,0.0,122 MB,127926272.0,122 MB,127926272.0,99289,https://arxiv.org/abs/2402.00786,none,"---
license: unknown
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
dataset_info:
  features:
  - name: artist_name
    dtype: string
  - name: album_name
    dtype: string
  - name: year
    dtype: int64
  - name: title
    dtype: string
  - name: number
    dtype: int64
  - name: original_version
    dtype: string
  - name: french_version
    dtype: string
  - name: language
    dtype: string
  splits:
  - name: train
    num_bytes: 250317845
    num_examples: 99289
  download_size: 122323323
  dataset_size: 250317845
task_categories:
- translation
- text-generation
language:
- fr
- en
- es
- it
- de
- ko
- id
- pt
- 'no'
- fi
- sv
- sw
- hr
- so
- ca
- tl
- ja
- nl
- ru
- et
- tr
- ro
- cy
- vi
- af
- hu
- sk
- sl
- cs
- da
- pl
- sq
- el
- he
- zh
- th
- bg
- ar
tags:
- music
- parallel
- parallel data
pretty_name: SYFT
size_categories:
- 10K<n<100K
---

# Original Songs Lyrics with French Translation

### Dataset Summary

Dataset of 99289 songs containing their metadata (author, album, release date, song number), original lyrics and lyrics translated into French.

Details of the number of songs by language of origin can be found in the table below:

| Original language | Number of songs |
|---|:---|
| en | 75786 |
| fr | 18486 |
| es | 1743 |
| it | 803 |
| de | 691 |
| sw | 529 |
| ko | 193 |
| id | 169 |
| pt | 142 |
| no | 122 |
| fi | 113 |
| sv | 70 |
| hr | 53 |
| so | 43 |
| ca | 41 |
| tl | 36 |
| ja | 35 |
| nl | 32 |
| ru | 29 |
| et | 27 |
| tr | 22 |
| ro | 19 |
| cy | 14 |
| vi | 14 |
| af | 13 |
| hu | 10 |
| sk | 10 |
| sl | 10 |
| cs | 7 |
| da | 6 |
| pl | 5 |
| sq | 4 |
| el | 4 |
| he | 3 |
| zh-cn | 2 |
| th | 1 |
| bg | 1 |
| ar | 1 |


## Citation

Our work can be cited as:

```bash
@misc{faysse2024croissantllm,
      title={CroissantLLM: A Truly Bilingual French-English Language Model}, 
      author={Manuel Faysse and Patrick Fernandes and Nuno Guerreiro and António Loison and Duarte Alves and Caio Corro and Nicolas Boizard and João Alves and Ricardo Rei and Pedro Martins and Antoni Bigata Casademunt and François Yvon and André Martins and Gautier Viaud and Céline Hudelot and Pierre Colombo},
      year={2024},
      eprint={2402.00786},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```",High,4.0
Translation,M-A-D/Mixed-Arabic-Dataset-Main,6.0,173.0,2023-10-06 17:56:33+00:00,none,3.0,791 MB,829423616.0,791 MB,829423616.0,131393,none,none,"---
language:
- ar
task_categories:
- conversational
- text-generation
- text2text-generation
- translation
- summarization
pretty_name: MAD
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
dataset_info:
  features:
  - name: GenId
    dtype: int64
  - name: SubId
    dtype: int64
  - name: DatasetName
    dtype: string
  - name: DatasetLink
    dtype: string
  - name: Text
    dtype: string
  - name: MetaData
    struct:
    - name: AboutAuthor
      dtype: string
    - name: AboutBook
      dtype: string
    - name: Author
      dtype: string
    - name: AuthorName
      dtype: string
    - name: BookLink
      dtype: string
    - name: BookName
      dtype: string
    - name: ChapterLink
      dtype: string
    - name: ChapterName
      dtype: string
    - name: Tags
      dtype: float64
    - name: __index_level_0__
      dtype: float64
    - name: created_date
      dtype: string
    - name: deleted
      dtype: bool
    - name: detoxify
      dtype: 'null'
    - name: emojis
      struct:
      - name: count
        sequence: int32
      - name: name
        sequence: string
    - name: id
      dtype: string
    - name: labels
      struct:
      - name: count
        sequence: int32
      - name: name
        sequence: string
      - name: value
        sequence: float64
    - name: lang
      dtype: string
    - name: message_id
      dtype: string
    - name: message_tree_id
      dtype: string
    - name: model_name
      dtype: 'null'
    - name: parent_id
      dtype: string
    - name: query_id
      dtype: string
    - name: rank
      dtype: float64
    - name: review_count
      dtype: float64
    - name: review_result
      dtype: bool
    - name: role
      dtype: string
    - name: synthetic
      dtype: bool
    - name: title
      dtype: string
    - name: tree_state
      dtype: string
    - name: url
      dtype: string
    - name: user_id
      dtype: string
  - name: ConcatenatedText
    dtype: int64
  - name: __index_level_0__
    dtype: float64
  splits:
  - name: train
    num_bytes: 1990497610
    num_examples: 131393
  download_size: 790648134
  dataset_size: 1990497610
---
# Dataset Card for ""Mixed-Arabic-Dataset""

## Mixed Arabic Datasets (MAD)

The Mixed Arabic Datasets (MAD) project provides a comprehensive collection of diverse Arabic-language datasets, sourced from various repositories, platforms, and domains. These datasets cover a wide range of text types, including books, articles, Wikipedia content, stories, and more.

### MAD Repo vs. MAD Main

#### MAD Repo
- **Versatility**: In the MAD Repository (MAD Repo), datasets are made available in their original, native form. Researchers and practitioners can selectively download specific datasets that align with their specific interests or requirements.
- **Independent Access**: Each dataset is self-contained, enabling users to work with individual datasets independently, allowing for focused analyses and experiments.

#### MAD Main or simply MAD
- **Unified Dataframe**: MAD Main represents a harmonized and unified dataframe, incorporating all datasets from the MAD Repository. It provides a seamless and consolidated view of the entire MAD collection, making it convenient for comprehensive analyses and applications.
- **Holistic Perspective**: Researchers can access a broad spectrum of Arabic-language content within a single dataframe, promoting holistic exploration and insights across diverse text sources.

### Why MAD Main?
- **Efficiency**: Working with MAD Main streamlines the data acquisition process by consolidating multiple datasets into one structured dataframe. This is particularly beneficial for large-scale projects or studies requiring diverse data sources.
- **Interoperability**: With MAD Main, the datasets are integrated into a standardized format, enhancing interoperability and compatibility with a wide range of data processing and analysis tools.
- **Meta-Analysis**: Researchers can conduct comprehensive analyses, such as cross-domain studies, trend analyses, or comparative studies, by leveraging the combined richness of all MAD datasets.

### Getting Started
- To access individual datasets in their original form, refer to the MAD Repository ([Link to MAD Repo](https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Datasets-Repo)).
- For a unified view of all datasets, conveniently organized in a dataframe, you are here in the right place.
```python
from datasets import load_dataset

dataset = load_dataset(""M-A-D/Mixed-Arabic-Dataset-Main"")
```

### Join Us on Discord

For discussions, contributions, and community interactions, join us on Discord! [![Discord](https://img.shields.io/discord/798499298231726101?label=Join%20us%20on%20Discord&logo=discord&logoColor=white&style=for-the-badge)](https://discord.gg/2NpJ9JGm)

### How to Contribute

Want to contribute to the Mixed Arabic Datasets project? Follow our comprehensive guide on Google Colab for step-by-step instructions: [Contribution Guide](https://colab.research.google.com/drive/1w7_7lL6w7nM9DcDmTZe1Vfiwkio6SA-w?usp=sharing).

**Note**: If you'd like to test a contribution before submitting it, feel free to do so on the [MAD Test Dataset](https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Dataset-test).

## Citation

```
@dataset{ 
title = {Mixed Arabic Datasets (MAD)},
author = {MAD Community},
howpublished = {Dataset},
url = {https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Datasets-Repo},
year = {2023},
}
```",Medium,3.0
Translation,BounharAbdelaziz/English-to-Moroccan-Darija,10.0,10.0,2024-04-05 14:26:03+00:00,none,1.0,1.7 MB,1782579.2,1.7 MB,1782579.2,16089,none,none,"---
language:
- ar
size_categories:
- 10K<n<100K
task_categories:
- translation
dataset_info:
  features:
  - name: english
    dtype: string
  - name: darija
    dtype: string
  - name: includes_arabizi
    dtype: bool
  splits:
  - name: train
    num_bytes: 2898935
    num_examples: 16089
  download_size: 1704472
  dataset_size: 2898935
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
---
# Dataset Card for ""English-to-Moroccan-Darija""

[More Information needed](https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)",Low,1.0
Translation,PeepDaSlan9/DIDI,2.0,21.0,2023-10-18 15:51:49+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- conversational
- text-classification
- table-question-answering
- question-answering
- translation
- summarization
- text-generation
- text2text-generation
- text-to-speech
- automatic-speech-recognition
- text-to-audio
- voice-activity-detection
language:
- ko
- ru
- ig
- es
- en
- ar
- fr
- de
- am
pretty_name: 'DIDI_3.5 '
size_categories:
- 100M<n<1B
---",Low,1.0
Translation,jhu-clsp/seamless-align,12.0,150.0,2024-06-02 17:03:04+00:00,mit,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2308.11596,none,"---
license: mit
task_categories:
- translation
- audio-to-audio
language:
- mt
- en
- cy
- te
- kn
- be
- ta
- uz
- tg
- ca
- ur
- zh
- th
- ko
- hi
- da
- cs
- vi
- sw
- rn
- uk
- tr
- ar
- id
- fi
- sk
- sv
- pl
- it
- pt
- ru
- de
- nl
- fr
---
# Dataset Card for Seamless-Align (WIP). Inspired by https://huggingface.co/datasets/allenai/nllb

## Table of Contents
- [Table of Contents](#table-of-contents)
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** [Needs More Information]
- **Repository:** [Needs More Information]
- **Paper:** [Needs More Information]
- **Leaderboard:** [Needs More Information]
- **Point of Contact:** [Needs More Information]

### Dataset Summary

This dataset was created based on [metadata](https://github.com/facebookresearch/seamless_communication/blob/main/docs/m4t/seamless_align_README.md) for mined Speech-to-Speech(S2S), Text-to-Speech(TTS) and Speech-to-Text(S2T) released by Meta AI.  The S2S contains data for 35 language pairs. The S2S dataset is ~1000GB compressed.


#### How to use the data
There are two ways to access the data:
* Via the Hugging Face Python datasets library 

```
Scripts coming soon
```

* Clone the git repo
```
git lfs install
git clone https://huggingface.co/datasets/jhu-clsp/seamless-align
```

### Supported Tasks and Leaderboards

N/A

### Languages

Language pairs can be found [here](https://github.com/facebookresearch/seamless_communication/blob/main/docs/m4t/seamless_align_README.md).

## Dataset Structure

The S2S dataset contains two gzipped files src.tar.gz annd tgt.tar.gz


### Data Instances

The number of instances for each language pair can be found in the [dataset_infos.json](https://huggingface.co/datasets/allenai/nllb/blob/main/dataset_infos.json) file.

### Data Fields

Data Field can be found [here](https://github.com/facebookresearch/seamless_communication/blob/main/docs/m4t/seamless_align_README.md).
 

### Data Splits

The data is not split.


## Dataset Creation

### Curation Rationale



### Source Data

Inspect links in metadata

#### Who are the source language producers?

Speech and Text was collected from the web many of which are web crawls. 
### Annotations

#### Annotation process

Parallel sentences were identified using SONAR encoders. (Duquenne et al., 2023)

#### Who are the annotators?

The data was not human annotated.

### Personal and Sensitive Information

Data may contain personally identifiable information, sensitive content, or toxic content that was publicly shared on the Internet.  

## Considerations for Using the Data

### Social Impact of Dataset

This dataset provides data for training machine learning systems for many languages.

### Discussion of Biases

Biases in the data have not been specifically studied, however as the original source of data is World Wide Web it is likely that the data has biases similar to those prevalent in the Internet. The data may also exhibit biases introduced by language identification and data filtering techniques; lower resource languages generally have lower accuracy.  

### Other Known Limitations

Some of the translations are in fact machine translations.  While some website machine translation tools are identifiable from HTML source, these tools were not filtered out en mass because raw HTML was not available from some sources and CommonCrawl processing started from WET files. 

## Additional Information

### Dataset Curators

The data was not curated.

### Licensing Information

The dataset is released under the terms of [MIT](https://opensource.org/license/mit/). **PLEASE, USE DATA RESPONSIBLY**


### Citation Information

Seamless Communication et al, SeamlessM4T: Massively Multilingual & Multimodal Machine Translation. arXiv https://arxiv.org/abs/2308.11596, 2023. <br>
Duquenne et al, SONAR: Sentence-Level Multimodal and Language-Agnostic Representations. arXiv https://arxiv.org/abs/2308.11466, 2023

### Contributions

We thank the Seamless Communication Meta AI team for open sourcing the meta data and instructions on how to use it with special thanks to   Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-jussà, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzmán, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang. We also thank the Center for Language and Speech Processing(CLSP) for hosting and releasing this data, including Bismarck Bamfo Odoom and Philipp Koehn (for engineering efforts to host the data, and releasing the huggingface dataset), and  Alexandre Mourachko  (for organizing the connection).",High,4.0
Translation,lingvanex/lingvanex_test_references,2.0,59.0,2023-10-24 13:48:54+00:00,cc-by-nc-sa-4.0,0.0,12.5 MB,13107200.0,8.84 MB,9269411.84,84091,none,none,"---
task_categories:
  - translation
multilinguality:
  - translation
task_ids: []
language:
  - af
  - ar
  - be
  - bp
  - bs
  - cs
  - da
  - de
  - el
  - es
  - et
  - fa
  - fi
  - fr
  - he
  - hu
  - id
  - it
  - lt
  - lv
  - no
  - pl
  - pt
  - ro
  - ru
  - sq
  - sr
  - sv
  - tr
  - uk
size_categories:
  - 1K<n<10K

configs:
  - config_name: en-cs
    data_files:
      - split: test
        path: data/en-cs.csv
  - config_name: en-et
    data_files:
      - split: test
        path: data/en-et.csv
  - config_name: en-hu
    data_files:
      - split: test
        path: data/en-hu.csv
  - config_name: en-no
    data_files:
      - split: test
        path: data/en-no.csv
  - config_name: en-ru
    data_files:
      - split: test
        path: data/en-ru.csv
  - config_name: en-uk
    data_files:
      - split: test
        path: data/en-uk.csv
  - config_name: en-af
    data_files:
      - split: test
        path: data/en-af.csv
  - config_name: en-da
    data_files:
      - split: test
        path: data/en-da.csv
  - config_name: en-fa
    data_files:
      - split: test
        path: data/en-fa.csv
  - config_name: en-id
    data_files:
      - split: test
        path: data/en-id.csv
  - config_name: en-pl
    data_files:
      - split: test
        path: data/en-pl.csv
  - config_name: en-sq
    data_files:
      - split: test
        path: data/en-sq.csv
  - config_name: en-ar
    data_files:
      - split: test
        path: data/en-ar.csv
  - config_name: en-de
    data_files:
      - split: test
        path: data/en-de.csv
  - config_name: en-fi
    data_files:
      - split: test
        path: data/en-fi.csv
  - config_name: en-it
    data_files:
      - split: test
        path: data/en-it.csv
  - config_name: en-pt_br
    data_files:
      - split: test
        path: data/en-pt_br.csv
  - config_name: en-sr
    data_files:
      - split: test
        path: data/en-sr.csv
  - config_name: en-be
    data_files:
      - split: test
        path: data/en-be.csv
  - config_name: en-el
    data_files:
      - split: test
        path: data/en-el.csv
  - config_name: en-fr
    data_files:
      - split: test
        path: data/en-fr.csv
  - config_name: en-lt
    data_files:
      - split: test
        path: data/en-lt.csv
  - config_name: en-pt
    data_files:
      - split: test
        path: data/en-pt.csv
  - config_name: en-sv
    data_files:
      - split: test
        path: data/en-sv.csv
  - config_name: en-bs
    data_files:
      - split: test
        path: data/en-bs.csv
  - config_name: en-es
    data_files:
      - split: test
        path: data/en-es.csv
  - config_name: en-he
    data_files:
      - split: test
        path: data/en-he.csv
  - config_name: en-lv
    data_files:
      - split: test
        path: data/en-lv.csv
  - config_name: en-ro
    data_files:
      - split: test
        path: data/en-ro.csv
  - config_name: en-tr
    data_files:
      - split: test
        path: data/en-tr.csv
  - config_name: en-uk
    data_files:
      - split: test
        path: data/en-uk.csv

license: cc-by-nc-sa-4.0
---

# LTR
LTR -- Lingvanex Test References for MT Evaluation from English into a total of 30 target languages for a big variety of cases.

## TEST CASES

| Parameter | Description |
|-----------|-------------|
| Length    | Sentences from 1 to 100 words.       |
| Domain    | Medicine (12%), Automobile (11%), Finance (8%) |
| Tokenizer    | Jupiter is 1.000.000 km far. Ask Mr. Johnson for training       |
| Tags    | I want to eat <tag> and swim       |
| Capitalisation (Case)    | HELLO my Dear frIEND |
| Different languages in one text (Up to 3 languages)    | I see ""Купалинка"" performance near the theater. |
| Styling    | Hello Dude!      |
| Errors (Grammar, OCR)    | I neet (need) to buy a kat (cat)|
| Abbreviations    | The model was named 15.BVcX-10     |
| Named Entities    | Let’s go to New York city      |
| Idioms    | A piece of cake. Once in a blue moon’       |
| Formulas (Math, Physics, Chemistry)    | Cr2(SO4)3 + CO2 + H2O + K2SO4 + KNO3 |
| Romanian numbers    | It was in MCMXVII year |
| Unicode Special Characters    | №%&*/#  |

# Updates
Data package has been released on 11/18/2023. Enjoy!

# License
The LTR data set is released under the [CC BY-SA 4.0 license](https://huggingface.co/datasets/lingvanex/lingvanex_test_references/blob/main/LICENSE.md).
# How to Cite
```
@inproceedings{
    title = ""{LTR} Lingvanex Test References for {MT} Evaluation"",
    author = ""Aliaksei Rudak"",
    url = ""https://lingvanex.com"",
    Year = ""2023""
}
```
",High,4.0
Translation,slone/nllb-200-10M-sample,10.0,95.0,2023-11-20 13:15:10+00:00,odc-by,1.0,1.83 GB,1964947537.92,1.83 GB,1964947537.92,9983398,https://arxiv.org/abs/2207.04672,none,"---
dataset_info:
  features:
  - name: laser_score
    dtype: float64
  - name: lang1
    dtype: string
  - name: text1
    dtype: string
  - name: lang2
    dtype: string
  - name: text2
    dtype: string
  - name: blaser_sim
    dtype: float64
  splits:
  - name: train
    num_bytes: 2279333006.0
    num_examples: 9983398
  download_size: 1825697094
  dataset_size: 2279333006.0
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: odc-by
task_categories:
- translation
pretty_name: nllb-200-10M-sample
size_categories:
- 1M<n<10M
language:
  - ak   # aka_Latn Akan
  - am   # amh_Ethi Amharic
  - ar   # arb_Arab Modern Standard Arabic
  - awa  # awa_Deva Awadhi
  - azj  # azj_Latn North Azerbaijani
  - bm   # bam_Latn Bambara
  - ban  # ban_Latn Balinese
  - be   # bel_Cyrl Belarusian
  - bem  # bem_Latn Bemba
  - bn   # ben_Beng Bengali
  - bho  # bho_Deva Bhojpuri
  - bjn  # bjn_Latn Banjar (Latin script)
  - bug  # bug_Latn Buginese
  - bg   # bul_Cyrl Bulgarian
  - ca   # cat_Latn Catalan
  - ceb  # ceb_Latn Cebuano
  - cs   # ces_Latn Czech
  - cjk  # cjk_Latn Chokwe
  - ckb  # ckb_Arab Central Kurdish
  - crh  # crh_Latn Crimean Tatar
  - da   # dan_Latn Danish
  - de   # deu_Latn German
  - dik  # dik_Latn Southwestern Dinka
  - dyu  # dyu_Latn Dyula
  - el   # ell_Grek Greek
  - en   # eng_Latn English
  - eo   # epo_Latn Esperanto
  - et   # est_Latn Estonian
  - ee   # ewe_Latn Ewe
  - fo   # fao_Latn Faroese
  - fj   # fij_Latn Fijian
  - fi   # fin_Latn Finnish
  - fon  # fon_Latn Fon
  - fr   # fra_Latn French
  - fur  # fur_Latn Friulian
  - ff   # fuv_Latn Nigerian Fulfulde
  - gaz  # gaz_Latn West Central Oromo
  - gd   # gla_Latn Scottish Gaelic
  - ga   # gle_Latn Irish
  - gl   # glg_Latn Galician
  - gn   # grn_Latn Guarani
  - gu   # guj_Gujr Gujarati
  - ht   # hat_Latn Haitian Creole
  - ha   # hau_Latn Hausa
  - he   # heb_Hebr Hebrew
  - hi   # hin_Deva Hindi
  - hne  # hne_Deva Chhattisgarhi
  - hr   # hrv_Latn Croatian
  - hu   # hun_Latn Hungarian
  - hy   # hye_Armn Armenian
  - ig   # ibo_Latn Igbo
  - ilo  # ilo_Latn Ilocano
  - id   # ind_Latn Indonesian
  - is   # isl_Latn Icelandic
  - it   # ita_Latn Italian
  - jv   # jav_Latn Javanese
  - ja   # jpn_Jpan Japanese
  - kab  # kab_Latn Kabyle
  - kac  # kac_Latn Jingpho
  - kam  # kam_Latn Kamba
  - kn   # kan_Knda Kannada
  - ks   # kas_Arab Kashmiri (Arabic script)
  - ks   # kas_Deva Kashmiri (Devanagari script)
  - ka   # kat_Geor Georgian
  - kk   # kaz_Cyrl Kazakh
  - kbp  # kbp_Latn Kabiyè
  - kea  # kea_Latn Kabuverdianu
  - mn   # khk_Cyrl Halh Mongolian
  - km   # khm_Khmr Khmer
  - ki   # kik_Latn Kikuyu
  - rw   # kin_Latn Kinyarwanda
  - ky   # kir_Cyrl Kyrgyz
  - kmb  # kmb_Latn Kimbundu
  - kmr  # kmr_Latn Northern Kurdish
  - kr   # knc_Arab Central Kanuri (Arabic script)
  - kr   # knc_Latn Central Kanuri (Latin script)
  - kg   # kon_Latn Kikongo
  - ko   # kor_Hang Korean
  - lo   # lao_Laoo Lao
  - lij  # lij_Latn Ligurian
  - li   # lim_Latn Limburgish
  - ln   # lin_Latn Lingala
  - lt   # lit_Latn Lithuanian
  - lmo  # lmo_Latn Lombard
  - ltg  # ltg_Latn Latgalian
  - lb   # ltz_Latn Luxembourgish
  - lua  # lua_Latn Luba-Kasai
  - lg   # lug_Latn Ganda
  - luo  # luo_Latn Luo
  - lus  # lus_Latn Mizo
  - lv   # lvs_Latn Standard Latvian
  - mag  # mag_Deva Magahi
  - mai  # mai_Deva Maithili
  - ml   # mal_Mlym Malayalam
  - mr   # mar_Deva Marathi
  - min  # min_Latn Minangkabau (Latin script)
  - mk   # mkd_Cyrl Macedonian
  - mt   # mlt_Latn Maltese
  - mni  # mni_Beng Meitei (Bengali script)
  - mos  # mos_Latn Mossi
  - mi   # mri_Latn Maori
  - my   # mya_Mymr Burmese
  - nl   # nld_Latn Dutch
  - nb   # nob_Latn Norwegian Bokmål
  - ne   # npi_Deva Nepali
  - nso  # nso_Latn Northern Sotho
  - nus  # nus_Latn Nuer
  - ny   # nya_Latn Nyanja
  - oc   # oci_Latn Occitan
  - ory  # ory_Orya Odia
  - pag  # pag_Latn Pangasinan
  - pa   # pan_Guru Eastern Panjabi
  - pap  # pap_Latn Papiamento
  - pbt  # pbt_Arab Southern Pashto
  - fa   # pes_Arab Western Persian
  - plt  # plt_Latn Plateau Malagasy
  - pl   # pol_Latn Polish
  - pt   # por_Latn Portuguese
  - prs  # prs_Arab Dari
  - qu   # quy_Latn Ayacucho Quechua
  - ro   # ron_Latn Romanian
  - rn   # run_Latn Rundi
  - ru   # rus_Cyrl Russian
  - sg   # sag_Latn Sango
  - sa   # san_Deva Sanskrit
  - sat  # sat_Beng ?
  - scn  # scn_Latn Sicilian
  - shn  # shn_Mymr Shan
  - si   # sin_Sinh Sinhala
  - sk   # slk_Latn Slovak
  - sl   # slv_Latn Slovenian
  - sm   # smo_Latn Samoan
  - sn   # sna_Latn Shona
  - sd   # snd_Arab Sindhi
  - so   # som_Latn Somali
  - st   # sot_Latn Southern Sotho
  - es   # spa_Latn Spanish
  - sc   # srd_Latn Sardinian
  - sr   # srp_Cyrl Serbian
  - ss   # ssw_Latn Swati
  - su   # sun_Latn Sundanese
  - sv   # swe_Latn Swedish
  - sw   # swh_Latn Swahili
  - szl  # szl_Latn Silesian
  - ta   # tam_Taml Tamil
  - taq  # taq_Latn Tamasheq (Latin script)
  - tt   # tat_Cyrl Tatar
  - te   # tel_Telu Telugu
  - tg   # tgk_Cyrl Tajik
  - tl   # tgl_Latn Tagalog
  - ti   # tir_Ethi Tigrinya
  - tpi  # tpi_Latn Tok Pisin
  - tn   # tsn_Latn Tswana
  - ts   # tso_Latn Tsonga
  - tk   # tuk_Latn Turkmen
  - tum  # tum_Latn Tumbuka
  - tr   # tur_Latn Turkish
  - tw   # twi_Latn Twi
  - tzm  # tzm_Tfng Central Atlas Tamazight
  - ug   # uig_Arab Uyghur
  - uk   # ukr_Cyrl Ukrainian
  - umb  # umb_Latn Umbundu
  - ur   # urd_Arab Urdu
  - uz   # uzn_Latn Northern Uzbek
  - vec  # vec_Latn Venetian
  - vi   # vie_Latn Vietnamese
  - war  # war_Latn Waray
  - wo   # wol_Latn Wolof
  - xh   # xho_Latn Xhosa
  - yi   # ydd_Hebr Eastern Yiddish
  - yo   # yor_Latn Yoruba
  - zh   # zho_Hans Chinese (Simplified)
  - zh   # zho_Hant Chinese (Traditional)
  - ms   # zsm_Latn Standard Malay
  - zu   # zul_Latn Zulu
---
# Dataset Card for ""nllb-200-10M-sample""

This is a sample of nearly 10M sentence pairs from the [NLLB-200](https://arxiv.org/abs/2207.04672) 
mined dataset [allenai/nllb](https://huggingface.co/datasets/allenai/nllb), 
scored with the model [facebook/blaser-2.0-qe](https://huggingface.co/facebook/blaser-2.0-qe) 
described in the [SeamlessM4T](https://arxiv.org/abs/2308.11596) paper.

The sample is not random; instead, we just took the top `n` sentence pairs from each translation direction.
The number `n` was computed with the goal of upsamping the directions that contain underrepresented languages.
Nevertheless, the 187 languoids (language and script combinations) are not represented equally,
with most languoids totaling 36K to 200K sentences.
Over 60% of the sentence pairs have BLASER-QE score above 3.5.

This dataset can be used for fine-tuning massively multilingual translation models. 
We suggest the following scenario:
- Filter the dataset by the value of `blaser_sim` (the recommended threshold is 3.0 or 3.5);
- Randomly swap the source/target roles in the sentence pairs during data loading;
- Use that data to augment the dataset while fine-tuning an NLLB-like model for a new translation direction,
in order to mitigate forgetting of all the other translation directions.

The dataset is released under the terms of [ODC-BY](https://opendatacommons.org/licenses/by/1-0/). 
By using this, you are also bound to the respective Terms of Use and License of the original source.

Citation:
- NLLB Team et al, *No Language Left Behind: Scaling Human-Centered Machine Translation*, Arxiv https://arxiv.org/abs/2207.04672, 2022.
- Seamless Communication et al, *SeamlessM4T — Massively Multilingual & Multimodal Machine Translation*, Arxiv https://arxiv.org/abs/2308.11596, 2023.

The following language codes are supported. The mapping between languages and codes can be found in the [NLLB-200 paper](https://arxiv.org/abs/2207.04672)
or in the [FLORES-200 repository](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200).
```
aka_Latn amh_Ethi arb_Arab awa_Deva azj_Latn bam_Latn ban_Latn bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Latn
bug_Latn bul_Cyrl cat_Latn ceb_Latn ces_Latn cjk_Latn ckb_Arab crh_Latn dan_Latn deu_Latn dik_Latn dyu_Latn
ell_Grek eng_Latn epo_Latn est_Latn ewe_Latn fao_Latn fij_Latn fin_Latn fon_Latn fra_Latn fur_Latn fuv_Latn
gaz_Latn gla_Latn gle_Latn glg_Latn grn_Latn guj_Gujr hat_Latn hau_Latn heb_Hebr hin_Deva hne_Deva hrv_Latn
hun_Latn hye_Armn ibo_Latn ilo_Latn ind_Latn isl_Latn ita_Latn jav_Latn jpn_Jpan kab_Latn kac_Latn kam_Latn
kan_Knda kas_Arab kas_Deva kat_Geor kaz_Cyrl kbp_Latn kea_Latn khk_Cyrl khm_Khmr kik_Latn kin_Latn kir_Cyrl
kmb_Latn kmr_Latn knc_Arab knc_Latn kon_Latn kor_Hang lao_Laoo lij_Latn lim_Latn lin_Latn lit_Latn lmo_Latn
ltg_Latn ltz_Latn lua_Latn lug_Latn luo_Latn lus_Latn lvs_Latn mag_Deva mai_Deva mal_Mlym mar_Deva min_Latn
mkd_Cyrl mlt_Latn mni_Beng mos_Latn mri_Latn mya_Mymr nld_Latn nob_Latn npi_Deva nso_Latn nus_Latn nya_Latn
oci_Latn ory_Orya pag_Latn pan_Guru pap_Latn pbt_Arab pes_Arab plt_Latn pol_Latn por_Latn prs_Arab quy_Latn
ron_Latn run_Latn rus_Cyrl sag_Latn san_Deva sat_Beng scn_Latn shn_Mymr sin_Sinh slk_Latn slv_Latn smo_Latn
sna_Latn snd_Arab som_Latn sot_Latn spa_Latn srd_Latn srp_Cyrl ssw_Latn sun_Latn swe_Latn swh_Latn szl_Latn
tam_Taml taq_Latn tat_Cyrl tel_Telu tgk_Cyrl tgl_Latn tir_Ethi tpi_Latn tsn_Latn tso_Latn tuk_Latn tum_Latn
tur_Latn twi_Latn tzm_Tfng uig_Arab ukr_Cyrl umb_Latn urd_Arab uzn_Latn vec_Latn vie_Latn war_Latn wol_Latn
xho_Latn ydd_Hebr yor_Latn zho_Hans zho_Hant zsm_Latn zul_Latn
```
",Medium,3.0
Translation,mhmtcrkglu/autotrain-data-testtranslation,0.0,41.0,2023-10-31 08:55:45+00:00,none,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
language:
- tr
- ar
task_categories:
- translation

---
# AutoTrain Dataset for project: testtranslation

## Dataset Description

This dataset has been automatically processed by AutoTrain for project testtranslation.

### Languages

The BCP-47 code for the dataset's language is tr2ar.

## Dataset Structure

### Data Instances

A sample from this dataset looks as follows:

```json
[
  {
    ""source"": ""TrueMood"",
    ""target"": ""\u062a\u0631\u0648\u0645\u0648\u062f""
  },
  {
    ""source"": ""cleanwax"",
    ""target"": ""\u0643\u0644\u064a\u0646\u0648\u0627\u0643\u0633""
  }
]
```

### Dataset Fields

The dataset has the following fields (also called ""features""):

```json
{
  ""source"": ""Value(dtype='string', id=None)"",
  ""target"": ""Value(dtype='string', id=None)""
}
```

### Dataset Splits

This dataset is split into a train and validation split. The split sizes are as follow:

| Split name   | Num samples         |
| ------------ | ------------------- |
| train        | 24 |
| valid        | 6 |
",Low,1.0
Translation,asas-ai/financial_news,1.0,20.0,2023-11-01 05:34:18+00:00,apache-2.0,0.0,8.98 MB,9416212.48,2.86 MB,2998927.36,7560,none,none,"---
license: apache-2.0
task_categories:
- translation
language:
- ar
- en
tags:
- finance
pretty_name: Financial News Pairs (Cleaned)
size_categories:
- 1K<n<10K
---",Low,1.0
Translation,ReDUB/ComfyOpenSubtitles,0.0,24.0,2023-11-20 05:03:53+00:00,unknown,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: unknown
task_categories:
- translation
language:
- en
- ru
- fr
- es
- ar
- zh
- ko
- ja
- de
pretty_name: ComfyOpenSubtitles
size_categories:
- 10M<n<100M
---

# ComfyOpenSubtitles

## Dataset Description

ComfyOpenSubtitles is a multilingual dataset that contains parallel translations of subtitles from various languages. It includes pairs of input and target languages, along with the corresponding subtitles.

### Languages

The dataset supports the following languages:
- English (en)
- Russian (ru)
- French (fr)
- Spanish (es)
- Arabic (ar)
- Simplified Chinese (zh-cn)
- Korean (ko)
- Japanese (ja)
- German (de)

## Dataset Structure

### Data Instances

Here are some examples of data instances:

- Input Language: English
  Target Language: Russian
  Input Text: ""Oh, bud... what have you done?""
  Output Text: ""Эх, Кореш... Что ж вы наделали?""

- Input Language: Spanish
  Target Language: French
  Input Text: ""This is a beautiful sunset.""
  Output Text: ""C'est un magnifique coucher de soleil.""

### Data Fields

The dataset includes the following fields for each instance:
- `input_language`: The language of the input text.
- `target_language`: The language of the target translation.
- `input_text`: The input text in the source language.
- `output_text`: The corresponding translation in the target language.

### Data Splits

The dataset is typically divided into training splits with varying sizes.

## Dataset Creation

### Curation Rationale

The dataset was created to provide a multilingual collection of subtitles and their translations for research and natural language processing tasks.

### Source Data

The source data for this dataset consists of subtitles from various movies and TV shows.

### Personal and Sensitive Information

The dataset may contain text from movies and TV shows, which may include personal or sensitive information related to the content of those shows.

### Other Known Limitations

Some data may be inaccurate. Be careful.

## Acknowledgments

- https://huggingface.co/datasets/open_subtitles",Medium,3.0
Translation,ReDUB/SoundHarvest,2.0,66.0,2023-12-14 22:51:51+00:00,other,0.0,113 Bytes,unknown,1.14 kB,unknown,2,none,none,"---
license: other
task_categories:
- translation
- audio-to-audio
language:
- ar
- es
- fr
- hi
- id
- ja
- ko
- pt
- ru
- th
- tr
- vi
- en
tags:
- speech2speech
pretty_name: SoundHarvest
size_categories:
- 1K<n<10K
---

## Data Format

The dataset is organized in the following structure:

```yaml
dataset/
├── video_id_1/
│ ├── audio_language_1.wav
│ ├── audio_language_2.wav
│ ├── subtitle_language_1.vtt
│ ├── subtitle_language_2.vtt
│ └── unmatched/
│ └── ...
├── video_id_2/
│ ├── ...
└── ...
```

Original version with the channel (MrBeast) will contain 487 hours 27 minutes 59 seconds of audio files.

## Limitations

- **Copyright**: Please be aware of copyright restrictions when using this dataset. Ensure that you have the necessary permissions to use the audio and subtitle data for your intended purposes.

- **Inaccuracies**: While efforts have been made to align audio and subtitles accurately, there may be occasional mismatches or inaccuracies in the dataset. We recommend verifying the quality and alignment of the data for your specific use case.

## Generating Dataset

For generating the dataset launch:
1. `generate_urls.py` - to generate video URLs based on `channel_urls.txt`
2. `generate_dataset.py` - for generating dataset (can take **a lot** of time...)
3. `polish_dataset.py` - for cleaning the folders without any useful data

## Usage

The SoundHarvest dataset can be utilized for a variety of applications, including:

### 1. Automatic Speech Recognition (ASR)

Train ASR models to convert spoken language into text. SoundHarvest provides diverse language samples, making it suitable for multilingual ASR tasks.

### 2. Multilingual Natural Language Processing (NLP)

Leverage the dataset for multilingual NLP tasks, such as:

- Speech sentiment analysis.
- Language identification.

### 3. Linguistic Research and Analysis

Conduct linguistic research and analysis to explore various aspects of languages, including phonetics, dialects, and language evolution.

### 4. Speech-to-Speech Translation

Use the dataset to develop and evaluate speech-to-speech translation models. Translate spoken content from one language to another, expanding the dataset's applications to cross-lingual communication.

## Acknowledgments

We would like to express our gratitude to the YouTube content creators for providing valuable multilingual audio content that makes this dataset possible.",Medium,2.0
Translation,Tamazight-NLP/AmaWar,2.0,51.0,2024-01-07 18:08:33+00:00,none,0.0,333 KB,340992.0,213 KB,218112.0,2342,none,none,"---
configs:
- config_name: examples
  data_files: examples.tsv
  sep: ""\t""
  default: true
- config_name: expressions
  data_files: expressions.tsv
  sep: ""\t""
- config_name: proverbs
  data_files: proverbs.tsv
  sep: ""\t""
- config_name: riddles
  data_files: riddles.tsv
  sep: ""\t""
- config_name: stories
  data_files: ""stories/*.tsv""
  sep: ""\t""
- config_name: poems
  data_files: ""poems/*.tsv""
  sep: ""\t""
task_categories:
- translation
- text2text-generation
language:
- ber
- tzm
- ar
pretty_name: Amawal Warayni
size_categories:
- 1K<n<10K
---
# Amawal Warayni

Bitext scraped from the online [AmaWar](https://amawalwarayni.com/) dictionary of the Tamazight dialect of Ait Warain spoken in northeastern Morocco.

Contains sentences, stories, and poems in Tamazight along with their translations into Modern Standard Arabic.

Big thanks to Dr. Noureddine Amhaoui for his amazing work.

# Citation
```
نور الدين أمهاوي. (2021). معجم محوسب لمعاني الأسماء والأفعال الأمازيغية الوارينية أمازيغي-عربي.
تاريخ الاسترداد 15 11، 2023، من https://amawalwarayni.com/
```
",Medium,2.0
Translation,M-A-D/DarijaBridge,6.0,52.0,2023-11-26 14:17:11+00:00,apache-2.0,0.0,134 MB,140509184.0,134 MB,140509184.0,1235091,none,none,"---
dataset_info:
  features:
  - name: sentence
    dtype: string
  - name: translation
    dtype: string
  - name: translated
    dtype: bool
  - name: corrected
    dtype: bool
  - name: correction
    dtype: string
  - name: quality
    dtype: int64
  - name: metadata
    struct:
    - name: config
      dtype: string
    - name: dataset
      dtype: string
    - name: language
      dtype: string
    - name: split
      dtype: string
    - name: template
      dtype: string
  splits:
  - name: train
    num_bytes: 343412514
    num_examples: 1235091
  download_size: 133902523
  dataset_size: 343412514
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: apache-2.0
language:
- ar
- en
task_categories:
- translation
pretty_name: DarijaBridge
size_categories:
- 1M<n<10M

---

# DarijaBridge Dataset Card

### General Information

- **Dataset Name:** DarijaBridge
- **Version:** 1.0
- **Creator:** MAD-Community
- **Language:** Darija (Moroccan Arabic) and English
- **Total Tokens:** 41,845,467 (in 'sentence' column)
- **Task:** Machine Translation

### Dataset Summary

DarijaBridge is a community-driven bilingual corpus designed for machine translation tasks between Darija (Moroccan Arabic) and English. Created by MAD-Community, it encompasses a wide range of the Moroccan ""dialects"" and colloquial expressions, reflecting the linguistic diversity of Morocco. The dataset is particularly valuable for developing and fine-tuning leading MT models like NLLB, improving translation accuracy and cultural relevance.

### Intended Use

This dataset is intended for use in machine translation research and applications, especially for those focusing on underrepresented languages and dialects like Darija. It's suitable for training models to translate between English and Darija and can be a crucial resource for linguistic studies and fostering cross-cultural communication.

## Data Collection and Preparation

### Data Source

The data in DarijaBridge has been contributed by the MAD-Community, comprising native Darija speakers and language experts. Contributions are ongoing, and the dataset is regularly updated with new translations and linguistic input.

### Methodology

Data is collected through community contributions, ensuring a diverse representation of dialects and usage. Each sentence in Darija is paired with its English translation, reviewed and corrected by language experts and expert models (like GPT-4) for accuracy.

## Dataset Structure

### Data Fields

- `sentence`: Contains the original sentence in Darija.
- `translation`: Contains the corresponding English translation of the Darija sentence.
- `quality`: Indicates the quality of the text in the sentence column (1 for high quality).
- `metadata`: Includes additional information like language, dialect, source, etc.

### Data Splits

The dataset is currently not split into standard training, validation, and test sets. Users are encouraged to create splits as per their specific research or application needs.

## Additional Information

### Limitations and Bias

As the dataset is community-contributed, there may be variations in translation quality and style. Efforts are made to standardize and review translations, but users should be aware of potential inconsistencies.

### Licensing Information

The DarijaBridge dataset is provided under the Apache 2.0 license.",Medium,3.0
Translation,ayymen/Pontoon-Translations,15.0,835.0,2024-01-19 21:32:07+00:00,mpl-2.0,1.0,442 MB,463470592.0,221 MB,231735296.0,3559829,none,none,"---
configs:
- config_name: en-ht
  data_files: en-ht.tsv
- config_name: en-ab
  data_files: en-ab.tsv
- config_name: en-cs
  data_files: en-cs.tsv
- config_name: en-nyn
  data_files: en-nyn.tsv
- config_name: en-fi
  data_files: en-fi.tsv
- config_name: en-nr
  data_files: en-nr.tsv
- config_name: en-ace
  data_files: en-ace.tsv
- config_name: en-yua
  data_files: en-yua.tsv
- config_name: en-zh-CN
  data_files: en-zh-CN.tsv
- config_name: en-bs
  data_files: en-bs.tsv
- config_name: en-de
  data_files: en-de.tsv
- config_name: en-ny
  data_files: en-ny.tsv
- config_name: en-ca-valencia
  data_files: en-ca-valencia.tsv
- config_name: en-lij
  data_files: en-lij.tsv
- config_name: en-cv
  data_files: en-cv.tsv
- config_name: en-xh
  data_files: en-xh.tsv
- config_name: en-son
  data_files: en-son.tsv
- config_name: en-bm
  data_files: en-bm.tsv
- config_name: en-gn
  data_files: en-gn.tsv
- config_name: en-lb
  data_files: en-lb.tsv
- config_name: en-lv
  data_files: en-lv.tsv
- config_name: en-pl
  data_files: en-pl.tsv
- config_name: en-bo
  data_files: en-bo.tsv
- config_name: en-es-AR
  data_files: en-es-AR.tsv
- config_name: en-tig
  data_files: en-tig.tsv
- config_name: en-nb-NO
  data_files: en-nb-NO.tsv
- config_name: en-tk
  data_files: en-tk.tsv
- config_name: en-xcl
  data_files: en-xcl.tsv
- config_name: en-ann
  data_files: en-ann.tsv
- config_name: en-en-CA
  data_files: en-en-CA.tsv
- config_name: en-yo
  data_files: en-yo.tsv
- config_name: en-mix
  data_files: en-mix.tsv
- config_name: en-tn
  data_files: en-tn.tsv
- config_name: en-mai
  data_files: en-mai.tsv
- config_name: en-cy
  data_files: en-cy.tsv
- config_name: en-kmr
  data_files: en-kmr.tsv
- config_name: en-bas
  data_files: en-bas.tsv
- config_name: en-anp
  data_files: en-anp.tsv
- config_name: en-skr
  data_files: en-skr.tsv
- config_name: en-quy
  data_files: en-quy.tsv
- config_name: en-gu-IN
  data_files: en-gu-IN.tsv
- config_name: en-it
  data_files: en-it.tsv
- config_name: en-tzm
  data_files: en-tzm.tsv
- config_name: en-ne-NP
  data_files: en-ne-NP.tsv
- config_name: en-uk
  data_files: en-uk.tsv
- config_name: en-lzz
  data_files: en-lzz.tsv
- config_name: en-zza
  data_files: en-zza.tsv
- config_name: en-gv
  data_files: en-gv.tsv
- config_name: en-vi
  data_files: en-vi.tsv
- config_name: en-te
  data_files: en-te.tsv
- config_name: en-hil
  data_files: en-hil.tsv
- config_name: en-quc
  data_files: en-quc.tsv
- config_name: en-mr
  data_files: en-mr.tsv
- config_name: en-eo
  data_files: en-eo.tsv
- config_name: en-ar
  data_files: en-ar.tsv
- config_name: en-zam
  data_files: en-zam.tsv
- config_name: en-rm-sursilv
  data_files: en-rm-sursilv.tsv
- config_name: en-shi
  data_files: en-shi.tsv
- config_name: en-sl
  data_files: en-sl.tsv
- config_name: en-th
  data_files: en-th.tsv
- config_name: en-ks
  data_files: en-ks.tsv
- config_name: en-ses
  data_files: en-ses.tsv
- config_name: en-pt-PT
  data_files: en-pt-PT.tsv
- config_name: en-br
  data_files: en-br.tsv
- config_name: en-es-ES
  data_files: en-es-ES.tsv
- config_name: en-ppl
  data_files: en-ppl.tsv
- config_name: en-ast
  data_files: en-ast.tsv
- config_name: en-ia
  data_files: en-ia.tsv
- config_name: en-id
  data_files: en-id.tsv
- config_name: en-cnh
  data_files: en-cnh.tsv
- config_name: en-gd
  data_files: en-gd.tsv
- config_name: en-tr
  data_files: en-tr.tsv
- config_name: en-es-MX
  data_files: en-es-MX.tsv
- config_name: en-fo
  data_files: en-fo.tsv
- config_name: en-hus
  data_files: en-hus.tsv
- config_name: en-tw
  data_files: en-tw.tsv
- config_name: en-brx
  data_files: en-brx.tsv
- config_name: en-hi
  data_files: en-hi.tsv
- config_name: en-lt
  data_files: en-lt.tsv
- config_name: en-ky
  data_files: en-ky.tsv
- config_name: en-si
  data_files: en-si.tsv
- config_name: en-csb
  data_files: en-csb.tsv
- config_name: en-ca
  data_files: en-ca.tsv
- config_name: en-bg
  data_files: en-bg.tsv
- config_name: en-fa
  data_files: en-fa.tsv
- config_name: en-ig
  data_files: en-ig.tsv
- config_name: en-kab
  data_files: en-kab.tsv
- config_name: en-ay
  data_files: en-ay.tsv
- config_name: en-oc
  data_files: en-oc.tsv
- config_name: en-hye
  data_files: en-hye.tsv
- config_name: en-ru
  data_files: en-ru.tsv
- config_name: en-snk
  data_files: en-snk.tsv
- config_name: en-ee
  data_files: en-ee.tsv
- config_name: en-fur
  data_files: en-fur.tsv
- config_name: en-gor
  data_files: en-gor.tsv
- config_name: en-udm
  data_files: en-udm.tsv
- config_name: en-es
  data_files: en-es.tsv
- config_name: en-az
  data_files: en-az.tsv
- config_name: en-nia
  data_files: en-nia.tsv
- config_name: en-sw
  data_files: en-sw.tsv
- config_name: en-nan-tw
  data_files: en-nan-tw.tsv
- config_name: en-ja
  data_files: en-ja.tsv
- config_name: en-da
  data_files: en-da.tsv
- config_name: en-hu
  data_files: en-hu.tsv
- config_name: en-nhe
  data_files: en-nhe.tsv
- config_name: en-he
  data_files: en-he.tsv
- config_name: en-mn
  data_files: en-mn.tsv
- config_name: en-os
  data_files: en-os.tsv
- config_name: en-mni
  data_files: en-mni.tsv
- config_name: en-sc
  data_files: en-sc.tsv
- config_name: en-hyw
  data_files: en-hyw.tsv
- config_name: en-pt
  data_files: en-pt.tsv
- config_name: en-ts
  data_files: en-ts.tsv
- config_name: en-ady
  data_files: en-ady.tsv
- config_name: en-ga-IE
  data_files: en-ga-IE.tsv
- config_name: en-sr
  data_files: en-sr.tsv
- config_name: en-bxr
  data_files: en-bxr.tsv
- config_name: en-mk
  data_files: en-mk.tsv
- config_name: en-lo
  data_files: en-lo.tsv
- config_name: en-ckb
  data_files: en-ckb.tsv
- config_name: en-sah
  data_files: en-sah.tsv
- config_name: en-kk
  data_files: en-kk.tsv
- config_name: en-nn-NO
  data_files: en-nn-NO.tsv
- config_name: en-eu
  data_files: en-eu.tsv
- config_name: en-ro
  data_files: en-ro.tsv
- config_name: en-es-CL
  data_files: en-es-CL.tsv
- config_name: en-cak
  data_files: en-cak.tsv
- config_name: en-st
  data_files: en-st.tsv
- config_name: en-am
  data_files: en-am.tsv
- config_name: en-as
  data_files: en-as.tsv
- config_name: en-kw
  data_files: en-kw.tsv
- config_name: en-vot
  data_files: en-vot.tsv
- config_name: en-tg
  data_files: en-tg.tsv
- config_name: en-kn
  data_files: en-kn.tsv
- config_name: en-ml
  data_files: en-ml.tsv
- config_name: en-vec
  data_files: en-vec.tsv
- config_name: en-ss
  data_files: en-ss.tsv
- config_name: en-sn
  data_files: en-sn.tsv
- config_name: en-pap-AW
  data_files: en-pap-AW.tsv
- config_name: en-ha
  data_files: en-ha.tsv
- config_name: en-ps
  data_files: en-ps.tsv
- config_name: en-azb
  data_files: en-azb.tsv
- config_name: en-en-GB
  data_files: en-en-GB.tsv
- config_name: en-ewo
  data_files: en-ewo.tsv
- config_name: en-tl
  data_files: en-tl.tsv
- config_name: en-gl
  data_files: en-gl.tsv
- config_name: en-bn-BD
  data_files: en-bn-BD.tsv
- config_name: en-rw
  data_files: en-rw.tsv
- config_name: en-mg
  data_files: en-mg.tsv
- config_name: en-tok
  data_files: en-tok.tsv
- config_name: en-tyv
  data_files: en-tyv.tsv
- config_name: en-fy-NL
  data_files: en-fy-NL.tsv
- config_name: en-dyu
  data_files: en-dyu.tsv
- config_name: en-kpv
  data_files: en-kpv.tsv
- config_name: en-pa-IN
  data_files: en-pa-IN.tsv
- config_name: en-jv
  data_files: en-jv.tsv
- config_name: en-meh
  data_files: en-meh.tsv
- config_name: en-azz
  data_files: en-azz.tsv
- config_name: en-pa-PK
  data_files: en-pa-PK.tsv
- config_name: en-rm-vallader
  data_files: en-rm-vallader.tsv
- config_name: en-nhi
  data_files: en-nhi.tsv
- config_name: en-hsb
  data_files: en-hsb.tsv
- config_name: en-be
  data_files: en-be.tsv
- config_name: en-ba
  data_files: en-ba.tsv
- config_name: en-en-ZA
  data_files: en-en-ZA.tsv
- config_name: en-ug
  data_files: en-ug.tsv
- config_name: en-ka
  data_files: en-ka.tsv
- config_name: en-mhr
  data_files: en-mhr.tsv
- config_name: en-sd
  data_files: en-sd.tsv
- config_name: en-tt
  data_files: en-tt.tsv
- config_name: en-yue
  data_files: en-yue.tsv
- config_name: en-arn
  data_files: en-arn.tsv
- config_name: en-ve
  data_files: en-ve.tsv
- config_name: en-fr
  data_files: en-fr.tsv
- config_name: en-lus
  data_files: en-lus.tsv
- config_name: en-kaa
  data_files: en-kaa.tsv
- config_name: en-el
  data_files: en-el.tsv
- config_name: en-dag
  data_files: en-dag.tsv
- config_name: en-hy-AM
  data_files: en-hy-AM.tsv
- config_name: en-nl
  data_files: en-nl.tsv
- config_name: en-pt-BR
  data_files: en-pt-BR.tsv
- config_name: en-ti
  data_files: en-ti.tsv
- config_name: en-trs
  data_files: en-trs.tsv
- config_name: en-zgh
  data_files: en-zgh.tsv
  default: true
- config_name: en-ban
  data_files: en-ban.tsv
- config_name: en-is
  data_files: en-is.tsv
- config_name: en-ceb
  data_files: en-ceb.tsv
- config_name: en-hi-IN
  data_files: en-hi-IN.tsv
- config_name: en-nv
  data_files: en-nv.tsv
- config_name: en-dsb
  data_files: en-dsb.tsv
- config_name: en-ltg
  data_files: en-ltg.tsv
- config_name: en-ln
  data_files: en-ln.tsv
- config_name: en-ur
  data_files: en-ur.tsv
- config_name: en-sat
  data_files: en-sat.tsv
- config_name: en-om
  data_files: en-om.tsv
- config_name: en-yi
  data_files: en-yi.tsv
- config_name: en-fuf
  data_files: en-fuf.tsv
- config_name: en-mt
  data_files: en-mt.tsv
- config_name: en-zh-TW
  data_files: en-zh-TW.tsv
- config_name: en-sq
  data_files: en-sq.tsv
- config_name: en-qvi
  data_files: en-qvi.tsv
- config_name: en-ff
  data_files: en-ff.tsv
- config_name: en-et
  data_files: en-et.tsv
- config_name: en-guc
  data_files: en-guc.tsv
- config_name: en-af
  data_files: en-af.tsv
- config_name: en-gom
  data_files: en-gom.tsv
- config_name: en-ilo
  data_files: en-ilo.tsv
- config_name: en-co
  data_files: en-co.tsv
- config_name: en-rm
  data_files: en-rm.tsv
- config_name: en-sv-SE
  data_files: en-sv-SE.tsv
- config_name: en-ko
  data_files: en-ko.tsv
- config_name: en-jbo
  data_files: en-jbo.tsv
- config_name: en-sk
  data_files: en-sk.tsv
- config_name: en-kbd
  data_files: en-kbd.tsv
- config_name: en-ta
  data_files: en-ta.tsv
- config_name: en-myv
  data_files: en-myv.tsv
- config_name: en-syr
  data_files: en-syr.tsv
- config_name: en-uz
  data_files: en-uz.tsv
- config_name: en-crh
  data_files: en-crh.tsv
- config_name: en-mrj
  data_files: en-mrj.tsv
- config_name: en-szl
  data_files: en-szl.tsv
- config_name: en-tsz
  data_files: en-tsz.tsv
- config_name: en-ach
  data_files: en-ach.tsv
- config_name: en-mdf
  data_files: en-mdf.tsv
- config_name: en-hr
  data_files: en-hr.tsv
- config_name: en-ixl
  data_files: en-ixl.tsv
- config_name: en-ie
  data_files: en-ie.tsv
- config_name: en-sco
  data_files: en-sco.tsv
- config_name: en-zh-HK
  data_files: en-zh-HK.tsv
- config_name: en-wo
  data_files: en-wo.tsv
- config_name: en-bn
  data_files: en-bn.tsv
- config_name: en-bn-IN
  data_files: en-bn-IN.tsv
- config_name: en-nso
  data_files: en-nso.tsv
- config_name: en-dv
  data_files: en-dv.tsv
- config_name: en-jiv
  data_files: en-jiv.tsv
- config_name: en-an
  data_files: en-an.tsv
- config_name: en-km
  data_files: en-km.tsv
- config_name: en-or
  data_files: en-or.tsv
- config_name: en-zu
  data_files: en-zu.tsv
- config_name: en-su
  data_files: en-su.tsv
- config_name: en-pai
  data_files: en-pai.tsv
- config_name: en-my
  data_files: en-my.tsv
- config_name: en-scn
  data_files: en-scn.tsv
- config_name: en-frp
  data_files: en-frp.tsv
- config_name: en-ms
  data_files: en-ms.tsv
- config_name: en-lg
  data_files: en-lg.tsv
language:
- ab
- ace
- ach
- ady
- af
- am
- an
- ann
- anp
- ar
- arn
- as
- ast
- ay
- az
- azb
- azz
- ba
- ban
- bas
- be
- bg
- bm
- bn
- bo
- br
- brx
- bs
- bxr
- ca
- cak
- ceb
- ckb
- cnh
- co
- crh
- cs
- csb
- cv
- cy
- da
- dag
- de
- dsb
- dv
- dyu
- ee
- el
- en
- eo
- es
- et
- eu
- ewo
- fa
- ff
- fi
- fo
- fr
- frp
- fuf
- fur
- fy
- ga
- gd
- gl
- gn
- gom
- gor
- gu
- guc
- gv
- ha
- he
- hi
- hil
- hr
- hsb
- ht
- hu
- hus
- hy
- hye
- hyw
- ia
- id
- ie
- ig
- ilo
- is
- it
- ixl
- ja
- jbo
- jiv
- jv
- ka
- kaa
- kab
- kbd
- kk
- km
- kmr
- kn
- ko
- kpv
- ks
- kw
- ky
- lb
- lg
- lij
- ln
- lo
- lt
- ltg
- lus
- lv
- lzz
- mai
- mdf
- meh
- mg
- mhr
- mix
- mk
- ml
- mn
- mni
- mr
- mrj
- ms
- mt
- my
- myv
- nan
- nb
- ne
- nhe
- nhi
- nia
- nl
- nn
- nr
- nso
- nv
- ny
- nyn
- oc
- om
- or
- os
- pa
- pai
- pap
- pl
- ppl
- ps
- pt
- quc
- quy
- qvi
- rm
- ro
- ru
- rw
- sah
- sat
- sc
- scn
- sco
- sd
- ses
- shi
- si
- sk
- skr
- sl
- sn
- snk
- son
- sq
- sr
- ss
- st
- su
- sv
- sw
- syr
- szl
- ta
- te
- tg
- th
- ti
- tig
- tk
- tl
- tn
- tok
- tr
- trs
- ts
- tsz
- tt
- tw
- tyv
- tzm
- udm
- ug
- uk
- ur
- uz
- ve
- vec
- vi
- vot
- wo
- xcl
- xh
- yi
- yo
- yua
- yue
- zam
- zgh
- zh
- zu
- zza
license: mpl-2.0
task_categories:
- translation
- text2text-generation
pretty_name: Pontoon Translations
annotations_creators:
- crowdsourced
---
# Dataset Card for Pontoon Translations

<!-- Provide a quick summary of the dataset. -->

This is a dataset containing strings from various Mozilla projects on Mozilla's [Pontoon](https://pontoon.mozilla.org) localization platform and their translations into more than 200 languages.
Source strings are in English.

To avoid rows with values like ""None"" and ""N/A"" being interpreted as missing values, pass the keep_default_na parameter like this:
```
from datasets import load_dataset

dataset = load_dataset(""ayymen/Pontoon-Translations"", keep_default_na=False)
```

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** Per [Pontoons's terms](https://pontoon.mozilla.org/terms/) ""Translations are governed by the [Mozilla Public License 2.0](https://www.mozilla.org/en-US/MPL/2.0/), or another license or set of licenses acceptable to the Mozilla Foundation.""

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->
- Machine Translation
- Language Identification

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

- Sentence pairs with empty/missing elements were dropped.
- Identical pairs were dropped.
- Rows where the english string does not contain any letters were dropped.
- Leading and trailing whitespace was stripped.
- Rows were deduplicated.

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

Pontoon users.

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Translation,ImruQays/Rasaif-Classical-Arabic-English-Parallel-texts,8.0,116.0,2024-03-22 13:22:34+00:00,cc-by-4.0,0.0,32.4 MB,33973862.4,17.6 MB,18454937.6,64391,none,none,"---
task_categories:
- translation
language:
- ar
- en
size_categories:
- 10K<n<100K
license: cc-by-4.0
---

# Introduction

This dataset represents a curated collection of parallel Arabic-English texts, featuring the translations of 24 historically and culturally significant books. These texts provide a portal to the intellectual and literary heritage of the Arabic-speaking world during its classical period.
# Content Details

Contained within this dataset are English translations of the following texts, sourced from the [Rasaif website](https://rasaif.com/):

- A Muslim Manual of War
- Al-Hanin Ila'l-Awtan
- Avarice and the Avaricious
- Contemplation
- Diseases of the Hearts and Their Cures
- Hayy ibn Yaqzan
- Ibn Khallikan's Biographical Dictionary
- Kitab al-I'tibar
- Knowledge Mandates Action
- Morals and Behaviour
- Nahj al-Balagha
- The Book of Strangers
- The Canon Of Medicine of Avicenna
- The Epistle on Legal Theory
- The Heavenly Dispute
- The Islamic Conquest of Syria
- The Journey of the Strangers
- The Key to Medicine and a Guide for Students
- The Muqaddimah: An Introduction to History
- The Optics of Ibn Al-Haytham
- The Rare and Excellent History of Saladin
- The Ring of the Dove
- The Strangers
- The Travels Of Ibn Battuta, 1325 – 1354

# Purpose and Application

The overarching objective of this dataset is to highlight the superior literary quality of Classical Arabic, which stands in stark contrast to the language's later developments, particularly due to the mass translations of European texts in the 19th and 20th centuries. It aims to:

- Refine Machine Translation (MT): With its intricate grammatical structure and rich lexicon, Classical Arabic presents an ideal challenge for MT systems, which, when honed on such high-caliber content, can achieve greater accuracy and fluency.

- Language Models: By incorporating texts of such linguistic finesse, this dataset becomes a cornerstone for developing Large Language Models (LLMs) that can grasp and replicate the sophistication inherent in Classical Arabic.

- Preserve Linguistic Heritage: This dataset acts as a conduit for preserving the exceptional literary form of Classical Arabic, providing a benchmark of quality against which contemporary writings can be measured.

# Suggested Research Application: Iterative Translation Refinement

A novel application for this dataset involves utilizing existing translation models to back-translate the English texts into Arabic, likely resulting in a less sophisticated form of the language. This process, known as back-translation, can generate a large corpus of imperfect Arabic text. Subsequently, a new model could be trained to refine this weaker form of Arabic by comparing it to the original Classical Arabic texts in the dataset. The resultant model can be used to enhance current Arabic texts by making it sound more ""Classical"".


# Credits
[The Rasaif Website](https://rasaif.com/): For updates and more information about their work, follow them on [Twitter](https://twitter.com/rasaif_com), and follow Ahmad Alghamdi's [Telegram channel](https://t.me/ahmedhassg)",Low,1.0
Translation,ImruQays/Thaqalayn-Classical-Arabic-English-Parallel-texts,8.0,29.0,2024-03-22 13:23:23+00:00,cc-by-4.0,0.0,50.1 MB,52533657.6,21.3 MB,22334668.8,26171,none,none,"---
task_categories:
- translation
language:
- ar
- en
size_categories:
- 10K<n<100K
license: cc-by-4.0
---

# Introduction

This dataset represents a comprehensive collection of parallel Arabic-English texts from the Thaqalayn Hadith Library, a premier source for exploring the classical hadith tradition of the Imāmī Shia Muslim school of thought. The library focuses on making primary historical sources accessible, serving as a bridge between past wisdom and contemporary study. The dataset features translations of significant classical Imāmī hadith texts, allowing for a deep dive into the linguistic and cultural heritage of this era.
# Content Details

The Thaqalayn Hadith Library includes Arabic-English parallel texts from the following classical collections:

- Al-Kāfi (The Sufficient)
- Muʿjam al-Aḥādīth al-Muʿtabara (A Comprehensive Compilation of Reliable Narrations)
- Al-Khiṣāl (The Book of Characteristics)
- ʿUyūn akhbār al-Riḍā (The Source of Traditions on Imam al-Riḍā)
- Al-Amālī (The Dictations) by Shaykh Muḥammad b. Muḥammad al-Mufīd
- Al-Amālī (The Dictations) by Shaykh Muḥammad b. ʿAlī al-Ṣaduq
- Al-Tawḥīd (The Book of Divine Unity)
- Kitāb al-Ḍuʿafāʾ (The Weakened Ones)
- Kitāb al-Ghayba (The Book of Occultation) by Abū ʿAbd Allah Muḥammad b. Ibrāhīm al-Nuʿmānī
- Kitāb al-Ghayba (The Book of Occultation) by Shaykh Muḥammad b. al-Ḥasan al-Ṭūsī
- Thawāb al-Aʿmāl wa ʿiqāb al-Aʿmāl (The Rewards & Punishments of Deeds)
- Kāmil al-Ziyārāt (The Complete Pilgrimage Guide)
- Faḍaʾil al-Shīʿa (Virtues of the Shīʿa)
- Ṣifāt al-Shīʿa (Attributes of the Shīʿa)
- Maʿānī al-ʾAkhbār (The Meanings of Reports)
- Kitāb al-Muʾmin (The Book of the Believer)
- Kitāb al-Zuhd (The Book of Asceticism)
- Nahj al-Balāgha (The Peak of Eloquence)

# Purpose and Application

The dataset aims to showcase the unmatched literary quality of Classical Arabic, distinguished from Modern Standard Arabic, particularly in its preservation from the European translation trends of the 19th and 20th centuries:

- Refinement of Machine Translation (MT): The complex grammatical structures and rich lexicon of Classical Arabic present a unique challenge for MT systems, pushing the boundaries of translation accuracy and fluency.
- Development of Language Models: These texts serve as a foundation for training sophisticated Large Language Models (LLMs) capable of understanding and replicating the depth of Classical Arabic.
- Preservation of Linguistic Heritage: This dataset preserves the original form of Classical Arabic, providing a standard of excellence against which modern writings can be compared.

# Suggested Research Application: Iterative Translation Refinement

A notable application of this dataset is the enhancement of contemporary Arabic writing through back-translation. Existing models can back-translate English texts into Arabic, potentially producing a less sophisticated form. This offers an opportunity to:

- Generate Imperfect Arabic Corpus: Use back-translation to create a corpus of Arabic text that is less refined than the original Classical Arabic.
- Train Refinement Models: Develop models that refine the imperfect Arabic by comparing it to the original texts, aiming to restore the classical eloquence.
- Enhance Contemporary Arabic Writing: Apply these models to modern Arabic texts, elevating their literary quality by infusing classical stylistic elements, making the language resonate with its classical roots.
 
# Credits
Credits go the [Thaqalayn website](https://thaqalayn.net/) for their compilation of Arabic and English texts. Also, the original webscrape is done by [jenusi](https://github.com/jenusi) on GitHub in this [repo](https://github.com/jenusi/ThaqalaynScraper). I only compiled it in the form of two columns for texts from all books. I also converted the numbers from western Arabic (0123456789) to eastern Arabic (٠١٢٣٤٥٦٧٨٩).",Low,1.0
Translation,ImruQays/Quran-Classical-Arabic-English-Parallel-texts,6.0,31.0,2023-12-29 16:54:05+00:00,cc-by-nc-4.0,0.0,17.5 MB,18350080.0,9.69 MB,10160701.44,6235,none,none,"---
task_categories:
- translation
language:
- ar
- en
size_categories:
- 10K<n<100K
license: cc-by-nc-4.0
---

# Introduction

This dataset presents a collection of parallel texts of the Holy Quran in Arabic (Imla'ei & Uthmanic scripts) alongside 17 different English translations.
# Contents

The dataset includes the Holy Quran in Classical Arabic with the following English translations:

- ""al-Qur’ân: A Contemporary Translation"" by Ahmed Ali
- ""Kanz-ul-Iman"" by Ahmed Raza Khan
- ""The Koran Interpreted"" by Arthur John Arberry
- ""The Message of The Qur'an"" by Muhammad Asad
- ""Qur'an English Commentary"" by Abdul Majid Daryabadi
- ""Noble Qur'an"" by Muhammad Muhsin Khan and Muhammad Taqi-ud-Din al-Hilali
- ""Clear Quran"" by Talal Itani
- ""Tafheem ul Quran"" by Abul Ala Maududi
- Translation by Safi-ur-Rahman al-Mubarakpuri
- Translation by Mohammed Marmaduke William Pickthall
- Translation by Ali Quli Qarai
- Translation by Hasan al-Fatih Qaribullah and Ahmad Darwish
- ""Saheeh International""
- ""The Arabic Text and English Translation"" by Muhammad Sarwar
- ""The Holy Quran"" by M. H. Shakir (author disputed)
- Translation by Wahiduddin Khan
- ""The Holy Qur'an: Text, Translation and Commentary"" by Abdullah Yusuf Ali

# Note on English Translations

It is essential to emphasize that the English translations included in this dataset are not considered the Quran itself. The Quran, by definition, is only in Arabic. The translations serve as interpretations or renderings of the meanings of the Quranic text, designed to convey its message to those who do not understand Arabic. They provide valuable insights but cannot substitute for the original Arabic text, which holds a unique status in Islamic tradition as the literal word of God.
# Credits
## Original Compilation

The [original compilation](https://huggingface.co/datasets/M-AI-C/quran_en_translations) of this dataset was undertaken by [M-AI-C](https://huggingface.co/M-AI-C) and appears to be sourced from [Tanzil](https://tanzil.net/trans/).

## Modification
The Imla'ei script was added and the tafseers were removed.",Medium,2.0
Translation,HeshamHaroon/ArzEn-MultiGenre,11.0,582.0,2023-12-31 14:41:09+00:00,cc-by-4.0,0.0,2.34 MB,2453667.84,1.4 MB,1468006.4,26047,none,none,"---
license: cc-by-4.0
task_categories:
- translation
language:
- ar
- en
size_categories:
- 1K<n<10K
---
# ArzEn-MultiGenre: A Comprehensive Parallel Dataset

## Overview
ArzEn-MultiGenre is a distinctive parallel dataset that encompasses a diverse collection of Egyptian Arabic content. This collection includes song lyrics, novels, and TV show subtitles, all of which have been meticulously translated and aligned with their English counterparts. The dataset serves as an invaluable tool for various linguistic and computational applications.
**Published:** 28 December 2023  
**Version:** 3  
**DOI:** 10.17632/6k97jty9xg.3  
**Contributor:** Rania Al-Sabbagh  

## Dataset Details
- **Total Segment Pairs:** 25,557
- **Languages:** Egyptian Arabic and English
- **Content Types:** Song Lyrics, Novels, TV Show Subtitles

## Applications
- **Machine Translation Benchmarking:** Ideal for testing and improving new machine translation models.
- **Language Model Fine-Tuning:** Suitable for enhancing large language models in few-shot settings.
- **Commercial Application Adaptation:** Can be used to refine tools like Google Translate for better performance with Egyptian Arabic.

## Research Relevance
This dataset is a significant resource for research in fields such as translation studies, cross-linguistic analysis, and lexical semantics.

## Unique Contributions
1. **Diverse Textual Genres:** The dataset includes genres not typically found in parallel datasets for Egyptian Arabic and English.
2. **Gold-Standard Quality:** Translated and aligned by human experts, ensuring high accuracy and reliability.

## Citation
Please cite this dataset as follows:  
Al-Sabbagh, Rania (2023). “ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations.” Mendeley Data, V3, DOI: 10.17632/6k97jty9xg.3
## Related Links
- [Article](https://ijaes2011.net/index.php/IJAES/article/view/560)
## Institutions
- University of Sharjah",Medium,3.0
Translation,rayliuca/WikidataLabels,2.0,1283.0,2024-01-11 04:17:57+00:00,cc0-1.0,0.0,21.9 GB,23514945945.6,21.9 GB,23514945945.6,654052298,none,none,"---
license: cc0-1.0
dataset_info:
- config_name: aa
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 13986211
    num_examples: 436895
  download_size: 9821312
  dataset_size: 13986211
- config_name: ab
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 5012532
    num_examples: 159908
  download_size: 3013706
  dataset_size: 5012532
- config_name: abs
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4252728
    num_examples: 143986
  download_size: 2567450
  dataset_size: 4252728
- config_name: ace
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 19105673
    num_examples: 574712
  download_size: 13573374
  dataset_size: 19105673
- config_name: ady
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4444259
    num_examples: 148627
  download_size: 2705754
  dataset_size: 4444259
- config_name: ady-cyrl
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4412556
    num_examples: 147884
  download_size: 2682170
  dataset_size: 4412556
- config_name: aeb
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4305734
    num_examples: 145198
  download_size: 2606368
  dataset_size: 4305734
- config_name: aeb-arab
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4467930
    num_examples: 148796
  download_size: 2722169
  dataset_size: 4467930
- config_name: aeb-latn
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 12770359
    num_examples: 404946
  download_size: 8886489
  dataset_size: 12770359
- config_name: af
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 58561042
    num_examples: 1643153
  download_size: 42539052
  dataset_size: 58561042
- config_name: agq
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 1317
    num_examples: 33
  download_size: 2906
  dataset_size: 1317
- config_name: ak
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 14198715
    num_examples: 443037
  download_size: 9991525
  dataset_size: 14198715
- config_name: aln
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 13811116
    num_examples: 432089
  download_size: 9673418
  dataset_size: 13811116
- config_name: als
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 20691
    num_examples: 543
  download_size: 17540
  dataset_size: 20691
- config_name: alt
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 108390
    num_examples: 1814
  download_size: 59046
  dataset_size: 108390
- config_name: am
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 5231176
    num_examples: 163038
  download_size: 3187164
  dataset_size: 5231176
- config_name: ami
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 21519
    num_examples: 686
  download_size: 16640
  dataset_size: 21519
- config_name: an
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 240345072
    num_examples: 5921087
  download_size: 164895205
  dataset_size: 240345072
- config_name: ang
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 14275715
    num_examples: 443461
  download_size: 10063758
  dataset_size: 14275715
- config_name: anp
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 8558258
    num_examples: 241612
  download_size: 4381360
  dataset_size: 8558258
- config_name: ar
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 291173732
    num_examples: 5724064
  download_size: 159369497
  dataset_size: 291173732
- config_name: arc
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4473283
    num_examples: 150006
  download_size: 2722619
  dataset_size: 4473283
- config_name: arn
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 13879729
    num_examples: 433912
  download_size: 9715431
  dataset_size: 13879729
- config_name: arq
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4346991
    num_examples: 146004
  download_size: 2636972
  dataset_size: 4346991
- config_name: ary
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 5358568
    num_examples: 171568
  download_size: 3313402
  dataset_size: 5358568
- config_name: arz
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 81806333
    num_examples: 1669699
  download_size: 49423508
  dataset_size: 81806333
- config_name: as
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 21658610
    num_examples: 450074
  download_size: 9641626
  dataset_size: 21658610
- config_name: ase
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4252943
    num_examples: 143986
  download_size: 2568106
  dataset_size: 4252943
- config_name: ast
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 1385628786
    num_examples: 20696237
  download_size: 955908362
  dataset_size: 1385628786
- config_name: atj
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 12996229
    num_examples: 411639
  download_size: 9057557
  dataset_size: 12996229
- config_name: av
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4722934
    num_examples: 153781
  download_size: 2880103
  dataset_size: 4722934
- config_name: avk
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 13194485
    num_examples: 414598
  download_size: 9200917
  dataset_size: 13194485
- config_name: awa
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 8599312
    num_examples: 242320
  download_size: 4411751
  dataset_size: 8599312
- config_name: ay
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 14269432
    num_examples: 443521
  download_size: 10029939
  dataset_size: 14269432
- config_name: az
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 21049248
    num_examples: 516732
  download_size: 14117527
  dataset_size: 21049248
- config_name: azb
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 30781587
    num_examples: 607562
  download_size: 16028687
  dataset_size: 30781587
- config_name: ba
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 11525351
    num_examples: 261509
  download_size: 6733777
  dataset_size: 11525351
- config_name: ban
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 13674052
    num_examples: 426706
  download_size: 9513747
  dataset_size: 13674052
- config_name: ban-bali
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 50961
    num_examples: 748
  download_size: 25817
  dataset_size: 50961
- config_name: bar
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 54783034
    num_examples: 1566120
  download_size: 40389830
  dataset_size: 54783034
- config_name: bbc
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 12820895
    num_examples: 406960
  download_size: 8917054
  dataset_size: 12820895
- config_name: bcc
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 8017228
    num_examples: 241977
  download_size: 4344579
  dataset_size: 8017228
- config_name: be
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 30978832
    num_examples: 564184
  download_size: 17461174
  dataset_size: 30978832
- config_name: be-tarask
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 18931909
    num_examples: 374396
  download_size: 10871239
  dataset_size: 18931909
- config_name: bg
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 200628708
    num_examples: 4383953
  download_size: 137745533
  dataset_size: 200628708
- config_name: bgn
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 7999280
    num_examples: 241566
  download_size: 4331249
  dataset_size: 7999280
- config_name: bi
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 14040026
    num_examples: 438382
  download_size: 9867032
  dataset_size: 14040026
- config_name: bjn
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 8375348
    num_examples: 254558
  download_size: 5722334
  dataset_size: 8375348
- config_name: bm
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 18145787
    num_examples: 549694
  download_size: 13129193
  dataset_size: 18145787
- config_name: bn
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 815803977
    num_examples: 9767284
  download_size: 261147329
  dataset_size: 815803977
- config_name: bo
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 11671330
    num_examples: 278307
  download_size: 5669602
  dataset_size: 11671330
- config_name: bpy
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 15497749
    num_examples: 347458
  download_size: 6991190
  dataset_size: 15497749
- config_name: bqi
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 8017455
    num_examples: 241984
  download_size: 4345123
  dataset_size: 8017455
- config_name: br
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 58304963
    num_examples: 1653800
  download_size: 42722031
  dataset_size: 58304963
- config_name: brh
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 5328437
    num_examples: 171504
  download_size: 3376189
  dataset_size: 5328437
- config_name: bs
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 30441466
    num_examples: 858190
  download_size: 21606575
  dataset_size: 30441466
- config_name: btm
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4252525
    num_examples: 143980
  download_size: 2567218
  dataset_size: 4252525
- config_name: bto
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 12841721
    num_examples: 407470
  download_size: 8934218
  dataset_size: 12841721
- config_name: bug
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 7595464
    num_examples: 235268
  download_size: 5129941
  dataset_size: 7595464
- config_name: bxr
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4713699
    num_examples: 153707
  download_size: 2869313
  dataset_size: 4713699
- config_name: ca
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 408509932
    num_examples: 9936886
  download_size: 288474980
  dataset_size: 408509932
- config_name: cbk-zam
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 14108232
    num_examples: 440345
  download_size: 9920793
  dataset_size: 14108232
- config_name: cdo
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 6503254
    num_examples: 201362
  download_size: 4137841
  dataset_size: 6503254
- config_name: ce
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 28093148
    num_examples: 607767
  download_size: 16367596
  dataset_size: 28093148
- config_name: ceb
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 332947091
    num_examples: 7769402
  download_size: 219525737
  dataset_size: 332947091
- config_name: ch
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 13983906
    num_examples: 436785
  download_size: 9817385
  dataset_size: 13983906
- config_name: cho
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 13950786
    num_examples: 435869
  download_size: 9791296
  dataset_size: 13950786
- config_name: chr
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 5386793
    num_examples: 172855
  download_size: 3419676
  dataset_size: 5386793
- config_name: chy
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 13994916
    num_examples: 437007
  download_size: 9830465
  dataset_size: 13994916
- config_name: ckb
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 23343034
    num_examples: 511183
  download_size: 11459344
  dataset_size: 23343034
- config_name: co
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 47080480
    num_examples: 1346929
  download_size: 34551346
  dataset_size: 47080480
- config_name: cps
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 12849864
    num_examples: 407695
  download_size: 8941921
  dataset_size: 12849864
- config_name: cr
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 5516556
    num_examples: 176667
  download_size: 3532952
  dataset_size: 5516556
- config_name: crh
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 10864382
    num_examples: 336709
  download_size: 7542853
  dataset_size: 10864382
- config_name: crh-cyrl
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4419064
    num_examples: 148046
  download_size: 2688683
  dataset_size: 4419064
- config_name: crh-latn
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 14201429
    num_examples: 442905
  download_size: 9986290
  dataset_size: 14201429
- config_name: cs
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 140189244
    num_examples: 3384048
  download_size: 97516751
  dataset_size: 140189244
- config_name: csb
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 20177120
    num_examples: 619275
  download_size: 14528772
  dataset_size: 20177120
- config_name: cv
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 8047221
    num_examples: 215611
  download_size: 4857718
  dataset_size: 8047221
- config_name: cy
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 89241808
    num_examples: 2244550
  download_size: 62686006
  dataset_size: 89241808
- config_name: da
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 130931077
    num_examples: 3448894
  download_size: 98202417
  dataset_size: 130931077
- config_name: dag
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 2664957
    num_examples: 78534
  download_size: 2052615
  dataset_size: 2664957
- config_name: de
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 765398522
    num_examples: 17531361
  download_size: 527642124
  dataset_size: 765398522
- config_name: de-at
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 53043722
    num_examples: 1515373
  download_size: 38761571
  dataset_size: 53043722
- config_name: de-ch
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 53480908
    num_examples: 1528137
  download_size: 39349412
  dataset_size: 53480908
- config_name: de-formal
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4256391
    num_examples: 144061
  download_size: 2571862
  dataset_size: 4256391
- config_name: din
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 12819746
    num_examples: 406591
  download_size: 8922303
  dataset_size: 12819746
- config_name: diq
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 7570161
    num_examples: 232674
  download_size: 5057742
  dataset_size: 7570161
- config_name: dsb
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 16135830
    num_examples: 491423
  download_size: 11412316
  dataset_size: 16135830
- config_name: dtp
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 13867373
    num_examples: 433733
  download_size: 9720699
  dataset_size: 13867373
- config_name: dty
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 8839082
    num_examples: 246026
  download_size: 4551845
  dataset_size: 8839082
- config_name: dua
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 2631
    num_examples: 87
  download_size: 3877
  dataset_size: 2631
- config_name: dv
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 81396462
    num_examples: 2103276
  download_size: 45332104
  dataset_size: 81396462
- config_name: dz
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 8590239
    num_examples: 242196
  download_size: 4406353
  dataset_size: 8590239
- config_name: ee
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 14377017
    num_examples: 447208
  download_size: 10136064
  dataset_size: 14377017
- config_name: egl
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 13068224
    num_examples: 413551
  download_size: 9121776
  dataset_size: 13068224
- config_name: el
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 32978562
    num_examples: 592016
  download_size: 19577876
  dataset_size: 32978562
- config_name: eml
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 14768563
    num_examples: 458847
  download_size: 10453636
  dataset_size: 14768563
- config_name: en
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 6327454281
    num_examples: 81801560
  download_size: 4224231068
  dataset_size: 6327454281
- config_name: en-ca
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 73305274
    num_examples: 1909970
  download_size: 53060194
  dataset_size: 73305274
- config_name: en-gb
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 115978412
    num_examples: 2520405
  download_size: 78924421
  dataset_size: 115978412
- config_name: en-us
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 14815
    num_examples: 332
  download_size: 9953
  dataset_size: 14815
- config_name: eo
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 256196064
    num_examples: 6285304
  download_size: 177219679
  dataset_size: 256196064
- config_name: es
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 730214298
    num_examples: 17233968
  download_size: 514588069
  dataset_size: 730214298
- config_name: es-419
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4355180
    num_examples: 146476
  download_size: 2659218
  dataset_size: 4355180
- config_name: es-formal
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4280933
    num_examples: 144717
  download_size: 2592085
  dataset_size: 4280933
- config_name: et
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 65123623
    num_examples: 1820762
  download_size: 48197302
  dataset_size: 65123623
- config_name: eu
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 290282374
    num_examples: 7109758
  download_size: 197889378
  dataset_size: 290282374
- config_name: ext
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 223257222
    num_examples: 5359047
  download_size: 147078789
  dataset_size: 223257222
- config_name: fa
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 123727757
    num_examples: 2142642
  download_size: 65952114
  dataset_size: 123727757
- config_name: ff
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 14116652
    num_examples: 440614
  download_size: 9920388
  dataset_size: 14116652
- config_name: fi
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 286539944
    num_examples: 6905698
  download_size: 209916638
  dataset_size: 286539944
- config_name: fit
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 20217258
    num_examples: 620391
  download_size: 14566702
  dataset_size: 20217258
- config_name: fj
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 14159041
    num_examples: 441745
  download_size: 9956108
  dataset_size: 14159041
- config_name: fkv
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 4328482
    num_examples: 145988
  download_size: 2619845
  dataset_size: 4328482
- config_name: fo
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 24474476
    num_examples: 731732
  download_size: 17876981
  dataset_size: 24474476
- config_name: fr
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 774128723
    num_examples: 17908351
  download_size: 534489308
  dataset_size: 774128723
- config_name: frc
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 17896106
    num_examples: 547258
  download_size: 12953740
  dataset_size: 17896106
- config_name: frp
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 40902510
    num_examples: 1191134
  download_size: 29778105
  dataset_size: 40902510
- config_name: frr
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 16979214
    num_examples: 515350
  download_size: 12069637
  dataset_size: 16979214
- config_name: fur
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 42077410
    num_examples: 1221071
  download_size: 30714082
  dataset_size: 42077410
- config_name: ga
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: string
  splits:
  - name: label
    num_bytes: 471527543
    num_examples: 11524282
  download_size: 320967189
  dataset_size: 471527543
- config_name: gag
  features:
  - name: wikidata_id
    dtype: string
  - name: lastrevid
    dtype: int64
  - name: label
    dtype: strin",Medium,3.0
Translation,ayymen/Weblate-Translations,15.0,1824.0,2024-04-02 10:16:03+00:00,other,0.0,1.35 GB,1449551462.4,691 MB,724566016.0,11745911,none,none,"---
configs:
- config_name: en-lk
  data_files: en-lk.tsv
- config_name: en-en-rAU
  data_files: en-en-rAU.tsv
- config_name: en-hy-rAM
  data_files: en-hy-rAM.tsv
- config_name: en-qt
  data_files: en-qt.tsv
- config_name: en-se
  data_files: en-se.tsv
- config_name: en-en_AU
  data_files: en-en_AU.tsv
- config_name: en-in
  data_files: en-in.tsv
- config_name: en_US-id
  data_files: en_US-id.tsv
- config_name: en-ajp
  data_files: en-ajp.tsv
- config_name: en-en_US_rude
  data_files: en-en_US_rude.tsv
- config_name: en_GB-sw
  data_files: en_GB-sw.tsv
- config_name: en_GB-tzm
  data_files: en_GB-tzm.tsv
- config_name: dev-pt
  data_files: dev-pt.tsv
- config_name: de-nb_NO
  data_files: de-nb_NO.tsv
- config_name: en_devel-bn_BD
  data_files: en_devel-bn_BD.tsv
- config_name: messages-fr
  data_files: messages-fr.tsv
- config_name: en-de-CH
  data_files: en-de-CH.tsv
- config_name: en-gu_IN
  data_files: en-gu_IN.tsv
- config_name: en-be_BY
  data_files: en-be_BY.tsv
- config_name: eo-sk
  data_files: eo-sk.tsv
- config_name: en-brx
  data_files: en-brx.tsv
- config_name: en-en_US
  data_files: en-en_US.tsv
- config_name: en_GB-an
  data_files: en_GB-an.tsv
- config_name: en-korean
  data_files: en-korean.tsv
- config_name: en_GB-fr-FR
  data_files: en_GB-fr-FR.tsv
- config_name: en_devel-si
  data_files: en_devel-si.tsv
- config_name: en_US-sr_Cyrl
  data_files: en_US-sr_Cyrl.tsv
- config_name: en-fr@formal
  data_files: en-fr@formal.tsv
- config_name: en_devel-zh_tw
  data_files: en_devel-zh_tw.tsv
- config_name: en-en_ud
  data_files: en-en_ud.tsv
- config_name: en_GB-bi
  data_files: en_GB-bi.tsv
- config_name: en-sq_AL
  data_files: en-sq_AL.tsv
- config_name: en-README_zh-CN
  data_files: en-README_zh-CN.tsv
- config_name: en_US-ml_IN
  data_files: en_US-ml_IN.tsv
- config_name: nb_NO-nn
  data_files: nb_NO-nn.tsv
- config_name: en_devel-es_419
  data_files: en_devel-es_419.tsv
- config_name: en-de-DE
  data_files: en-de-DE.tsv
- config_name: en-dua
  data_files: en-dua.tsv
- config_name: en-gu-rIN
  data_files: en-gu-rIN.tsv
- config_name: en-ty
  data_files: en-ty.tsv
- config_name: nl-pl
  data_files: nl-pl.tsv
- config_name: en_US-bo
  data_files: en_US-bo.tsv
- config_name: en_devel-ru_RU
  data_files: en_devel-ru_RU.tsv
- config_name: en_GB-cy_GB
  data_files: en_GB-cy_GB.tsv
- config_name: en_US-zh-TW
  data_files: en_US-zh-TW.tsv
- config_name: en_US-zh-hk
  data_files: en_US-zh-hk.tsv
- config_name: en-DE
  data_files: en-DE.tsv
- config_name: en_US-lzh
  data_files: en_US-lzh.tsv
- config_name: sv-sma
  data_files: sv-sma.tsv
- config_name: en_GB-fi_FI
  data_files: en_GB-fi_FI.tsv
- config_name: en_US-zu
  data_files: en_US-zu.tsv
- config_name: en_devel-mr
  data_files: en_devel-mr.tsv
- config_name: en_US-he-IL
  data_files: en_US-he-IL.tsv
- config_name: en_GB-fur
  data_files: en_GB-fur.tsv
- config_name: en-fr_CH
  data_files: en-fr_CH.tsv
- config_name: en-en-CA
  data_files: en-en-CA.tsv
- config_name: en-ro_MD
  data_files: en-ro_MD.tsv
- config_name: en_US-yue_HK
  data_files: en_US-yue_HK.tsv
- config_name: es-mr
  data_files: es-mr.tsv
- config_name: en_GB-ace
  data_files: en_GB-ace.tsv
- config_name: en_GB-lt
  data_files: en_GB-lt.tsv
- config_name: en-es-rES
  data_files: en-es-rES.tsv
- config_name: en-ksh
  data_files: en-ksh.tsv
- config_name: en_GB-ti
  data_files: en_GB-ti.tsv
- config_name: en-zh-rSG
  data_files: en-zh-rSG.tsv
- config_name: en-ms_Arab
  data_files: en-ms_Arab.tsv
- config_name: en-README_CZ
  data_files: en-README_CZ.tsv
- config_name: en-ug-CN
  data_files: en-ug-CN.tsv
- config_name: en-ar-rYE
  data_files: en-ar-rYE.tsv
- config_name: en-pk
  data_files: en-pk.tsv
- config_name: en_US-pt
  data_files: en_US-pt.tsv
- config_name: en_devel-pt-br
  data_files: en_devel-pt-br.tsv
- config_name: en-de_formal
  data_files: en-de_formal.tsv
- config_name: en-zh_TW
  data_files: en-zh_TW.tsv
- config_name: en-hu-rHU
  data_files: en-hu-rHU.tsv
- config_name: en-lv-LV
  data_files: en-lv-LV.tsv
- config_name: en-hr_HR
  data_files: en-hr_HR.tsv
- config_name: en-en_devel
  data_files: en-en_devel.tsv
- config_name: en-ka
  data_files: en-ka.tsv
- config_name: en_GB-da_DK
  data_files: en_GB-da_DK.tsv
- config_name: en-ar-AR
  data_files: en-ar-AR.tsv
- config_name: en-om
  data_files: en-om.tsv
- config_name: en_US-id-ID
  data_files: en_US-id-ID.tsv
- config_name: en-cs_CZ
  data_files: en-cs_CZ.tsv
- config_name: it-es_ES
  data_files: it-es_ES.tsv
- config_name: en-zh_HK
  data_files: en-zh_HK.tsv
- config_name: dev-ko
  data_files: dev-ko.tsv
- config_name: en-cr
  data_files: en-cr.tsv
- config_name: en-sr_Cyrl
  data_files: en-sr_Cyrl.tsv
- config_name: en-nl_BE
  data_files: en-nl_BE.tsv
- config_name: en_GB-zh-rTW
  data_files: en_GB-zh-rTW.tsv
- config_name: en-da-DK
  data_files: en-da-DK.tsv
- config_name: en-ang
  data_files: en-ang.tsv
- config_name: en-ur-IN
  data_files: en-ur-IN.tsv
- config_name: en-HU
  data_files: en-HU.tsv
- config_name: en-kw
  data_files: en-kw.tsv
- config_name: en_GB-fo
  data_files: en_GB-fo.tsv
- config_name: en-sr-SP
  data_files: en-sr-SP.tsv
- config_name: en-pl
  data_files: en-pl.tsv
- config_name: en-or
  data_files: en-or.tsv
- config_name: en-en-gb
  data_files: en-en-gb.tsv
- config_name: en-en
  data_files: en-en.tsv
- config_name: en_GB-fa_IR
  data_files: en_GB-fa_IR.tsv
- config_name: en-bn-IN
  data_files: en-bn-IN.tsv
- config_name: en-pl_pl
  data_files: en-pl_pl.tsv
- config_name: en_US-ro_RO
  data_files: en_US-ro_RO.tsv
- config_name: en-es_mx
  data_files: en-es_mx.tsv
- config_name: en-kk_KZ
  data_files: en-kk_KZ.tsv
- config_name: en-ab
  data_files: en-ab.tsv
- config_name: en_UK-de_DE
  data_files: en_UK-de_DE.tsv
- config_name: eo-de
  data_files: eo-de.tsv
- config_name: en_US-fil
  data_files: en_US-fil.tsv
- config_name: en-bp
  data_files: en-bp.tsv
- config_name: en-ta_IN
  data_files: en-ta_IN.tsv
- config_name: en-round
  data_files: en-round.tsv
- config_name: en-gd
  data_files: en-gd.tsv
- config_name: en_US-en@uwu
  data_files: en_US-en@uwu.tsv
- config_name: en-dum
  data_files: en-dum.tsv
- config_name: en-ja_JP
  data_files: en-ja_JP.tsv
- config_name: en-ryu
  data_files: en-ryu.tsv
- config_name: en-b+en+001
  data_files: en-b+en+001.tsv
- config_name: en-en-US
  data_files: en-en-US.tsv
- config_name: en-sl_SI
  data_files: en-sl_SI.tsv
- config_name: de-it
  data_files: de-it.tsv
- config_name: en_GB-sr_RS
  data_files: en_GB-sr_RS.tsv
- config_name: en_US-da
  data_files: en_US-da.tsv
- config_name: en_GB-tk
  data_files: en_GB-tk.tsv
- config_name: en-bn
  data_files: en-bn.tsv
- config_name: en_devel-es_bo
  data_files: en_devel-es_bo.tsv
- config_name: en-ja_CARES
  data_files: en-ja_CARES.tsv
- config_name: en-km-KH
  data_files: en-km-KH.tsv
- config_name: en_US-de_DE
  data_files: en_US-de_DE.tsv
- config_name: en_US-hu_HU
  data_files: en_US-hu_HU.tsv
- config_name: en-ta-rIN
  data_files: en-ta-rIN.tsv
- config_name: en_US-ml
  data_files: en_US-ml.tsv
- config_name: en-sr_RS
  data_files: en-sr_RS.tsv
- config_name: en_US-eu
  data_files: en_US-eu.tsv
- config_name: pl-es
  data_files: pl-es.tsv
- config_name: en_US-ka
  data_files: en_US-ka.tsv
- config_name: en-bulgarian
  data_files: en-bulgarian.tsv
- config_name: fr-en
  data_files: fr-en.tsv
- config_name: en_devel-nb-rNO
  data_files: en_devel-nb-rNO.tsv
- config_name: en_GB-ce
  data_files: en_GB-ce.tsv
- config_name: en_US-bs
  data_files: en_US-bs.tsv
- config_name: en-en@uwu
  data_files: en-en@uwu.tsv
- config_name: en_GB-nn
  data_files: en_GB-nn.tsv
- config_name: en-pa_PK
  data_files: en-pa_PK.tsv
- config_name: en-wae
  data_files: en-wae.tsv
- config_name: en-ar_EG
  data_files: en-ar_EG.tsv
- config_name: en_GB-lt_LT
  data_files: en_GB-lt_LT.tsv
- config_name: en-zh-Hant-HK
  data_files: en-zh-Hant-HK.tsv
- config_name: messages-de
  data_files: messages-de.tsv
- config_name: en-ur_IN
  data_files: en-ur_IN.tsv
- config_name: en-in-rID
  data_files: en-in-rID.tsv
- config_name: en-lo-LA
  data_files: en-lo-LA.tsv
- config_name: en-el-rGR
  data_files: en-el-rGR.tsv
- config_name: en-es-ES
  data_files: en-es-ES.tsv
- config_name: en_devel-et
  data_files: en_devel-et.tsv
- config_name: en-fr-rCH
  data_files: en-fr-rCH.tsv
- config_name: en-en_CA
  data_files: en-en_CA.tsv
- config_name: en-b+uz+Latn
  data_files: en-b+uz+Latn.tsv
- config_name: en_GB-tig
  data_files: en_GB-tig.tsv
- config_name: en_GB-hi_IN
  data_files: en_GB-hi_IN.tsv
- config_name: de-pl
  data_files: de-pl.tsv
- config_name: en-zh-rCN
  data_files: en-zh-rCN.tsv
- config_name: en-hi-rIN
  data_files: en-hi-rIN.tsv
- config_name: en-ba
  data_files: en-ba.tsv
- config_name: en-fy
  data_files: en-fy.tsv
- config_name: en-el-GR
  data_files: en-el-GR.tsv
- config_name: en-tum
  data_files: en-tum.tsv
- config_name: en-ru-RU
  data_files: en-ru-RU.tsv
- config_name: en_US-fa
  data_files: en_US-fa.tsv
- config_name: en_GB-ka
  data_files: en_GB-ka.tsv
- config_name: es-nb-rNO
  data_files: es-nb-rNO.tsv
- config_name: en_US-ckb
  data_files: en_US-ckb.tsv
- config_name: en-hi_IN
  data_files: en-hi_IN.tsv
- config_name: eo-pa
  data_files: eo-pa.tsv
- config_name: en_devel-zh_TW
  data_files: en_devel-zh_TW.tsv
- config_name: en_GB-ch
  data_files: en_GB-ch.tsv
- config_name: en-sdh
  data_files: en-sdh.tsv
- config_name: en-lzh
  data_files: en-lzh.tsv
- config_name: en-zh_HANS-CN
  data_files: en-zh_HANS-CN.tsv
- config_name: en-li
  data_files: en-li.tsv
- config_name: en_devel-zh_cn
  data_files: en_devel-zh_cn.tsv
- config_name: en_GB-mk
  data_files: en_GB-mk.tsv
- config_name: en_GB-ay
  data_files: en_GB-ay.tsv
- config_name: en-sq-rAL
  data_files: en-sq-rAL.tsv
- config_name: en-nl_TND
  data_files: en-nl_TND.tsv
- config_name: en-th
  data_files: en-th.tsv
- config_name: messages-id
  data_files: messages-id.tsv
- config_name: en-bo
  data_files: en-bo.tsv
- config_name: en-hy
  data_files: en-hy.tsv
- config_name: en_US-gd
  data_files: en_US-gd.tsv
- config_name: en-tok
  data_files: en-tok.tsv
- config_name: pt_BR-en
  data_files: pt_BR-en.tsv
- config_name: fr-pt
  data_files: fr-pt.tsv
- config_name: en-bs-rBA
  data_files: en-bs-rBA.tsv
- config_name: en-zh-hant
  data_files: en-zh-hant.tsv
- config_name: en_US-fr
  data_files: en_US-fr.tsv
- config_name: en-eu-ES
  data_files: en-eu-ES.tsv
- config_name: en-lv_LV
  data_files: en-lv_LV.tsv
- config_name: und-fr
  data_files: und-fr.tsv
- config_name: en-af-rZA
  data_files: en-af-rZA.tsv
- config_name: en-da
  data_files: en-da.tsv
- config_name: en-os
  data_files: en-os.tsv
- config_name: en-fr-CH
  data_files: en-fr-CH.tsv
- config_name: en-es_MX
  data_files: en-es_MX.tsv
- config_name: nl-bg
  data_files: nl-bg.tsv
- config_name: en_GB-ckb
  data_files: en_GB-ckb.tsv
- config_name: en-ar-rEG
  data_files: en-ar-rEG.tsv
- config_name: en_US-mr
  data_files: en_US-mr.tsv
- config_name: en_US-cs-CZ
  data_files: en_US-cs-CZ.tsv
- config_name: en_devel-fi
  data_files: en_devel-fi.tsv
- config_name: en-mhr
  data_files: en-mhr.tsv
- config_name: en-no-rNO
  data_files: en-no-rNO.tsv
- config_name: en-it_it
  data_files: en-it_it.tsv
- config_name: en-ar-rSA
  data_files: en-ar-rSA.tsv
- config_name: en_GB-nso
  data_files: en_GB-nso.tsv
- config_name: en-ti
  data_files: en-ti.tsv
- config_name: en-iw_HE
  data_files: en-iw_HE.tsv
- config_name: en-szl
  data_files: en-szl.tsv
- config_name: en_GB-ba
  data_files: en_GB-ba.tsv
- config_name: en_devel-cs
  data_files: en_devel-cs.tsv
- config_name: en_GB-pl_PL
  data_files: en_GB-pl_PL.tsv
- config_name: en-ta_LK
  data_files: en-ta_LK.tsv
- config_name: en-uz@latin
  data_files: en-uz@latin.tsv
- config_name: en-el
  data_files: en-el.tsv
- config_name: en_GB-cs
  data_files: en_GB-cs.tsv
- config_name: en-bul_BG
  data_files: en-bul_BG.tsv
- config_name: en-fa_IR
  data_files: en-fa_IR.tsv
- config_name: en-gsw
  data_files: en-gsw.tsv
- config_name: en-ko-KR
  data_files: en-ko-KR.tsv
- config_name: en-bs_BA
  data_files: en-bs_BA.tsv
- config_name: en_GB-wo
  data_files: en_GB-wo.tsv
- config_name: en_devel-it
  data_files: en_devel-it.tsv
- config_name: en_US-bn
  data_files: en_US-bn.tsv
- config_name: en_devel-pl
  data_files: en_devel-pl.tsv
- config_name: en-rm
  data_files: en-rm.tsv
- config_name: en-night
  data_files: en-night.tsv
- config_name: eo-ca
  data_files: eo-ca.tsv
- config_name: en_US-ps
  data_files: en_US-ps.tsv
- config_name: en_GB-sd
  data_files: en_GB-sd.tsv
- config_name: en-th-TH
  data_files: en-th-TH.tsv
- config_name: en-sv-rSE
  data_files: en-sv-rSE.tsv
- config_name: en-b+zh+Hans
  data_files: en-b+zh+Hans.tsv
- config_name: en_devel-uk
  data_files: en_devel-uk.tsv
- config_name: en_US-it_IT
  data_files: en_US-it_IT.tsv
- config_name: en-b+hrx
  data_files: en-b+hrx.tsv
- config_name: en-my
  data_files: en-my.tsv
- config_name: en_GB-sc
  data_files: en_GB-sc.tsv
- config_name: en-de_DE_rude
  data_files: en-de_DE_rude.tsv
- config_name: en_GB-ff
  data_files: en_GB-ff.tsv
- config_name: en_devel-nl
  data_files: en_devel-nl.tsv
- config_name: en-shn
  data_files: en-shn.tsv
- config_name: en_GB-ca
  data_files: en_GB-ca.tsv
- config_name: en-hu_HU
  data_files: en-hu_HU.tsv
- config_name: ru-be
  data_files: ru-be.tsv
- config_name: es-ml
  data_files: es-ml.tsv
- config_name: en_GB-na
  data_files: en_GB-na.tsv
- config_name: en_devel-ja
  data_files: en_devel-ja.tsv
- config_name: en-pt-rPT-v26
  data_files: en-pt-rPT-v26.tsv
- config_name: en_devel-pt_BR
  data_files: en_devel-pt_BR.tsv
- config_name: en_US-ar_AA
  data_files: en_US-ar_AA.tsv
- config_name: en_US-en_GB
  data_files: en_US-en_GB.tsv
- config_name: en-de_FORM
  data_files: en-de_FORM.tsv
- config_name: en_US-et
  data_files: en_US-et.tsv
- config_name: pl-it
  data_files: pl-it.tsv
- config_name: messages-ru
  data_files: messages-ru.tsv
- config_name: en_devel-en
  data_files: en_devel-en.tsv
- config_name: en-te_IN
  data_files: en-te_IN.tsv
- config_name: en_US-it-IT
  data_files: en_US-it-IT.tsv
- config_name: en-zh-rMO
  data_files: en-zh-rMO.tsv
- config_name: en-fy-NL
  data_files: en-fy-NL.tsv
- config_name: en-iw-rIL
  data_files: en-iw-rIL.tsv
- config_name: en-zh-Hant
  data_files: en-zh-Hant.tsv
- config_name: en-es_uy
  data_files: en-es_uy.tsv
- config_name: en_GB-or
  data_files: en_GB-or.tsv
- config_name: en-tt
  data_files: en-tt.tsv
- config_name: de-pt
  data_files: de-pt.tsv
- config_name: en-zh-Hans
  data_files: en-zh-Hans.tsv
- config_name: en-ar-TN
  data_files: en-ar-TN.tsv
- config_name: en_US-si_LK
  data_files: en_US-si_LK.tsv
- config_name: en-so
  data_files: en-so.tsv
- config_name: en_GB-csb
  data_files: en_GB-csb.tsv
- config_name: en-fr-CA
  data_files: en-fr-CA.tsv
- config_name: en-es_BO
  data_files: en-es_BO.tsv
- config_name: en_devel-es_pa
  data_files: en_devel-es_pa.tsv
- config_name: en-vi-VN
  data_files: en-vi-VN.tsv
- config_name: en_devel-sw
  data_files: en_devel-sw.tsv
- config_name: en-es-rMX
  data_files: en-es-rMX.tsv
- config_name: en-eu-rES
  data_files: en-eu-rES.tsv
- config_name: en_GB-pi
  data_files: en_GB-pi.tsv
- config_name: en_devel-bg
  data_files: en_devel-bg.tsv
- config_name: en-ja-JP
  data_files: en-ja-JP.tsv
- config_name: en_US-uk
  data_files: en_US-uk.tsv
- config_name: en_GB-km
  data_files: en_GB-km.tsv
- config_name: en_US-ko
  data_files: en_US-ko.tsv
- config_name: en-gmh
  data_files: en-gmh.tsv
- config_name: en_US-hy
  data_files: en_US-hy.tsv
- config_name: en_GB-ml
  data_files: en_GB-ml.tsv
- config_name: en-bn-rIN
  data_files: en-bn-rIN.tsv
- config_name: en-ach
  data_files: en-ach.tsv
- config_name: en-pt-rBR-v26
  data_files: en-pt-rBR-v26.tsv
- config_name: en_US-zh
  data_files: en_US-zh.tsv
- config_name: en-sw-rKE
  data_files: en-sw-rKE.tsv
- config_name: en_GB-ha
  data_files: en_GB-ha.tsv
- config_name: en-en-rGB
  data_files: en-en-rGB.tsv
- config_name: en_devel-pt
  data_files: en_devel-pt.tsv
- config_name: en-no_NB
  data_files: en-no_NB.tsv
- config_name: en-no_NO
  data_files: en-no_NO.tsv
- config_name: en-es_es
  data_files: en-es_es.tsv
- config_name: en-kk
  data_files: en-kk.tsv
- config_name: en-bm
  data_files: en-bm.tsv
- config_name: en-pl-PL
  data_files: en-pl-PL.tsv
- config_name: en_GB-id
  data_files: en_GB-id.tsv
- config_name: en-sr-Latn
  data_files: en-sr-Latn.tsv
- config_name: en_US-ms
  data_files: en_US-ms.tsv
- config_name: en-et_ET
  data_files: en-et_ET.tsv
- config_name: en-b+es+419
  data_files: en-b+es+419.tsv
- config_name: en_GB-kw
  data_files: en_GB-kw.tsv
- config_name: en-no
  data_files: en-no.tsv
- config_name: en-wa
  data_files: en-wa.tsv
- config_name: en-ber
  data_files: en-ber.tsv
- config_name: en_US-es_MX
  data_files: en_US-es_MX.tsv
- config_name: en-de_1901
  data_files: en-de_1901.tsv
- config_name: en-ja-rJP
  data_files: en-ja-rJP.tsv
- config_name: en_US-uk_UA
  data_files: en_US-uk_UA.tsv
- config_name: en_US-ja_JP
  data_files: en_US-ja_JP.tsv
- config_name: en-b+fr
  data_files: en-b+fr.tsv
- config_name: en-pt-br
  data_files: en-pt-br.tsv
- config_name: en-te
  data_files: en-te.tsv
- config_name: en-np
  data_files: en-np.tsv
- config_name: en_GB-gu
  data_files: en_GB-gu.tsv
- config_name: en_GB-ki
  data_files: en_GB-ki.tsv
- config_name: en-kab-KAB
  data_files: en-kab-KAB.tsv
- config_name: de-fr
  data_files: de-fr.tsv
- config_name: en-ru_old
  data_files: en-ru_old.tsv
- config_name: en_devel-es_do
  data_files: en_devel-es_do.tsv
- config_name: en-ua
  data_files: en-ua.tsv
- config_name: en-et_EE
  data_files: en-et_EE.tsv
- config_name: ia-it
  data_files: ia-it.tsv
- config_name: en_GB-ro
  data_files: en_GB-ro.tsv
- config_name: en_US-pt-rPT
  data_files: en_US-pt-rPT.tsv
- config_name: en-ur_PK
  data_files: en-ur_PK.tsv
- config_name: en-pa-rPK
  data_files: en-pa-rPK.tsv
- config_name: en-vec
  data_files: en-vec.tsv
- config_name: en-nl-rBE
  data_files: en-nl-rBE.tsv
- config_name: en-lv
  data_files: en-lv.tsv
- config_name: en-ar-rBH
  data_files: en-ar-rBH.tsv
- config_name: en-an
  data_files: en-an.tsv
- config_name: en_US-sr
  data_files: en_US-sr.tsv
- config_name: en-Ukrainian
  data_files: en-Ukrainian.tsv
- config_name: en_US-mk
  data_files: en_US-mk.tsv
- config_name: en_GB-br
  data_files: en_GB-br.tsv
- config_name: en-de@informal
  data_files: en-de@informal.tsv
- config_name: en-dz
  data_files: en-dz.tsv
- config_name: en_US-he_IL
  data_files: en_US-he_IL.tsv
- config_name: en_GB-mr
  data_files: en_GB-mr.tsv
- config_name: en-cs-CARES
  data_files: en-cs-CARES.tsv
- config_name: en_US-hi_IN
  data_files: en_US-hi_IN.tsv
- config_name: en_US-ro
  data_files: en_US-ro.tsv
- config_name: en_US-fr_CA
  data_files: en_US-fr_CA.tsv
- config_name: en-as
  data_files: en-as.tsv
- config_name: en_GB-ro_MD
  data_files: en_GB-ro_MD.tsv
- config_name: en_US-lt-LT
  data_files: en_US-lt-LT.tsv
- config_name: fr-ca
  data_files: fr-ca.tsv
- config_name: en-be_Latn
  data_files: en-be_Latn.tsv
- config_name: en-en-AU
  data_files: en-en-AU.tsv
- config_name: en_US-fr_FR
  data_files: en_US-fr_FR.tsv
- config_name: en-de-de
  data_files: en-de-de.tsv
- config_name: en-nds
  data_files: en-nds.tsv
- config_name: en_US-ja
  data_files: en_US-ja.tsv
- config_name: en-es-AR
  data_files: en-es-AR.tsv
- config_name: en-ms
  data_files: en-ms.tsv
- config_name: en-zh-CHS
  data_files: en-zh-CHS.tsv
- config_name: en_devel-bs
  data_files: en_devel-bs.tsv
- config_name: en-arn
  data_files: en-arn.tsv
- config_name: zh_Hans-en
  data_files: zh_Hans-en.tsv
- config_name: en-co
  data_files: en-co.tsv
- config_name: en-uz_Latn
  data_files: en-uz_Latn.tsv
- config_name: en-cs-rCZ
  data_files: en-cs-rCZ.tsv
- config_name: en-ku
  data_files: en-ku.tsv
- config_name: en-ha
  data_files: en-ha.tsv
- config_name: en-de-zuerich-lernt
  data_files: en-de-zuerich-lernt.tsv
- config_name: en_US-be
  data_files: en_US-be.tsv
- config_name: en-tr
  data_files: en-tr.tsv
- config_name: en-ru_ru
  data_files: en-ru_ru.tsv
- config_name: en-kl
  data_files: en-kl.tsv
- config_name: en-it
  data_files: en-it.tsv
- config_name: en-b+be+Latn
  data_files: en-b+be+Latn.tsv
- config_name: en_devel-mk
  data_files: en_devel-mk.tsv
- config_name: en_US-vi
  data_files: en_US-vi.tsv
- config_name: en-zh_CMN-HANT
  data_files: en-zh_CMN-HANT.tsv
- config_name: en-mnw
  data_files: en-mnw.tsv
- config_name: en_US-sv-SE
  data_files: en_US-sv-SE.tsv
- config_name: en-gum
  data_files: en-gum.tsv
- config_name: en-my_MM
  data_files: en-my_MM.tsv
- config_name: en_GB-mk_MK
  data_files: en_GB-mk_MK.tsv
- config_name: en_devel-es_ec
  data_files: en_devel-es_ec.tsv
- config_name: en_US-ne
  data_files: en_US-ne.tsv
- config_name: nl-zh_Hans
  data_files: nl-zh_Hans.tsv
- config_name: en-zh_hans
  data_files: en-zh_hans.tsv
- config_name: en-sr-rCS
  data_files: en-sr-rCS.tsv
- config_name: en-es_NI
  data_files: en-es_NI.tsv
- config_name: en_GB-bs
  data_files: en_GB-bs.tsv
- config_name: en_GB-tr_TR
  data_files: en_GB-tr_TR.tsv
- config_name: ru-en
  data_files: ru-en.tsv
- config_name: en_US-my
  data_files: en_US-my.tsv
- config_name: en-ia
  data_files: en-ia.tsv
- config_name: en-hu-HU
  data_files: en-hu-HU.tsv
- config_name: en-nn_NO
  data_files: en-nn_NO.tsv
- config_name: en_GB-es_419
  data_files: en_GB-es_419.tsv
- config_name: en-ca-rES
  data_files: en-ca-rES.tsv
- config_name: en_US-zh-CN
  data_files: en_US-zh-CN.tsv
- config_name: en_US-tzm
  data_files: en_US-tzm.tsv
- config_name: en-it_CARES
  data_files: en-it_CARES.tsv
- config_name: en_GB-he
  data_files: en_GB-he.tsv
- config_name: en_US-sn
  data_files: en_US-sn.tsv
- config_name: en-ml_IN
  data_files: en-ml_IN.tsv
- config_name: en-guc
  data_files: en-guc.tsv
- config_name: zh_Hans-ru
  data_files: zh_Hans-ru.tsv
- config_name: en-csb
  data_files: en-csb.tsv
- config_name: en-nan
  data_files: en-nan.tsv
- config_name: en-fa-IR
  data_files: en-fa-IR.tsv
- config_name: en_US-en_CA
  data_files: en_US-en_CA.tsv
- config_name: en_GB-ar
  data_files: en_GB-ar.tsv
- config_name: en_GB-ia_FR
  data_files: en_GB-ia_FR.tsv
- config_name: en_US-es-MX
  data_files: en_US-es-MX.tsv
- config_name: en_devel-el
  data_files: en_devel-el.tsv
- config_name: en_GB-ach
  data_files: en_GB-ach.tsv
- config_name: en-Italian
  data_files: en-Italian.tsv
- config_name: en_devel-az
  data_files: en_devel-az.tsv
- config_name: eo-ru
  data_files: eo-ru.tsv
- config_name: en-es_US
  data_files: en-es_US.tsv
- config_name: en_devel-cy
  data_files: en_devel-cy.tsv
- config_name: en-es-mx
  data_files: en-es-mx.tsv
- config_name: en-en-rCA
  data_files: en-en-rCA.tsv
- config_name: en-kn-IN
  data_files: en-kn-IN.tsv
- config_name: en_devel-zh_CN
  data_files: en_devel-zh_CN.tsv
- config_name: en_US-lt_LT
  data_files: en_US-lt_LT.tsv
- config_name: en_GB-id_ID
  data_files: en_GB-id_ID.tsv
- config_name: en-mt
  data_files: en-mt.tsv
- config_name: en-bar
  data_files: en-bar.tsv
- config_name: en-kr
  data_files: en-kr.tsv
- config_name: en_GB-de-DE
  data_files: en_GB-de-DE.tsv
- config_name: en-zgh
  data_files: en-zgh.tsv
  default: true
- config_name: en-german
  data_files: en-german.tsv
- config_name: en-de_ch
  data_files: en-de_ch.tsv
- config_name: en_devel-hy
  data_files: en_devel-hy.tsv
- config_name: en_GB-hr
  data_files: en_GB-hr.tsv
- config_name: en_GB-ca_AD
  data_files: en_GB-ca_AD.tsv
- config_name: en-b+ca+VALENCIA
  data_files: en-b+ca+VALENCIA.tsv
- config_name: en-rw
  data_files: en-rw.tsv
- config_name: en-fil-FIL
  data_files: en-fil-FIL.tsv
- config_name: it-de
  data_files: it-de.tsv
- config_name: en_US-es-rMX
  data_files: en_US-es-rMX.tsv
- config_name: en-sk-SK
  data_files: en-sk-SK.tsv
- config_name: en-my-MM
  data_files: en-my-MM.tsv
- config_name: en-es_ve
  data_files: en-es_ve.tsv
- config_name: en-fra-rFR
  data_files: en-fra-rFR.tsv
- config_name: en_GB-gv
  data_files: en_GB-gv.tsv
- config_name: en-ml-IN
  data_files: en-ml-IN.tsv
- config_name: en_US-zh-rHK
  data_files: en_US-zh-rHK.tsv
- config_name: en-fur
  data_files: en-fur.tsv
- config_name: en_GB-sv
  data_files: en_GB-sv.tsv
- config_name: en-ne-rNP
  data_files: en-ne-rNP.tsv
- config_name: en_GB-fr
  data_files: en_GB-fr.tsv
- config_name: en_US-qya
  data_files: en_US-qya.tsv
- config_name: en-ja_KS
  data_files: en-ja_KS.tsv
- config_name: en-en_uwu_x
  data_files: en-en_uwu_x.tsv
- config_name: en-zh_CN
  data_files: en-zh_CN.tsv
- config_name: en-az_AZ
  data_files: en-az_AZ.tsv
- config_name: en-bem
  data_files: en-bem.tsv
- config_name: en-ars
  data_files: en-ars.tsv
- config_name: en-xh
  data_files: en-xh.tsv
- config_name: en_US-zh_Hant_HK
  data_files: en_US-zh_Hant_HK.tsv
- config_name: en_US-en-rGB
  data_files: en_US-en-rGB.tsv
- config_name: en-pam
  data_files: en-pam.tsv
- config_name: en_devel-zh-rCN
  data_files: en_devel-zh-rCN.tsv
- config_name: en-zh_LATN@pinyin
  data_files: en-zh_LATN@pinyin.tsv
- config_name: en_US-en_NZ
  data_files: en_US-en_NZ.tsv
- config_name: en-nb_no
  data_files: en-nb_no.tsv
- config_name: en-bn-rBD
  data_files: en-bn-rBD.tsv
- config_name: en-pl_PL
  data_files: en-pl_PL.tsv
- config_name: en-romanian
  data_files: en-romanian.tsv
- config_name: en_US-ja_KANJI
  data_files: en_US-ja_KANJI.tsv
- config_name: en_US-zh-rCN
  data_files: en_US-zh-rCN.tsv
- config_name: en-ca_es
  data_files: en-ca_es.tsv
- config_name: en-de_de
  data_files: en-de_de.tsv
- config_name: en-rom
  data_files: en-rom.tsv
- config_name: en_devel-lv
  data_files: en_devel-lv.tsv
- config_name: en-ro
  data_files: en-ro.tsv
- config_name: en_US-th-TH
  data_files: en_US-th-TH.tsv
- config_name: en_GB-wal
  data_files: en_GB-wal.tsv
- config_name: en_US-fi-FI
  data_files: en_US-fi-FI.tsv
- config_name: en-ar_AR
  data_files: en-ar_AR.tsv
- config_name: en_US-el
  data_files: en_US-el.tsv
- config_name: en_GB-chr
  data_files: en_GB-chr.tsv
- config_name: en-pbb
  data_files: en-pbb.tsv
- config_name: en-ar-rXB
  data_files: en-ar-rXB.tsv
- config_name: en-tzm
  data_files: en-tzm.tsv
- config_name: en-mr-rIN
  data_files: en-mr-rIN.tsv
- config_name: en-ms-rMY
  data_files: en-ms-rMY.tsv
- config_name: en-apc
  data_files: en-apc.tsv
- config_name: en_GB-fi
  data_files: en_GB-fi.tsv
- config_name: en_US-hi
  data_files: en_US-hi.tsv
- config_name: en-hz
  data_files: en-hz.tsv
- config_name: en_GB-mi
  data_files: en_GB-mi.tsv
- config_name: en-sai
  data_files: en-sai.tsv
- config_name: en-ig
  data_files: en-ig.tsv
- config_name: en-en_Shaw
  data_files: en-en_Shaw.tsv
- config_name: en_US-fa_IR
  data_files: en_US-fa_IR.tsv
- config_name: en-mr
  data_files: en-mr.tsv
- config_name: en-pl_PL_rude
  data_files: en-pl_PL_rude.tsv
- config_name: en-cv
  data_files: en-cv.tsv
- config_name: messages-ar
  data_files: messages-ar.tsv
- config_name: en-ko_KO
  data_files: en-ko_KO.tsv
- config_name: en_US-zh-hans
  data_files: en_US-zh-hans.tsv
- config_name: en-ga-IE
  data_files: en-ga-IE.tsv
- config_name: en-am
  data_files: en-am.tsv
- config_name: en-ug
  data_files: en-ug.tsv
- config_name: en-af_ZA
  data_files: en-af_ZA.tsv
- config_name: en-ES
  data_files: en-ES.tsv
- config_name: en_US-ru_RU
  data_files: en_US-ru_RU.tsv
- config_name: en_GB-lv
  data_files: en_GB-lv.tsv
- config_name: en-yi
  data_files: en-yi.tsv
- config_name: en_GB-pl
  data_files: en_GB-pl.tsv
- config_name: en_GB-tl
  data_files: en_GB-tl.tsv
- config_name: en-km
  data_files: en-km.tsv
- config_name: en-azb
  data_files: en-azb.tsv
- config_name: en_devel-fr
  data_files: en_devel-fr.tsv
- config_name: en-pa-PK
  data_files: en-pa-PK.tsv
- config_name: en-tn
  data_files: en-tn.tsv
- config_name: en-mjw
  data_files: en-mjw.tsv
- config_name: en-frs
  data_files: en-frs.tsv
- config_name: en-it-IT
  data_files: en-it-IT.tsv
- config_name: en-ro_RO
  data_files: en-ro_RO.tsv
- config_name: en_US-nl_NL
  data_files: en_US-nl_NL.tsv
- config_name: en-ht
  data_files: en-ht.tsv
- config_name: en_devel-es_cr
  data_files: en_devel-es_cr.tsv
- config_name: en_US-zh-rTW
  data_files: en_US-zh-rTW.tsv
- config_name: en-fo
  data_files: en-fo.tsv
- config_name: en-skr
  data_files: en-skr.tsv
- config_name: en-ak
  data_files: en-ak.tsv
- config_name: en_GB-sr@latin
  data_files: en_GB-sr@latin.tsv
- config_name: en_US-de_CH
  data_files: en_US-de_CH.tsv
- config_name: en_US-uk-UA
  data_files: en_US-uk-UA.tsv
- config_name: en-ko_KR
  data_files: en-ko_KR.tsv
- config_name: en-cy
  data_files: en-cy.tsv
- config_name: en-galo
  data_files: en-galo.tsv
- config_name: en-bn_BD
  data_files: en-bn_BD.tsv
- config_name: en_devel-ms
  data_files: en_devel-ms.tsv
- config_name: fr-it
  data_files: fr-it.tsv
- config_name: en-ny
  data_files: en-ny.tsv
- config_name: en-tet
  data_files: en-tet.tsv
- config_name: en_GB-sk
  data_files: en_GB-sk.tsv
- config_name: eo-ar
  data_files: eo-ar.tsv
- config_name: eo-es
  data_files: eo-es.tsv
- config_name: en-bho
  data_files: en-bho.tsv
- config_name: en-pap
  data_files: en-pap.tsv
- config_name: en-vi_VN
  data_files: en-vi_VN.tsv
- config_name: en_US-ar
  data_files: en_US-ar.tsv
- config_name: en_devel-nb
  data_files: en_devel-nb.tsv
- config_name: en_devel-es_mx
  data_files: en_devel-es_mx.tsv
- config_name: es-ca
  data_files: es-ca.tsv
- config_name: en_GB-kn
  data_files: en_GB-kn.tsv
- config_name: en-ru_UA
  data_files: en-ru_UA.tsv
- config_name: sv-nb
  data_files: sv-nb.tsv
- config_name: en_GB-zh_Hans
  data_files: en_GB-zh_Hans.tsv
- config_name: en-he-IL
  data_files: en-he-IL.tsv
- config_name: en_GB-et
  data_files: en_GB-et.tsv
- config_name: es-pl
  data_files: es-pl.tsv
- config_name: en-hy-AM
  data_files: en-hy-AM.tsv
- config_name: en_US-cy
  data_files: en_US-cy.tsv
- config_name: en-hu-rZZ
  data_files: en-hu-rZZ.tsv
- config_name: en-by
  data_files: en-by.tsv
- config_name: en_GB-hy
  data_files: en_GB-hy.tsv
- config_name: en_US-zh-Hant
  data_files: en_US-zh-Hant.tsv
- config_name: en-gu-IN
  data_files: en-gu-IN.tsv
- config_name: en_GB-ml_IN
  data_files: en_GB-ml_IN.tsv
- config_name: de-nl
  data_files: de-nl.tsv
- config_name: en_devel-ur
  data_files: en_devel-ur.tsv
- config_name: en-ca-ES
  data_files: en-ca-ES.tsv
- config_name: en_GB-kl
  data_files: en_GB-kl.tsv
- config_name: en_US-ta_IN
  data_files: en_US-ta_IN.tsv
- config_name: en_US-sk_SK
  data_files: en_US-sk_SK.tsv
- config_name: en-zh_Latn
  data_files: en-zh_Latn.tsv
- config_name: en_GB-es
  data_files: en_GB-es.tsv
- config_name: en-en_uk
  data_files: en-en_uk.tsv
- config_name: en_GB-ru
  data_files: en_GB-ru.tsv
- config_name: en-gu
  data_files: en-gu.tsv
- config_name: en_US-km
  data_files: en_US-km.tsv
- config_name: en_GB-uz
  data_files: en_GB-uz.tsv
- config_name: en_US-yue-HK
  data_files: en_US-yue-HK.tsv
- config_name: en-ceb
  data_files: en-ceb.tsv
- config_name: en-is
  data_files: en-is.tsv
- config_name: en-ug@Arab
  data_files: en-ug@Arab.tsv
- config_name: es-ru
  data_files: es-ru.tsv
- config_name: en-pt
  data_files: en-pt.tsv
- config_name: en-es-US
  data_files: en-es-US.tsv
- config_name: en-zh-rCMN-HANT
  data_files: en-zh-rCMN-HANT.tsv
- config_name: en-jbo-EN
  data_files: en-jbo-EN.tsv
- config_name: en_US-pa
  data_files: en_US-pa.tsv
- config_name: en_US-or
  data_files: en_US-or.tsv
- config_name: dev-hu
  data_files: dev-hu.tsv
- config_name: en-b+ast
  data_files: en-b+ast.tsv
- config_name: messages-vi
  data_files: messages-vi.tsv
- config_name: en-ht-HT
  data_files: en-ht-HT.tsv
- config_name: en-ar_AA
  data_files: en-ar_AA.tsv
- config_name: en-mcc234
  data_files: en-mcc234.tsv
- config_name: en_GB-he_IL
  data_files: en_GB-he_IL.tsv
- config_name: en-fr_FR
  data_files: en-fr_FR.tsv
- config_name: en-es_ES
  data_files: en-es_ES.tsv
- config_name: en-tr-v26
  data_files: en-tr-v26.tsv
- config_name: ru-kk
  data_files: ru-kk.tsv
- config_name: en_GB-ky
  data_files: en_GB-ky.tsv
- config_name: en-st
  data_files: en-st.tsv
- config_name: en-ky
  data_files: en-ky.tsv
- config_name: en_GB-fa
  data_files: en_GB-fa.tsv
- config_name: en-ta
  data_files: en-ta.tsv
- config_name: en_US-ru-RU
  data_files: en_US-ru-RU.tsv
- config_name: en_US-it
  data_files: en_US-it.tsv
- config_name: en-mai
  data_files: en-mai.tsv
- config_name: en_GB-ga
  data_files: en_GB-ga.tsv
- config_name: en-ay
  data_files: en-ay.tsv
- config_name: en-pt_PT
  data_files: en-pt_PT.tsv
- config_name: en-fa-rIR
  data_files: en-fa-rIR.tsv
- config_name: en-sk_SK
  data_files: en-sk_SK.tsv
- config_name: en-ru_sov
  data_files: en-ru_sov.tsv
- config_name: en-pt-PT
  data_files: en-pt-PT.tsv
- config_name: en_US-ko-KR
  data_files: en_US-ko-KR.tsv
- con",High,4.0
Translation,google/mittens,6.0,57.0,2024-01-17 19:17:58+00:00,cc-by-4.0,0.0,3.6 MB,3774873.6,1.58 MB,1656750.08,3632,none,none,"---
license: cc-by-4.0
task_categories:
- translation
language:
- ar
- fi
- om
- lg
- as
- tr
- fa
- id
- bn
- de
- hi
- pt
- ru
- zh
- ja
- pl
- te
- th
- cs
- fr
- am
- it
- es
tags:
- multilingual
- i18n
size_categories:
- 1K<n<10K
---

# MiTTenS: A Dataset for Evaluating Misgendering in Translation
Misgendering is the act of referring to someone in a way that does not reflect their gender identity.  Translation systems, including foundation models capable of translation, can produce errors that result in misgendering harms. To measure the extent of such potential harms when translating into and out of English, we introduce a dataset, MiTTenS, covering 26 languages from a variety of language families and scripts, including several traditionally underpresented in digital resources. The dataset is constructed with handcrafted passages that target known failure patterns, longer synthetically generated passages, and natural passages sourced from multiple domains. We demonstrate the usefulness of the dataset by evaluating both dedicated neural machine translation systems and foundation models, and show that all systems exhibit errors resulting in misgendering harms, even in high resource languages.

## HuggingFace dataset
This mirrors the GitHub repository at https://github.com/google-research-datasets/mittens
",Low,1.0
Translation,wecover/OPUS,0.0,3689.0,2024-05-23 09:15:01+00:00,none,0.0,13.9 GB,14925011353.6,13.9 GB,14925011353.6,98713492,https://arxiv.org/abs/2004.09813,none,"---
task_categories:
- translation
language:
- af
- am
- ar
- as
- az
- be
- bg
- bn
- br
- bs
- ca
- cs
- cy
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fr
- fy
- ga
- gd
- gl
- ha
- he
- hi
- hr
- hu
- hy
- id
- is
- it
- ja
- jv
- ka
- kk
- km
- kn
- ko
- ku
- ky
- la
- lo
- lt
- mg
- mk
- ml
- mn
- mr
- ms
- my
- ne
- nl
- 'no'
- om
- or
- pa
- pl
- ps
- pt
- ro
- ru
- sa
- sd
- si
- sk
- sl
- so
- sq
- sr
- su
- sv
- sw
- ta
- te
- th
- tl
- tr
- ug
- uk
- ur
- uz
- vi
- xh
- yi
- zh
---

# Collection of OPUS

Corpus from https://opus.nlpl.eu has been collected. The following corpora have been included:
- [UNPC](https://cms.unov.org/UNCorpus)
- [GlobalVoices](https://globalvoices.org)
- [TED2020](https://www.ted.com/participate/translate)
- [News-Commentary](https://opus.nlpl.eu/News-Commentary/corpus/version/News-Commentary)
- [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)
- [Tatoeba](https://tatoeba.org/ko/)
- [Europarl](http://www.statmt.org/europarl/)
- [OpenSubtitles](http://www.opensubtitles.org/)

25,000 samples (randomly sampled within the first 100,000 samples) per language pair of each corpus were collected, with no modification of data.

# Licenses
### OPUS

```
@inproceedings{tiedemann2012parallel,
  title={Parallel data, tools and interfaces in OPUS.},
  author={Tiedemann, J{\""o}rg},
  booktitle={Lrec},
  volume={2012},
  pages={2214--2218},
  year={2012},
  organization={Citeseer}
}
```

### Tatoeba

CC BY 2.0 FR

### TED2020 

CC BY–NC–ND 4.0
```
@inproceedings{reimers-2020-multilingual-sentence-bert,
    title = ""Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation"",
    author = ""Reimers, Nils and Gurevych, Iryna"",
    booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"",
    month = ""11"",
    year = ""2020"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://arxiv.org/abs/2004.09813"",
}
```

### WikiMatrix 
```
CC-BY-SA 4.0
@article{schwenk2019wikimatrix,
  title={Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia},
  author={Schwenk, Holger and Chaudhary, Vishrav and Sun, Shuo and Gong, Hongyu and Guzm{\'a}n, Francisco},
  journal={arXiv preprint arXiv:1907.05791},
  year={2019}
}
```

### UNPC
```
@inproceedings{ziemski2016united,
  title={The united nations parallel corpus v1. 0},
  author={Ziemski, Micha{\l} and Junczys-Dowmunt, Marcin and Pouliquen, Bruno},
  booktitle={Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)},
  pages={3530--3534},
  year={2016}
}
```
",Medium,2.0
Translation,imomayiz/darija-english,8.0,1326.0,2024-04-17 00:00:58+00:00,cc,0.0,6.34 MB,6647971.84,4.19 MB,4393533.44,87785,none,none,"---
language:
- ar
- en
license: cc
task_categories:
- translation
configs:
- config_name: sentences
  data_files:
  - split: sentences
    path: sentences.csv
  sep: "",""
- config_name: submissions
  data_files:
  - split: submissions
    path: submissions/submissions*.json
---
This work is part of [DODa](https://darija-open-dataset.github.io/).
",Low,1.0
Translation,2A2I/Arabic_Aya,14.0,1753.0,2024-03-15 11:08:04+00:00,apache-2.0,0.0,12.5 GB,13421772800.0,12.5 GB,13421772800.0,41472592,https://arxiv.org/abs/2402.06619,none,"---
language:
- ar
license: apache-2.0
size_categories:
- 1M<n<10M
task_categories:
- text-classification
- translation
- summarization
pretty_name: 2A
dataset_info:
- config_name: CohereForAI-aya_collection-aya_dataset
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: string
  - name: language_code
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 7555482
    num_examples: 13960
  download_size: 3687445
  dataset_size: 7555482
- config_name: CohereForAI-aya_collection-aya_human_annotated
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: test
    num_bytes: 222650
    num_examples: 250
  download_size: 120393
  dataset_size: 222650
- config_name: CohereForAI-aya_collection-templated_afrisenti
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 5070578
    num_examples: 14468
  - name: test
    num_bytes: 2674428
    num_examples: 7838
  - name: validation
    num_bytes: 643036
    num_examples: 1816
  download_size: 2330165
  dataset_size: 8388042
- config_name: CohereForAI-aya_collection-templated_mintaka
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 20413129
    num_examples: 70000
  - name: test
    num_bytes: 5799667
    num_examples: 20000
  - name: validation
    num_bytes: 2976183
    num_examples: 10000
  download_size: 6746433
  dataset_size: 29188979
- config_name: CohereForAI-aya_collection-templated_ntx_llm
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 199809
    num_examples: 111
  download_size: 34306
  dataset_size: 199809
- config_name: CohereForAI-aya_collection-templated_xcsqa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: validation
    num_bytes: 393580
    num_examples: 1000
  download_size: 137233
  dataset_size: 393580
- config_name: CohereForAI-aya_collection-templated_xlel_wd
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 97691354
    num_examples: 90760
  - name: test
    num_bytes: 15499274
    num_examples: 14791
  - name: validation
    num_bytes: 10752041
    num_examples: 9768
  download_size: 57959575
  dataset_size: 123942669
- config_name: CohereForAI-aya_collection-translated_adversarial_qa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 147727007
    num_examples: 100000
  - name: test
    num_bytes: 16108000
    num_examples: 10000
  - name: validation
    num_bytes: 14862183
    num_examples: 10000
  download_size: 52642775
  dataset_size: 178697190
- config_name: CohereForAI-aya_collection-translated_cnn_dailymail
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 3578924407
    num_examples: 1000000
  - name: test
    num_bytes: 415594340
    num_examples: 114900
  - name: validation
    num_bytes: 486698663
    num_examples: 133680
  download_size: 2209523190
  dataset_size: 4481217410
- config_name: CohereForAI-aya_collection-translated_dolly
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: gcp_source
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: alphabet
    dtype: string
  - name: split
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 213140804
    num_examples: 148080
  download_size: 96189154
  dataset_size: 213140804
- config_name: CohereForAI-aya_collection-translated_flan_coqa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 245744048
    num_examples: 64090
  download_size: 124335769
  dataset_size: 245744048
- config_name: CohereForAI-aya_collection-translated_flan_cot
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 634249526
    num_examples: 919100
  download_size: 273491678
  dataset_size: 634249526
- config_name: CohereForAI-aya_collection-translated_flan_gem_wiki
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 961863533.277311
    num_examples: 271470
  download_size: 485152798
  dataset_size: 961863533.277311
- config_name: CohereForAI-aya_collection-translated_flan_lambada
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 16531932
    num_examples: 42790
  download_size: 7457248
  dataset_size: 16531932
- config_name: CohereForAI-aya_collection-translated_flan_qa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 2989244
    num_examples: 5400
  download_size: 1292664
  dataset_size: 2989244
- config_name: CohereForAI-aya_collection-translated_hotpotqa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 1154195031
    num_examples: 3554760
  - name: validation
    num_bytes: 69779681
    num_examples: 224000
  download_size: 420699282
  dataset_size: 1223974712
- config_name: CohereForAI-aya_collection-translated_joke_explaination
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 8219049
    num_examples: 7540
  download_size: 3600136
  dataset_size: 8219049
- config_name: CohereForAI-aya_collection-translated_mintaka
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 40908047
    num_examples: 140000
  - name: test
    num_bytes: 11646781
    num_examples: 40000
  - name: validation
    num_bytes: 5951801
    num_examples: 20000
  download_size: 12723211
  dataset_size: 58506629
- config_name: CohereForAI-aya_collection-translated_mlqa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: test
    num_bytes: 331062576
    num_examples: 231800
  - name: validation
    num_bytes: 31900260
    num_examples: 22960
  download_size: 146571384
  dataset_size: 362962836
- config_name: CohereForAI-aya_collection-translated_nqopen
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 397677612
    num_examples: 1758500
  - name: validation
    num_bytes: 16780970
    num_examples: 72200
  download_size: 136208663
  dataset_size: 414458582
- config_name: CohereForAI-aya_collection-translated_paws
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 303643575
    num_examples: 494010
  - name: test
    num_bytes: 49242541
    num_examples: 80000
  - name: validation
    num_bytes: 49475307
    num_examples: 80000
  download_size: 66436419
  dataset_size: 402361423
- config_name: CohereForAI-aya_collection-translated_piqa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 113290227
    num_examples: 161130
  - name: validation
    num_bytes: 12924744
    num_examples: 18380
  download_size: 45954644
  dataset_size: 126214971
- config_name: CohereForAI-aya_collection-translated_soda
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 6230916321
    num_examples: 11915820
  - name: test
    num_bytes: 777982873
    num_examples: 1489680
  - name: validation
    num_bytes: 772817056
    num_examples: 1463460
  download_size: 2804874077
  dataset_size: 7781716250
- config_name: CohereForAI-aya_collection-translated_wiki_split
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 6349516377
    num_examples: 9899440
  - name: test
    num_bytes: 32058254
    num_examples: 50000
  - name: validation
    num_bytes: 32284536
    num_examples: 50000
  download_size: 2446037624
  dataset_size: 6413859167
- config_name: CohereForAI-aya_collection-translated_wikiqa
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 5014300
    num_examples: 10400
  - name: test
    num_bytes: 1378807
    num_examples: 2930
  - name: validation
    num_bytes: 685770
    num_examples: 1400
  download_size: 2872586
  dataset_size: 7078877
- config_name: CohereForAI-aya_collection-translated_xlel_wd
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: dataset_name
    dtype: string
  - name: sub_dataset_name
    dtype: string
  - name: task_type
    dtype: string
  - name: template_id
    dtype: int64
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: split
    dtype: string
  splits:
  - name: train
    num_bytes: 5250663186
    num_examples: 5231120
  - name: test
    num_bytes: 721821743
    num_examples: 729740
  - name: validation
    num_bytes: 635907993
    num_examples: 632640
  download_size: 3091503409
  dataset_size: 6608392922
- config_name: CohereForAI-aya_dataset
  features:
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: language
    dtype: string
  - name: language_code
    dtype: string
  - name: annotation_type
    dtype: string
  - name: user_id
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: train
    num_bytes: 8314232
    num_examples: 13960
  - name: test
    num_bytes: 246400
    num_examples: 250
  download_size: 3778631
  dataset_size: 8560632
- config_name: CohereForAI-aya_evaluation_suite-aya_human_annotated
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: test
    num_bytes: 222650
    num_examples: 250
  download_size: 120393
  dataset_size: 222650
- config_name: CohereForAI-aya_evaluation_suite-dolly_human_edited
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: source_id
    dtype: int64
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: test
    num_bytes: 188495
    num_examples: 200
  download_size: 100291
  dataset_size: 188495
- config_name: CohereForAI-aya_evaluation_suite-dolly_machine_translated
  features:
  - name: id
    dtype: int64
  - name: inputs
    dtype: string
  - name: targets
    dtype: string
  - name: language
    dtype: string
  - name: script
    dtype: string
  - name: source_id
    dtype: int64
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: test
    num_bytes: 3491803
    num_examples: 2000
  download_size: 1762303
  dataset_size: 3491803
configs:
- config_name: CohereForAI-aya_collection-aya_dataset
  data_files:
  - split: train
    path: CohereForAI-aya_collection-aya_dataset/train-*
- config_name: CohereForAI-aya_collection-aya_human_annotated
  data_files:
  - split: test
    path: CohereForAI-aya_collection-aya_human_annotated/test-*
- config_name: CohereForAI-aya_collection-templated_afrisenti
  data_files:
  - split: train
    path: CohereForAI-aya_collection-templated_afrisenti/train-*
  - split: test
    path: CohereForAI-aya_collection-templated_afrisenti/test-*
  - split: validation
    path: CohereForAI-aya_collection-templated_afrisenti/validation-*
- config_name: CohereForAI-aya_collection-templated_mintaka
  data_files:
  - split: train
    path: CohereForAI-aya_collection-templated_mintaka/train-*
  - split: test
    path: CohereForAI-aya_collection-templated_mintaka/test-*
  - split: validation
    path: CohereForAI-aya_collection-templated_mintaka/validation-*
- config_name: CohereForAI-aya_collection-templated_ntx_llm
  data_files:
  - split: train
    path: CohereForAI-aya_collection-templated_ntx_llm/train-*
- config_name: CohereForAI-aya_collection-templated_xcsqa
  data_files:
  - split: validation
    path: CohereForAI-aya_collection-templated_xcsqa/validation-*
- config_name: CohereForAI-aya_collection-templated_xlel_wd
  data_files:
  - split: train
    path: CohereForAI-aya_collection-templated_xlel_wd/train-*
  - split: test
    path: CohereForAI-aya_collection-templated_xlel_wd/test-*
  - split: validation
    path: CohereForAI-aya_collection-templated_xlel_wd/validation-*
- config_name: CohereForAI-aya_collection-translated_adversarial_qa
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_adversarial_qa/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_adversarial_qa/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_adversarial_qa/validation-*
- config_name: CohereForAI-aya_collection-translated_cnn_dailymail
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_cnn_dailymail/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_cnn_dailymail/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_cnn_dailymail/validation-*
- config_name: CohereForAI-aya_collection-translated_dolly
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_dolly/train-*
- config_name: CohereForAI-aya_collection-translated_flan_coqa
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_flan_coqa/train-*
- config_name: CohereForAI-aya_collection-translated_flan_cot
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_flan_cot/train-*
- config_name: CohereForAI-aya_collection-translated_flan_gem_wiki
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_flan_gem_wiki/train-*
- config_name: CohereForAI-aya_collection-translated_flan_lambada
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_flan_lambada/train-*
- config_name: CohereForAI-aya_collection-translated_flan_qa
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_flan_qa/train-*
- config_name: CohereForAI-aya_collection-translated_hotpotqa
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_hotpotqa/train-*
  - split: validation
    path: CohereForAI-aya_collection-translated_hotpotqa/validation-*
- config_name: CohereForAI-aya_collection-translated_joke_explaination
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_joke_explaination/train-*
- config_name: CohereForAI-aya_collection-translated_mintaka
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_mintaka/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_mintaka/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_mintaka/validation-*
- config_name: CohereForAI-aya_collection-translated_mlqa
  data_files:
  - split: test
    path: CohereForAI-aya_collection-translated_mlqa/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_mlqa/validation-*
- config_name: CohereForAI-aya_collection-translated_nqopen
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_nqopen/train-*
  - split: validation
    path: CohereForAI-aya_collection-translated_nqopen/validation-*
- config_name: CohereForAI-aya_collection-translated_paws
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_paws/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_paws/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_paws/validation-*
- config_name: CohereForAI-aya_collection-translated_piqa
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_piqa/train-*
  - split: validation
    path: CohereForAI-aya_collection-translated_piqa/validation-*
- config_name: CohereForAI-aya_collection-translated_soda
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_soda/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_soda/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_soda/validation-*
- config_name: CohereForAI-aya_collection-translated_wiki_split
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_wiki_split/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_wiki_split/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_wiki_split/validation-*
- config_name: CohereForAI-aya_collection-translated_wikiqa
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_wikiqa/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_wikiqa/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_wikiqa/validation-*
- config_name: CohereForAI-aya_collection-translated_xlel_wd
  data_files:
  - split: train
    path: CohereForAI-aya_collection-translated_xlel_wd/train-*
  - split: test
    path: CohereForAI-aya_collection-translated_xlel_wd/test-*
  - split: validation
    path: CohereForAI-aya_collection-translated_xlel_wd/validation-*
- config_name: CohereForAI-aya_dataset
  data_files:
  - split: train
    path: CohereForAI-aya_dataset/train-*
  - split: test
    path: CohereForAI-aya_dataset/test-*
- config_name: CohereForAI-aya_evaluation_suite-aya_human_annotated
  data_files:
  - split: test
    path: CohereForAI-aya_evaluation_suite-aya_human_annotated/test-*
- config_name: CohereForAI-aya_evaluation_suite-dolly_human_edited
  data_files:
  - split: test
    path: CohereForAI-aya_evaluation_suite-dolly_human_edited/test-*
- config_name: CohereForAI-aya_evaluation_suite-dolly_machine_translated
  data_files:
  - split: test
    path: CohereForAI-aya_evaluation_suite-dolly_machine_translated/test-*
---
# Dataset Card for : Arabic Aya (2A)

<!-- Provide a quick summary of the dataset. -->

<!-- This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).-->

## **Arabic Aya (2A) : A Curated Subset of the Aya Collection for Arabic Language Processing**

### Dataset Sources & Infos
- **Data Origin**: Derived from 69 subsets of the original Aya datasets : [CohereForAI/aya_collection](https://huggingface.co/datasets/CohereForAI/aya_collection), [CohereForAI/aya_dataset](https://huggingface.co/datasets/CohereForAI/aya_dataset), and [CohereForAI/aya_evaluation_suite](https://huggingface.co/datasets/CohereForAI/aya_evaluation_suite).
- **Languages**: Modern Standard Arabic (MSA) and a variety of Arabic dialects ( 'arb', 'arz', 'ary', 'ars', 'knc', 'acm', 'apc', 'aeb', 'ajp', 'acq' )
- **Applications**: `Language Modeling`,  `Text Classification`, `Sentiment Analysis`, `Dialect Identification`, `Translation`
- **Paper:** [2402.06619](https://huggingface.co/papers/2402.06619)
- **Maintainer:** [Elfilali Ali](https://huggingface.co/Ali-C137)
- **License:** Apache-2.0

### Overview
`Arabic Aya` is a meticulously curated dataset derived from the comprehensive Aya collection by [CohereForAI](https://huggingface.co/CohereForAI), specifically focusing on Arabic text data. This dataset aggregates content from the [CohereForAI/aya_collection](https://huggingface.co/datasets/CohereForAI/aya_collection), [CohereForAI/aya_dataset](https://huggingface.co/datasets/CohereForAI/aya_dataset), and [CohereForAI/aya_evaluation_suite](https://huggingface.co/datasets/CohereForAI/aya_evaluation_suite), filtering out all but the Arabic content, including both Modern Standard Arabic (MSA) and various regional dialects. 

### Purpose
The aim of 'Arabic Aya' is to provide researchers, technologists, and linguists with a ready-to-use Arabic text resource, significantly reducing the time and effort required for data preprocessing in NLP and AI projects focused on the Arabic language.
- Use the Aya datasets out of the box for your Arabic applications and research 😀

### Usage
This dataset serves as a foundational tool for those embarking on Arabic language projects, from academic research to commercial applications. By providing a pre-filtered source of Arabic text, 'Arabic Aya' enables users to dive straight into model training, analysis, and application development without the preliminary hassle of data cleaning and language filtering.

#### Use with HuggingFace's datasets library
To load this dataset with Datasets, you'll need to install Datasets as `pip install datasets --upgrade` and then use a similar code to the following:

```python
from datasets import load_dataset

dataset = load_dataset(""2A2I/Arabic_Aya"", ""CohereForAI-aya_collection-templated_mintaka"")
```
In the above code snippet, ""CohereForAI-aya_collection-templated_mintaka"" refers to the arabic version (100k rows) of the original ""templated_mintaka"" subset (780k rows) of the aya_collection. You can load other subsets by specifying its name at the time of loading the dataset.


### Access and Contribution
Available on the Hugging Face Hub under [2A2I/Arabic_Aya](https://huggingface.co/datasets/2A2I/Arabic_Aya), 'Arabic Aya' invites contributions from the community. Users are encouraged to offer feedback, suggest improvements.

### Support and Collaboration
We are committed to fostering an inclusive and supportive environment around Arabic AI and NLP research. For support, collaboration, or queries regarding the dataset, please reach out through the Hugging Face Hub's discussion section or reach out at [2A2I Contact Email](arabic.ai.initiative@gmail.com).




# Original Dataset Card of Aya by CohereForAI


![Aya Header](https://huggingface.co/datasets/CohereForAI/aya_collection/resolve/main/aya_header.png)

# Dataset Summary
The Aya Collection is a massive multilingual collection consisting of 513 million instances of prompts and completions covering a wide range of tasks.
This collection incorporates instruction-style templates from fluent speakers and applies them to a curated list of datasets, as well as translations of instruction-style datasets into 101 languages. Aya Dataset, a human-curated multilingual instruction and response dataset, is also part of this collection. See our paper for more details regarding the collection.   

- **Curated by:** Contributors of [Aya Open Science Intiative](https://cohere.com/research/aya)
- **Language(s):** 115 languages
- **License:** [Apache 2.0](https://opensource.org/license/apache-2-0)
  
- **Aya Datasets Family:**
| Name | Explanation |
|------|--------------|
| [aya_dataset](https://huggingface.co/datasets/CohereForAI/aya_dataset) | Human-annotated multilingual instruction finetuning dataset, comprising over 204K instances across 65 languages. |
| [aya_collection](https://huggingface.co/datasets/CohereForAI/aya_collection) | Created by applying instruction-style templates from fluent speakers to 44 datasets, including translations of 19 instruction-style datasets into 101 languages.|
| [aya_evaluation_suite](https://huggingface.co/datasets/CohereForAI/aya_evaluation_suite) | A diverse evaluation set for multilingual open-ended generation, featuring 250 culturally grounded prompts in 7 languages, 200 translated prompts in 24 languages, and human-edited versions selected for cross-cultural relevance from English Dolly in 6 languages.|


# Dataset
The `Aya Collection` is a comprehensive, large corpus of datasets that can be used by researchers around the world to train multilingual models. Our goal is only to include datasets with permissive licensing for manipulation and redistribution.

The `Aya Collection` consists of three different sources of data:

1. Templated data: We collaborated with fluent speakers to create templates that allowed for the automatic expansion of existing datasets into various languages.
2. Translated data: We translated a hand-selected subset of 19 datasets into 101 languages (114 dialects) using the NLLB 3.3B parameter machine translation model.
3. Aya Dataset: We release the [Aya Dataset](https://huggingface.co/datasets/CohereForAI/aya_dataset) as a subset of the overall collection. This is the only dataset in the collection that is human-annotated in its entirety.

## Load with Datasets
To load ",High,6.0
Translation,louam123/darja-tounsi1,0.0,28.0,2024-02-14 19:49:39+00:00,none,0.0,419 KB,429056.0,269 KB,275456.0,7879,none,none,"---
task_categories:
- translation
language:
- ar
- en
tags:
- darja
- tunisian
pretty_name: 'darja-tounsi-louam '
size_categories:
- 1K<n<10K
---",Low,0.0
Translation,xezpeleta/ccmatrix,0.0,57.0,2024-02-19 07:56:12+00:00,unknown,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/1911.04944,none,"---
annotations_creators:
- found
language_creators:
- found
language:
- af
- am
- ar
- ast
- az
- be
- bg
- bn
- br
- ca
- ceb
- cs
- cy
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fr
- fy
- ga
- gd
- gl
- ha
- he
- hi
- hr
- hu
- hy
- id
- ig
- ilo
- is
- it
- ja
- jv
- ka
- kk
- km
- ko
- la
- lb
- lg
- lt
- lv
- mg
- mk
- ml
- mr
- ms
- my
- ne
- nl
- 'no'
- oc
- om
- or
- pl
- pt
- ro
- ru
- sd
- si
- sk
- sl
- so
- sq
- sr
- su
- sv
- sw
- ta
- tl
- tr
- tt
- uk
- ur
- uz
- vi
- wo
- xh
- yi
- yo
- zh
- zu
- se
license:
- unknown
multilinguality:
- multilingual
size_categories:
- 100M<n<1B
source_datasets:
- original
task_categories:
- text2text-generation
- translation
task_ids: []
paperswithcode_id: ccmatrix
pretty_name: CCMatrixV1
tags:
- conditional-text-generation
---

# Dataset Card for CCMatrix v1

## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description
- **Homepage:** https://opus.nlpl.eu/CCMatrix.php
- **Repository:** None
- **Paper:** https://arxiv.org/abs/1911.04944
### Dataset Summary

This corpus has been extracted from web crawls using the margin-based bitext mining techniques described at https://github.com/facebookresearch/LASER/tree/master/tasks/CCMatrix.

* 90 languages, 1,197 bitexts
* total number of files: 90
* total number of tokens: 112.14G
* total number of sentence fragments: 7.37G

### Supported Tasks and Leaderboards
[More Information Needed]

### Languages

Configs are generated for all language pairs in both directions.
You can find the valid pairs in Homepage section of Dataset Description: https://opus.nlpl.eu/CCMatrix.php
E.g.

```
from datasets import load_dataset
dataset = load_dataset(""yhavinga/ccmatrix"", ""en-nl"", streaming=True)
```

This will open the `en-nl` dataset in streaming mode. Without streaming, download and prepare will take tens of minutes.
You can inspect elements with:

```
print(next(iter(dataset['train'])))
{'id': 0, 'score': 1.2499677, 'translation': {'en': 'They come from all parts of Egypt, just like they will at the day of His coming.', 'nl': 'Zij kwamen uit alle delen van Egypte, evenals zij op de dag van Zijn komst zullen doen.'}}
```

## Dataset Structure
### Data Instances
For example:

```json
{
        ""id"": 1,
        ""score"": 1.2498379,
        ""translation"": {
            ""nl"": ""En we moeten elke waarheid vals noemen die niet minstens door een lach vergezeld ging.”"",
            ""en"": ""And we should call every truth false which was not accompanied by at least one laugh.”""
        }
    }
```

### Data Fields
Each example contains an integer id starting with 0, a score, and a translation dictionary with the language 1 and
language 2 texts.

### Data Splits
Only a `train` split is provided.

## Dataset Creation
### Curation Rationale
[More Information Needed]
### Source Data
[More Information Needed]
#### Initial Data Collection and Normalization
[More Information Needed]
#### Who are the source language producers?
[More Information Needed]
### Annotations
[More Information Needed]
#### Annotation process
[More Information Needed]
#### Who are the annotators?
[More Information Needed]
### Personal and Sensitive Information
[More Information Needed]
## Considerations for Using the Data
### Social Impact of Dataset
[More Information Needed]
### Discussion of Biases
[More Information Needed]
### Other Known Limitations
[More Information Needed]
## Additional Information
### Dataset Curators
[More Information Needed]
### Licensing Information
[More Information Needed]
### Citation Information

IMPORTANT: Please cite reference [2][3] if you use this data.

1. **[CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data](https://arxiv.org/abs/1911.00359)**
   by *Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Jouli
    and Edouard Grave*.
2. **[CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB](https://arxiv.org/abs/1911.04944)** by *Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave and Armand Joulin*.
3. **[Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125)** by *Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines,
    Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky,
    Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin.*

This HuggingFace CCMatrix dataset is a wrapper around the service and files prepared and hosted by OPUS:

* **[Parallel Data, Tools and Interfaces in OPUS](https://www.aclweb.org/anthology/L12-1246/)** by *Jörg Tiedemann*.

### Contributions
",High,4.0
Translation,Tamazight-NLP/DGLAI,2.0,28.0,2024-02-23 03:33:22+00:00,none,0.0,147 KB,150528.0,88.4 KB,90521.6,1834,none,none,"---
task_categories:
- translation
- text2text-generation
language:
- ber
- zgh
- fr
- ar
size_categories:
- 1K<n<10K
pretty_name: DGLAI examples
---

# Dataset Card for DGLAI examples

<!-- Provide a quick summary of the dataset. -->

Parallel sentences scraped from the DGLAI (Dictionnaire Général de la Langue Amazighe) published by IRCAM.

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** IRCAM.
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** Standard Moroccan Tamazight, French, Arabic.
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,5.0
Translation,alpindale/subscene,3.0,144.0,2024-03-05 13:21:42+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- translation
- text-generation
language:
- en
- ar
- fa
- da
- zh
- tr
pretty_name: subscene
size_categories:
- 10M<n<100M
---

# Subscene
A dump of the [subscene](https://subscene.com) website.",Low,1.0
Translation,atlasia/darija-translation,16.0,18.0,2024-04-25 22:20:20+00:00,cc,0.0,2.04 MB,unknown,2.04 MB,unknown,45103,none,none,"---
license: cc
task_categories:
- translation
language:
- ar
- fr
- en
---
Welcome to the AtlasIA Darija Translation repo!

This repository is part of an open-source initiative aimed at collecting a dataset for Darija-English translation. 
Our project involves using the interface https://atlasia.ma for collecting and storing the data. Here's an overview of how our project works:

## Project Overview:

* Translation Interface: Using the user-friendly interface of [AtlasIA](https://atlasia.ma), we present Darija sentences for translation, then contributors provide translations in English or French for the given Darija sentences.
* Additional Features: Optionally, contributors can also provide audio recordings of the Darija sentences, along with a confidence level associated with the translation.
* Data Storage: The collected data, including Darija sentences, translations, audio recordings, and confidence levels, are stored initially in a Firestore DB. After processing, the data is moved to this repository to be open-sourced.

## How to Contribute:

* Visit [AtlasIA](https://atlasia.ma).
* Translate Sentences: Read the provided sentence, select the translation language (EN/FR), and translate the sentence into the chosen language.
* Optional Features: Contributors can record an audio reading of the original sentence (optional) and submit.
* Reporting Incorrect Sentences: If you encounter an incorrect sentence, you can use the ""Report"" button to send an alert about it.
* Pre-Processing: Before adding data to this repo, our team will pre-process it to ensure data quality and consistency.
* Contributions Welcome: We invite contributions from individuals interested in Darija-English translation. Feel free to join our efforts by contributing translations or improving the dataset.

## Dataset Structure:

The dataset is organized in a structured format, with each entry containing:
* Darija Sentence
* English or French Translation
* Optional Audio Recording
* Confidence Level

## License:
This dataset is released under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License ([CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)). It is open to everyone for research and open-source work.

## Contact Us:
If you have any questions, suggestions, or feedback regarding the dataset or project, please don't hesitate to contact one of the project collaborators or join our Discord community.

Thank you for your interest and contribution to the Darija-English Translation Dataset!",Medium,2.0
Translation,Selimx2001x/Arabic-Text-to-Sign-Language-Translation,0.0,15.0,2024-03-16 21:27:40+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- translation
language:
- ar
tags:
- Sign Language
- Signs
- Arabic
- Translation
pretty_name: SignLang
size_categories:
- 1K<n<10K
---",Low,1.0
Translation,MohamedRashad/rasaif-translations,4.0,69.0,2024-03-19 15:23:34+00:00,none,0.0,246 KB,251904.0,246 KB,251904.0,1951,none,none,"---
dataset_info:
  features:
  - name: arabic
    dtype: string
  - name: english
    dtype: string
  splits:
  - name: train
    num_bytes: 458802
    num_examples: 1951
  download_size: 245732
  dataset_size: 458802
task_categories:
- translation
language:
- ar
- en
pretty_name: Rasaif Translation
size_categories:
- 1K<n<10K
---
# Dataset Source

https://rasaif.com",Low,1.0
Translation,felixludos/babel-briefings,5.0,213.0,2024-03-29 20:53:36+00:00,cc-by-nc-sa-4.0,0.0,UNKNOWN,UNKNOWN,2.9 GB,3113851289.6,3113851289.6,https://arxiv.org/abs/2403.19352,none,"---
license: cc-by-nc-sa-4.0
pretty_name: Babel Briefings
language:
- en
- es
- de
- fr
- zh
- ar
- pt
- bg
- cs
- el
- he
- hu
- id
- it
- ja
- ko
- lt
- lv
- nl
- 'no'
- pl
- ro
- ru
- sk
- sl
- sr
- sv
- th
- tr
- uk
task_categories:
- text-classification
- translation
- zero-shot-classification
- feature-extraction
- text-generation
tags:
- news
- headlines
- business
- science
- technology
- sports
- health
- entertainment
size_categories:
- 1M<n<10M
---

# Babel Briefings News Headlines Dataset README

> Break Free from the Language Barrier

Version: 1 - Date: 30 Oct 2023

Collected and Prepared by Felix Leeb (Max Planck Institute for Intelligent Systems, Tübingen, Germany)

License: Babel Briefings Headlines Dataset © 2023 by Felix Leeb is licensed under [CC BY-NC-SA 4.0](http://creativecommons.org/licenses/by-nc-sa/4.0/) 

Check out our paper on [arxiv](https://arxiv.org/abs/2403.19352).

This dataset contains 4,719,199 news headlines across 30 different languages collected between 8 August 2020 and 29 November 2021. The headlines were collected using the [News API](https://newsapi.org/) by collecting the top headlines (usually about 30-70 articles) separately for each combination of the 54 locations x 7 categories almost every day. Note, that the same article may occur more than once across different locations, categories, or dates (which is recorded in the `instances` property), so in total 7,419,089 instances were collected.

For non-English articles, the article data is translated to English using Google Translate (see `en-title`, `en-description`, and `en-content` properties).

The dataset is provided in the form of 54 JSON files, one for each location containing the all the unique headlines that appeared for the first time in the corresponding location. Each headline is represented as a JSON object with the following properties:

- `ID`: (integer) a unique ID for each article 
- `title`: (string) the headline text in the original language
- `description`: (string) the article description in the original language
- `content`: (string) the first few words of the article in the original language
- `author`: (string) the author of the article
- `source-id`: (string) the news aggregator (e.g. Google-News)
- `source-name`: (string) usually the domain of the source where the article was published
- `url`: (string) the URL of the article
- `urlToImage`: (string) the URL to an image associated with the article
- `publishedAt`: (date) the article was published
- `instances`: (list) specific time and place where this article was posted. Each element contains:
  - `collectedAt`: (date) date and time when the article was collected
  - `category`: (string) of the article from 7 possible values (see below for full list)
  - `location`: (string) of the article from 54 possible values (see below for full list)
- `language`: (string) ISO-639 2-letter code for the language (inferred from location)
- `en-title`: (string) the headline text translated to English (if necessary)
- `en-description`: (string) the article description text translated to English (if necessary)
- `en-content`: (string) the first few words of the article translated to English (if necessary)


## Notes

- Unfortunately, due to an issue with News API, the `content` of articles originally in a non-latin based script (e.g. Chinese, Arabic, Japanese, Greek, Russian, etc.) are usually not available. However, for the most part all other articles should have a meaningful `content` property, and the `title` and `descriptions` appear unaffected.
- All properties except `language`, `en-title`, `en-description`, and `en-content` are taken directly from the News API responses. The language is inferred from the location, and the English translations are collected using Google Translate.


## Statistics

Here are a few basic summary statistics about the dataset.

### Articles by Language

| Code   | Language   |   Articles | Locations                                          |
|--------|------------|------------|----------------------------------------------------|
| en     | English    |    1128233 | au, ca, gb, ie, in, my, ng, nz, ph, sa, sg, us, za |
| es     | Spanish    |     455952 | ar, co, cu, mx, ve                                 |
| fr     | French     |     288328 | be, fr, ma                                         |
| zh     | Chinese    |     270887 | cn, hk, tw                                         |
| de     | German     |     259718 | at, ch, de                                         |
| pt     | Portuguese |     243829 | br, pt                                             |
| ar     | Arabic     |     178854 | ae, eg                                             |
| id     | Indonesian |     131252 | id                                                 |
| it     | Italian    |     129005 | it                                                 |
| tr     | Turkish    |     122724 | tr                                                 |
| el     | Greek      |     119940 | gr                                                 |
| ja     | Japanese   |     118475 | jp                                                 |
| pl     | Polish     |     116904 | pl                                                 |
| ru     | Russian    |     113395 | ru                                                 |
| nl     | Dutch      |     104031 | nl                                                 |
| th     | Thai       |      90708 | th                                                 |
| sv     | Swedish    |      86838 | se                                                 |
| ko     | Korean     |      83090 | kr                                                 |
| sr     | Serbian    |      80040 | rs                                                 |
| hu     | Hungarian  |      73509 | hu                                                 |
| cs     | Czech      |      70647 | cz                                                 |
| he     | Hebrew     |      67794 | il                                                 |
| bg     | Bulgarian  |      67223 | bg                                                 |
| uk     | Ukrainian  |      65610 | ua                                                 |
| ro     | Romanian   |      54601 | ro                                                 |
| no     | Norwegian  |      46804 | no                                                 |
| sk     | Slovak     |      43057 | sk                                                 |
| lv     | Latvian    |      40006 | lv                                                 |
| lt     | Lithuanian |      34719 | lt                                                 |
| sl     | Slovenian  |      33026 | si                                                 |

### Instances by category

| Category      |   Instances |
|---------------|-------------|
| sports        |     1132542 |
| entertainment |      982479 |
| business      |      840748 |
| technology    |      802933 |
| general       |      704692 |
| health        |      424188 |
| science       |      388281 |

### Instances by location

| Code   | Location             |   Instances |
|--------|----------------------|-------------|
| ae     | United Arab Emirates |      214256 |
| ar     | Argentina            |      159139 |
| ph     | Philippines          |      155365 |
| ng     | Nigeria              |      155112 |
| in     | India                |      145536 |
| us     | United States        |      144800 |
| ca     | Canada               |      143928 |
| sa     | Saudi Arabia         |      143382 |
| cu     | Cuba                 |      138675 |
| au     | Australia            |      138408 |
| br     | Brazil               |      136101 |
| ma     | Morocco              |      131974 |
| id     | Indonesia            |      131252 |
| eg     | Egypt                |      129382 |
| it     | Italy                |      129005 |
| gb     | United Kingdom       |      127391 |
| ie     | Ireland              |      126640 |
| mx     | Mexico               |      124499 |
| tr     | Turkey               |      122724 |
| gr     | Greece               |      119940 |
| de     | Germany              |      119917 |
| jp     | Japan                |      118475 |
| za     | South Africa         |      117351 |
| fr     | France               |      117210 |
| pl     | Poland               |      116904 |
| pt     | Portugal             |      115976 |
| co     | Colombia             |      115325 |
| my     | Malaysia             |      115223 |
| ru     | Russian Federation   |      113395 |
| at     | Austria              |      111867 |
| nz     | New Zealand          |      108809 |
| tw     | Taiwan               |      108652 |
| nl     | Netherlands          |      104031 |
| sg     | Singapore            |      101251 |
| be     | Belgium              |       99460 |
| cn     | China                |       91561 |
| ve     | Venezuela            |       91045 |
| th     | Thailand             |       90708 |
| se     | Sweden               |       86838 |
| kr     | Korea                |       83090 |
| hk     | Hong Kong            |       83051 |
| rs     | Serbia               |       80040 |
| hu     | Hungary              |       73509 |
| cz     | Czechia              |       70647 |
| ch     | Switzerland          |       68846 |
| il     | Israel               |       67794 |
| bg     | Bulgaria             |       67223 |
| ua     | Ukraine              |       65610 |
| ro     | Romania              |       54601 |
| no     | Norway               |       46804 |
| sk     | Slovakia             |       43057 |
| lv     | Latvia               |       40006 |
| lt     | Lithuania            |       34719 |
| si     | Slovenia             |       33026 |",Medium,3.0
Translation,ymoslem/CoVoST2-EN-AR,4.0,128.0,2024-12-04 13:15:08+00:00,cc-by-nc-4.0,0.0,14 GB,15032385536.0,14 GB,15032385536.0,326228,https://arxiv.org/abs/2007.10310,none,"---
dataset_info:
- config_name: ar-en
  features:
  - name: client_id
    dtype: string
  - name: file
    dtype: string
  - name: audio
    dtype:
      audio:
        sampling_rate: 16000
  - name: sentence
    dtype: string
  - name: translation
    dtype: string
  - name: id
    dtype: string
  splits:
  - name: train
    num_examples: 2283
  - name: test
    num_examples: 1695
  - name: validation
    num_examples: 1758
- config_name: en-ar
  features:
  - name: client_id
    dtype: string
  - name: file
    dtype: string
  - name: audio
    dtype:
      audio:
        sampling_rate: 16000
  - name: sentence
    dtype: string
  - name: translation
    dtype: string
  - name: id
    dtype: string
  splits:
  - name: train
    num_examples: 289430
  - name: test
    num_examples: 15531
  - name: validation
    num_examples: 15531
configs:
- config_name: ar-en
  data_files:
  - split: train
    path: ar-en/train-*
  - split: validation
    path: ar-en/validation-*
  - split: test
    path: ar-en/test-*
- config_name: en-ar
  data_files:
  - split: train
    path: en-ar/train-*
  - split: validation
    path: en-ar/validation-*
  - split: test
    path: en-ar/test-*
license: cc-by-nc-4.0
task_categories:
- automatic-speech-recognition
- text-to-speech
- translation
language:
- ar
- en
size_categories:
- 100K<n<1M
---


## Dataset Description

CoVoST 2 is a large-scale multilingual speech translation corpus based on Common Voice, developed by FAIR. This is the English-to-Arabic portion of the dataset. The original dataset can be found [here](https://github.com/facebookresearch/covost).


## Data Splits (EN-AR)

| lang  | train  | validation | test  |
|-------|--------|------------|-------|
| EN-AR | 289430 | 15531      | 15531 |
| AR-EN | 2283	 | 1758	      | 1695  |

## Citation

```
@misc{wang2020covost,
    title={CoVoST 2: A Massively Multilingual Speech-to-Text Translation Corpus},
    author={Changhan Wang and Anne Wu and Juan Pino},
    year={2020},
    eprint={2007.10310},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```",High,5.0
Translation,atlasia/Synthetic-Data-English-Darija,2.0,38.0,2024-11-03 21:35:34+00:00,none,0.0,170 kB,unknown,170 kB,unknown,1011,none,none,"---
language:
- ar
- en
size_categories:
- 1K<n<10K
task_categories:
- translation
dataset_info:
  features:
  - name: English
    dtype: string
  - name: Moroccan Darija
    dtype: string
  splits:
  - name: train
    num_bytes: 366143
    num_examples: 1011
  download_size: 170098
  dataset_size: 366143
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
---

Synthetic data generated by proprietary llms.",Low,1.0
Translation,ziozzang/osx_dictionary_translation_pairs,1.0,34.0,2024-04-02 04:16:05+00:00,none,0.0,300 MB,unknown,106 MB,unknown,2450540,none,none,"---
task_categories:
- translation
language:
- ko
- en
- cs
- ar
- nl
- fi
- fr
- de
- hu
- hi
- el
- pl
- id
- it
- pt
- ru
- vi
- tr
- te
- es
- zh
- th
- ja
---

Apple's Internal dictionary extracted.
- the pairs are word level example of translation pairs (usage case, or example pairs)
- Original data are Human curated.
- This can be used for make machine generated training data.

License
- I have no claim of license.

Expected Usecase
- This dataset is for simple test, tasks for translation case.

---
Pipeline example
- feed as example. and LLM can generate translation pairs to better translation.

References
- apple-peeler: https://pypi.org/project/apple-peeler/
",Medium,3.0
Translation,AbderrahmanSkiredj1/data_translation_quran_yusufali_10k,0.0,6.0,2024-07-15 19:04:49+00:00,none,0.0,4.4 MB,4613734.4,4.4 MB,4613734.4,9873,none,none,"---
dataset_info:
  features:
  - name: text
    dtype: string
  - name: label
    dtype: string
  - name: prompt
    dtype: string
  splits:
  - name: train
    num_bytes: 9893887
    num_examples: 9873
  download_size: 4396886
  dataset_size: 9893887
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- translation
language:
- ar
- en
---",Low,1.0
Translation,AbderrahmanSkiredj1/ahadith_translation_34k,1.0,9.0,2024-07-15 19:00:51+00:00,none,0.0,19.5 MB,20447232.0,19.5 MB,20447232.0,34088,none,none,"---
dataset_info:
  features:
  - name: text
    dtype: string
  - name: label
    dtype: string
  - name: prompt
    dtype: string
  splits:
  - name: train
    num_bytes: 56339232
    num_examples: 34088
  download_size: 19500537
  dataset_size: 56339232
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- translation
language:
- ar
---",Low,1.0
Translation,AbderrahmanSkiredj1/data_un_parallel_ar_fr_40k,1.0,29.0,2025-01-03 22:32:37+00:00,none,0.0,29.9 MB,31352422.4,29.9 MB,31352422.4,38811,none,none,"---
dataset_info:
  features:
  - name: text
    dtype: string
  - name: label
    dtype: string
  splits:
  - name: train
    num_bytes: 67856777
    num_examples: 38811
  download_size: 29916888
  dataset_size: 67856777
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- translation
language:
- ar
- fr
---",Low,1.0
Translation,miugod/ikcest2022,1.0,6.0,2024-04-22 11:48:08+00:00,none,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
task_categories:
- translation
language:
- zh
- en
- fr
- ru
- th
- ar
pretty_name: ikcest2022
size_categories:
- 100K<n<1M
dataset_info:
  - config_name: ikcest2022-zh-fr
    features:
      - name: translation
        dtype:
          translation:
            languages:
              - zh
              - fr
    splits:
      - name: train
      - name: test
      - name: validation
  - config_name: ikcest2022-zh-ru
    features:
      - name: translation
        dtype:
          translation:
            languages:
              - zh
              - ru
    splits:
      - name: train
      - name: test
      - name: validation
  - config_name: ikcest2022-zh-th
    features:
      - name: translation
        dtype:
          translation:
            languages:
              - zh
              - th
    splits:
      - name: train
      - name: test
      - name: validation
  - config_name: ikcest2022-zh-ar
    features:
      - name: translation
        dtype:
          translation:
            languages:
              - zh
              - ar
    splits:
      - name: train
      - name: test
      - name: validation
  - config_name: ikcest2022-zh-en
    features:
      - name: translation
        dtype:
          translation:
            languages:
              - zh
              - en
    splits:
      - name: train
      - name: test
      - name: validation

---

# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->

|      | train  | dev  | test   |
| ---- | ------ | ---- | ------ |
| zhen | 100000 | 1000 | 1000  |
| zhfr | 100000 | 1000 | 1000 |
| zhru | 150000 | 1000 | 1000 |
| zhth | 100000 | 1000 | 1000 |
| zhar | 50000  | 1000 | 1000   |


- **Curated by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Translation,davidstap/ted_talks,3.0,116.0,2024-04-16 15:00:08+00:00,cc-by-nc-nd-4.0,4.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,"['https://aclanthology.org/N18-2084"",']","---
language:
- ar 
- az 
- be 
- bg 
- bn 
- bs 
- cs 
- da 
- de 
- el 
- en 
- eo 
- es 
- et 
- eu 
- fa 
- fi 
- fr 
- gl 
- he 
- hi 
- hr 
- hu 
- hy 
- id 
- it 
- ja 
- ka 
- kk 
- ko 
- ku 
- lt 
- mk 
- mn 
- mr 
- ms 
- my 
- nb 
- nl 
- pl 
- pt 
- ro 
- ru 
- sk 
- sl 
- sq 
- sr 
- sv 
- ta 
- th 
- tr 
- uk 
- ur 
- vi 
- zh 
language_creators:
- expert-generated
annotations_creators:
- crowdsourced
license:
- cc-by-nc-nd-4.0
multilinguality:
- translation
pretty_name: TED_Talks
task_categories:
  - translation
---
## Dataset Description

Train, validation and test splits for TED talks as in http://phontron.com/data/ted_talks.tar.gz. Data is detokenized using moses. 

Example of loading:
```python
dataset = load_dataset(""davidstap/ted_talks"", ""ar_en"", trust_remote_code=True)
```

Note that `ar_en` and `en_ar` will result in the same data being loaded..


The following languages are available:
```
- ar 
- az 
- be 
- bg 
- bn 
- bs 
- cs 
- da 
- de 
- el 
- en 
- eo 
- es 
- et 
- eu 
- fa 
- fi 
- fr 
- fr-ca 
- gl 
- he 
- hi 
- hr 
- hu 
- hy 
- id 
- it 
- ja 
- ka 
- kk 
- ko 
- ku 
- lt 
- mk 
- mn 
- mr 
- ms 
- my 
- nb 
- nl 
- pl 
- pt 
- pt-br 
- ro 
- ru 
- sk 
- sl 
- sq 
- sr 
- sv 
- ta 
- th 
- tr 
- uk 
- ur 
- vi 
- zh 
- zh-cn 
- zh-tw
```


### Citation Information

```
@inproceedings{qi-etal-2018-pre,
    title = ""When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?"",
    author = ""Qi, Ye  and
      Sachan, Devendra  and
      Felix, Matthieu  and
      Padmanabhan, Sarguna  and
      Neubig, Graham"",
    booktitle = ""Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)"",
    month = jun,
    year = ""2018"",
    address = ""New Orleans, Louisiana"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/N18-2084"",
    doi = ""10.18653/v1/N18-2084"",
    pages = ""529--535"",
}
```
",High,5.0
Translation,AbderrahmanSkiredj1/hadiths_ar_fr_extracted_from_hadeethenc_dot_com_2k5,0.0,22.0,2025-01-03 22:33:18+00:00,none,0.0,317 KB,324608.0,317 KB,324608.0,2443,none,none,"---
dataset_info:
  features:
  - name: text
    dtype: string
  - name: label
    dtype: string
  splits:
  - name: train
    num_bytes: 759687
    num_examples: 2443
  download_size: 316825
  dataset_size: 759687
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- translation
language:
- ar
- fr
---",Low,1.0
Translation,davidstap/NTREX,3.0,91.0,2024-04-23 17:47:51+00:00,cc-by-sa-4.0,0.0,49.9 MB,52323942.4,26.6 MB,27892121.6,255616,none,"['https://aclanthology.org/2022.sumeval-1.4"",', 'https://aclanthology.org/W19-5301"",']","---
annotations_creators:
- expert-generated
language_creators:
- expert-generated
language:
- af
- am
- ar
- az
- ba
- be
- bg
- bn
- bo
- bs
- ca
- cs
- cy
- da
- de
- dv
- dz
- ee
- el
- et
- eu
- fa
- fa
- fi
- fil
- fj
- fj
- fo
- fr
- gd
- gu
- ha
- he
- hi
- hmn
- hr
- hu
- hy
- id
- ig
- is
- it
- ja
- kk
- km
- kn
- ko
- ku
- ku
- ky
- lb
- lo
- lt
- lv
- mi
- mk
- mn
- mr
- ms
- ms
- mt
- my
- nb
- nd
- ne
- nl
- nn
- ny
- om
- oy
- pa
- ps
- pt
- ro
- ru
- rw
- sd
- sh
- shi
- si
- sk
- sl
- sm
- sn
- so
- sq
- sr
- ss
- st
- sv
- sw
- ta
- te
- tg
- th
- tk
- tn
- to
- tr
- tt
- ty
- uk
- ur
- uz
- ve
- vi
- wo
- xh
- yo
- zh
- zh
- zu
license:
- cc-by-sa-4.0
multilinguality:
- translation
task_categories:
- translation
size_categories:
- ""1997""
configs:
  - config_name: afr_Latn
    data_files:
      - split: test
        path: data/afr_Latn/newstest2019-ref.afr.txt
  - config_name: amh_Ethi
    data_files:
      - split: test
        path: data/amh_Ethi/newstest2019-ref.amh.txt
  - config_name: arb_Arab
    data_files:
      - split: test
        path: data/arb_Arab/newstest2019-ref.arb.txt
  - config_name: aze_Latn
    data_files:
      - split: test
        path: data/aze_Latn/newstest2019-ref.aze.txt
  - config_name: bak_Cyrl
    data_files:
      - split: test
        path: data/bak_Cyrl/newstest2019-ref.bak.txt
  - config_name: bel_Cyrl
    data_files:
      - split: test
        path: data/bel_Cyrl/newstest2019-ref.bel.txt
  - config_name: bem_Latn
    data_files:
      - split: test
        path: data/bem_Latn/newstest2019-ref.bem.txt
  - config_name: ben_Beng
    data_files:
      - split: test
        path: data/ben_Beng/newstest2019-ref.ben.txt
  - config_name: bod_Tibt
    data_files:
      - split: test
        path: data/bod_Tibt/newstest2019-ref.bod.txt
  - config_name: bos_Latn
    data_files:
      - split: test
        path: data/bos_Latn/newstest2019-ref.bos.txt
  - config_name: bul_Cyrl
    data_files:
      - split: test
        path: data/bul_Cyrl/newstest2019-ref.bul.txt
  - config_name: cat_Latn
    data_files:
      - split: test
        path: data/cat_Latn/newstest2019-ref.cat.txt
  - config_name: ces_Latn
    data_files:
      - split: test
        path: data/ces_Latn/newstest2019-ref.ces.txt
  - config_name: ckb_Arab
    data_files:
      - split: test
        path: data/ckb_Arab/newstest2019-ref.ckb.txt
  - config_name: cym_Latn
    data_files:
      - split: test
        path: data/cym_Latn/newstest2019-ref.cym.txt
  - config_name: dan_Latn
    data_files:
      - split: test
        path: data/dan_Latn/newstest2019-ref.dan.txt
  - config_name: deu_Latn
    data_files:
      - split: test
        path: data/deu_Latn/newstest2019-ref.deu.txt
  - config_name: div_Thaa
    data_files:
      - split: test
        path: data/div_Thaa/newstest2019-ref.div.txt
  - config_name: dzo_Tibt
    data_files:
      - split: test
        path: data/dzo_Tibt/newstest2019-ref.dzo.txt
  - config_name: ell_Grek
    data_files:
      - split: test
        path: data/ell_Grek/newstest2019-ref.ell.txt
  - config_name: eng-GB_Latn
    data_files:
      - split: test
        path: data/eng-GB_Latn/newstest2019-ref.eng-GB.txt
  - config_name: eng-IN_Latn
    data_files:
      - split: test
        path: data/eng-IN_Latn/newstest2019-ref.eng-IN.txt
  - config_name: eng-US_Latn
    data_files:
      - split: test
        path: data/eng-US_Latn/newstest2019-ref.eng-US.txt
  - config_name: eng_Latn
    data_files:
      - split: test
        path: data/eng_Latn/newstest2019-ref.eng.txt
  - config_name: est_Latn
    data_files:
      - split: test
        path: data/est_Latn/newstest2019-ref.est.txt
  - config_name: eus_Latn
    data_files:
      - split: test
        path: data/eus_Latn/newstest2019-ref.eus.txt
  - config_name: ewe_Latn
    data_files:
      - split: test
        path: data/ewe_Latn/newstest2019-ref.ewe.txt
  - config_name: fao_Latn
    data_files:
      - split: test
        path: data/fao_Latn/newstest2019-ref.fao.txt
  - config_name: fas_Arab
    data_files:
      - split: test
        path: data/fas_Arab/newstest2019-ref.fas.txt
  - config_name: fij_Latn
    data_files:
      - split: test
        path: data/fij_Latn/newstest2019-ref.fij.txt
  - config_name: fil_Latn
    data_files:
      - split: test
        path: data/fil_Latn/newstest2019-ref.fil.txt
  - config_name: fin_Latn
    data_files:
      - split: test
        path: data/fin_Latn/newstest2019-ref.fin.txt
  - config_name: fra-CA_Latn
    data_files:
      - split: test
        path: data/fra-CA_Latn/newstest2019-ref.fra-CA.txt
  - config_name: fra_Latn
    data_files:
      - split: test
        path: data/fra_Latn/newstest2019-ref.fra.txt
  - config_name: fuc_Latn
    data_files:
      - split: test
        path: data/fuc_Latn/newstest2019-ref.fuc.txt
  - config_name: gle_Latn
    data_files:
      - split: test
        path: data/gle_Latn/newstest2019-ref.gle.txt
  - config_name: glg_Latn
    data_files:
      - split: test
        path: data/glg_Latn/newstest2019-ref.glg.txt
  - config_name: guj_Gujr
    data_files:
      - split: test
        path: data/guj_Gujr/newstest2019-ref.guj.txt
  - config_name: hau_Latn
    data_files:
      - split: test
        path: data/hau_Latn/newstest2019-ref.hau.txt
  - config_name: heb_Hebr
    data_files:
      - split: test
        path: data/heb_Hebr/newstest2019-ref.heb.txt
  - config_name: hin_Deva
    data_files:
      - split: test
        path: data/hin_Deva/newstest2019-ref.hin.txt
  - config_name: hmn_Latn
    data_files:
      - split: test
        path: data/hmn_Latn/newstest2019-ref.hmn.txt
  - config_name: hrv_Latn
    data_files:
      - split: test
        path: data/hrv_Latn/newstest2019-ref.hrv.txt
  - config_name: hun_Latn
    data_files:
      - split: test
        path: data/hun_Latn/newstest2019-ref.hun.txt
  - config_name: hye_Armn
    data_files:
      - split: test
        path: data/hye_Armn/newstest2019-ref.hye.txt
  - config_name: ibo_Latn
    data_files:
      - split: test
        path: data/ibo_Latn/newstest2019-ref.ibo.txt
  - config_name: ind_Latn
    data_files:
      - split: test
        path: data/ind_Latn/newstest2019-ref.ind.txt
  - config_name: isl_Latn
    data_files:
      - split: test
        path: data/isl_Latn/newstest2019-ref.isl.txt
  - config_name: ita_Latn
    data_files:
      - split: test
        path: data/ita_Latn/newstest2019-ref.ita.txt
  - config_name: jpn_Jpan
    data_files:
      - split: test
        path: data/jpn_Jpan/newstest2019-ref.jpn.txt
  - config_name: kan_Knda
    data_files:
      - split: test
        path: data/kan_Knda/newstest2019-ref.kan.txt
  - config_name: kat_Geor
    data_files:
      - split: test
        path: data/kat_Geor/newstest2019-ref.kat.txt
  - config_name: kaz_Cyrl
    data_files:
      - split: test
        path: data/kaz_Cyrl/newstest2019-ref.kaz.txt
  - config_name: khm_Khmr
    data_files:
      - split: test
        path: data/khm_Khmr/newstest2019-ref.khm.txt
  - config_name: kin_Latn
    data_files:
      - split: test
        path: data/kin_Latn/newstest2019-ref.kin.txt
  - config_name: kir_Cyrl
    data_files:
      - split: test
        path: data/kir_Cyrl/newstest2019-ref.kir.txt
  - config_name: kmr_Latn
    data_files:
      - split: test
        path: data/kmr_Latn/newstest2019-ref.kmr.txt
  - config_name: kor_Hang
    data_files:
      - split: test
        path: data/kor_Hang/newstest2019-ref.kor.txt
  - config_name: lao_Laoo
    data_files:
      - split: test
        path: data/lao_Laoo/newstest2019-ref.lao.txt
  - config_name: lav_Latn
    data_files:
      - split: test
        path: data/lav_Latn/newstest2019-ref.lav.txt
  - config_name: lit_Latn
    data_files:
      - split: test
        path: data/lit_Latn/newstest2019-ref.lit.txt
  - config_name: ltz_Latn
    data_files:
      - split: test
        path: data/ltz_Latn/newstest2019-ref.ltz.txt
  - config_name: mal_Mlym
    data_files:
      - split: test
        path: data/mal_Mlym/newstest2019-ref.mal.txt
  - config_name: mar_Deva
    data_files:
      - split: test
        path: data/mar_Deva/newstest2019-ref.mar.txt
  - config_name: mey_Arab
    data_files:
      - split: test
        path: data/mey_Arab/newstest2019-ref.mey.txt
  - config_name: mkd_Cyrl
    data_files:
      - split: test
        path: data/mkd_Cyrl/newstest2019-ref.mkd.txt
  - config_name: mlg_Latn
    data_files:
      - split: test
        path: data/mlg_Latn/newstest2019-ref.mlg.txt
  - config_name: mlt_Latn
    data_files:
      - split: test
        path: data/mlt_Latn/newstest2019-ref.mlt.txt
  - config_name: mon_Mong
    data_files:
      - split: test
        path: data/mon_Mong/newstest2019-ref.mon.txt
  - config_name: mri_Latn
    data_files:
      - split: test
        path: data/mri_Latn/newstest2019-ref.mri.txt
  - config_name: msa_Latn
    data_files:
      - split: test
        path: data/msa_Latn/newstest2019-ref.msa.txt
  - config_name: mya_Mymr
    data_files:
      - split: test
        path: data/mya_Mymr/newstest2019-ref.mya.txt
  - config_name: nde_Latn
    data_files:
      - split: test
        path: data/nde_Latn/newstest2019-ref.nde.txt
  - config_name: nep_Deva
    data_files:
      - split: test
        path: data/nep_Deva/newstest2019-ref.nep.txt
  - config_name: nld_Latn
    data_files:
      - split: test
        path: data/nld_Latn/newstest2019-ref.nld.txt
  - config_name: nno_Latn
    data_files:
      - split: test
        path: data/nno_Latn/newstest2019-ref.nno.txt
  - config_name: nob_Latn
    data_files:
      - split: test
        path: data/nob_Latn/newstest2019-ref.nob.txt
  - config_name: nso_Latn
    data_files:
      - split: test
        path: data/nso_Latn/newstest2019-ref.nso.txt
  - config_name: nya_Latn
    data_files:
      - split: test
        path: data/nya_Latn/newstest2019-ref.nya.txt
  - config_name: orm_Ethi
    data_files:
      - split: test
        path: data/orm_Ethi/newstest2019-ref.orm.txt
  - config_name: pan_Guru
    data_files:
      - split: test
        path: data/pan_Guru/newstest2019-ref.pan.txt
  - config_name: pol_Latn
    data_files:
      - split: test
        path: data/pol_Latn/newstest2019-ref.pol.txt
  - config_name: por-BR_Latn
    data_files:
      - split: test
        path: data/por-BR_Latn/newstest2019-ref.por-BR.txt
  - config_name: por_Latn
    data_files:
      - split: test
        path: data/por_Latn/newstest2019-ref.por.txt
  - config_name: prs_Arab
    data_files:
      - split: test
        path: data/prs_Arab/newstest2019-ref.prs.txt
  - config_name: pus_Arab
    data_files:
      - split: test
        path: data/pus_Arab/newstest2019-ref.pus.txt
  - config_name: ron_Latn
    data_files:
      - split: test
        path: data/ron_Latn/newstest2019-ref.ron.txt
  - config_name: rus_Cyrl
    data_files:
      - split: test
        path: data/rus_Cyrl/newstest2019-ref.rus.txt
  - config_name: shi_Arab
    data_files:
      - split: test
        path: data/shi_Arab/newstest2019-ref.shi.txt
  - config_name: sin_Sinh
    data_files:
      - split: test
        path: data/sin_Sinh/newstest2019-ref.sin.txt
  - config_name: slk_Latn
    data_files:
      - split: test
        path: data/slk_Latn/newstest2019-ref.slk.txt
  - config_name: slv_Latn
    data_files:
      - split: test
        path: data/slv_Latn/newstest2019-ref.slv.txt
  - config_name: smo_Latn
    data_files:
      - split: test
        path: data/smo_Latn/newstest2019-ref.smo.txt
  - config_name: sna_Latn
    data_files:
      - split: test
        path: data/sna_Latn/newstest2019-ref.sna.txt
  - config_name: snd_Arab
    data_files:
      - split: test
        path: data/snd_Arab/newstest2019-ref.snd.txt
  - config_name: som_Latn
    data_files:
      - split: test
        path: data/som_Latn/newstest2019-ref.som.txt
  - config_name: spa-MX_Latn
    data_files:
      - split: test
        path: data/spa-MX_Latn/newstest2019-ref.spa-MX.txt
  - config_name: spa_Latn
    data_files:
      - split: test
        path: data/spa_Latn/newstest2019-ref.spa.txt
  - config_name: sqi_Latn
    data_files:
      - split: test
        path: data/sqi_Latn/newstest2019-ref.sqi.txt
  - config_name: srp_Cyrl
    data_files:
      - split: test
        path: data/srp_Cyrl/newstest2019-ref.srp.txt
  - config_name: srp_Latn
    data_files:
      - split: test
        path: data/srp_Latn/newstest2019-ref.srp.txt
  - config_name: ssw_Latn
    data_files:
      - split: test
        path: data/ssw_Latn/newstest2019-ref.ssw.txt
  - config_name: swa_Latn
    data_files:
      - split: test
        path: data/swa_Latn/newstest2019-ref.swa.txt
  - config_name: swe_Latn
    data_files:
      - split: test
        path: data/swe_Latn/newstest2019-ref.swe.txt
  - config_name: tah_Latn
    data_files:
      - split: test
        path: data/tah_Latn/newstest2019-ref.tah.txt
  - config_name: tam_Taml
    data_files:
      - split: test
        path: data/tam_Taml/newstest2019-ref.tam.txt
  - config_name: tat_Cyrl
    data_files:
      - split: test
        path: data/tat_Cyrl/newstest2019-ref.tat.txt
  - config_name: tel_Telu
    data_files:
      - split: test
        path: data/tel_Telu/newstest2019-ref.tel.txt
  - config_name: tgk_Cyrl
    data_files:
      - split: test
        path: data/tgk_Cyrl/newstest2019-ref.tgk.txt
  - config_name: tha_Thai
    data_files:
      - split: test
        path: data/tha_Thai/newstest2019-ref.tha.txt
  - config_name: tir_Ethi
    data_files:
      - split: test
        path: data/tir_Ethi/newstest2019-ref.tir.txt
  - config_name: ton_Latn
    data_files:
      - split: test
        path: data/ton_Latn/newstest2019-ref.ton.txt
  - config_name: tsn_Latn
    data_files:
      - split: test
        path: data/tsn_Latn/newstest2019-ref.tsn.txt
  - config_name: tuk_Latn
    data_files:
      - split: test
        path: data/tuk_Latn/newstest2019-ref.tuk.txt
  - config_name: tur_Latn
    data_files:
      - split: test
        path: data/tur_Latn/newstest2019-ref.tur.txt
  - config_name: uig_Arab
    data_files:
      - split: test
        path: data/uig_Arab/newstest2019-ref.uig.txt
  - config_name: ukr_Cyrl
    data_files:
      - split: test
        path: data/ukr_Cyrl/newstest2019-ref.ukr.txt
  - config_name: urd_Arab
    data_files:
      - split: test
        path: data/urd_Arab/newstest2019-ref.urd.txt
  - config_name: uzb_Latn
    data_files:
      - split: test
        path: data/uzb_Latn/newstest2019-ref.uzb.txt
  - config_name: ven_Latn
    data_files:
      - split: test
        path: data/ven_Latn/newstest2019-ref.ven.txt
  - config_name: vie_Latn
    data_files:
      - split: test
        path: data/vie_Latn/newstest2019-ref.vie.txt
  - config_name: wol_Latn
    data_files:
      - split: test
        path: data/wol_Latn/newstest2019-ref.wol.txt
  - config_name: xho_Latn
    data_files:
      - split: test
        path: data/xho_Latn/newstest2019-ref.xho.txt
  - config_name: yor_Latn
    data_files:
      - split: test
        path: data/yor_Latn/newstest2019-ref.yor.txt
  - config_name: yue_Hant
    data_files:
      - split: test
        path: data/yue_Hant/newstest2019-ref.yue.txt
  - config_name: zho_Hans
    data_files:
      - split: test
        path: data/zho_Hans/newstest2019-ref.zho.txt
  - config_name: zho_Hant
    data_files:
      - split: test
        path: data/zho_Hant/newstest2019-ref.zho.txt
  - config_name: zul_Latn
    data_files:
      - split: test
        path: data/zul_Latn/newstest2019-ref.zul.txt
---
## Dataset Description

NTREX -- News Test References for MT Evaluation from English into a total of 128 target languages. See [original GitHub repo](https://github.com/MicrosoftTranslator/NTREX/tree/main) for full details.

Example of loading:
```python
dataset = load_dataset(""davidstap/NTREX"", ""rus_Cyrl"", trust_remote_code=True)
```

## Languages

The following languages are available:

| Language Code   | Language Name               |
|-----------------|-----------------------------|
| `afr_Latn`      | Afrikaans                   |
| `amh_Ethi`      | Amharic                     |
| `arb_Arab`      | Arabic                      |
| `aze_Latn`      | Azerbaijani                 |
| `bak_Cyrl`      | Bashkir                     |
| `bel_Cyrl`      | Belarusian                  |
| `bem_Latn`      | Bemba                       |
| `ben_Beng`      | Bengali                     |
| `bod_Tibt`      | Tibetan                     |
| `bos_Latn`      | Bosnian                     |
| `bul_Cyrl`      | Bulgarian                   |
| `cat_Latn`      | Catalan                     |
| `ces_Latn`      | Czech                       |
| `ckb_Arab`      | Sorani Kurdish              |
| `cym_Latn`      | Welsh                       |
| `dan_Latn`      | Danish                      |
| `deu_Latn`      | German                      |
| `div_Thaa`      | Dhivehi                     |
| `dzo_Tibt`      | Dzongkha                    |
| `ell_Grek`      | Greek                       |
| `eng-GB_Latn`   | English (Great Britain)     |
| `eng-IN_Latn`   | English (India)             |
| `eng-US_Latn`   | English (United States)     |
| `eng_Latn`      | English                     |
| `est_Latn`      | Estonian                    |
| `eus_Latn`      | Basque                      |
| `ewe_Latn`      | Ewe                         |
| `fao_Latn`      | Faroese                     |
| `fas_Arab`      | Persian                     |
| `fij_Latn`      | Fijian                      |
| `fil_Latn`      | Filipino                    |
| `fin_Latn`      | Finnish                     |
| `fra-CA_Latn`   | French (Canada)             |
| `fra_Latn`      | French                      |
| `fuc_Latn`      | Pulaar                      |
| `gle_Latn`      | Irish                       |
| `glg_Latn`      | Galician                    |
| `guj_Gujr`      | Gujarati                    |
| `hau_Latn`      | Hausa                       |
| `heb_Hebr`      | Hebrew                      |
| `hin_Deva`      | Hindi                       |
| `hmn_Latn`      | Hmong                       |
| `hrv_Latn`      | Croatian                    |
| `hun_Latn`      | Hungarian                   |
| `hye_Armn`      | Armenian                    |
| `ibo_Latn`      | Igbo                        |
| `ind_Latn`      | Indonesian                  |
| `isl_Latn`      | Icelandic                   |
| `ita_Latn`      | Italian                     |
| `jpn_Jpan`      | Japanese                    |
| `kan_Knda`      | Kannada                     |
| `kat_Geor`      | Georgian                    |
| `kaz_Cyrl`      | Kazakh                      |
| `khm_Khmr`      | Khmer                       |
| `kin_Latn`      | Kinyarwanda                 |
| `kir_Cyrl`      | Kyrgyz                      |
| `kmr_Latn`      | Northern Kurdish            |
| `kor_Hang`      | Korean                      |
| `lao_Laoo`      | Lao                         |
| `lav_Latn`      | Latvian                     |
| `lit_Latn`      | Lithuanian                  |
| `ltz_Latn`      | Luxembourgish               |
| `mal_Mlym`      | Malayalam                   |
| `mar_Deva`      | Marathi                     |
| `mey_Arab`      | Hassaniya Arabic            |
| `mkd_Cyrl`      | Macedonian                  |
| `mlg_Latn`      | Malagasy                    |
| `mlt_Latn`      | Maltese                     |
| `mon_Mong`      | Mongolian                   |
| `mri_Latn`      | Maori                       |
| `msa_Latn`      | Malay                       |
| `mya_Mymr`      | Burmese                     |
| `nde_Latn`      | Ndebele                     |
| `nep_Deva`      | Nepali                      |
| `nld_Latn`      | Dutch                       |
| `nno_Latn`      | Norwegian Nynorsk           |
| `nob_Latn`      | Norwegian Bokmål            |
| `nso_Latn`      | Northern Sotho              |
| `nya_Latn`      | Chichewa                    |
| `orm_Ethi`      | Oromo                       |
| `pan_Guru`      | Punjabi (Gurmukhi)          |
| `pol_Latn`      | Polish                      |
| `por-BR_Latn`   | Portuguese (Brazil)         |
| `por_Latn`      | Portuguese                  |
| `prs_Arab`      | Dari                        |
| `pus_Arab`      | Pashto                      |
| `ron_Latn`      | Romanian                    |
| `rus_Cyrl`      | Russian                     |
| `shi_Arab`      | Tachelhit                   |
| `sin_Sinh`      | Sinhala                     |
| `slk_Latn`      | Slovak                      |
| `slv_Latn`      | Slovenian                   |
| `smo_Latn`      | Samoan                      |
| `sna_Latn`      | Shona                       |
| `snd_Arab`      | Sindhi                      |
| `som_Latn`      | Somali                      |
| `spa-MX_Latn`   | Spanish (Mexico)            |
| `spa_Latn`      | Spanish                     |
| `sqi_Latn`      | Albanian                    |
| `srp_Cyrl`      | Serbian (Cyrillic)          |
| `srp_Latn`      | Serbian (Latin)             |
| `ssw_Latn`      | Swati                       |
| `swa_Latn`      | Swahili                     |
| `swe_Latn`      | Swedish                     |
| `tah_Latn`      | Tahitian                    |
| `tam_Taml`      | Tamil                       |
| `tat_Cyrl`      | Tatar                       |
| `tel_Telu`      | Telugu                      |
| `tgk_Cyrl`      | Tajik                       |
| `tha_Thai`      | Thai                        |
| `tir_Ethi`      | Tigrinya                    |
| `ton_Latn`      | Tongan                      |
| `tsn_Latn`      | Tswana                      |
| `tuk_Latn`      | Turkmen                     |
| `tur_Latn`      | Turkish                     |
| `uig_Arab`      | Uighur                      |
| `ukr_Cyrl`      | Ukrainian                   |
| `urd_Arab`      | Urdu                        |
| `uzb_Latn`      | Uzbek                       |
| `ven_Latn`      | Venda                       |
| `vie_Latn`      | Vietnamese                  |
| `wol_Latn`      | Wolof                       |
| `xho_Latn`      | Xhosa                       |
| `yor_Latn`      | Yoruba                      |
| `yue_Hant`      | Cantonese                   |
| `zho_Hans`      | Chinese (Simplified)        |
| `zho_Hant`      | Chinese (Traditional)       |
| `zul_Latn`      | Zulu                        |


### Citation Information
For the original NTREX-128 dataset, please cite:

```
@inproceedings{federmann-etal-2022-ntrex,
    title = ""{NTREX}-128 {--} News Test References for {MT} Evaluation of 128 Languages"",
    author = ""Federmann, Christian and Kocmi, Tom and Xin, Ying"",
    booktitle = ""Proceedings of the First Workshop on Scaling Up Multilingual Evaluation"",
    month = ""nov"",
    year = ""2022"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2022.sumeval-1.4"",
    pages = ""21--24"",
}
```

as well as the WMT 2019 paper that provided the English source data NTREX-128 is based on:

```
@inproceedings{barrault-etal-2019-findings,
    title = ""Findings of the 2019 Conference on Machine Translation ({WMT}19)"",
    author = {Barrault, Lo{\""\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Koehn, Philipp  and
      Malmasi, Shervin  and
      Monz, Christof  and
      M{\""u}ller, Mathias  and
      Pal, Santanu  and
      Post, Matt  and
      Zampieri, Marcos},
    editor = ""Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin"",
    booktitle = ""Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)"",
    month = aug,
    year = ""2019"",
    address = ""Florence, Italy"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/W19-5301"",
    doi = ""10.18653/v1/W19-5301"",
    pages = ""1--61"",
}
```",High,5.0
Translation,refine-ai/subscene,0.0,8984.0,2025-03-08 05:26:11+00:00,none,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
language:
- ar
- hy
- az
- eu
- be
- bn
- bs
- pt
- bg
- my
- km
- ca
- zh
- hr
- cs
- da
- nl
- en
- eo
- et
- fa
- fi
- fr
- ka
- de
- el
- he
- hi
- hu
- is
- it
- ja
- kn
- ko
- ku
- lv
- lt
- mk
- ms
- ml
- mn
- ne
- 'no'
- ps
- pl
- pa
- ro
- ru
- sr
- si
- sk
- sl
- so
- es
- su
- sw
- sv
- tl
- ta
- te
- th
- tr
- uk
- ur
- ve
- yo
- id
task_categories:
- text-generation
- translation
- text-classification
size_categories:
- 1B<n<10B
---


# Subscene
<center>
    <img src=""https://huggingface.co/datasets/khalidalt/subscene/resolve/main/subscene_logo.png"" alt=""Subscene: The largest collection of subtitles"">
</center>



## Dataset Summary

Subscene is a vast collection of multilingual subtitles, encompassing 65 different languages and consisting of more than 30 billion tokens with a total size of 410.70 GB. This dataset includes subtitles for movies, series, and animations gathered from the Subscene dump. It provides a rich resource for studying language variations and building multilingual NLP models. We have carefully applied a fastText classifier to remove any non-language content from incorrect subsets. Additionally, we performed basic cleaning and filtration. However, there is still room for further cleaning and refinement.

## Dataset Structure

### Data Instances

```
{""subtitle_name"": ""choir-girl"",
""file_name"": ""Choir.Girl.2019.HDRip.XviD.AC3-EVO.json"",
""transcript"": [{""id"": 1, ""start_time"": ""00:00:09,741"", ""end_time"": ""00:00:13,702"", ""text"": ""Subtitles by explosiveskull""}, {""id"": 2, ""start_time"": ""00:01:58,569"", ""end_time"": ""00:02:00,536"", ""text"": ""Get Julius on the phone.""}, {""id"": 3, ""start_time"": ""00:02:00,538"", ""end_time"": ""00:02:03,338"", ""text"": ""The sponsor's got an issue with sharing the page.""}, {""id"": 4, ""start_time"": ""00:02:08,412"", ""end_time"": ""00:02:09,411"", ""text"": ""Eugene.""}, {""id"": 5, ""start_time"": ""00:02:10,748"", ""end_time"": ""00:02:14,950"", ""text"": ""Yeah, uh, I'm supposed to meet Julius.""}, {""id"": 6, ""start_time"": ""00:02:14,952"", ""end_time"": ""00:02:19,888"", ""text"": ""I'm Anne Kellan, the editor. You won't be meeting him.""}, {""id"": 7, ""start_time"": ""00:02:19,890"", ""end_time"": ""00:02:22,391"", ""text"": ""He said on the phone that, um...""}, {""id"": 8, ""start_time"": ""00:02:22,393"", ""end_time"": ""00:02:24,293"", ""text"": ""We won't be publishing these.""}, {""id"": 9, ""start_time"": ""00:02:24,295"", ""end_time"": ""00:02:31,266"", ""text"": ""You have a child overdosing, an elderly woman being bashed""}, {""id"": 10, ""start_time"": ""00:02:31,268"", ""end_time"": ""00:02:33,302"", ""text"": ""and this,""}, {""id"": 11, ""start_time"": ""00:02:33,304"", ""end_time"": ""00:02:36,205"", ""text"": ""a girl either being sexually assaulted or prostituting herself.""}, {""id"": 12, ""start_time"": ""00:02:36,907"", ""end_time"": ""00:02:39,374"", ""text"": ""I spoke to Julius and...""}, ...]
}
```

```
{""subtitle_name"": ""the-fabulous-first-season"",
 ""file_name"": ""The.Fabulous.S01E06.DUBBED.WEBRip.x264-ION10_Arabic.json"",
 ""transcript"": [{""id"": ""1"", ""start_time"": ""00:00:06,006"", ""end_time"": ""00:00:09,926"", ""text"": ""‫\""مسلسلات NETFLIX\""‬""}, {""id"": ""2"", ""start_time"": ""00:00:18,309"", ""end_time"": ""00:00:19,849"", ""text"": ""‫- المجال مفتوح أمامك.‬ ‫- اركض!‬""}, {""id"": ""3"", ""start_time"": ""00:00:19,936"", ""end_time"": ""00:00:22,186"", ""text"": ""‫تلك فرصتك! هيا يا \""وو مين\""!‬""}, {""id"": ""4"", ""start_time"": ""00:00:23,189"", ""end_time"": ""00:00:24,229"", ""text"": ""‫- ويلاه. ‬ ‫- لا بأس.‬""}, {""id"": ""5"", ""start_time"": ""00:00:24,315"", ""end_time"": ""00:00:26,025"", ""text"": ""‫رائع، أحسنت صنعًا!‬""}, {""id"": ""6"", ""start_time"": ""00:00:26,109"", ""end_time"": ""00:00:27,189"", ""text"": ""‫أحسنت!‬""}, {""id"": ""7"", ""start_time"": ""00:00:27,277"", ""end_time"": ""00:00:28,397"", ""text"": ""‫ألن تخبرني؟‬""}, {""id"": ""8"", ""start_time"": ""00:00:29,320"", ""end_time"": ""00:00:30,490"", ""text"": ""‫الفتاة التي في الصورة.‬""}, {""id"": ""9"", ""start_time"": ""00:00:31,072"", ""end_time"": ""00:00:32,622"", ""text"": ""‫كم هو مثابر. العب الكرة فحسب.‬""}, ....]
}

```

### Data Fields

- `subtitle_name` (`str`): The name of the folder containing the subtitle file. This likely corresponds to the movie, series, or anime title.
- `file_name` (`str`): The specific filename of the subtitle file itself.
- `transcript` (`dict`): This dictionary holds the complete transcript information for the subtitle file. It contains nested structures to represent individual subtitle segments.
- `id` (`int`): (Within `transcript` dictionary) A unique identifier for a specific subtitle segment within the file.
- `start_time` (`str`): (Within `transcript` dictionary) The starting time of the subtitle segment, likely formatted according to a specific timecode standard.
- `end_time` (`str`): (Within `transcript` dictionary) The ending time of the subtitle segment, following the same timecode format as `start_time`.
- `text` (`str`): (Within `transcript` dictionary) The actual text content displayed for that specific subtitle segment.


## Using The HuggingFace datasets

```python
from datasets import load_dataset

# Load the Arabic subset only
ds = load_dataset(""refine-ai/Subscene"", ""arabic"", trust_remote_code=True)

```


## Dataset Statstics


| Language            | Raw Size   | Num Char   | Num Byte   | Num Words   | Num Token   |
|:------------------- |:-----------|:-----------|:-----------|:------------|:------------|
| Arabic              | 59.7 GB    | 12B        | 24B        | 3B          | 5B          |
| Armenian            | 933.7 KB   | 175K       | 372K       | 36K         | 177K        |
| Azerbaijani         | 5.7 MB     | 1M         | 2M         | 225K        | 701K        |
| Basque              | 36.4 MB    | 9M         | 10M        | 1M          | 3M          |
| Belarusian          | 3.2 MB     | 598K       | 1M         | 119K        | 433K        |
| Bengali             | 2.6 GB     | 429M       | 1B         | 88M         | 152M        |
| traditional_chinese | 1.1 GB     | 202M       | 323M       | 38M         | 110M        |
| Bosnian             | 51.0 MB    | 12M        | 15M        | 3M          | 6M          |
| Brazilian-Portuguese| 9.1 GB     | 2B         | 3B         | 463M        | 727M        |
| Bulgarian           | 324.7 MB   | 60M        | 126M       | 13M         | 38M         |
| Bulgarian-English   | 4.2 MB     | 851K       | 1M         | 179K        | 515K        |
| Burmese             | 724 MB     | 127M       | 378M       | 11M         | 237M        |
| Cambodian-Khmer     | 165.5 MB   | 28M        | 80M        | 2M          | 32M         |
| Catalan             | 70.8 MB    | 17M        | 20M        | 4M          | 6M          |
| simplified_chinese  | 3.9 GB     | 197M       | 505M       | 29M         | 129M        |
| Croatian            | 1.1 GB     | 260M       | 315M       | 54M         | 129M        |
| Czech               | 2.4 GB     | 519M       | 671M       | 108M        | 288M        |
| Danish              | 12.1 GB    | 3B         | 4B         | 716M        | 1B          |
| Dutch               | 5.5 GB     | 1B         | 2B         | 319M        | 610M        |
| Dutch-English       | 28.3 MB    | 7M         | 9M         | 2M          | 3M          |
| English             | 127.9 GB   | 30B        | 36B        | 7B          | 10B         |
| English-German      | 160.5 MB   | 39M        | 47M        | 8M          | 14M         |
| Esperanto           | 4.6 MB     | 1M         | 1M         | 224K        | 431K        |
| Estonian            | 345.9 MB   | 83M        | 98M        | 15M         | 39M         |
| Farsi_persian       | 40.3 GB    | 7B         | 15B        | 2B          | 5B          |
| Finnish             | 4.3 GB     | 1B         | 1B         | 179M        | 522M        |
| French              | 18 GB      | 5B         | 6B         | 1B          | 1B          |
| Georgian            | 2.3 MB     | 432K       | 1M         | 79K         | 378K        |
| German              | 3 GB       | 788M       | 931M       | 157M        | 307M        |
| Greek               | 4.5 GB     | 919M       | 2B         | 181M        | 737M        |
| Greenlandic         | 10.2 MB    | 3M         | 3M         | 298K        | 1M          |
| Hebrew              | 4.9 GB     | 839M       | 2B         | 194M        | 811M        |
| Hindi               | 575 MB     | 92M        | 285M       | 24M         | 36M         |
| Hungarian           | 1.2 GB     | 287M       | 353M       | 52M         | 141M        |
| Hungarian_English   | 600kb      | 126K       | 152K       | 25K         | 53K         |
| Icelandic           | 476.1 MB   | 110M       | 144M       | 23M         | 58M         |
| Indonesian          | 9.7 GB     | 10B        | 12B        | 2B          | 3B          |
| Italian             | 7 GB       | 2B         | 3B         | 501M        | 950M        |
| Japanese            | 2.0 GB     | 198M       | 596M       | 29M         | 190M        |
| Kannada             | 21.8 MB    | 4M         | 11M        | 564K        | 1M          |
| Korean              | 5.5 GB     | 553M       | 2B         | 188M        | 821M        |
| Kurdish             | 72.4 MB    | 14M        | 29M        | 3M          | 11M         |
| Latvian             | 157.2 MB   | 35M        | 43M        | 6M          | 18M         |
| Lithuanian          | 207.3 MB   | 47M        | 57M        | 8M          | 24M         |
| Macedonian          | 97 MB      | 20M        | 41M        | 4M          | 12M         |
| Malay               | 4.9 GB     | 1B         | 1B         | 221M        | 371M        |
| Malayalam           | 446.9 MB   | 86M        | 246M       | 11M         | 27M         |
| Manipuri            | 1000kb     | 93K        | 280K       | 14K         | 60K         |
| Mongolian           | 9.9 MB     | 2M         | 4M         | 390K        | 2M          |
| Nepali              | 37.8 MB    | 6M         | 19M        | 1M          | 2M          |
| Norwegian           | 6.8 GB     | 2B         | 2B         | 392M        | 765M        |
| Pashto              | 9.5 MB     | 2M         | 4M         | 417K        | 870K        |
| Polish              | 2 GB       | 467M       | 570M       | 85M         | 243M        |
| Portuguese          | 3.7 GB     | 899M       | 1B         | 191M        | 280M        |
| Punjabi             | 55.3 MB    | 87K        | 263K       | 22K         | 33K         |
| Romanian            | 3.5 GB     | 835M       | 1B         | 183M        | 402M        |
| Russian             | 1.9 GB     | 359M       | 740M       | 71M         | 220M        |
| Serbian             | 929.5 MB   | 225M       | 285M       | 48M         | 113M        |
| Sinhala             | 1.1 GB     | 185M       | 544M       | 37M         | 345M        |
| Slovak              | 649.2 MB   | 140M       | 179M       | 29M         | 75M         |
| Slovenian           | 654.8 MB   | 151M       | 183M       | 31M         | 74M         |
| Somali              | 1.1 MB     | 300K       | 353K       | 62K         | 134K        |
| Spanish             | 10.9 GB    | 3B         | 3B         | 548M        | 831M        |
| Sundanese           | 3.1 Mb     | 762K       | 881K       | 135K        | 302K        |
| Swahili             | 15.9 MB    | 4M         | 4M         | 661K        | 1M          |
| Swedish             | 6.9 GB     | 2B         | 2B         | 392M        | 795M        |
| Tagalog             | 188 MB     | 45M        | 52M        | 9M          | 18M         |
| Tamil               | 1 GB       | 50M        | 149M       | 7M          | 16M         |
| Telugu              | 223.5 MB   | 39M        | 115M       | 6M          | 14M         |
| Thai                | 6.9 GB     | 1B         | 3B         | 77M         | 1B          |
| Turkish             | 6.7 GB     | 2B         | 2B         | 268M        | 789M        |
| Ukrainian           | 891.1 MB   | 168M       | 346M       | 34M         | 116M        |
| Urdu                | 5 GB       | 53M        | 116M       | 15M         | 21M         |
| Vietnamese          | 16 GB      | 3B         | 5B         | 952M        | 1B          |
| Yoruba              | 820.6 KB   | 154K       | 230K       | 46K         | 80K         |


## Citation Information
If you use this corpus, please cite the paper:

```
@misc{refine-ai,
author = {Refine AI},
title = {Subscene: A Large-Scale Multilingual Subtitle Dataset},
year = {2024},
}
```

##  Acknowledgements

We would like to extend our thanks to everyone who has worked on and contributed to the Subscene website. A special thanks goes to eionlol for uploading the entire Subscene dump, organized by language. You can access the archive through the following link: https://archive.org/details/subscene-final-dump",Medium,2.0
Translation,berwart/SE-Chatting.en,2.0,135.0,2024-10-03 21:10:29+00:00,mit,0.0,1.01 GB,1084479242.24,129 MB,135266304.0,10000140,none,none,"---
license: mit
task_categories:
- question-answering
- translation
language:
- en
- fr
- ja
- zh
- ru
- ar
tags:
- chemistry
- biology
- finance
- legal
- music
- art
- code
- climate
- medical
pretty_name: SE.02
size_categories:
- 10M<n<100M
---

# SE.02 
***Dataset***

Hello, welcome to the official main dataset of SE.02 that's always getting updated, make sure to like to help us a lot.
this dataset contains pretty much everything from math to isk what to put but pretty much anything you think an ai can say.
anyways this is our biggest dataset yet, and out first one that I don't even know how I managed to make this.

you can use it to train your own ai if you want.
",Low,1.0
Translation,Mouwiya/SANAD,1.0,12.0,2024-05-24 08:53:56+00:00,none,0.0,86.4 MB,90596966.4,86.4 MB,90596966.4,45500,none,none,"---
dataset_info:
  features:
  - name: text
    dtype: string
  - name: label
    dtype: string
  splits:
  - name: train
    num_bytes: 181482477
    num_examples: 45500
  download_size: 86389093
  dataset_size: 181482477
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- text-classification
- translation
- summarization
language:
- ar
- en
size_categories:
- 10K<n<100K
---

# Arabic News Articles Dataset

## About Dataset
Context
SANAD Dataset is a large collection of Arabic news articles that can be used in different Arabic NLP tasks such as Text Classification and Word Embedding. The articles were collected using Python scripts written specifically for three popular news websites: AlKhaleej, AlArabiya and Akhbarona.

All datasets have seven categories [Culture, Finance, Medical, Politics, Religion, Sports and Tech], except AlArabiya which doesn’t have [Religion]. SANAD contains a total number of 190k+ articles.

## Content
a subset Khaleej (45500 articles in 7 categories) of SANAD. Labels are categorized in: Culture, Finance, Medical, Politics,Religion,Sports,Tech.

## Acknowledgements
We wouldn't be here without the help of Omar Einea, Ashraf Elnagar, Ridhwan Al-Debsi.
https://doi.org/10.1016/j.dib.2019.104076

## Contact
**Mouwiya S. A. Al-Qaisieh** 
mo3awiya@gmail.com",Low,1.0
Translation,atlasia/darija_english,10.0,85.0,2024-05-16 08:00:43+00:00,cc-by-nc-4.0,5.0,21.2 MB,22229811.2,12.2 MB,12792627.2,128289,none,none,"---
license: cc-by-nc-4.0
task_categories:
- translation
language:
- en
- ar
size_categories:
- 100K<n<1M
configs:
- config_name: web_data
  data_files: atlasia_web_data.csv
- config_name: comments
  data_files: atlasia_comments.csv
- config_name: stories
  data_files: atlasia_stories.csv
- config_name: doda
  data_files: atlasia_doda.csv
- config_name: transliteration
  data_files: atlasia_atam.csv
---

# Dataset Card for atlasia/darija-english
## Dataset Details

### Dataset Description

A compilation of Darija-English pairs curated by AtlasIA. 

- **Curated by:** AtlasIA
- **Language(s) (NLP):** Moroccan Darija, English
- **License:** CC-by-NC-4.0
- **Darija sentences sources (additionally to the web):**
  - doda: [AtlasIA platform contributions](https://huggingface.co/datasets/atlasia/darija-translation)
  - stories: [Mixed Arabic Datasets](https://huggingface.co/datasets/M-A-D/Mixed-Arabic-Datasets-Repo/viewer/Ary--Ali-C137--Darija-Stories-Dataset)
  - transliteration: [AtlasIA x DODa](https://huggingface.co/datasets/atlasia/ATAM). Can be used for transliteration task.
",Medium,2.0
Translation,aiana94/polynews-parallel,13.0,322.0,2024-06-21 08:35:51+00:00,cc-by-nc-4.0,1.0,617 MB,646971392.0,617 MB,646971392.0,5386846,https://arxiv.org/abs/2406.12634,none,"---
license: cc-by-nc-4.0
task_categories:
- translation
- text-retrieval
language:
- am
- ar
- ay
- bm
- bbj
- bn
- bg
- ca
- cs
- ku
- da
- de
- el
- en
- et
- ee
- fil
- fi
- fr
- fon
- gu
- ha
- he
- hi
- hu
- ig
- id
- it
- ja
- kk
- km
- ko
- lv
- lt
- lg
- luo
- mk
- mos
- my
- nl
- ne
- or
- pa
- pcm
- fa
- pl
- pt
- mg
- ro
- ru
- es
- sr
- sq
- sw
- sv
- tet
- tn
- tr
- tw
- ur
- wo
- yo
- zh
- zu
multilinguality:
- translation
- multilingual
pretty_name: PolyNewsParallel
size_categories:
- 1K<n<10K
source_datasets:
- mafand
- wmt-news
- globalvoices
tags:
- news
- polynews-parallel
- mafand
- globalvoices
- wmtnews
configs:
- config_name: ces_Latn-tur_Latn
  data_files:
  - split: train
    path: data/ces_Latn-tur_Latn/train.parquet.gzip
- config_name: mya_Mymr-rus_Cyrl
  data_files:
  - split: train
    path: data/mya_Mymr-rus_Cyrl/train.parquet.gzip
- config_name: plt_Latn-nld_Latn
  data_files:
  - split: train
    path: data/plt_Latn-nld_Latn/train.parquet.gzip
- config_name: hun_Latn-jpn_Jpan
  data_files:
  - split: train
    path: data/hun_Latn-jpn_Jpan/train.parquet.gzip
- config_name: bul_Cyrl-swh_Latn
  data_files:
  - split: train
    path: data/bul_Cyrl-swh_Latn/train.parquet.gzip
- config_name: amh_Ethi-deu_Latn
  data_files:
  - split: train
    path: data/amh_Ethi-deu_Latn/train.parquet.gzip
- config_name: cat_Latn-ell_Grek
  data_files:
  - split: train
    path: data/cat_Latn-ell_Grek/train.parquet.gzip
- config_name: cat_Latn-nld_Latn
  data_files:
  - split: train
    path: data/cat_Latn-nld_Latn/train.parquet.gzip
- config_name: deu_Latn-eng_Latn
  data_files:
  - split: train
    path: data/deu_Latn-eng_Latn/train.parquet.gzip
- config_name: ben_Beng-tet_Latn
  data_files:
  - split: train
    path: data/ben_Beng-tet_Latn/train.parquet.gzip
- config_name: bul_Cyrl-srp_Latn
  data_files:
  - split: train
    path: data/bul_Cyrl-srp_Latn/train.parquet.gzip
- config_name: arb_Arab-tur_Latn
  data_files:
  - split: train
    path: data/arb_Arab-tur_Latn/train.parquet.gzip
- config_name: bul_Cyrl-ita_Latn
  data_files:
  - split: train
    path: data/bul_Cyrl-ita_Latn/train.parquet.gzip
- config_name: ayr_Latn-plt_Latn
  data_files:
  - split: train
    path: data/ayr_Latn-plt_Latn/train.parquet.gzip
- config_name: hin_Deva-ita_Latn
  data_files:
  - split: train
    path: data/hin_Deva-ita_Latn/train.parquet.gzip
- config_name: cat_Latn-hun_Latn
  data_files:
  - split: train
    path: data/cat_Latn-hun_Latn/train.parquet.gzip
- config_name: cat_Latn-npi_Deva
  data_files:
  - split: train
    path: data/cat_Latn-npi_Deva/train.parquet.gzip
- config_name: ces_Latn-ind_Latn
  data_files:
  - split: train
    path: data/ces_Latn-ind_Latn/train.parquet.gzip
- config_name: ces_Latn-nld_Latn
  data_files:
  - split: train
    path: data/ces_Latn-nld_Latn/train.parquet.gzip
- config_name: arb_Arab-jpn_Jpan
  data_files:
  - split: train
    path: data/arb_Arab-jpn_Jpan/train.parquet.gzip
- config_name: eng_Latn-ibo_Latn
  data_files:
  - split: train
    path: data/eng_Latn-ibo_Latn/train.parquet.gzip
- config_name: ben_Beng-cat_Latn
  data_files:
  - split: train
    path: data/ben_Beng-cat_Latn/train.parquet.gzip
- config_name: srp_Latn-tur_Latn
  data_files:
  - split: train
    path: data/srp_Latn-tur_Latn/train.parquet.gzip
- config_name: ben_Beng-swh_Latn
  data_files:
  - split: train
    path: data/ben_Beng-swh_Latn/train.parquet.gzip
- config_name: deu_Latn-ron_Latn
  data_files:
  - split: train
    path: data/deu_Latn-ron_Latn/train.parquet.gzip
- config_name: heb_Hebr-ita_Latn
  data_files:
  - split: train
    path: data/heb_Hebr-ita_Latn/train.parquet.gzip
- config_name: pes_Arab-srp_Latn
  data_files:
  - split: train
    path: data/pes_Arab-srp_Latn/train.parquet.gzip
- config_name: eng_Latn-fin_Latn
  data_files:
  - split: train
    path: data/eng_Latn-fin_Latn/train.parquet.gzip
- config_name: ben_Beng-heb_Hebr
  data_files:
  - split: train
    path: data/ben_Beng-heb_Hebr/train.parquet.gzip
- config_name: bul_Cyrl-jpn_Jpan
  data_files:
  - split: train
    path: data/bul_Cyrl-jpn_Jpan/train.parquet.gzip
- config_name: kor_Hang-zho_Hans
  data_files:
  - split: train
    path: data/kor_Hang-zho_Hans/train.parquet.gzip
- config_name: nld_Latn-zho_Hant
  data_files:
  - split: train
    path: data/nld_Latn-zho_Hant/train.parquet.gzip
- config_name: hun_Latn-ron_Latn
  data_files:
  - split: train
    path: data/hun_Latn-ron_Latn/train.parquet.gzip
- config_name: npi_Deva-pol_Latn
  data_files:
  - split: train
    path: data/npi_Deva-pol_Latn/train.parquet.gzip
- config_name: ayr_Latn-bul_Cyrl
  data_files:
  - split: train
    path: data/ayr_Latn-bul_Cyrl/train.parquet.gzip
- config_name: ita_Latn-urd_Arab
  data_files:
  - split: train
    path: data/ita_Latn-urd_Arab/train.parquet.gzip
- config_name: ayr_Latn-mkd_Cyrl
  data_files:
  - split: train
    path: data/ayr_Latn-mkd_Cyrl/train.parquet.gzip
- config_name: ces_Latn-heb_Hebr
  data_files:
  - split: train
    path: data/ces_Latn-heb_Hebr/train.parquet.gzip
- config_name: ayr_Latn-ron_Latn
  data_files:
  - split: train
    path: data/ayr_Latn-ron_Latn/train.parquet.gzip
- config_name: mya_Mymr-sqi_Latn
  data_files:
  - split: train
    path: data/mya_Mymr-sqi_Latn/train.parquet.gzip
- config_name: fil_Latn-urd_Arab
  data_files:
  - split: train
    path: data/fil_Latn-urd_Arab/train.parquet.gzip
- config_name: sqi_Latn-srp_Latn
  data_files:
  - split: train
    path: data/sqi_Latn-srp_Latn/train.parquet.gzip
- config_name: por_Latn-tur_Latn
  data_files:
  - split: train
    path: data/por_Latn-tur_Latn/train.parquet.gzip
- config_name: plt_Latn-por_Latn
  data_files:
  - split: train
    path: data/plt_Latn-por_Latn/train.parquet.gzip
- config_name: ben_Beng-tur_Latn
  data_files:
  - split: train
    path: data/ben_Beng-tur_Latn/train.parquet.gzip
- config_name: khm_Khmr-zho_Hant
  data_files:
  - split: train
    path: data/khm_Khmr-zho_Hant/train.parquet.gzip
- config_name: ory_Orya-urd_Arab
  data_files:
  - split: train
    path: data/ory_Orya-urd_Arab/train.parquet.gzip
- config_name: ben_Beng-mkd_Cyrl
  data_files:
  - split: train
    path: data/ben_Beng-mkd_Cyrl/train.parquet.gzip
- config_name: eng_Latn-lug_Latn
  data_files:
  - split: train
    path: data/eng_Latn-lug_Latn/train.parquet.gzip
- config_name: hun_Latn-swh_Latn
  data_files:
  - split: train
    path: data/hun_Latn-swh_Latn/train.parquet.gzip
- config_name: spa_Latn-ckb_Arab
  data_files:
  - split: train
    path: data/spa_Latn-ckb_Arab/train.parquet.gzip
- config_name: por_Latn-srp_Latn
  data_files:
  - split: train
    path: data/por_Latn-srp_Latn/train.parquet.gzip
- config_name: kor_Hang-nld_Latn
  data_files:
  - split: train
    path: data/kor_Hang-nld_Latn/train.parquet.gzip
- config_name: amh_Ethi-zho_Hans
  data_files:
  - split: train
    path: data/amh_Ethi-zho_Hans/train.parquet.gzip
- config_name: ron_Latn-swe_Latn
  data_files:
  - split: train
    path: data/ron_Latn-swe_Latn/train.parquet.gzip
- config_name: dan_Latn-kor_Hang
  data_files:
  - split: train
    path: data/dan_Latn-kor_Hang/train.parquet.gzip
- config_name: amh_Ethi-nld_Latn
  data_files:
  - split: train
    path: data/amh_Ethi-nld_Latn/train.parquet.gzip
- config_name: ita_Latn-rus_Cyrl
  data_files:
  - split: train
    path: data/ita_Latn-rus_Cyrl/train.parquet.gzip
- config_name: jpn_Jpan-ory_Orya
  data_files:
  - split: train
    path: data/jpn_Jpan-ory_Orya/train.parquet.gzip
- config_name: ayr_Latn-ita_Latn
  data_files:
  - split: train
    path: data/ayr_Latn-ita_Latn/train.parquet.gzip
- config_name: eng_Latn-pcm_Latn
  data_files:
  - split: train
    path: data/eng_Latn-pcm_Latn/train.parquet.gzip
- config_name: ben_Beng-khm_Khmr
  data_files:
  - split: train
    path: data/ben_Beng-khm_Khmr/train.parquet.gzip
- config_name: ita_Latn-ory_Orya
  data_files:
  - split: train
    path: data/ita_Latn-ory_Orya/train.parquet.gzip
- config_name: hin_Deva-mya_Mymr
  data_files:
  - split: train
    path: data/hin_Deva-mya_Mymr/train.parquet.gzip
- config_name: deu_Latn-khm_Khmr
  data_files:
  - split: train
    path: data/deu_Latn-khm_Khmr/train.parquet.gzip
- config_name: nld_Latn-swe_Latn
  data_files:
  - split: train
    path: data/nld_Latn-swe_Latn/train.parquet.gzip
- config_name: spa_Latn-sqi_Latn
  data_files:
  - split: train
    path: data/spa_Latn-sqi_Latn/train.parquet.gzip
- config_name: ita_Latn-swe_Latn
  data_files:
  - split: train
    path: data/ita_Latn-swe_Latn/train.parquet.gzip
- config_name: fil_Latn-zho_Hans
  data_files:
  - split: train
    path: data/fil_Latn-zho_Hans/train.parquet.gzip
- config_name: plt_Latn-yor_Latn
  data_files:
  - split: train
    path: data/plt_Latn-yor_Latn/train.parquet.gzip
- config_name: ind_Latn-pol_Latn
  data_files:
  - split: train
    path: data/ind_Latn-pol_Latn/train.parquet.gzip
- config_name: amh_Ethi-ell_Grek
  data_files:
  - split: train
    path: data/amh_Ethi-ell_Grek/train.parquet.gzip
- config_name: fil_Latn-por_Latn
  data_files:
  - split: train
    path: data/fil_Latn-por_Latn/train.parquet.gzip
- config_name: ces_Latn-zho_Hant
  data_files:
  - split: train
    path: data/ces_Latn-zho_Hant/train.parquet.gzip
- config_name: npi_Deva-swh_Latn
  data_files:
  - split: train
    path: data/npi_Deva-swh_Latn/train.parquet.gzip
- config_name: mkd_Cyrl-nld_Latn
  data_files:
  - split: train
    path: data/mkd_Cyrl-nld_Latn/train.parquet.gzip
- config_name: deu_Latn-urd_Arab
  data_files:
  - split: train
    path: data/deu_Latn-urd_Arab/train.parquet.gzip
- config_name: srp_Latn-swe_Latn
  data_files:
  - split: train
    path: data/srp_Latn-swe_Latn/train.parquet.gzip
- config_name: khm_Khmr-zho_Hans
  data_files:
  - split: train
    path: data/khm_Khmr-zho_Hans/train.parquet.gzip
- config_name: plt_Latn-swe_Latn
  data_files:
  - split: train
    path: data/plt_Latn-swe_Latn/train.parquet.gzip
- config_name: ind_Latn-mkd_Cyrl
  data_files:
  - split: train
    path: data/ind_Latn-mkd_Cyrl/train.parquet.gzip
- config_name: dan_Latn-nld_Latn
  data_files:
  - split: train
    path: data/dan_Latn-nld_Latn/train.parquet.gzip
- config_name: heb_Hebr-zho_Hant
  data_files:
  - split: train
    path: data/heb_Hebr-zho_Hant/train.parquet.gzip
- config_name: ell_Grek-tur_Latn
  data_files:
  - split: train
    path: data/ell_Grek-tur_Latn/train.parquet.gzip
- config_name: cat_Latn-jpn_Jpan
  data_files:
  - split: train
    path: data/cat_Latn-jpn_Jpan/train.parquet.gzip
- config_name: bul_Cyrl-rus_Cyrl
  data_files:
  - split: train
    path: data/bul_Cyrl-rus_Cyrl/train.parquet.gzip
- config_name: deu_Latn-plt_Latn
  data_files:
  - split: train
    path: data/deu_Latn-plt_Latn/train.parquet.gzip
- config_name: rus_Cyrl-swh_Latn
  data_files:
  - split: train
    path: data/rus_Cyrl-swh_Latn/train.parquet.gzip
- config_name: rus_Cyrl-zho_Hans
  data_files:
  - split: train
    path: data/rus_Cyrl-zho_Hans/train.parquet.gzip
- config_name: pes_Arab-rus_Cyrl
  data_files:
  - split: train
    path: data/pes_Arab-rus_Cyrl/train.parquet.gzip
- config_name: ell_Grek-ind_Latn
  data_files:
  - split: train
    path: data/ell_Grek-ind_Latn/train.parquet.gzip
- config_name: urd_Arab-zho_Hant
  data_files:
  - split: train
    path: data/urd_Arab-zho_Hant/train.parquet.gzip
- config_name: ind_Latn-ita_Latn
  data_files:
  - split: train
    path: data/ind_Latn-ita_Latn/train.parquet.gzip
- config_name: pes_Arab-urd_Arab
  data_files:
  - split: train
    path: data/pes_Arab-urd_Arab/train.parquet.gzip
- config_name: ayr_Latn-eng_Latn
  data_files:
  - split: train
    path: data/ayr_Latn-eng_Latn/train.parquet.gzip
- config_name: pol_Latn-zho_Hant
  data_files:
  - split: train
    path: data/pol_Latn-zho_Hant/train.parquet.gzip
- config_name: fra_Latn-zho_Hant
  data_files:
  - split: train
    path: data/fra_Latn-zho_Hant/train.parquet.gzip
- config_name: ces_Latn-swe_Latn
  data_files:
  - split: train
    path: data/ces_Latn-swe_Latn/train.parquet.gzip
- config_name: ind_Latn-swe_Latn
  data_files:
  - split: train
    path: data/ind_Latn-swe_Latn/train.parquet.gzip
- config_name: ayr_Latn-hun_Latn
  data_files:
  - split: train
    path: data/ayr_Latn-hun_Latn/train.parquet.gzip
- config_name: amh_Ethi-hun_Latn
  data_files:
  - split: train
    path: data/amh_Ethi-hun_Latn/train.parquet.gzip
- config_name: cat_Latn-tur_Latn
  data_files:
  - split: train
    path: data/cat_Latn-tur_Latn/train.parquet.gzip
- config_name: cat_Latn-plt_Latn
  data_files:
  - split: train
    path: data/cat_Latn-plt_Latn/train.parquet.gzip
- config_name: dan_Latn-fra_Latn
  data_files:
  - split: train
    path: data/dan_Latn-fra_Latn/train.parquet.gzip
- config_name: deu_Latn-ory_Orya
  data_files:
  - split: train
    path: data/deu_Latn-ory_Orya/train.parquet.gzip
- config_name: fra_Latn-kor_Hang
  data_files:
  - split: train
    path: data/fra_Latn-kor_Hang/train.parquet.gzip
- config_name: ben_Beng-ita_Latn
  data_files:
  - split: train
    path: data/ben_Beng-ita_Latn/train.parquet.gzip
- config_name: arb_Arab-deu_Latn
  data_files:
  - split: train
    path: data/arb_Arab-deu_Latn/train.parquet.gzip
- config_name: por_Latn-tet_Latn
  data_files:
  - split: train
    path: data/por_Latn-tet_Latn/train.parquet.gzip
- config_name: plt_Latn-mkd_Cyrl
  data_files:
  - split: train
    path: data/plt_Latn-mkd_Cyrl/train.parquet.gzip
- config_name: ita_Latn-tet_Latn
  data_files:
  - split: train
    path: data/ita_Latn-tet_Latn/train.parquet.gzip
- config_name: dan_Latn-plt_Latn
  data_files:
  - split: train
    path: data/dan_Latn-plt_Latn/train.parquet.gzip
- config_name: arb_Arab-hun_Latn
  data_files:
  - split: train
    path: data/arb_Arab-hun_Latn/train.parquet.gzip
- config_name: eng_Latn-npi_Deva
  data_files:
  - split: train
    path: data/eng_Latn-npi_Deva/train.parquet.gzip
- config_name: ita_Latn-mkd_Cyrl
  data_files:
  - split: train
    path: data/ita_Latn-mkd_Cyrl/train.parquet.gzip
- config_name: ory_Orya-rus_Cyrl
  data_files:
  - split: train
    path: data/ory_Orya-rus_Cyrl/train.parquet.gzip
- config_name: fil_Latn-ind_Latn
  data_files:
  - split: train
    path: data/fil_Latn-ind_Latn/train.parquet.gzip
- config_name: ell_Grek-hun_Latn
  data_files:
  - split: train
    path: data/ell_Grek-hun_Latn/train.parquet.gzip
- config_name: deu_Latn-ita_Latn
  data_files:
  - split: train
    path: data/deu_Latn-ita_Latn/train.parquet.gzip
- config_name: ben_Beng-srp_Latn
  data_files:
  - split: train
    path: data/ben_Beng-srp_Latn/train.parquet.gzip
- config_name: hin_Deva-pan_Guru
  data_files:
  - split: train
    path: data/hin_Deva-pan_Guru/train.parquet.gzip
- config_name: ita_Latn-ron_Latn
  data_files:
  - split: train
    path: data/ita_Latn-ron_Latn/train.parquet.gzip
- config_name: hin_Deva-zho_Hant
  data_files:
  - split: train
    path: data/hin_Deva-zho_Hant/train.parquet.gzip
- config_name: hin_Deva-swh_Latn
  data_files:
  - split: train
    path: data/hin_Deva-swh_Latn/train.parquet.gzip
- config_name: heb_Hebr-swh_Latn
  data_files:
  - split: train
    path: data/heb_Hebr-swh_Latn/train.parquet.gzip
- config_name: ces_Latn-fil_Latn
  data_files:
  - split: train
    path: data/ces_Latn-fil_Latn/train.parquet.gzip
- config_name: bul_Cyrl-hin_Deva
  data_files:
  - split: train
    path: data/bul_Cyrl-hin_Deva/train.parquet.gzip
- config_name: ell_Grek-rus_Cyrl
  data_files:
  - split: train
    path: data/ell_Grek-rus_Cyrl/train.parquet.gzip
- config_name: rus_Cyrl-urd_Arab
  data_files:
  - split: train
    path: data/rus_Cyrl-urd_Arab/train.parquet.gzip
- config_name: nld_Latn-sqi_Latn
  data_files:
  - split: train
    path: data/nld_Latn-sqi_Latn/train.parquet.gzip
- config_name: deu_Latn-por_Latn
  data_files:
  - split: train
    path: data/deu_Latn-por_Latn/train.parquet.gzip
- config_name: deu_Latn-ind_Latn
  data_files:
  - split: train
    path: data/deu_Latn-ind_Latn/train.parquet.gzip
- config_name: ben_Beng-rus_Cyrl
  data_files:
  - split: train
    path: data/ben_Beng-rus_Cyrl/train.parquet.gzip
- config_name: eng_Latn-khm_Khmr
  data_files:
  - split: train
    path: data/eng_Latn-khm_Khmr/train.parquet.gzip
- config_name: fra_Latn-swe_Latn
  data_files:
  - split: train
    path: data/fra_Latn-swe_Latn/train.parquet.gzip
- config_name: heb_Hebr-por_Latn
  data_files:
  - split: train
    path: data/heb_Hebr-por_Latn/train.parquet.gzip
- config_name: swh_Latn-tet_Latn
  data_files:
  - split: train
    path: data/swh_Latn-tet_Latn/train.parquet.gzip
- config_name: amh_Ethi-hin_Deva
  data_files:
  - split: train
    path: data/amh_Ethi-hin_Deva/train.parquet.gzip
- config_name: npi_Deva-nld_Latn
  data_files:
  - split: train
    path: data/npi_Deva-nld_Latn/train.parquet.gzip
- config_name: ben_Beng-hin_Deva
  data_files:
  - split: train
    path: data/ben_Beng-hin_Deva/train.parquet.gzip
- config_name: ell_Grek-heb_Hebr
  data_files:
  - split: train
    path: data/ell_Grek-heb_Hebr/train.parquet.gzip
- config_name: rus_Cyrl-tur_Latn
  data_files:
  - split: train
    path: data/rus_Cyrl-tur_Latn/train.parquet.gzip
- config_name: ayr_Latn-tur_Latn
  data_files:
  - split: train
    path: data/ayr_Latn-tur_Latn/train.parquet.gzip
- config_name: jpn_Jpan-mya_Mymr
  data_files:
  - split: train
    path: data/jpn_Jpan-mya_Mymr/train.parquet.gzip
- config_name: eng_Latn-zho_Hans
  data_files:
  - split: train
    path: data/eng_Latn-zho_Hans/train.parquet.gzip
- config_name: khm_Khmr-rus_Cyrl
  data_files:
  - split: train
    path: data/khm_Khmr-rus_Cyrl/train.parquet.gzip
- config_name: ayr_Latn-tet_Latn
  data_files:
  - split: train
    path: data/ayr_Latn-tet_Latn/train.parquet.gzip
- config_name: ell_Grek-swe_Latn
  data_files:
  - split: train
    path: data/ell_Grek-swe_Latn/train.parquet.gzip
- config_name: eng_Latn-tsn_Latn
  data_files:
  - split: train
    path: data/eng_Latn-tsn_Latn/train.parquet.gzip
- config_name: ces_Latn-hun_Latn
  data_files:
  - split: train
    path: data/ces_Latn-hun_Latn/train.parquet.gzip
- config_name: ben_Beng-spa_Latn
  data_files:
  - split: train
    path: data/ben_Beng-spa_Latn/train.parquet.gzip
- config_name: ces_Latn-srp_Latn
  data_files:
  - split: train
    path: data/ces_Latn-srp_Latn/train.parquet.gzip
- config_name: fra_Latn-hin_Deva
  data_files:
  - split: train
    path: data/fra_Latn-hin_Deva/train.parquet.gzip
- config_name: eng_Latn-rus_Cyrl
  data_files:
  - split: train
    path: data/eng_Latn-rus_Cyrl/train.parquet.gzip
- config_name: amh_Ethi-pes_Arab
  data_files:
  - split: train
    path: data/amh_Ethi-pes_Arab/train.parquet.gzip
- config_name: srp_Latn-urd_Arab
  data_files:
  - split: train
    path: data/srp_Latn-urd_Arab/train.parquet.gzip
- config_name: fil_Latn-hun_Latn
  data_files:
  - split: train
    path: data/fil_Latn-hun_Latn/train.parquet.gzip
- config_name: amh_Ethi-ita_Latn
  data_files:
  - split: train
    path: data/amh_Ethi-ita_Latn/train.parquet.gzip
- config_name: ckb_Arab-plt_Latn
  data_files:
  - split: train
    path: data/ckb_Arab-plt_Latn/train.parquet.gzip
- config_name: pes_Arab-sqi_Latn
  data_files:
  - split: train
    path: data/pes_Arab-sqi_Latn/train.parquet.gzip
- config_name: pan_Guru-zho_Hans
  data_files:
  - split: train
    path: data/pan_Guru-zho_Hans/train.parquet.gzip
- config_name: por_Latn-ron_Latn
  data_files:
  - split: train
    path: data/por_Latn-ron_Latn/train.parquet.gzip
- config_name: rus_Cyrl-swe_Latn
  data_files:
  - split: train
    path: data/rus_Cyrl-swe_Latn/train.parquet.gzip
- config_name: dan_Latn-mkd_Cyrl
  data_files:
  - split: train
    path: data/dan_Latn-mkd_Cyrl/train.parquet.gzip
- config_name: ces_Latn-pes_Arab
  data_files:
  - split: train
    path: data/ces_Latn-pes_Arab/train.parquet.gzip
- config_name: ben_Beng-urd_Arab
  data_files:
  - split: train
    path: data/ben_Beng-urd_Arab/train.parquet.gzip
- config_name: kor_Hang-mkd_Cyrl
  data_files:
  - split: train
    path: data/kor_Hang-mkd_Cyrl/train.parquet.gzip
- config_name: swh_Latn-tur_Latn
  data_files:
  - split: train
    path: data/swh_Latn-tur_Latn/train.parquet.gzip
- config_name: pan_Guru-rus_Cyrl
  data_files:
  - split: train
    path: data/pan_Guru-rus_Cyrl/train.parquet.gzip
- config_name: deu_Latn-srp_Latn
  data_files:
  - split: train
    path: data/deu_Latn-srp_Latn/train.parquet.gzip
- config_name: ita_Latn-tur_Latn
  data_files:
  - split: train
    path: data/ita_Latn-tur_Latn/train.parquet.gzip
- config_name: pan_Guru-tur_Latn
  data_files:
  - split: train
    path: data/pan_Guru-tur_Latn/train.parquet.gzip
- config_name: dan_Latn-urd_Arab
  data_files:
  - split: train
    path: data/dan_Latn-urd_Arab/train.parquet.gzip
- config_name: arb_Arab-ita_Latn
  data_files:
  - split: train
    path: data/arb_Arab-ita_Latn/train.parquet.gzip
- config_name: cat_Latn-fil_Latn
  data_files:
  - split: train
    path: data/cat_Latn-fil_Latn/train.parquet.gzip
- config_name: amh_Ethi-ben_Beng
  data_files:
  - split: train
    path: data/amh_Ethi-ben_Beng/train.parquet.gzip
- config_name: ayr_Latn-ind_Latn
  data_files:
  - split: train
    path: data/ayr_Latn-ind_Latn/train.parquet.gzip
- config_name: arb_Arab-pes_Arab
  data_files:
  - split: train
    path: data/arb_Arab-pes_Arab/train.parquet.gzip
- config_name: amh_Ethi-cat_Latn
  data_files:
  - split: train
    path: data/amh_Ethi-cat_Latn/train.parquet.gzip
- config_name: arb_Arab-kor_Hang
  data_files:
  - split: train
    path: data/arb_Arab-kor_Hang/train.parquet.gzip
- config_name: spa_Latn-heb_Hebr
  data_files:
  - split: train
    path: data/spa_Latn-heb_Hebr/train.parquet.gzip
- config_name: bul_Cyrl-hun_Latn
  data_files:
  - split: train
    path: data/bul_Cyrl-hun_Latn/train.parquet.gzip
- config_name: srp_Latn-zho_Hant
  data_files:
  - split: train
    path: data/srp_Latn-zho_Hant/train.parquet.gzip
- config_name: arb_Arab-ind_Latn
  data_files:
  - split: train
    path: data/arb_Arab-ind_Latn/train.parquet.gzip
- config_name: bul_Cyrl-urd_Arab
  data_files:
  - split: train
    path: data/bul_Cyrl-urd_Arab/train.parquet.gzip
- config_name: ind_Latn-plt_Latn
  data_files:
  - split: train
    path: data/ind_Latn-plt_Latn/train.parquet.gzip
- config_name: dan_Latn-pes_Arab
  data_files:
  - split: train
    path: data/dan_Latn-pes_Arab/train.parquet.gzip
- config_name: mkd_Cyrl-urd_Arab
  data_files:
  - split: train
    path: data/mkd_Cyrl-urd_Arab/train.parquet.gzip
- config_name: ayr_Latn-npi_Deva
  data_files:
  - split: train
    path: data/ayr_Latn-npi_Deva/train.parquet.gzip
- config_name: jpn_Jpan-por_Latn
  data_files:
  - split: train
    path: data/jpn_Jpan-por_Latn/train.parquet.gzip
- config_name: heb_Hebr-jpn_Jpan
  data_files:
  - split: train
    path: data/heb_Hebr-jpn_Jpan/train.parquet.gzip
- config_name: kor_Hang-por_Latn
  data_files:
  - split: train
    path: data/kor_Hang-por_Latn/train.parquet.gzip
- config_name: cat_Latn-srp_Latn
  data_files:
  - split: train
    path: data/cat_Latn-srp_Latn/train.parquet.gzip
- config_name: ben_Beng-nld_Latn
  data_files:
  - split: train
    path: data/ben_Beng-nld_Latn/train.parquet.gzip
- config_name: ita_Latn-srp_Latn
  data_files:
  - split: train
    path: data/ita_Latn-srp_Latn/train.parquet.gzip
- config_name: pan_Guru-pol_Latn
  data_files:
  - split: train
    path: data/pan_Guru-pol_Latn/train.parquet.gzip
- config_name: jpn_Jpan-khm_Khmr
  data_files:
  - split: train
    path: data/jpn_Jpan-khm_Khmr/train.parquet.gzip
- config_name: amh_Ethi-pol_Latn
  data_files:
  - split: train
    path: data/amh_Ethi-pol_Latn/train.parquet.gzip
- config_name: ayr_Latn-ell_Grek
  data_files:
  - split: train
    path: data/ayr_Latn-ell_Grek/train.parquet.gzip
- config_name: arb_Arab-zho_Hant
  data_files:
  - split: train
    path: data/arb_Arab-zho_Hant/train.parquet.gzip
- config_name: fra_Latn-mya_Mymr
  data_files:
  - split: train
    path: data/fra_Latn-mya_Mymr/train.parquet.gzip
- config_name: fra_Latn-tet_Latn
  data_files:
  - split: train
    path: data/fra_Latn-tet_Latn/train.parquet.gzip
- config_name: deu_Latn-pan_Guru
  data_files:
  - split: train
    path: data/deu_Latn-pan_Guru/train.parquet.gzip
- config_name: cat_Latn-swh_Latn
  data_files:
  - split: train
    path: data/cat_Latn-swh_Latn/train.parquet.gzip
- config_name: ayr_Latn-swh_Latn
  data_files:
  - split: train
    path: data/ayr_Latn-swh_Latn/train.parquet.gzip
- config_name: mkd_Cyrl-por_Latn
  data_files:
  - split: train
    path: data/mkd_Cyrl-por_Latn/train.parquet.gzip
- config_name: eng_Latn-tur_Latn
  data_files:
  - split: train
    path: data/eng_Latn-tur_Latn/train.parquet.gzip
- config_name: ayr_Latn-zho_Hant
  data_files:
  - split: train
    path: data/ayr_Latn-zho_Hant/train.parquet.gzip
- config_name: hun_Latn-ind_Latn
  data_files:
  - split: train
    path: data/hun_Latn-ind_Latn/train.parquet.gzip
- config_name: nld_Latn-rus_Cyrl
  data_files:
  - split: train
    path: data/nld_Latn-rus_Cyrl/train.parquet.gzip
- config_name: bul_Cyrl-deu_Latn
  data_files:
  - split: train
    path: data/bul_Cyrl-deu_Latn/train.parquet.gzip
- config_name: ben_Beng-pes_Arab
  data_files:
  - split: train
    path: data/ben_Beng-pes_Arab/train.parquet.gzip
- config_name: jpn_Jpan-sqi_Latn
  data_files:
  - split: train
    path: data/jpn_Jpan-sqi_Latn/train.parquet.gzip
- config_name: ben_Beng-kor_Hang
  data_files:
  - split: train
    path: data/ben_Beng-kor_Hang/train.parquet.gzip
- config_name: ben_Beng-pan_Guru
  data_files:
  - split: train
    path: data/ben_Beng-pan_Guru/train.parquet.gzip
- config_name: hun_Latn-sqi_Latn
  data_files:
  - split: train
    path: data/hun_Latn-sqi_Latn/train.parquet.gzip
- config_name: nld_Latn-pan_Guru
  data_files:
  - split: train
    path: data/nld_Latn-pan_Guru/train.parquet.gzip
- config_name: kor_Hang-swh_Latn
  data_files:
  - split: train
    path: data/kor_Hang-swh_Latn/train.parquet.gzip
- config_name: fra_Latn-ind_Latn
  data_files:
  - split: train
    path: data/fra_Latn-ind_Latn/train.parquet.gzip
- config_name: ell_Grek-sqi_Latn
  data_files:
  - split: train
    path: data/ell_Grek-sqi_Latn/train.parquet.gzip
- config_name: jpn_Jpan-kor_Hang
  data_files:
  - split: train
    path: data/jpn_Jpan-kor_Hang/train.parquet.gzip
- config_name: tet_Latn-zho_Hant
  data_files:
  - split: train
    path: data/tet_Latn-zho_Hant/train.parquet.gzip
- config_name: fra_Latn-nld_Latn
  data_files:
  - split: train
    path: data/fra_Latn-nld_Latn/train.parquet.gzip
- config_name: eng_Latn-zho_Hant
  data_files:
  - split: train
    path: data/eng_Latn-zho_Hant/train.parquet.gzip
- config_name: ory_Orya-pol_Latn
  data_files:
  - split: train
    path: data/ory_Orya-pol_Latn/train.parquet.gzip
- config_name: plt_Latn-npi_Deva
  data_files:
  - split: train
    path: data/plt_Latn-npi_Deva/train.parquet.gzip
- config_name: ben_Beng-zho_Hans
  data_files:
  - split: train
    path: data/ben_Beng-zho_Hans/train.parquet.gzip
- config_name: cat_Latn-khm_Khmr
  data_files:
  - split: train
    path: data/cat_Latn-khm_Khmr/train.parquet.gzip
- config_name: swe_Latn-zho_Hant
  data_files:
  - split: train
    path: data/swe_Latn-zho_Hant/train.parquet.gzip
- config_name: arb_Arab-hin_Deva
  data_files:
  - split: train
    path: data/arb_Arab-hin_Deva/train.parquet.gzip
- config_name: pes_Arab-zho_Hans
  data_files:
  - split: train
    path: data/pes_Arab-zho_Hans/train.parquet.gzip
- config_name: ind_Latn-zho_Hant
  data_files:
  - split: train
    path: data/ind_Latn-zho_Hant/train.parquet.gzip
- config_name: por_Latn-urd_Arab
  data_files:
  - split: train
    path: data/por_Latn-urd_Arab/train.parquet.gzip
- config_name: khm_Khmr-por_Latn
  data_files:
  - split: train
    path: data/khm_Khmr-por_Latn/train.parquet.gzip
- config_name: heb_Hebr-pol_Latn
  data_files:
  - split: train
    path: data/heb_Hebr-pol_Latn/train.parquet.gzip
- config_name: ell_Grek-zho_Hant
  data_files:
  - split: train
    path: data/ell_Grek-zho_Hant/train.parquet.gzip
- config_name: por_Latn-swe_Latn
  data_files:
  - split: train
    path: data/por_Latn-swe_Latn/train.parquet.gzip
- config_name: ben_Beng-ind_Latn
  data_files:
  - split: train
    path: data/ben_Beng-ind_Latn/train.parquet.gzip
- config_name: spa_Latn-urd_Arab
  data_files:
  - split: train
    path: data/spa_Latn-urd_Arab/train.parquet.gzip
- config_name: srp_Latn-zho_Hans
  data_files:
  - split: train
    path: data/srp_Latn-zho_Hans/train.parquet.gzip
- config_name: deu_Latn-kor_Hang
  data_files:
  - split: train
    path: data/deu_Latn-kor_Hang/train.parquet.gzip
- config_name: khm_Khmr-plt_Latn
  data_files:
  - split: train
    path: data/khm_Khmr-plt_Latn/train.parquet.gzip
- config_name: plt_Latn-ory_Orya
  data_files:
  - split: train
    path: data/plt_Latn-ory_Orya/train.parquet.gzip
- config_name: bul_Cyrl-ell_Grek
  data_files:
  - split: train
    path: data/bul_Cyrl-ell_Grek/train.parquet.gzip
- config_name: spa_Latn-hun_Latn
  data_files:
  - split: train
    path: data/spa_Latn-hun_Latn/train.parquet.gzip
- config_name: heb_Hebr-mkd_Cyrl
  data_files:
  - split: train
    path: data/heb_Hebr-mkd_Cyrl/train.parquet.gzip
- config_name: jpn_Jpan-pan_Guru
  data_files:
  - split: train
    path: data/jpn_Jpan-pan_Guru/train.parquet.gzip
- config_name: tur_Latn-zho_Hant
  data_files:
  - split: train
    path: data/tur_Latn-zho_Hant/train.parquet.gzip
- config_name: plt_Latn-sqi_Latn
  data_files:
  - split: train
    path: data/plt_Latn-sqi_Latn/train.parquet.gzip
- config_name: fil_Latn-nld_Latn
  data_files:
  - split: train
    path: data/fil_Latn-nld_Latn/train.parquet.gzip
- config_name: arb_Arab-eng_Latn
  data_files:
  - split: train
    path: data/arb_Arab-eng_Latn/train.parquet.gzip
- config_name: pol_Latn-ron_Latn
  data_files:
  - split: train
    path: data/pol_Latn-ron_Latn/train.parquet.gzip
- config_name: hun_Latn-mya_Mymr
  data_files:
  - split: train
    path: data/hun_Latn-mya_Mymr/train.parquet.gzip
- config_name: ell_Grek-npi_Deva
  data_files:
  - split: train
    path: data/ell_Grek-npi_Deva/train.parquet.gzip
- config_name: amh_Ethi-ind_Latn
  data_files:
  - split: train
    path: data/amh_Ethi-ind_Latn/train.parquet.gzip
- config_name: fil_Latn-zho_Hant
  data_files:
  - split: train
    path: data/fil_Latn-zho_Hant/train.parquet.gzip
- config_name: arb_Arab-npi_Deva
  data_files:
  - split: train
    path: data/arb_Arab-npi_Deva/train.parquet.gzip
- config_name: fra_Latn-urd_Arab
  data_files:
  - split: train
    path: data/fra_Latn-urd_Arab/train.parquet.gzip
- config_name: arb_Arab-yor_Latn
  data_files:
  - split: train
    path: data/arb_Arab-yor_Latn/train.parquet.gzip
- config_name: mya_Mymr-zho_Hant
  data_files:
  - split: train
    path: data/mya_Mymr-zho_Hant/train.parquet.gzip
- config_name: ell_Grek-urd_Arab
  data_files:
  - split: train
    path: data/ell_Grek-urd_Arab/train.parquet.gzip
- config_name: ben_Beng-zho_Hant
  data_files:
  - split: train
    path: data/ben_Beng-zho_Hant/train.parquet.gzip
- config_name: arb_Arab-bul_Cyrl
  data_files:
  - split: train
    path: data/arb_Arab-bul_Cyrl/train.parquet.gzip
- config_name: ben_Beng-sqi_Latn
  data_files:
  - split: train
    path: data/ben_Beng-sqi_Latn/train.parquet.gzip
- config_name: cat_Latn-pes_Arab
  data_files:
  - split: train
    path: data/cat_Latn-pes_Arab/train.parquet.gzip
- config_name: ayr_Latn-sqi_Latn
  data_files:
  - split: train
    path: data/ayr_Latn-sqi_Latn/train.parquet.gzip
- config_name: pan_Guru-swh_Latn
  data_files:
  - split: train
    path: data/pan_Guru-swh_Latn/train.parquet.gzip
- config_name: kor_Hang-ron_Latn
  data_files:
  - split: train
    path: data/kor_Hang-ron_Latn/train.parquet.gzip
- config_name: spa_Latn-zho_Hans
  data_files:
  - split: train
    path: data/spa_Latn-zho_Hans/train.parquet.gzip
- config_name: ayr_Latn-por_Latn
  data_files:
  - split: train
    path: data/ayr_Latn-por_Latn/train.parquet.gzip
- config_name: eng_Latn-swh_Latn
  data_files:
  - split: train
    path: data/eng_Latn-swh_Latn/train.parquet.gzip
- config_name: heb_Hebr-nld_Latn
  data_files:
  - split: train
    path: data/heb_Hebr-nld_Latn/train.parquet.gzip
- config_name: mya_Mymr-pol_Latn
  data_files:
  - split: train
    path: data/mya_Mymr-pol_Latn/train.parquet.gzip
- config_name: deu_Latn-rus_Cyrl
  data_files:
  - split: train
    path: data/deu_Latn-rus_Cyrl/train.parquet.gzip
- config_name: pol_Latn-swh_Latn
  data_files:
  - split: train
    path: data/pol_Latn-swh_Latn/train.parquet.gzip
- config_name: nld_Latn-pol_Latn
  data_files:
  - split: train
    path: data/nld_Latn-pol_Latn/train.parquet.gzip
- config_name: hun_Latn-mkd_Cyrl
  data_files:
  - split: train
    path: data/hun_Latn-mkd_Cyrl/train.parqu",High,5.0
Translation,MBZUAI-Paris/DODa-10K,3.0,37.0,2024-09-27 06:33:03+00:00,cc-by-nc-4.0,0.0,1.28 MB,1342177.28,1.28 MB,1342177.28,10000,https://arxiv.org/abs/2409.17912,none,"---
language:
- ma
- ar
- en
- fr
task_categories:
- translation
- text2text-generation
multilinguality:
- multilingual
language_creators:
- expert-generated
annotations_creators:
- expert-generated
- machine-generated
source_datasets:
- original
size_categories:
- 1K<n<10K
license:
- cc-by-nc-4.0
---

# Dataset Card for DODa-10K

## Table of Contents
- [Table of Contents](#table-of-contents)
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks](#supported-tasks)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)

## Dataset Description

- **Homepage:** [https://hf.co/datasets/MBZUAI-Paris/DoDa-10K](https://hf.co/datasets/MBZUAI-Paris/DoDa-10K)
- **Paper:** [More Information Needed]

### Dataset Summary

DODa-10K is part of the Darija Open Dataset (DODa), a collaborative open-source project to collect Darija language resources. It includes translation quintuples between Darija (in both Arabic and Latin scripts), Modern Standard Arabic (MSA), English, and French. The dataset was augmented using GPT-4 for generating translations and was reviewed by native Darija speakers to ensure quality. The dataset also supports transliteration tasks for converting between Darija Arabic script and Latin script.

### Supported Tasks
  - Translating between Darija (Arabic and Latin), MSA, English, and French.
  - Transliteration between Darija Arabic script and Latin script.

### Languages

The dataset includes Moroccan Arabic (Darija in both Arabic and Latin scripts), Modern Standard Arabic (MSA), English, and French.

## Dataset Structure

The dataset contains translation quintuples, with each instance including the same sentence in Darija (Arabic script), Darija (Latin script), English, French, and MSA.

### Data Instances

Each data instance includes:
- **Darija_arabic**: The sentence in Darija, written in Arabic script.
- **Darija_latin**: The same sentence in Darija, written in Latin script.
- **English**: The English translation.
- **French**: The French translation.
- **MSA**: The Modern Standard Arabic translation.
- **index**: A unique identifier for the instance.

Example:
```
{
    ""Darija_arabic"": ""تانقتارح أنّاك تسمع كتر أُ تهدر قلّ"",
    ""Darija_latin"": ""tan9tar7 annak tsm3 ktr o thdr 9ell"",
    ""English"": ""I suggest you listen more and talk less"",
    ""French"": ""Je te suggère d'écouter plus et de parler moins"",
    ""MSA"": ""أقترح عليك أن تستمع أكثر وتتكلم أقل"",
    ""index"": 30
}
```

## Dataset Creation

### Curation Rationale

DODa-10K was created to provide a high-quality multilingual parallel corpus for Darija, MSA, English, and French, as well as to support transliteration tasks between Darija in Arabic and Latin scripts. The goal is to foster the development of multilingual models and transliteration systems for Moroccan Darija.

### Source Data

#### Initial Data Collection and Normalization

The dataset was collected as part of the Darija Open Dataset project, and 10,000 examples were augmented using GPT-4. The translations were then manually reviewed by native Darija speakers to ensure accuracy and quality.

#### Who are the source language producers?

The original DoDa dataset was augmented by GPT-4 and reviewed by native Darija speakers for accuracy. The original corpus was sourced from various open-source initiatives, such as Darija-English parallel corpora.

### Personal and Sensitive Information

The dataset does not contain any personal or sensitive information.

## Considerations for Using the Data

### Social Impact of Dataset

This dataset promotes the development of NLP tools for Moroccan Darija, a language with limited digital resources. It also facilitates research on translation and transliteration between several languages to Moroccan Darija.

### Discussion of Biases

The dataset was machine-generated and manually reviewed, but it may still reflect biases present in the machine translation models used, particularly in cultural nuances between the languages.

### Other Known Limitations

- The dataset is limited to the first 10K examples from the Darija Open Dataset project.

## Additional Information

### Dataset Curators

- MBZUAI-Paris team

### Licensing Information

- [Creative Commons Attribution-NonCommercial 4.0 International License](https://github.com/Gibberlings3/GitHub-Templates/blob/master/License-Templates/CC-BY-NC-ND-4.0/LICENSE-CC-BY-NC-ND-4.0.md)

### Citation Information

```
@article{shang2024atlaschatadaptinglargelanguage,
      title={Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect}, 
      author={Guokan Shang and Hadi Abdine and Yousef Khoubrane and Amr Mohamed and Yassine Abbahaddou and Sofiane Ennadir and Imane Momayiz and Xuguang Ren and Eric Moulines and Preslav Nakov and Michalis Vazirgiannis and Eric Xing},
      year={2024},
      eprint={2409.17912},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.17912}, 
}
```
```
@article{outchakoucht2024evolution,
  title={The Evolution of Darija Open Dataset: Introducing Version 2},
  author={Outchakoucht, Aissam and Es-Samaali, Hamza},
  journal={arXiv preprint arXiv:2405.13016},
  year={2024}
}
```

```
@article{outchakoucht2021moroccan,
  title={Moroccan dialect-darija-open dataset},
  author={Outchakoucht, Aissam and Es-Samaali, Hamza},
  journal={arXiv preprint arXiv:2103.09687},
  year={2021}
}
```",High,5.0
Translation,Mohamedfadil369/BrainData,1.0,16.0,2024-06-08 02:36:18+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- translation
- summarization
- text2text-generation
language:
- ar
- en
tags:
- legal
- finance
- medical
- webdataset
pretty_name: BrainData
size_categories:
- n>1T
---

# Dataset Card for BrainData

<!-- Provide a quick summary of the dataset. -->

This dataset card provides detailed information about the BrainData dataset. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->

BrainData is a comprehensive dataset designed for multiple NLP tasks including translation, summarization, and text-to-text generation. It encompasses a variety of domains such as legal, finance, and medical, with content available in both Arabic and English. This extensive dataset is ideal for training robust multilingual models.

- **Curated by:** Dr. Mohamed El Fadil
- **Funded by:** BRAINSAIT LTD
- **Shared by:** Dr. Mohamed El Fadil
- **Language(s) (NLP):** Arabic, English
- **License:** Apache-2.0

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [BrainData GitHub Repository](https://github.com/brainsait/dataset)
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

BrainData can be used for training models in translation, summarization, and text generation. It is particularly useful for applications in the legal, finance, and medical sectors, where multilingual support is required.

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

The dataset is not suitable for tasks unrelated to text processing, such as image recognition or speech-to-text. Additionally, it should not be used for generating inappropriate or harmful content.

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

The dataset includes text data in both Arabic and English, covering multiple domains. It is structured to support easy access and processing, with clear separations between different task categories and languages.

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

BrainData was created to address the need for high-quality multilingual datasets in the fields of legal, finance, and medical text processing. It aims to facilitate the development of advanced NLP models that can operate across different languages and domains.

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

The source data includes legal documents, financial reports, medical records, and web data, ensuring a diverse and representative sample of text for each domain.

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

Data was collected from reputable sources in the legal, finance, and medical fields. It underwent thorough filtering and normalization to ensure consistency and quality. Tools such as Python's NLTK and SpaCy libraries were used for text preprocessing.

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

The source data was produced by professionals and organizations in the legal, finance, and medical sectors, ensuring authoritative and accurate content.

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

Annotations were performed by domain experts using tools like Prodigy and Labelbox. Guidelines were provided to ensure consistency, and inter-annotator agreement was regularly checked to maintain quality.

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

The annotators were professionals with expertise in legal, finance, and medical fields, ensuring high-quality and accurate annotations.

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

The dataset contains sensitive information, particularly in the medical and financial domains. All personal identifiers have been removed or anonymized to protect privacy.

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

The dataset may contain biases inherent to the source data, such as regional or institutional biases. Users should be aware of these limitations and consider them when developing models.

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should perform bias analysis and fairness checks when using the dataset, especially for critical applications. Regular updates and retraining with new data are recommended to mitigate biases.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

```bibtex
@misc{braindata2024dataset,
  title={BrainData: A Multilingual Dataset for Legal, Finance, and Medical Text Processing},
  author={El Fadil, Mohamed and BrainSAIT Team},
  year={2024},
  url={https://github.com/brainsait/dataset}
}
",High,4.0
Translation,Svngoku/xP3x-Kongo,4.0,39.0,2024-06-24 21:33:28+00:00,apache-2.0,3.0,300 MB,314572800.0,300 MB,314572800.0,1223481,https://arxiv.org/abs/2211.01786,none,"---
annotations_creators:
- expert-generated
- crowdsourced
language:
- af
- ar
- az
- be
- bg
- bn
- br
- bs
- ca
- ch
- cs
- cv
- cy
- da
- de
- el
- en
- eo
- es
- et
- eu
- fa
- fi
- fo
- fr
- fy
- ga
- gd
- gl
- gn
- he
- hi
- hr
- hu
- hy
- ia
- id
- ie
- io
- is
- it
- ja
- jv
- ka
- kk
- km
- ko
- ku
- kw
- la
- lb
- lt
- lv
- mi
- mk
- ml
- mn
- mr
- ms
- mt
- my
- nb
- nl
- nn
- 'no'
- oc
- pl
- pt
- qu
- rn
- ro
- ru
- sh
- sl
- sq
- sr
- sv
- sw
- ta
- te
- th
- tk
- tl
- tr
- tt
- ug
- uk
- ur
- uz
- vi
- vo
- yi
- zh
- ace
- acm
- acq
- aeb
- af
- ajp
- ak
- als
- am
- apc
- ar
- ars
- ary
- arz
- as
- ast
- awa
- ayr
- azb
- azj
- ba
- bm
- ban
- be
- bem
- bn
- bho
- bjn
- bo
- bs
- bug
- bg
- ca
- ceb
- cs
- cjk
- ckb
- crh
- cy
- da
- de
- dik
- dyu
- dz
- el
- en
- eo
- et
- eu
- ee
- fo
- fj
- fi
- fon
- fr
- fur
- fuv
- gaz
- gd
- ga
- gl
- gn
- gu
- ht
- ha
- he
- hi
- hne
- hr
- hu
- hy
- ig
- ilo
- id
- is
- it
- jv
- ja
- kab
- kac
- kam
- kn
- ks
- ka
- kk
- kbp
- kea
- khk
- km
- ki
- rw
- ky
- kmb
- kmr
- knc
- kg
- ko
- lo
- lij
- li
- ln
- lt
- lmo
- ltg
- lb
- lua
- lg
- luo
- lus
- lvs
- mag
- mai
- ml
- mar
- min
- mk
- mt
- mni
- mos
- mi
- my
- nl
- nn
- nb
- npi
- nso
- nus
- ny
- oc
- ory
- pag
- pa
- pap
- pbt
- pes
- plt
- pl
- pt
- prs
- quy
- ro
- rn
- ru
- sg
- sa
- sat
- scn
- shn
- si
- sk
- sl
- sm
- sn
- sd
- so
- st
- es
- sc
- sr
- ss
- su
- sv
- swh
- szl
- ta
- taq
- tt
- te
- tg
- tl
- th
- ti
- tpi
- tn
- ts
- tk
- tum
- tr
- tw
- tzm
- ug
- uk
- umb
- ur
- uzn
- vec
- vi
- war
- wo
- xh
- ydd
- yo
- yue
- zh
- zsm
- zu
programming_language:
- Java
- Python
- Jupyter-Notebook
license:
- apache-2.0
multilinguality:
- multilingual
pretty_name: xP3x
size_categories:
- 100M<n<1B
task_categories:
- other
- translation
---

# Dataset Card for xP3x Kikongo Focus

## Table of Contents
- [Table of Contents](#table-of-contents)
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
- [Additional Information](#additional-information)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Repository:** https://github.com/bigscience-workshop/xmtf
- **Paper:** [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786)
- **Point of Contact:** [Niklas Muennighoff](mailto:n.muennighoff@gmail.com)

### Dataset Summary

> xP3x (Crosslingual Public Pool of Prompts eXtended) is a collection of prompts & datasets across 277 languages & 16 NLP tasks. It contains all of xP3 + much more! It is used for training future contenders of mT0 & BLOOMZ at project Aya @[C4AI](https://cohere.for.ai/) 🧡
> 
- **Creation:** The dataset can be recreated using instructions available [here](https://github.com/bigscience-workshop/xmtf#create-xp3) together with the file in this repository named `xp3x_create.py`. We provide this version to save processing time.
- **Languages:** 277
- **xP3 Dataset Family:**

<table>
  <tr>
<th>Name</th>
<th>Explanation</th>
<th>Example models</th>
</tr>
<tr>
<td><a href=https://huggingface.co/datasets/Muennighoff/xP3x>xP3x</a></t> 
<td>Mixture of 17 tasks in 277 languages with English prompts</td>
<td>WIP - Join us at Project Aya @<a href=https://cohere.for.ai/>C4AI</a> to help!</td>
</tr>
<tr>
<td><a href=https://huggingface.co/datasets/bigscience/xP3>xP3</a></t> 
<td>Mixture of 13 training tasks in 46 languages with English prompts</td>
<td><a href=https://huggingface.co/bigscience/bloomz>bloomz</a> & <a href=https://huggingface.co/bigscience/mt0-xxl>mt0-xxl</a></td>
</tr>
<tr>
<td><a href=https://huggingface.co/datasets/bigscience/xP3mt>xP3mt</a></t> 
<td>Mixture of 13 training tasks in 46 languages with prompts in 20 languages (machine-translated from English)</td>
<td><a href=https://huggingface.co/bigscience/bloomz-mt>bloomz-mt</a> & <a href=https://huggingface.co/bigscience/mt0-xxl-mt>mt0-xxl-mt</a></td>
</tr>
<tr>
<td><a href=https://huggingface.co/datasets/bigscience/xP3all>xP3all</a></t> 
<td>xP3 + evaluation datasets adding an additional 3 tasks for a total of 16 tasks in 46 languages with English prompts</td>
<td></td>
</tr>
<tr>
<td><a href=https://huggingface.co/datasets/bigscience/xP3megds>xP3megds</a></t> 
<td><a href=https://github.com/bigscience-workshop/Megatron-DeepSpeed>Megatron-DeepSpeed</a> processed version of xP3</td>
<td><a href=https://huggingface.co/bigscience/bloomz>bloomz</a></td>
</tr>
<tr>
<td><a href=https://huggingface.co/datasets/Muennighoff/P3>P3</a></t> 
<td>Repreprocessed version of the English-only <a href=https://huggingface.co/datasets/bigscience/P3>P3</a> with 8 training tasks</td>
<td><a href=https://huggingface.co/bigscience/bloomz-p3>bloomz-p3</a> & <a href=https://huggingface.co/bigscience/mt0-xxl-p3>mt0-xxl-p3</a></td>
</tr>
</table>

## Dataset Structure


### Data Instances

An example looks as follows:

```json
{
  'inputs': '11月、遂にクロームはファイヤーフォックスを引き離し始めた。_はインターネットユーザーの評価が高まったのだ。\nReplace the _ in the above sentence with the correct option: \n- ファイヤーフォックス\n- クローム',
  'targets': 'クローム',
  'language': 'jpn_Jpan',
  'split': 'test',
  'template': 'Replace',
  'dataset': 'Muennighoff/xwinograd',
  'config': 'jp'
}
```

### Data Fields

The data fields are the same among all splits:
- `inputs`: the natural language input fed to the model
- `targets`: the natural language target that the model has to generate
- `language`: The language code. The codes are an extension of the FLORES-200 codes, where the first part is the language code and the second part the script code.
- `template`: The name of the prompt used.
- `dataset`: The Hugging Face dataset identifier of where the data stems from.
- `config`: The config of the Hugging Face dataset. 

### Usage

The dataset has 680 gigabytes and 530 million samples. You may want to filter it and then deduplicate depending on your needs.

Loading by language:

```python
# pip install -q datasets
from datasets import load_dataset
ds = load_dataset(""Muennighoff/xP3x"", ""zho_Hans"", streaming=True) # Use streaming to not download all at once
for x in ds[""train""]:
    print(x)
    break
```

You can then filter down by the data fields to e.g. only get certain configs or datasets.
As every dataset-config-template is its own jsonl file, you can also decide on the datasets, configs and templates you want and only download them.
For example, to download all Japanese xwinograd samples, you could do:

```python
# pip install -q datasets
from datasets import load_dataset
import multiprocessing
# pip install --upgrade huggingface-hub
from huggingface_hub import HfFileSystem, hf_hub_url

fs = HfFileSystem()
fps = fs.glob(f""datasets/CohereForAI/xP3x/data/jpn_Jpan/*xwinograd*"")
resolved_paths = [fs.resolve_path(file) for file in fps]
data_files = [hf_hub_url(resolved_path.repo_id, resolved_path.path_in_repo, repo_type=resolved_path.repo_type) for resolved_path in resolved_paths]

ds = load_dataset(""json"", data_files=data_files, num_proc=8)[""train""]
```

Sometimes it may be faster to clone the entire repo. To download all English files, you could do e.g.
```bash
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/datasets/CohereForAI/xP3x
cd xP3x
git lfs pull --include=""data/eng_Latn/*""
```

### Data Splits

|Language|Code|Kilobytes|%|Samples|%|
|--------|------:|------:|-:|---:|-:|
|Kikongo|kon_Latn|648,992|0.1|1,223,481|0.23|

#### Language specifics

- `Japanese`: Data in `jpn_Hira`, `jpn_Kana`, `jpn_Hani` is guaranteed to have Hiragana, Katakana or Kanji, respectively in each sample. However, they may still include other styles. So while all samples in `jpn_Kana` are guaranteed to have Katakana, there may still be Hiragana or Kanji.

## Dataset Creation

### Source Data


#### Training datasets

- Code Miscellaneous
  - [CodeComplex](https://huggingface.co/datasets/codeparrot/codecomplex)
  - [Docstring Corpus](https://huggingface.co/datasets/teven/code_docstring_corpus)
  - [GreatCode](https://huggingface.co/datasets/great_code)
  - [State Changes](https://huggingface.co/datasets/Fraser/python-state-changes)
- Closed-book QA
  - [Hotpot QA](https://huggingface.co/datasets/hotpot_qa)
  - [Trivia QA](https://huggingface.co/datasets/trivia_qa)
  - [Web Questions](https://huggingface.co/datasets/web_questions)
  - [Wiki QA](https://huggingface.co/datasets/wiki_qa)  
- Extractive QA
  - [Adversarial QA](https://huggingface.co/datasets/adversarial_qa)
  - [CMRC2018](https://huggingface.co/datasets/cmrc2018)
  - [DRCD](https://huggingface.co/datasets/clue)
  - [DuoRC](https://huggingface.co/datasets/duorc)
  - [MLQA](https://huggingface.co/datasets/mlqa)      
  - [Quoref](https://huggingface.co/datasets/quoref)
  - [ReCoRD](https://huggingface.co/datasets/super_glue)  
  - [ROPES](https://huggingface.co/datasets/ropes)
  - [SQuAD v2](https://huggingface.co/datasets/squad_v2)
  - [xQuAD](https://huggingface.co/datasets/xquad)
  - TyDI QA
    - [Primary](https://huggingface.co/datasets/khalidalt/tydiqa-primary)
    - [Goldp](https://huggingface.co/datasets/khalidalt/tydiqa-goldp)
- Multiple-Choice QA
  - [ARC](https://huggingface.co/datasets/ai2_arc)
  - [C3](https://huggingface.co/datasets/c3)  
  - [CoS-E](https://huggingface.co/datasets/cos_e)
  - [Cosmos](https://huggingface.co/datasets/cosmos)
  - [DREAM](https://huggingface.co/datasets/dream)
  - [MultiRC](https://huggingface.co/datasets/super_glue)
  - [OpenBookQA](https://huggingface.co/datasets/openbookqa)
  - [PiQA](https://huggingface.co/datasets/piqa)  
  - [QUAIL](https://huggingface.co/datasets/quail)
  - [QuaRel](https://huggingface.co/datasets/quarel)
  - [QuaRTz](https://huggingface.co/datasets/quartz)
  - [QASC](https://huggingface.co/datasets/qasc)
  - [RACE](https://huggingface.co/datasets/race)
  - [SciQ](https://huggingface.co/datasets/sciq)    
  - [Social IQA](https://huggingface.co/datasets/social_i_qa)
  - [Wiki Hop](https://huggingface.co/datasets/wiki_hop)
  - [WiQA](https://huggingface.co/datasets/wiqa)  
- Paraphrase Identification
  - [MRPC](https://huggingface.co/datasets/super_glue)
  - [PAWS](https://huggingface.co/datasets/paws)
  - [PAWS-X](https://huggingface.co/datasets/paws-x)  
  - [QQP](https://huggingface.co/datasets/qqp)  
- Program Synthesis
  - [APPS](https://huggingface.co/datasets/codeparrot/apps)
  - [CodeContests](https://huggingface.co/datasets/teven/code_contests)
  - [JupyterCodePairs](https://huggingface.co/datasets/codeparrot/github-jupyter-text-code-pairs)
  - [MBPP](https://huggingface.co/datasets/Muennighoff/mbpp)
  - [NeuralCodeSearch](https://huggingface.co/datasets/neural_code_search)
  - [XLCoST](https://huggingface.co/datasets/codeparrot/xlcost-text-to-code)  
- Structure-to-text
  - [Common Gen](https://huggingface.co/datasets/common_gen)
  - [Wiki Bio](https://huggingface.co/datasets/wiki_bio)
- Sentiment
  - [Amazon](https://huggingface.co/datasets/amazon_polarity)
  - [App Reviews](https://huggingface.co/datasets/app_reviews)
  - [IMDB](https://huggingface.co/datasets/imdb)
  - [Rotten Tomatoes](https://huggingface.co/datasets/rotten_tomatoes)
  - [Yelp](https://huggingface.co/datasets/yelp_review_full)
- Simplification
  - [BiSECT](https://huggingface.co/datasets/GEM/BiSECT)
- Summarization
  - [CNN Daily Mail](https://huggingface.co/datasets/cnn_dailymail)
  - [Gigaword](https://huggingface.co/datasets/gigaword)
  - [MultiNews](https://huggingface.co/datasets/multi_news)
  - [SamSum](https://huggingface.co/datasets/samsum)
  - [Wiki-Lingua](https://huggingface.co/datasets/GEM/wiki_lingua)
  - [XLSum](https://huggingface.co/datasets/GEM/xlsum)
  - [XSum](https://huggingface.co/datasets/xsum)
- Topic Classification
  - [AG News](https://huggingface.co/datasets/ag_news)
  - [DBPedia](https://huggingface.co/datasets/dbpedia_14)
  - [TNEWS](https://huggingface.co/datasets/clue)  
  - [TREC](https://huggingface.co/datasets/trec)
  - [CSL](https://huggingface.co/datasets/clue) 
- Translation
  - [Flores-200](https://huggingface.co/datasets/Muennighoff/flores200)
  - [Tatoeba](https://huggingface.co/datasets/Helsinki-NLP/tatoeba_mt)
  - [MultiEURLEX](https://huggingface.co/datasets/multi_eurlex)
- Word Sense disambiguation
  - [WiC](https://huggingface.co/datasets/super_glue)
  - [XL-WiC](https://huggingface.co/datasets/pasinit/xlwic)
- Natural Language Inference (NLI)
  - [ANLI](https://huggingface.co/datasets/anli)
  - [CB](https://huggingface.co/datasets/super_glue)
  - [RTE](https://huggingface.co/datasets/super_glue)
  - [XNLI](https://huggingface.co/datasets/xnli)
- Coreference Resolution
  - [Winogrande](https://huggingface.co/datasets/winogrande)
  - [XWinograd](https://huggingface.co/datasets/Muennighoff/xwinograd)
- Sentence Completion
  - [COPA](https://huggingface.co/datasets/super_glue)
  - [Story Cloze](https://huggingface.co/datasets/story_cloze)
  - [XCOPA](https://huggingface.co/datasets/xcopa)  
  - [XStoryCloze](https://huggingface.co/datasets/Muennighoff/xstory_cloze)

#### Dataset specifics

- Flores-200: There are three prompts for Flores: `continuation`, `question`, `command`, which represent three commonly used prompting styles, i.e. making a prompt seem like a natural continuation, turning it into a question or commanding the model to do something.
- tatoeba_mt: Contains duplicates. For example, it has data that is both classified as `jpn_Kana` and `jpn_Jpan`, so you may want to deduplicate.

## Additional Information

### Licensing Information

The dataset collection is released under Apache 2.0. Note that individual datasets may have different licenses.

### Citation Information

```bibtex
@article{muennighoff2022crosslingual,
  title={Crosslingual generalization through multitask finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}
```

### Contributions

Thanks to the contributors of [promptsource](https://github.com/bigscience-workshop/promptsource/graphs/contributors) for adding many prompts used in this dataset.
Thanks to the Aya team @[C4AI](https://cohere.for.ai/) 🧡",High,6.0
Translation,ymoslem/UN-Arabic-English-Filtered,2.0,160.0,2024-07-09 17:14:55+00:00,cc-by-4.0,0.0,5.16 GB,5540507811.84,5.16 GB,5540507811.84,19296911,none,"['https://aclanthology.org/L16-1561"",']","---
dataset_info:
  features:
  - name: text_en
    dtype: string
  - name: text_ar
    dtype: string
  splits:
  - name: train
    num_bytes: 9128039564
    num_examples: 19279407
  - name: test
    num_bytes: 3677574
    num_examples: 8752
  - name: dev
    num_bytes: 3714475
    num_examples: 8752
  download_size: 5159323292
  dataset_size: 9135431613
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: test
    path: data/test-*
  - split: dev
    path: data/dev-*
task_categories:
- translation
language:
- ar
- en
size_categories:
- 10M<n<100M
license: cc-by-4.0
---

## Dataset Details

MultiUN + UNPC datasets, with rule-based and semantic filtering (train > 0.45 - test/dev > 0.9)
as well as (>= 0.1) fasttext language detection.


### Dataset Structure

```
DatasetDict({
    train: Dataset({
        features: ['text_en', 'text_ar'],
        num_rows: 19279407
    })
    test: Dataset({
        features: ['text_en', 'text_ar'],
        num_rows: 8752
    })
    dev: Dataset({
        features: ['text_en', 'text_ar'],
        num_rows: 8752
    })
})
```

### Citations

```
@inproceedings{eisele-chen-2010-multiun,
    title = ""{M}ulti{UN}: A Multilingual Corpus from United Nation Documents"",
    author = ""Eisele, Andreas  and
      Chen, Yu"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel"",
    booktitle = ""Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)"",
    month = may,
    year = ""2010"",
    address = ""Valletta, Malta"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_Paper.pdf"",
}
```

```
@inproceedings{ziemski-etal-2016-united,
    title = ""The {U}nited {N}ations Parallel Corpus v1.0"",
    author = ""Ziemski, Micha{\l}  and
      Junczys-Dowmunt, Marcin  and
      Pouliquen, Bruno"",
    editor = ""Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios"",
    booktitle = ""Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)"",
    month = may,
    year = ""2016"",
    address = ""Portoro{\v{z}}, Slovenia"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""https://aclanthology.org/L16-1561"",
    pages = ""3530--3534"",
}
```

```
@INPROCEEDINGS{Tiedemann2012-OPUS,
  title     = ""{Parallel Data, Tools and Interfaces in {OPUS}}"",
  booktitle = ""{Proceedings of the Eighth International Conference on Language
               Resources and Evaluation ({LREC}'12)}"",
  author    = ""Tiedemann, J{\""o}rg"",
  publisher = ""European Language Resources Association (ELRA)"",
  pages     = ""2214--2218"",
  month     =  may,
  year      =  2012,
  url       = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf"",
  address   = ""Istanbul, Turkey""
}
```",High,4.0
Translation,Washedashore/thepower,2.0,11.0,2024-07-15 02:56:09+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- text-classification
- question-answering
- summarization
- text-generation
- table-question-answering
- token-classification
- zero-shot-classification
- translation
- fill-mask
- depth-estimation
- automatic-speech-recognition
language:
- aa
- ab
- ae
- af
- ak
- am
- an
- ar
- ay
- bm
- as
- av
- az
- br
- en
- ce
- ba
- be
- bg
- bh
- bi
- bn
- cy
tags:
- chemistry
- biology
- medical
- finance
- art
- code
- synthetic
pretty_name: pow
size_categories:
- 100B<n<1T
---
# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [Washed Ashore Relics]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Translation,ymoslem/CoVoST2-EN-AR-Text,1.0,61.0,2024-12-04 13:15:34+00:00,cc-by-nc-4.0,0.0,28.8 MB,30198988.8,28.8 MB,30198988.8,269380,https://arxiv.org/abs/2007.10310,none,"---
dataset_info:
  features:
  - name: text_en
    dtype: string
  - name: text_ar
    dtype: string
  splits:
  - name: train
    num_bytes: 43878549
    num_examples: 269380
  download_size: 28782159
  dataset_size: 43878549
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: cc-by-nc-4.0
task_categories:
- translation
language:
- ar
- en
size_categories:
- 1K<n<10K
---

## Dataset Details

This is a text-only filtered version of the English-Arabic [CoVoST2-EN-AR](https://huggingface.co/datasets/ymoslem/CoVoST2-EN-AR).

### Dataset Structure

```
Dataset({
    features: ['text_en', 'text_ar'],
    num_rows: 269380
})
```

### Citation

```
@misc{wang2020covost,
    title={CoVoST 2: A Massively Multilingual Speech-to-Text Translation Corpus},
    author={Changhan Wang and Anne Wu and Juan Pino},
    year={2020},
    eprint={2007.10310},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

```",High,4.0
Translation,ymoslem/MS-UI-EN-AR,1.0,34.0,2024-12-15 03:11:34+00:00,none,0.0,19.4 MB,20342374.4,19.4 MB,20342374.4,303402,none,none,"---
dataset_info:
  features:
  - name: text_en
    dtype: string
  - name: text_ar
    dtype: string
  splits:
  - name: train
    num_bytes: 37408246
    num_examples: 303402
  download_size: 19382320
  dataset_size: 37408246
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- translation
language:
- ar
- en
---


## Dataset Details

English-to-Arabic Microsoft UI strings

### Dataset Structure

```
Dataset({
    features: ['Source Term', 'Translation'],
    num_rows: 310307
})
```
",Low,1.0
Translation,ymoslem/MS-Glossary-EN-AR,0.0,30.0,2024-08-12 21:50:36+00:00,none,0.0,675 KB,691200.0,675 KB,691200.0,22061,none,none,"---
language:
- ar
- en
size_categories:
- 1K<n<10K
task_categories:
- translation
dataset_info:
  features:
  - name: text_en
    dtype: string
  - name: text_ar
    dtype: string
  splits:
  - name: train
    num_bytes: 1107907
    num_examples: 22061
  download_size: 674925
  dataset_size: 1107907
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
---

## Dataset Details

The English-to-Arabic portion of the Microsoft Software Localization Glossary.

### Dataset Structure

```
Dataset({
    features: ['en_term', 'ar_term', 'en_def'],
    num_rows: 25761
})
```

",Low,1.0
Translation,ReySajju742/Quran,2.0,56.0,2024-08-08 10:46:45+00:00,cc,0.0,27.3 MB,28626124.8,12.1 MB,12687769.6,62360,none,none,"---
license: cc
task_categories:
- translation
language:
- en
- ar
- ur
- tr
- es
- sv
- bn
- id
- fr
- ru
- zh
tags:
- Islam
- Quran
- translation
pretty_name: Quran Dataset With various Translations
size_categories:
- 10K<n<100K
---

Quran Dataset with Various Translations

License: Creative Commons (CC)
Task Categories: Translation
Languages:
Arabic (ar)
English (en)
Urdu (ur)
Turkish (tr)
Spanish (es)
Swedish (sv)
Bengali (bn)
Indonesian (id)
French (fr)
Russian (ru)
Chinese (zh)

Tags: Islam, Quran, Translation

Description:

This dataset contains complete translations of the Quran in various languages. The Quran is the holy book of Islam, and this dataset provides access to its text in multiple languages, making it a valuable resource for researchers, developers, and anyone interested in Islamic studies.
Dataset Details:

Size: 10K < n < 100K

Format: Text files for each language

Content: Complete translations of the Quran in each language

Usage:
This dataset is licensed under Creative Commons, allowing for free use, sharing, and adaptation. It can be used for various purposes, such as:
Text analysis and natural language processing tasks

Quranic research and studies

Development of Islamic apps and websites
Language translation and localization projects

Acknowledgement:

Please acknowledge the source of this dataset in any publication or project that uses it.",Medium,3.0
Translation,Abdulmohsena/Classic-Arabic-English-Language-Pairs,1.0,169.0,2025-06-12 00:56:10+00:00,none,0.0,4.46 MB,4676648.96,4.46 MB,4676648.96,26912,none,none,"---
language:
- ar
- en
task_categories:
- translation
dataset_info:
  features:
  - name: ar
    dtype: string
  - name: en
    dtype: string
  splits:
  - name: quran
    num_bytes: 1498476
    num_examples: 9474
  - name: hadith
    num_bytes: 1243667
    num_examples: 4107
  - name: books
    num_bytes: 5818972
    num_examples: 13331
  download_size: 4457485
  dataset_size: 8561115
configs:
- config_name: default
  data_files:
  - split: quran
    path: data/quran-*
  - split: hadith
    path: data/hadith-*
  - split: books
    path: data/books-*
---



### Source Data

- **ImruQays:** [Quran Parallel Texts](https://huggingface.co/datasets/ImruQays/Quran-Classical-Arabic-English-Parallel-texts)
- **Shamela Books:** [Shamela Open Library](https://shamela.ws/)

### Limitations

- **Translation Quality:** The translation of Shamela Books was done using Google Translate, which showed some inaccuracies with complex classical Arabic terms.",Low,1.0
Translation,liboaccn/MIT-10M,9.0,175.0,2025-03-13 03:09:43+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,"['https://aclanthology.org/2025.coling-main.346/', 'https://aclanthology.org/2025.coling-main.346/', 'https://aclanthology.org/2025.coling-main.346/"",']","---
license: apache-2.0
configs:
- config_name: default
  data_files:
  - split: train
    path:
    - train/EN.jsonl
    - train/ZH.jsonl
    - train/PT.jsonl
    - train/JA.jsonl
    - train/FR.jsonl
    - train/ES.jsonl
    - train/IT.jsonl
    - train/DE.jsonl
  - split: test
    path:
    - test/EN.jsonl
    - test/ZH.jsonl
    - test/PT.jsonl
    - test/JA.jsonl
    - test/FR.jsonl
    - test/ES.jsonl
    - test/IT.jsonl
    - test/DE.jsonl
language:
- en
- zh
- pt
- ja
- fr
- es
- it
- de
- ru
- ar
- ko
- tr
- th
- hi
task_categories:
- translation
- image-to-text
size_categories:
- 10M<n<100M
---

## MIT-10M

**Paper:** https://aclanthology.org/2025.coling-main.346/

**Introduction:**

Image Translation (IT) holds immense potential across diverse domains, enabling the translation of textual content within images into various languages.
However, existing datasets often suffer from limitations in scale, diversity, and quality, hindering the development and evaluation of IT models.
To address this issue, we introduce MIT-10M, a large-scale parallel corpus of multilingual image translation with over 10M image-text pairs derived from real-world data, which has undergone extensive data cleaning and multilingual translation validation. 
It contains 0.8M images in three sizes, 28 categories, tasks with three levels of difficulty and 14 languages image-text pairs, which is a considerable improvement on existing datasets. 




### Citation Information
You can cite our paper https://aclanthology.org/2025.coling-main.346/
```
@inproceedings{li-etal-2025-mit,
    title = ""{MIT}-10{M}: A Large Scale Parallel Corpus of Multilingual Image Translation"",
    author = ""Li, Bo  and Zhu, Shaolin and Wen, Lijie"",
    booktitle = ""Proceedings of the 31st International Conference on Computational Linguistics"",
    month = jan,
    year = ""2025"",
    address = ""Abu Dhabi, UAE"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2025.coling-main.346/"",
    pages = ""5154--5167""
}

```
",Medium,3.0
Translation,TutlaytAI/Multilang-to-Kabyle-Translation,1.0,50.0,2025-01-29 14:16:56+00:00,none,0.0,16.9 MB,17720934.4,16.9 MB,17720934.4,325085,none,none,"---
dataset_info:
  features:
  - name: Translated Sentence
    dtype: string
  - name: Kab Sentence
    dtype: string
  splits:
  - name: fr
    num_bytes: 13997080
    num_examples: 162117
  - name: en
    num_bytes: 10362834
    num_examples: 111893
  - name: ara
    num_bytes: 264201
    num_examples: 3360
  - name: ber
    num_bytes: 1462167
    num_examples: 18494
  - name: deu
    num_bytes: 74077
    num_examples: 1120
  - name: spa
    num_bytes: 2099729
    num_examples: 28101
  download_size: 16942474
  dataset_size: 28260088
configs:
- config_name: default
  data_files:
  - split: fr
    path: data/fr-*
  - split: en
    path: data/en-*
  - split: ara
    path: data/ara-*
  - split: ber
    path: data/ber-*
  - split: deu
    path: data/deu-*
  - split: spa
    path: data/spa-*
task_categories:
- translation
language:
- kab
- ber
- ar
- de
- es
- en
- fr
---",Low,1.0
Translation,robinhad/long_flores,0.0,24.0,2024-08-29 22:09:04+00:00,cc-by-sa-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2207.04672,none,"---
annotations_creators:
- found
language_creators:
- expert-generated
language:
- ace
- acm
- acq
- aeb
- af
- ajp
- ak
- als
- am
- apc
- ar
- ars
- ary
- arz
- as
- ast
- awa
- ayr
- azb
- azj
- ba
- bm
- ban
- be
- bem
- bn
- bho
- bjn
- bo
- bs
- bug
- bg
- ca
- ceb
- cs
- cjk
- ckb
- crh
- cy
- da
- de
- dik
- dyu
- dz
- el
- en
- eo
- et
- eu
- ee
- fo
- fj
- fi
- fon
- fr
- fur
- fuv
- gaz
- gd
- ga
- gl
- gn
- gu
- ht
- ha
- he
- hi
- hne
- hr
- hu
- hy
- ig
- ilo
- id
- is
- it
- jv
- ja
- kab
- kac
- kam
- kn
- ks
- ka
- kk
- kbp
- kea
- khk
- km
- ki
- rw
- ky
- kmb
- kmr
- knc
- kg
- ko
- lo
- lij
- li
- ln
- lt
- lmo
- ltg
- lb
- lua
- lg
- luo
- lus
- lvs
- mag
- mai
- ml
- mar
- min
- mk
- mt
- mni
- mos
- mi
- my
- nl
- nn
- nb
- npi
- nso
- nus
- ny
- oc
- ory
- pag
- pa
- pap
- pbt
- pes
- plt
- pl
- pt
- prs
- quy
- ro
- rn
- ru
- sg
- sa
- sat
- scn
- shn
- si
- sk
- sl
- sm
- sn
- sd
- so
- st
- es
- sc
- sr
- ss
- su
- sv
- swh
- szl
- ta
- taq
- tt
- te
- tg
- tl
- th
- ti
- tpi
- tn
- ts
- tk
- tum
- tr
- tw
- tzm
- ug
- uk
- umb
- ur
- uzn
- vec
- vi
- war
- wo
- xh
- ydd
- yo
- yue
- zh
- zsm
- zu
license:
- cc-by-sa-4.0
multilinguality:
- multilingual
- translation
size_categories:
- unknown
source_datasets:
- extended|flores
task_categories:
- text2text-generation
- translation
task_ids: []
paperswithcode_id: flores
pretty_name: flores200
language_details: ace_Arab, ace_Latn, acm_Arab, acq_Arab, aeb_Arab, afr_Latn, ajp_Arab,
  aka_Latn, amh_Ethi, apc_Arab, arb_Arab, ars_Arab, ary_Arab, arz_Arab, asm_Beng,
  ast_Latn, awa_Deva, ayr_Latn, azb_Arab, azj_Latn, bak_Cyrl, bam_Latn, ban_Latn,bel_Cyrl,
  bem_Latn, ben_Beng, bho_Deva, bjn_Arab, bjn_Latn, bod_Tibt, bos_Latn, bug_Latn,
  bul_Cyrl, cat_Latn, ceb_Latn, ces_Latn, cjk_Latn, ckb_Arab, crh_Latn, cym_Latn,
  dan_Latn, deu_Latn, dik_Latn, dyu_Latn, dzo_Tibt, ell_Grek, eng_Latn, epo_Latn,
  est_Latn, eus_Latn, ewe_Latn, fao_Latn, pes_Arab, fij_Latn, fin_Latn, fon_Latn,
  fra_Latn, fur_Latn, fuv_Latn, gla_Latn, gle_Latn, glg_Latn, grn_Latn, guj_Gujr,
  hat_Latn, hau_Latn, heb_Hebr, hin_Deva, hne_Deva, hrv_Latn, hun_Latn, hye_Armn,
  ibo_Latn, ilo_Latn, ind_Latn, isl_Latn, ita_Latn, jav_Latn, jpn_Jpan, kab_Latn,
  kac_Latn, kam_Latn, kan_Knda, kas_Arab, kas_Deva, kat_Geor, knc_Arab, knc_Latn,
  kaz_Cyrl, kbp_Latn, kea_Latn, khm_Khmr, kik_Latn, kin_Latn, kir_Cyrl, kmb_Latn,
  kon_Latn, kor_Hang, kmr_Latn, lao_Laoo, lvs_Latn, lij_Latn, lim_Latn, lin_Latn,
  lit_Latn, lmo_Latn, ltg_Latn, ltz_Latn, lua_Latn, lug_Latn, luo_Latn, lus_Latn,
  mag_Deva, mai_Deva, mal_Mlym, mar_Deva, min_Latn, mkd_Cyrl, plt_Latn, mlt_Latn,
  mni_Beng, khk_Cyrl, mos_Latn, mri_Latn, zsm_Latn, mya_Mymr, nld_Latn, nno_Latn,
  nob_Latn, npi_Deva, nso_Latn, nus_Latn, nya_Latn, oci_Latn, gaz_Latn, ory_Orya,
  pag_Latn, pan_Guru, pap_Latn, pol_Latn, por_Latn, prs_Arab, pbt_Arab, quy_Latn,
  ron_Latn, run_Latn, rus_Cyrl, sag_Latn, san_Deva, sat_Beng, scn_Latn, shn_Mymr,
  sin_Sinh, slk_Latn, slv_Latn, smo_Latn, sna_Latn, snd_Arab, som_Latn, sot_Latn,
  spa_Latn, als_Latn, srd_Latn, srp_Cyrl, ssw_Latn, sun_Latn, swe_Latn, swh_Latn,
  szl_Latn, tam_Taml, tat_Cyrl, tel_Telu, tgk_Cyrl, tgl_Latn, tha_Thai, tir_Ethi,
  taq_Latn, taq_Tfng, tpi_Latn, tsn_Latn, tso_Latn, tuk_Latn, tum_Latn, tur_Latn,
  twi_Latn, tzm_Tfng, uig_Arab, ukr_Cyrl, umb_Latn, urd_Arab, uzn_Latn, vec_Latn,
  vie_Latn, war_Latn, wol_Latn, xho_Latn, ydd_Hebr, yor_Latn, yue_Hant, zho_Hans,
  zho_Hant, zul_Latn
tags:
- conditional-text-generation
---
# Long-context version of Flores 200 dataset

This repository contains a long-context version of Flores 200 dataset. Below there is an original model card from Meta.


# Dataset Card for Flores 200

## Table of Contents

- [Dataset Card for Flores 200](#dataset-card-for-flores-200)
  - [Table of Contents](#table-of-contents)
  - [Dataset Description](#dataset-description)
    - [Dataset Summary](#dataset-summary)
    - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
    - [Languages](#languages)
  - [Dataset Structure](#dataset-structure)
    - [Data Instances](#data-instances)
    - [Data Fields](#data-fields)
    - [Data Splits](#data-splits)
    - [Dataset Creation](#dataset-creation)
  - [Additional Information](#additional-information)
    - [Dataset Curators](#dataset-curators)
    - [Licensing Information](#licensing-information)
    - [Citation Information](#citation-information)

## Dataset Description

- **Home:** [Flores](https://github.com/facebookresearch/flores)
- **Repository:** [Github](https://github.com/facebookresearch/flores)

### Dataset Summary

FLORES is a benchmark dataset for machine translation between English and low-resource languages.

>The creation of FLORES-200 doubles the existing language coverage of FLORES-101. 
Given the nature of the new languages, which have less standardization and require 
more specialized professional translations, the verification process became more complex. 
This required modifications to the translation workflow. FLORES-200 has several languages 
which were not translated from English. Specifically, several languages were translated 
from Spanish, French, Russian and Modern Standard Arabic. Moreover, FLORES-200 also 
includes two script alternatives for four languages. FLORES-200 consists of translations 
from 842 distinct web articles, totaling 3001 sentences. These sentences are divided 
into three splits: dev, devtest, and test (hidden). On average, sentences are approximately 
21 words long.

**Disclaimer**: *The Flores-200 dataset is hosted by the Facebook and licensed under the [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).
### Supported Tasks and Leaderboards
#### Multilingual Machine Translation
Refer to the [Dynabench leaderboard](https://dynabench.org/flores/Flores%20MT%20Evaluation%20(FULL)) for additional details on model evaluation on FLORES-101 in the context of the WMT2021 shared task on [Large-Scale Multilingual Machine Translation](http://www.statmt.org/wmt21/large-scale-multilingual-translation-task.html). Flores 200 is an extention of this.

### Languages
The dataset contains parallel sentences for 200 languages, as mentioned in the original [Github](https://github.com/facebookresearch/flores/blob/master/README.md) page for the project. Languages are identified with the ISO 639-3 code (e.g. `eng`, `fra`, `rus`) plus an additional code describing the script (e.g., ""eng_Latn"", ""ukr_Cyrl""). See [the webpage for code descriptions](https://github.com/facebookresearch/flores/blob/main/flores200/README.md).

Use the configuration `all` to access the full set of parallel sentences for all the available languages in a single command. 

Use a hyphenated pairing to get two langauges in one datapoint (e.g., ""eng_Latn-ukr_Cyrl"" will provide sentences in the format below).

## Dataset Structure
### Data Instances
A sample from the `dev` split for the Ukrainian language (`ukr_Cyrl` config) is provided below. All configurations have the same structure, and all sentences are aligned across configurations and splits.
```python
{
	'id': 1,
	'sentence': 'У понеділок, науковці зі Школи медицини Стенфордського університету оголосили про винайдення нового діагностичного інструменту, що може сортувати клітини за їх видами: це малесенький друкований чіп, який можна виготовити за допомогою стандартних променевих принтерів десь по одному центу США за штуку.',
	'URL': 'https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet',
	'domain': 'wikinews',
	'topic': 'health',
	'has_image': 0,
	'has_hyperlink': 0
}
```
When using a hyphenated pairing or using the `all` function, data will be presented as follows:

```python
{
    'id': 1, 
    'URL': 'https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet', 
    'domain': 'wikinews', 
    'topic': 'health', 
    'has_image': 0, 
    'has_hyperlink': 0, 
    'sentence_eng_Latn': 'On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each.', 
    'sentence_ukr_Cyrl': 'У понеділок, науковці зі Школи медицини Стенфордського університету оголосили про винайдення нового діагностичного інструменту, що може сортувати клітини за їх видами: це малесенький друкований чіп, який можна виготовити за допомогою стандартних променевих принтерів десь по одному центу США за штуку.'
}
```


The text is provided as-in the original dataset, without further preprocessing or tokenization.
### Data Fields
- `id`: Row number for the data entry, starting at 1.
- `sentence`: The full sentence in the specific language (may have _lang for pairings)
- `URL`: The URL for the English article from which the sentence was extracted.
- `domain`: The domain of the sentence.
- `topic`: The topic of the sentence.
- `has_image`: Whether the  original article contains an image.
- `has_hyperlink`: Whether the  sentence contains a hyperlink.
### Data Splits
|            config| `dev`| `devtest`|
|-----------------:|-----:|---------:|
|all configurations|   997|     1012:|
### Dataset Creation
Please refer to the original article [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) for additional information on dataset creation.
## Additional Information
### Dataset Curators
See paper for details.
### Licensing Information
Licensed with Creative Commons Attribution Share Alike 4.0. License available [here](https://creativecommons.org/licenses/by-sa/4.0/).
### Citation Information
Please cite the authors if you use these corpora in your work:
```bibtex
@article{nllb2022,
  author    = {NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi,  Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang},
  title     = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  year      = {2022}
}
```

Please also cite prior work that this dataset builds on:

```bibtex
@inproceedings{,
  title={The FLORES-101  Evaluation Benchmark for Low-Resource and Multilingual Machine Translation},
  author={Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc'Aurelio and Guzm\'{a}n, Francisco and Fan, Angela},
  year={2021}
}
```

```bibtex
@inproceedings{,
  title={Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English},
  author={Guzm\'{a}n, Francisco and Chen, Peng-Jen and Ott, Myle and Pino, Juan and Lample, Guillaume and Koehn, Philipp and Chaudhary, Vishrav and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1902.01382},
  year={2019}
}
```",High,4.0
Translation,DrAliGomaa/ar-eg-dataset,0.0,36.0,2025-05-14 22:57:43+00:00,apache-2.0,0.0,2.67 GB,2866890670.08,2.67 GB,2866890670.08,6370,none,none,"---
language:
- ar
license: apache-2.0
task_categories:
- automatic-speech-recognition
- translation
pretty_name: Egyptian Arabic
dataset_info:
  features:
  - name: audio_path
    dtype: string
  - name: sentence
    dtype: string
  - name: audio
    dtype: audio
  splits:
  - name: train
    num_bytes: 2211062502.6
    num_examples: 5100
  - name: validation
    num_bytes: 557950258.18
    num_examples: 1270
  download_size: 14910942412
  dataset_size: 2769012760.7799997
configs:
- config_name: default
  data_files:
  - split: validation
    path: data/validation-*
  - split: train
    path: data/train-*
---
An in-progress dataset for arabic-egyptian-dialect, specifically made from transcripton of DrAliGomaa videos on youtube.
Dr Ali Gomaa is a famous Egyptian Islamic Scholar and he was the mufti of Egypt from 2003-2013
* Link to his youtube channel: https://www.youtube.com/@DrAliGomaa
* Link to his page on facebook: https://www.facebook.com/DrAliGomaa",Medium,2.0
Translation,AnasAber/DoDA_sentences_darija_english,3.0,77.0,2024-09-06 17:39:42+00:00,none,2.0,2.93 MB,3072327.68,2.93 MB,3072327.68,45371,none,none,"---
dataset_info:
  features:
  - name: darija
    dtype: string
  - name: eng
    dtype: string
  - name: darija_ar
    dtype: string
  splits:
  - name: train
    num_bytes: 4828255.079553969
    num_examples: 45371
  download_size: 2933551
  dataset_size: 4828255.079553969
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- translation
- text-generation
language:
- en
- ar
tags:
- darija
- moroccan_arabic
pretty_name: darija english translation
size_categories:
- 10K<n<100K
---

Dataset of english/moroccan arabic translations, from DoDA dataset.",Low,1.0
Translation,openlanguagedata/flores_plus,46.0,3206.0,2025-03-23 20:17:57+00:00,cc-by-sa-4.0,0.0,117 MB,unknown,117 MB,unknown,867798,https://arxiv.org/abs/2207.04672,none,"---
annotations_creators:
- found
language_creators:
- expert-generated
language:
- ace
- acm
- acq
- aeb
- af
- ajp
- ak
- als
- am
- apc
- ar
- ars
- ary
- arz
- as
- ast
- awa
- ayr
- azb
- azj
- ba
- bm
- ban
- be
- bem
- bn
- bho
- bjn
- bo
- bs
- bug
- bg
- ca
- ceb
- cs
- cjk
- ckb
- crh
- cy
- da
- de
- dar
- dik
- dyu
- dz
- el
- en
- eo
- et
- eu
- ee
- fo
- fj
- fi
- fon
- fr
- fur
- fuv
- gaz
- gd
- ga
- gl
- gn
- gu
- ht
- ha
- he
- hi
- hne
- hr
- hu
- hy
- ig
- ilo
- id
- is
- it
- jv
- ja
- kab
- kac
- kam
- kn
- ks
- ka
- kk
- kbp
- kea
- khk
- km
- ki
- rw
- ky
- kmb
- kmr
- knc
- kg
- ko
- lo
- lij
- li
- ln
- lt
- lmo
- ltg
- lb
- lua
- lg
- luo
- lus
- lvs
- mag
- mai
- ml
- mar
- min
- mk
- mt
- mni
- mos
- mi
- my
- nl
- nn
- nb
- npi
- nso
- nus
- ny
- oc
- ory
- pag
- pa
- pap
- pbt
- pes
- plt
- pl
- pt
- prs
- quy
- ro
- rn
- ru
- sg
- sa
- sat
- scn
- shn
- si
- sk
- sl
- sm
- sn
- sd
- so
- st
- es
- sc
- sr
- ss
- su
- sv
- swh
- szl
- ta
- taq
- tt
- te
- tg
- tl
- th
- ti
- tpi
- tn
- ts
- tk
- tum
- tr
- tw
- tzm
- ug
- uk
- umb
- ur
- uzn
- vec
- vi
- war
- wo
- xh
- ydd
- yo
- yue
- zh
- zsm
- zu
license:
- cc-by-sa-4.0
multilinguality:
- multilingual
- translation
size_categories:
- unknown
source_datasets:
- extended|flores
task_categories:
- text2text-generation
- translation
task_ids: []
pretty_name: flores+
language_details: ace_Arab, ace_Latn, acm_Arab, acq_Arab, aeb_Arab, afr_Latn, ajp_Arab,
  aka_Latn, amh_Ethi, apc_Arab, arb_Arab, ars_Arab, ary_Arab, arz_Arab, asm_Beng,
  ast_Latn, awa_Deva, ayr_Latn, azb_Arab, azj_Latn, bak_Cyrl, bam_Latn, ban_Latn, bel_Cyrl,
  bem_Latn, ben_Beng, bho_Deva, bjn_Arab, bjn_Latn, bod_Tibt, bos_Latn, bug_Latn,
  bul_Cyrl, cat_Latn, ceb_Latn, ces_Latn, cjk_Latn, ckb_Arab, crh_Latn, cym_Latn,
  dan_Latn, deu_Latn, dar_Cyrl, dik_Latn, dyu_Latn, dzo_Tibt, ell_Grek, eng_Latn, epo_Latn,
  est_Latn, eus_Latn, ewe_Latn, fao_Latn, pes_Arab, fij_Latn, fin_Latn, fon_Latn,
  fra_Latn, fur_Latn, fuv_Latn, gla_Latn, gle_Latn, glg_Latn, grn_Latn, guj_Gujr,
  hat_Latn, hau_Latn, heb_Hebr, hin_Deva, hne_Deva, hrv_Latn, hun_Latn, hye_Armn,
  ibo_Latn, ilo_Latn, ind_Latn, isl_Latn, ita_Latn, jav_Latn, jpn_Jpan, kab_Latn,
  kac_Latn, kam_Latn, kan_Knda, kas_Arab, kas_Deva, kat_Geor, knc_Arab, knc_Latn,
  kaz_Cyrl, kbp_Latn, kea_Latn, khm_Khmr, kik_Latn, kin_Latn, kir_Cyrl, kmb_Latn,
  kon_Latn, kor_Hang, kmr_Latn, lao_Laoo, lvs_Latn, lij_Latn, lim_Latn, lin_Latn,
  lit_Latn, lmo_Latn, ltg_Latn, ltz_Latn, lua_Latn, lug_Latn, luo_Latn, lus_Latn,
  mag_Deva, mai_Deva, mal_Mlym, mar_Deva, min_Latn, mkd_Cyrl, plt_Latn, mlt_Latn,
  mni_Beng, khk_Cyrl, mos_Latn, mri_Latn, zsm_Latn, mya_Mymr, nld_Latn, nno_Latn,
  nob_Latn, npi_Deva, nso_Latn, nus_Latn, nya_Latn, oci_Latn, gaz_Latn, ory_Orya,
  pag_Latn, pan_Guru, pap_Latn, pol_Latn, por_Latn, prs_Arab, pbt_Arab, quy_Latn,
  ron_Latn, run_Latn, rus_Cyrl, sag_Latn, san_Deva, sat_Beng, scn_Latn, shn_Mymr,
  sin_Sinh, slk_Latn, slv_Latn, smo_Latn, sna_Latn, snd_Arab, som_Latn, sot_Latn,
  spa_Latn, als_Latn, srd_Latn, srp_Cyrl, ssw_Latn, sun_Latn, swe_Latn, swh_Latn,
  szl_Latn, tam_Taml, tat_Cyrl, tel_Telu, tgk_Cyrl, tgl_Latn, tha_Thai, tir_Ethi,
  taq_Latn, taq_Tfng, tpi_Latn, tsn_Latn, tso_Latn, tuk_Latn, tum_Latn, tur_Latn,
  twi_Latn, tzm_Tfng, uig_Arab, ukr_Cyrl, umb_Latn, urd_Arab, uzn_Latn, vec_Latn,
  vie_Latn, war_Latn, wol_Latn, xho_Latn, ydd_Hebr, yor_Latn, yue_Hant, zho_Hans,
  zho_Hant, zul_Latn
tags:
- text
configs:
- config_name: default
  data_files:
  - split: dev
    path: ""dev/*.parquet""
  - split: devtest
    path: ""devtest/*.parquet""
# all configs below were generated automatically based on the filenames of the dataset contents
- config_name: ace_Arab
  data_files:
  - path: dev/ace_Arab.parquet
    split: dev
  - path: devtest/ace_Arab.parquet
    split: devtest
- config_name: ace_Latn
  data_files:
  - path: dev/ace_Latn.parquet
    split: dev
  - path: devtest/ace_Latn.parquet
    split: devtest
- config_name: acm_Arab
  data_files:
  - path: dev/acm_Arab.parquet
    split: dev
  - path: devtest/acm_Arab.parquet
    split: devtest
- config_name: acq_Arab
  data_files:
  - path: dev/acq_Arab.parquet
    split: dev
  - path: devtest/acq_Arab.parquet
    split: devtest
- config_name: aeb_Arab
  data_files:
  - path: dev/aeb_Arab.parquet
    split: dev
  - path: devtest/aeb_Arab.parquet
    split: devtest
- config_name: afr_Latn
  data_files:
  - path: dev/afr_Latn.parquet
    split: dev
  - path: devtest/afr_Latn.parquet
    split: devtest
- config_name: als_Latn
  data_files:
  - path: dev/als_Latn.parquet
    split: dev
  - path: devtest/als_Latn.parquet
    split: devtest
- config_name: amh_Ethi
  data_files:
  - path: dev/amh_Ethi.parquet
    split: dev
  - path: devtest/amh_Ethi.parquet
    split: devtest
- config_name: apc_Arab_nort3139
  data_files:
  - path: dev/apc_Arab_nort3139.parquet
    split: dev
  - path: devtest/apc_Arab_nort3139.parquet
    split: devtest
- config_name: apc_Arab_sout3123
  data_files:
  - path: dev/apc_Arab_sout3123.parquet
    split: dev
  - path: devtest/apc_Arab_sout3123.parquet
    split: devtest
- config_name: arb_Arab
  data_files:
  - path: dev/arb_Arab.parquet
    split: dev
  - path: devtest/arb_Arab.parquet
    split: devtest
- config_name: arb_Latn
  data_files:
  - path: dev/arb_Latn.parquet
    split: dev
  - path: devtest/arb_Latn.parquet
    split: devtest
- config_name: arg_Latn
  data_files:
  - path: dev/arg_Latn.parquet
    split: dev
  - path: devtest/arg_Latn.parquet
    split: devtest
- config_name: ars_Arab
  data_files:
  - path: dev/ars_Arab.parquet
    split: dev
  - path: devtest/ars_Arab.parquet
    split: devtest
- config_name: ary_Arab
  data_files:
  - path: dev/ary_Arab.parquet
    split: dev
  - path: devtest/ary_Arab.parquet
    split: devtest
- config_name: arz_Arab
  data_files:
  - path: dev/arz_Arab.parquet
    split: dev
  - path: devtest/arz_Arab.parquet
    split: devtest
- config_name: asm_Beng
  data_files:
  - path: dev/asm_Beng.parquet
    split: dev
  - path: devtest/asm_Beng.parquet
    split: devtest
- config_name: ast_Latn
  data_files:
  - path: dev/ast_Latn.parquet
    split: dev
  - path: devtest/ast_Latn.parquet
    split: devtest
- config_name: awa_Deva
  data_files:
  - path: dev/awa_Deva.parquet
    split: dev
  - path: devtest/awa_Deva.parquet
    split: devtest
- config_name: ayr_Latn
  data_files:
  - path: dev/ayr_Latn.parquet
    split: dev
  - path: devtest/ayr_Latn.parquet
    split: devtest
- config_name: azb_Arab
  data_files:
  - path: dev/azb_Arab.parquet
    split: dev
  - path: devtest/azb_Arab.parquet
    split: devtest
- config_name: azj_Latn
  data_files:
  - path: dev/azj_Latn.parquet
    split: dev
  - path: devtest/azj_Latn.parquet
    split: devtest
- config_name: bak_Cyrl
  data_files:
  - path: dev/bak_Cyrl.parquet
    split: dev
  - path: devtest/bak_Cyrl.parquet
    split: devtest
- config_name: bam_Latn
  data_files:
  - path: dev/bam_Latn.parquet
    split: dev
  - path: devtest/bam_Latn.parquet
    split: devtest
- config_name: ban_Latn
  data_files:
  - path: dev/ban_Latn.parquet
    split: dev
  - path: devtest/ban_Latn.parquet
    split: devtest
- config_name: bel_Cyrl
  data_files:
  - path: dev/bel_Cyrl.parquet
    split: dev
  - path: devtest/bel_Cyrl.parquet
    split: devtest
- config_name: bem_Latn
  data_files:
  - path: dev/bem_Latn.parquet
    split: dev
  - path: devtest/bem_Latn.parquet
    split: devtest
- config_name: ben_Beng
  data_files:
  - path: dev/ben_Beng.parquet
    split: dev
  - path: devtest/ben_Beng.parquet
    split: devtest
- config_name: bho_Deva
  data_files:
  - path: dev/bho_Deva.parquet
    split: dev
  - path: devtest/bho_Deva.parquet
    split: devtest
- config_name: bjn_Arab
  data_files:
  - path: dev/bjn_Arab.parquet
    split: dev
  - path: devtest/bjn_Arab.parquet
    split: devtest
- config_name: bjn_Latn
  data_files:
  - path: dev/bjn_Latn.parquet
    split: dev
  - path: devtest/bjn_Latn.parquet
    split: devtest
- config_name: bod_Tibt
  data_files:
  - path: dev/bod_Tibt.parquet
    split: dev
  - path: devtest/bod_Tibt.parquet
    split: devtest
- config_name: bos_Latn
  data_files:
  - path: dev/bos_Latn.parquet
    split: dev
  - path: devtest/bos_Latn.parquet
    split: devtest
- config_name: brx_Deva
  data_files:
  - path: dev/brx_Deva.parquet
    split: dev
- config_name: bug_Latn
  data_files:
  - path: dev/bug_Latn.parquet
    split: dev
  - path: devtest/bug_Latn.parquet
    split: devtest
- config_name: bul_Cyrl
  data_files:
  - path: dev/bul_Cyrl.parquet
    split: dev
  - path: devtest/bul_Cyrl.parquet
    split: devtest
- config_name: cat_Latn
  data_files:
  - path: dev/cat_Latn.parquet
    split: dev
  - path: devtest/cat_Latn.parquet
    split: devtest
- config_name: cat_Latn_vale1252
  data_files:
  - path: devtest/cat_Latn_vale1252.parquet
    split: devtest
- config_name: ceb_Latn
  data_files:
  - path: dev/ceb_Latn.parquet
    split: dev
  - path: devtest/ceb_Latn.parquet
    split: devtest
- config_name: ces_Latn
  data_files:
  - path: dev/ces_Latn.parquet
    split: dev
  - path: devtest/ces_Latn.parquet
    split: devtest
- config_name: chv_Cyrl
  data_files:
  - path: dev/chv_Cyrl.parquet
    split: dev
  - path: devtest/chv_Cyrl.parquet
    split: devtest
- config_name: cjk_Latn
  data_files:
  - path: dev/cjk_Latn.parquet
    split: dev
  - path: devtest/cjk_Latn.parquet
    split: devtest
- config_name: ckb_Arab
  data_files:
  - path: dev/ckb_Arab.parquet
    split: dev
  - path: devtest/ckb_Arab.parquet
    split: devtest
- config_name: cmn_Hans
  data_files:
  - path: dev/cmn_Hans.parquet
    split: dev
  - path: devtest/cmn_Hans.parquet
    split: devtest
- config_name: cmn_Hant
  data_files:
  - path: dev/cmn_Hant.parquet
    split: dev
  - path: devtest/cmn_Hant.parquet
    split: devtest
- config_name: crh_Latn
  data_files:
  - path: dev/crh_Latn.parquet
    split: dev
  - path: devtest/crh_Latn.parquet
    split: devtest
- config_name: cym_Latn
  data_files:
  - path: dev/cym_Latn.parquet
    split: dev
  - path: devtest/cym_Latn.parquet
    split: devtest
- config_name: dan_Latn
  data_files:
  - path: dev/dan_Latn.parquet
    split: dev
  - path: devtest/dan_Latn.parquet
    split: devtest
- config_name: dar_Cyrl
  data_files:
  - path: dev/dar_Cyrl.parquet
    split: dev
- config_name: deu_Latn
  data_files:
  - path: dev/deu_Latn.parquet
    split: dev
  - path: devtest/deu_Latn.parquet
    split: devtest
- config_name: dgo_Deva
  data_files:
  - path: dev/dgo_Deva.parquet
    split: dev
- config_name: dik_Latn
  data_files:
  - path: dev/dik_Latn.parquet
    split: dev
  - path: devtest/dik_Latn.parquet
    split: devtest
- config_name: dyu_Latn
  data_files:
  - path: dev/dyu_Latn.parquet
    split: dev
  - path: devtest/dyu_Latn.parquet
    split: devtest
- config_name: dzo_Tibt
  data_files:
  - path: dev/dzo_Tibt.parquet
    split: dev
  - path: devtest/dzo_Tibt.parquet
    split: devtest
- config_name: ekk_Latn
  data_files:
  - path: dev/ekk_Latn.parquet
    split: dev
  - path: devtest/ekk_Latn.parquet
    split: devtest
- config_name: ell_Grek
  data_files:
  - path: dev/ell_Grek.parquet
    split: dev
  - path: devtest/ell_Grek.parquet
    split: devtest
- config_name: eng_Latn
  data_files:
  - path: dev/eng_Latn.parquet
    split: dev
  - path: devtest/eng_Latn.parquet
    split: devtest
- config_name: epo_Latn
  data_files:
  - path: dev/epo_Latn.parquet
    split: dev
  - path: devtest/epo_Latn.parquet
    split: devtest
- config_name: eus_Latn
  data_files:
  - path: dev/eus_Latn.parquet
    split: dev
  - path: devtest/eus_Latn.parquet
    split: devtest
- config_name: ewe_Latn
  data_files:
  - path: dev/ewe_Latn.parquet
    split: dev
  - path: devtest/ewe_Latn.parquet
    split: devtest
- config_name: fao_Latn
  data_files:
  - path: dev/fao_Latn.parquet
    split: dev
  - path: devtest/fao_Latn.parquet
    split: devtest
- config_name: fij_Latn
  data_files:
  - path: dev/fij_Latn.parquet
    split: dev
  - path: devtest/fij_Latn.parquet
    split: devtest
- config_name: fil_Latn
  data_files:
  - path: dev/fil_Latn.parquet
    split: dev
  - path: devtest/fil_Latn.parquet
    split: devtest
- config_name: fin_Latn
  data_files:
  - path: dev/fin_Latn.parquet
    split: dev
  - path: devtest/fin_Latn.parquet
    split: devtest
- config_name: fon_Latn
  data_files:
  - path: dev/fon_Latn.parquet
    split: dev
  - path: devtest/fon_Latn.parquet
    split: devtest
- config_name: fra_Latn
  data_files:
  - path: dev/fra_Latn.parquet
    split: dev
  - path: devtest/fra_Latn.parquet
    split: devtest
- config_name: fur_Latn
  data_files:
  - path: dev/fur_Latn.parquet
    split: dev
  - path: devtest/fur_Latn.parquet
    split: devtest
- config_name: fuv_Latn
  data_files:
  - path: dev/fuv_Latn.parquet
    split: dev
  - path: devtest/fuv_Latn.parquet
    split: devtest
- config_name: gaz_Latn
  data_files:
  - path: dev/gaz_Latn.parquet
    split: dev
  - path: devtest/gaz_Latn.parquet
    split: devtest
- config_name: gla_Latn
  data_files:
  - path: dev/gla_Latn.parquet
    split: dev
  - path: devtest/gla_Latn.parquet
    split: devtest
- config_name: gle_Latn
  data_files:
  - path: dev/gle_Latn.parquet
    split: dev
  - path: devtest/gle_Latn.parquet
    split: devtest
- config_name: glg_Latn
  data_files:
  - path: dev/glg_Latn.parquet
    split: dev
  - path: devtest/glg_Latn.parquet
    split: devtest
- config_name: gom_Deva
  data_files:
  - path: dev/gom_Deva.parquet
    split: dev
- config_name: gug_Latn
  data_files:
  - path: dev/gug_Latn.parquet
    split: dev
  - path: devtest/gug_Latn.parquet
    split: devtest
- config_name: guj_Gujr
  data_files:
  - path: dev/guj_Gujr.parquet
    split: dev
  - path: devtest/guj_Gujr.parquet
    split: devtest
- config_name: hat_Latn
  data_files:
  - path: dev/hat_Latn.parquet
    split: dev
  - path: devtest/hat_Latn.parquet
    split: devtest
- config_name: hau_Latn
  data_files:
  - path: dev/hau_Latn.parquet
    split: dev
  - path: devtest/hau_Latn.parquet
    split: devtest
- config_name: heb_Hebr
  data_files:
  - path: dev/heb_Hebr.parquet
    split: dev
  - path: devtest/heb_Hebr.parquet
    split: devtest
- config_name: hin_Deva
  data_files:
  - path: dev/hin_Deva.parquet
    split: dev
  - path: devtest/hin_Deva.parquet
    split: devtest
- config_name: hne_Deva
  data_files:
  - path: dev/hne_Deva.parquet
    split: dev
  - path: devtest/hne_Deva.parquet
    split: devtest
- config_name: hrv_Latn
  data_files:
  - path: dev/hrv_Latn.parquet
    split: dev
  - path: devtest/hrv_Latn.parquet
    split: devtest
- config_name: hun_Latn
  data_files:
  - path: dev/hun_Latn.parquet
    split: dev
  - path: devtest/hun_Latn.parquet
    split: devtest
- config_name: hye_Armn
  data_files:
  - path: dev/hye_Armn.parquet
    split: dev
  - path: devtest/hye_Armn.parquet
    split: devtest
- config_name: ibo_Latn
  data_files:
  - path: dev/ibo_Latn.parquet
    split: dev
  - path: devtest/ibo_Latn.parquet
    split: devtest
- config_name: ilo_Latn
  data_files:
  - path: dev/ilo_Latn.parquet
    split: dev
  - path: devtest/ilo_Latn.parquet
    split: devtest
- config_name: ind_Latn
  data_files:
  - path: dev/ind_Latn.parquet
    split: dev
  - path: devtest/ind_Latn.parquet
    split: devtest
- config_name: isl_Latn
  data_files:
  - path: dev/isl_Latn.parquet
    split: dev
  - path: devtest/isl_Latn.parquet
    split: devtest
- config_name: ita_Latn
  data_files:
  - path: dev/ita_Latn.parquet
    split: dev
  - path: devtest/ita_Latn.parquet
    split: devtest
- config_name: jav_Latn
  data_files:
  - path: dev/jav_Latn.parquet
    split: dev
  - path: devtest/jav_Latn.parquet
    split: devtest
- config_name: jpn_Jpan
  data_files:
  - path: dev/jpn_Jpan.parquet
    split: dev
  - path: devtest/jpn_Jpan.parquet
    split: devtest
- config_name: kaa_Latn
  data_files:
  - path: devtest/kaa_Latn.parquet
    split: devtest
- config_name: kab_Latn
  data_files:
  - path: dev/kab_Latn.parquet
    split: dev
  - path: devtest/kab_Latn.parquet
    split: devtest
- config_name: kac_Latn
  data_files:
  - path: dev/kac_Latn.parquet
    split: dev
  - path: devtest/kac_Latn.parquet
    split: devtest
- config_name: kam_Latn
  data_files:
  - path: dev/kam_Latn.parquet
    split: dev
  - path: devtest/kam_Latn.parquet
    split: devtest
- config_name: kan_Knda
  data_files:
  - path: dev/kan_Knda.parquet
    split: dev
  - path: devtest/kan_Knda.parquet
    split: devtest
- config_name: kas_Arab
  data_files:
  - path: dev/kas_Arab.parquet
    split: dev
  - path: devtest/kas_Arab.parquet
    split: devtest
- config_name: kas_Deva
  data_files:
  - path: dev/kas_Deva.parquet
    split: dev
  - path: devtest/kas_Deva.parquet
    split: devtest
- config_name: kat_Geor
  data_files:
  - path: dev/kat_Geor.parquet
    split: dev
  - path: devtest/kat_Geor.parquet
    split: devtest
- config_name: kaz_Cyrl
  data_files:
  - path: dev/kaz_Cyrl.parquet
    split: dev
  - path: devtest/kaz_Cyrl.parquet
    split: devtest
- config_name: kbp_Latn
  data_files:
  - path: dev/kbp_Latn.parquet
    split: dev
  - path: devtest/kbp_Latn.parquet
    split: devtest
- config_name: kea_Latn
  data_files:
  - path: dev/kea_Latn.parquet
    split: dev
  - path: devtest/kea_Latn.parquet
    split: devtest
- config_name: khk_Cyrl
  data_files:
  - path: dev/khk_Cyrl.parquet
    split: dev
  - path: devtest/khk_Cyrl.parquet
    split: devtest
- config_name: khm_Khmr
  data_files:
  - path: dev/khm_Khmr.parquet
    split: dev
  - path: devtest/khm_Khmr.parquet
    split: devtest
- config_name: kik_Latn
  data_files:
  - path: dev/kik_Latn.parquet
    split: dev
  - path: devtest/kik_Latn.parquet
    split: devtest
- config_name: kin_Latn
  data_files:
  - path: dev/kin_Latn.parquet
    split: dev
  - path: devtest/kin_Latn.parquet
    split: devtest
- config_name: kir_Cyrl
  data_files:
  - path: dev/kir_Cyrl.parquet
    split: dev
  - path: devtest/kir_Cyrl.parquet
    split: devtest
- config_name: kmb_Latn
  data_files:
  - path: dev/kmb_Latn.parquet
    split: dev
  - path: devtest/kmb_Latn.parquet
    split: devtest
- config_name: kmr_Latn
  data_files:
  - path: dev/kmr_Latn.parquet
    split: dev
  - path: devtest/kmr_Latn.parquet
    split: devtest
- config_name: knc_Arab
  data_files:
  - path: dev/knc_Arab.parquet
    split: dev
  - path: devtest/knc_Arab.parquet
    split: devtest
- config_name: knc_Latn
  data_files:
  - path: dev/knc_Latn.parquet
    split: dev
  - path: devtest/knc_Latn.parquet
    split: devtest
- config_name: kor_Hang
  data_files:
  - path: dev/kor_Hang.parquet
    split: dev
  - path: devtest/kor_Hang.parquet
    split: devtest
- config_name: ktu_Latn
  data_files:
  - path: dev/ktu_Latn.parquet
    split: dev
  - path: devtest/ktu_Latn.parquet
    split: devtest
- config_name: lao_Laoo
  data_files:
  - path: dev/lao_Laoo.parquet
    split: dev
  - path: devtest/lao_Laoo.parquet
    split: devtest
- config_name: lij_Latn
  data_files:
  - path: dev/lij_Latn.parquet
    split: dev
  - path: devtest/lij_Latn.parquet
    split: devtest
- config_name: lim_Latn
  data_files:
  - path: dev/lim_Latn.parquet
    split: dev
  - path: devtest/lim_Latn.parquet
    split: devtest
- config_name: lin_Latn
  data_files:
  - path: dev/lin_Latn.parquet
    split: dev
  - path: devtest/lin_Latn.parquet
    split: devtest
- config_name: lit_Latn
  data_files:
  - path: dev/lit_Latn.parquet
    split: dev
  - path: devtest/lit_Latn.parquet
    split: devtest
- config_name: lmo_Latn
  data_files:
  - path: dev/lmo_Latn.parquet
    split: dev
  - path: devtest/lmo_Latn.parquet
    split: devtest
- config_name: ltg_Latn
  data_files:
  - path: dev/ltg_Latn.parquet
    split: dev
  - path: devtest/ltg_Latn.parquet
    split: devtest
- config_name: ltz_Latn
  data_files:
  - path: dev/ltz_Latn.parquet
    split: dev
  - path: devtest/ltz_Latn.parquet
    split: devtest
- config_name: lua_Latn
  data_files:
  - path: dev/lua_Latn.parquet
    split: dev
  - path: devtest/lua_Latn.parquet
    split: devtest
- config_name: lug_Latn
  data_files:
  - path: dev/lug_Latn.parquet
    split: dev
  - path: devtest/lug_Latn.parquet
    split: devtest
- config_name: luo_Latn
  data_files:
  - path: dev/luo_Latn.parquet
    split: dev
  - path: devtest/luo_Latn.parquet
    split: devtest
- config_name: lus_Latn
  data_files:
  - path: dev/lus_Latn.parquet
    split: dev
  - path: devtest/lus_Latn.parquet
    split: devtest
- config_name: lvs_Latn
  data_files:
  - path: dev/lvs_Latn.parquet
    split: dev
  - path: devtest/lvs_Latn.parquet
    split: devtest
- config_name: mag_Deva
  data_files:
  - path: dev/mag_Deva.parquet
    split: dev
  - path: devtest/mag_Deva.parquet
    split: devtest
- config_name: mai_Deva
  data_files:
  - path: dev/mai_Deva.parquet
    split: dev
  - path: devtest/mai_Deva.parquet
    split: devtest
- config_name: mal_Mlym
  data_files:
  - path: dev/mal_Mlym.parquet
    split: dev
  - path: devtest/mal_Mlym.parquet
    split: devtest
- config_name: mar_Deva
  data_files:
  - path: dev/mar_Deva.parquet
    split: dev
  - path: devtest/mar_Deva.parquet
    split: devtest
- config_name: mhr_Cyrl
  data_files:
  - path: dev/mhr_Cyrl.parquet
    split: dev
- config_name: min_Arab
  data_files:
  - path: dev/min_Arab.parquet
    split: dev
  - path: devtest/min_Arab.parquet
    split: devtest
- config_name: min_Latn
  data_files:
  - path: dev/min_Latn.parquet
    split: dev
  - path: devtest/min_Latn.parquet
    split: devtest
- config_name: mkd_Cyrl
  data_files:
  - path: dev/mkd_Cyrl.parquet
    split: dev
  - path: devtest/mkd_Cyrl.parquet
    split: devtest
- config_name: mlt_Latn
  data_files:
  - path: dev/mlt_Latn.parquet
    split: dev
  - path: devtest/mlt_Latn.parquet
    split: devtest
- config_name: mni_Beng
  data_files:
  - path: dev/mni_Beng.parquet
    split: dev
  - path: devtest/mni_Beng.parquet
    split: devtest
- config_name: mni_Mtei
  data_files:
  - path: dev/mni_Mtei.parquet
    split: dev
- config_name: mos_Latn
  data_files:
  - path: dev/mos_Latn.parquet
    split: dev
  - path: devtest/mos_Latn.parquet
    split: devtest
- config_name: mri_Latn
  data_files:
  - path: dev/mri_Latn.parquet
    split: dev
  - path: devtest/mri_Latn.parquet
    split: devtest
- config_name: mya_Mymr
  data_files:
  - path: dev/mya_Mymr.parquet
    split: dev
  - path: devtest/mya_Mymr.parquet
    split: devtest
- config_name: myv_Cyrl
  data_files:
  - path: dev/myv_Cyrl.parquet
    split: dev
  - path: devtest/myv_Cyrl.parquet
    split: devtest
- config_name: nld_Latn
  data_files:
  - path: dev/nld_Latn.parquet
    split: dev
  - path: devtest/nld_Latn.parquet
    split: devtest
- config_name: nno_Latn
  data_files:
  - path: dev/nno_Latn.parquet
    split: dev
  - path: devtest/nno_Latn.parquet
    split: devtest
- config_name: nob_Latn
  data_files:
  - path: dev/nob_Latn.parquet
    split: dev
  - path: devtest/nob_Latn.parquet
    split: devtest
- config_name: npi_Deva
  data_files:
  - path: dev/npi_Deva.parquet
    split: dev
  - path: devtest/npi_Deva.parquet
    split: devtest
- config_name: nqo_Nkoo
  data_files:
  - path: dev/nqo_Nkoo.parquet
    split: dev
  - path: devtest/nqo_Nkoo.parquet
    split: devtest
- config_name: nso_Latn
  data_files:
  - path: dev/nso_Latn.parquet
    split: dev
  - path: devtest/nso_Latn.parquet
    split: devtest
- config_name: nus_Latn
  data_files:
  - path: dev/nus_Latn.parquet
    split: dev
  - path: devtest/nus_Latn.parquet
    split: devtest
- config_name: nya_Latn
  data_files:
  - path: dev/nya_Latn.parquet
    split: dev
  - path: devtest/nya_Latn.parquet
    split: devtest
- config_name: oci_Latn
  data_files:
  - path: dev/oci_Latn.parquet
    split: dev
  - path: devtest/oci_Latn.parquet
    split: devtest
- config_name: oci_Latn_aran1260
  data_files:
  - path: dev/oci_Latn_aran1260.parquet
    split: dev
  - path: devtest/oci_Latn_aran1260.parquet
    split: devtest
- config_name: ory_Orya
  data_files:
  - path: dev/ory_Orya.parquet
    split: dev
  - path: devtest/ory_Orya.parquet
    split: devtest
- config_name: pag_Latn
  data_files:
  - path: dev/pag_Latn.parquet
    split: dev
  - path: devtest/pag_Latn.parquet
    split: devtest
- config_name: pan_Guru
  data_files:
  - path: dev/pan_Guru.parquet
    split: dev
  - path: devtest/pan_Guru.parquet
    split: devtest
- config_name: pap_Latn
  data_files:
  - path: dev/pap_Latn.parquet
    split: dev
  - path: devtest/pap_Latn.parquet
    split: devtest
- config_name: pbt_Arab
  data_files:
  - path: dev/pbt_Arab.parquet
    split: dev
  - path: devtest/pbt_Arab.parquet
    split: devtest
- config_name: pes_Arab
  data_files:
  - path: dev/pes_Arab.parquet
    split: dev
  - path: devtest/pes_Arab.parquet
    split: devtest
- config_name: plt_Latn
  data_files:
  - path: dev/plt_Latn.parquet
    split: dev
  - path: devtest/plt_Latn.parquet
    split: devtest
- config_name: pol_Latn
  data_files:
  - path: dev/pol_Latn.parquet
    split: dev
  - path: devtest/pol_Latn.parquet
    split: devtest
- config_name: por_Latn
  data_files:
  - path: dev/por_Latn.parquet
    split: dev
  - path: devtest/por_Latn.parquet
    split: devtest
- config_name: prs_Arab
  data_files:
  - path: dev/prs_Arab.parquet
    split: dev
  - path: devtest/prs_Arab.parquet
    split: devtest
- config_name: quy_Latn
  data_files:
  - path: dev/quy_Latn.parquet
    split: dev
  - path: devtest/quy_Latn.parquet
    split: devtest
- config_name: ron_Latn
  data_files:
  - path: dev/ron_Latn.parquet
    split: dev
  - path: devtest/ron_Latn.parquet
    split: devtest
- config_name: run_Latn
  data_files:
  - path: dev/run_Latn.parquet
    split: dev
  - path: devtest/run_Latn.parquet
    split: devtest
- config_name: rus_Cyrl
  data_files:
  - path: dev/rus_Cyrl.parquet
    split: dev
  - path: devtest/rus_Cyrl.parquet
    split: devtest
- config_name: sag_Latn
  data_files:
  - path: dev/sag_Latn.parquet
    split: dev
  - path: devtest/sag_Latn.parquet
    split: devtest
- config_name: san_Deva
  data_files:
  - path: dev/san_Deva.parquet
    split: dev
  - path: devtest/san_Deva.parquet
    split: devtest
- config_name: sat_Olck
  data_files:
  - path: dev/sat_Olck.parquet
    split: dev
  - path: devtest/sat_Olck.parquet
    split: devtest
- config_name: scn_Latn
  data_files:
  - path: dev/scn_Latn.parquet
    split: dev
  - path: devtest/scn_Latn.parquet
    split: devtest
- config_name: shn_Mymr
  data_files:
  - path: dev/shn_Mymr.parquet
    split: dev
  - path: devtest/shn_Mymr.parquet
    split: devtest
- config_name: sin_Sinh
  data_files:
  - path: dev/sin_Sinh.parquet
    split: dev
  - path: devtest/sin_Sinh.parquet
    split: devtest
- config_name: slk_Latn
  data_files:
  - path: dev/slk_Latn.parquet
    split: dev
  - path: devtest/slk_Latn.parquet
    split: devtest
- config_name: slv_Latn
  data_files:
  - path: dev/slv_Latn.parquet
    split: dev
  - path: devtest/slv_Latn.parquet
    split: devtest
- config_name: smo_Latn
  data_files:
  - path: dev/smo_Latn.parquet
    split: dev
  - path: devtest/smo_Latn.parquet
    split: devtest
- config_name: sna_Latn
  data_files:
  - path: dev/sna_Latn.parquet
    split: dev
  - path: devtest/sna_Latn.parquet
    split: devtest
- config_name: snd_Arab
  data_files:
  - path: dev/snd_Arab.parquet
    split: dev
  - path: devtest/snd_Arab.parquet
    split: devtest
- config_name: snd_Deva
  data_files:
  - path: dev/snd_Deva.parquet
    split: dev
- config_name: som_Latn
  data_files:
  - path: dev/som_Latn.parquet
    split: dev
  - path: devtest/som_Latn.parquet
    split: devtest
- config_name: sot_Latn
  data_files:
  - path: dev/sot_Latn.parquet
    split: dev
  - path: devtest/sot_Latn.parquet
    split: devtest
- config_name: spa_Latn
  data_files:
  - path: dev/spa_Latn.parquet
    split: dev
  - path: devtest/spa_Latn.parquet
    split: devtest
- config_name: srd_Latn
  data_files:
  - path: dev/srd_Latn.parquet
    split: dev
  - path: devtest/srd_Latn.parquet
    split: devtest
- config_name: srp_Cyrl
  data_files:
  - path: dev/srp_Cyrl.parquet
    split: dev
  - path: devtest/srp_Cyrl.parquet
    split: devtest
- config_name: ssw_Latn
  data_files:
  - path: dev/ssw_Latn.parquet
    split: dev
  - path: devtest/ssw_Latn.parquet
    split: devtest
- config_name: sun_Latn
  data_files:
  - path: dev/sun_Latn.parquet
    split: dev
  - path: devtest/sun_Latn.parquet
    split: devtest
- config_name: swe_Latn
  data_files:
  - path: dev/swe_Latn.parquet
    split: dev
  - path: devtest/swe_Latn.parquet
    split: devtest
- config_name: swh_Latn
  data_files:
  - path: dev/swh_Latn.parquet
    split: dev
  - path: devtest/swh_Latn.parquet
    split: devtest
- config_name: szl_Latn
  data_files:
  - path: dev/szl_Latn.parquet
    split: dev
  - path: devtest/szl_Latn.parquet
    split: devtest
- config_name: tam_Taml
  data_files:
  - path: dev/tam_Taml.parquet
    split: dev
  - path: devtest/tam_Taml.parquet
    split: devtest
- config_name: taq_Latn
  data_files:
  - path: dev/taq_Latn.parquet
    split: dev
  - path: devtest/taq_Latn.parquet
    split: devtest
- config_name: taq_Tfng
  data_files:
  - path: dev/taq_Tfng.parquet
    split: dev
  - path: devtest/taq_Tfng.parquet
    split: devtest
- config_name: tat_Cyrl
  data_files:
  - path: dev/tat_Cyrl.parquet
    split: dev
  - path: devtest/tat_Cyrl.parquet
    split: devtest
- config_name: tel_Telu
  data_files:
  - path: dev/tel_Telu.parquet
    split: dev
  - path: devtest/tel_Telu.parquet
    split: devtest
- config_name: tgk_Cyrl
  data_files:
  - path: dev/tgk_Cyrl.parquet
    split: dev
  - path: devtest/tgk_Cyrl.parquet
    split: devtest
- config_name: tha_Thai
  data_files:
  - path: dev/tha_Thai.parquet
    split: dev
  - path: devtest/tha_Thai.parquet
    split: devtest
- config_name: tir_Ethi
  data_files:
  - path: dev/tir_Ethi.parquet
    split: dev
  - path: devtest/tir_Ethi.parquet
    split: devtest
- config_name: tpi_Latn
  data_files:
  - path: dev/tpi_Latn.parquet
    split: dev
  - path: devtest/tpi_Latn.parquet
    split: devtest
- config_name: tsn_Latn
  data_files:
  - path: dev/tsn_Latn.parquet
    split: dev
  - path: devtest/tsn_Latn.parquet
    split: devtest
- config_name: tso_Latn
  data_files:
  - path: dev/tso_Latn.parquet
    split: dev
  - path: devtest/tso_Latn.parquet
    split: devtest
- config_name: tuk_Latn
  data_files:
  - path: dev/tuk_Latn.parquet
    split: dev
  - path: devtest/tuk_Latn.parquet
    split: devtest
- config_name: tum_Latn
  data_files:
  - path: dev/tum_Latn.parquet
    split: dev
  - path: devtest/tum_Latn.parquet
    split: devtest
- config_name: tur_Latn
  data_files:
  - path: dev/tur_Latn.parquet
    split: dev
  - path: devtest/tur_Latn.parquet
    split: devtest
- config_name: twi_Latn_akua1239
  data_files:
  - path: dev/twi_Latn_akua1239.parquet
    split: dev
  - path: devtest/twi_Latn_akua1239.parquet
    split: devtest
- config_name: twi_Latn_asan1239
  data_files:
  - path: dev/twi_Latn_asan1239.parquet
    split: dev
  - path: devtest/twi_Latn_asan1239.parquet
    split: devtest
- config_name: tyv_Cyrl
  data_files:
  - path: dev/tyv_Cyrl.parquet
    split: dev
  - path: devtest/tyv_Cyrl.parquet
    split: devtest
- config_name: uig_Arab
  data_files:
  - path: dev/uig_Arab.parquet
    split: dev
  - path: devtest/uig_Arab.parquet
    split: devtest
- config_name: ukr_Cyrl
  data_files:
  - path: dev/ukr_Cyrl.parquet
    split: dev
  - path: devtest/ukr_Cyrl.parquet
    split: devtest
- config_name: umb_Latn
  data_files:
  - path: dev/umb_Latn.parquet
    split: dev
  - path: devtest/umb_Latn.parquet
    split: devtest
- config_name: urd_Arab
  data_files:
  - path: dev/urd_Arab.parquet
    split: dev
  - path: devtest/urd_Arab.parquet
    split: devtest
- config_name: uzn_Latn
  data_files:
  - path: dev/uzn_Latn.parquet
    split: dev
  - path: devtest/uzn_Latn.parquet
    split: devtest
- config_name: vec_Latn
  data_files:
  - path: dev/vec_Latn.parquet
    split: dev
  - path: devtest/vec_Latn.parquet
    split: devtest
- config_name: vie_Latn
  data_files:
  - path: dev/vie_Latn.parquet
    split: dev
  - path: devtest/vie_Latn.parquet
    split: devtest
- config_name: vmw_Latn
  data_files:
  - path: dev/vmw_Latn.parquet
    split: dev
  - path: devtest/vmw_Latn.parquet
    split: devtest
- config_name: war_Latn
  data_files:
  - path: dev/war_Latn.parquet
    split: dev
  - path: devtest/war_Latn.parquet
    split: devtest
- config_name: wol_Latn
  data_files:
  - path: dev/wol_Latn.parquet
    split: dev
  - path: devtest/wol_Latn.parque",High,4.0
Translation,espnet/floras,10.0,5169.0,2024-11-29 20:12:19+00:00,cc-by-3.0,0.0,UNKNOWN,UNKNOWN,10.4 GB,11166914969.6,2044,none,none,"---
license: cc-by-3.0
dataset_info:
- config_name: monolingual
  features:
  - name: id
    dtype: string
  - name: language
    dtype: string
  - name: score
    dtype: string
  - name: audio
    dtype:
      audio:
        sampling_rate: 16000
  - name: text
    dtype: string
  - name: summary
    dtype: string
  - name: translation
    dtype: string
  splits:
  - name: train
    num_bytes: 2250087924
    num_examples: 50814
  - name: dev
    num_bytes: 3730403898.0
    num_examples: 81
  - name: test
    num_bytes: 6882657690.0
    num_examples: 116
  download_size: 27806858743
  dataset_size: 21226123202.0
- config_name: multilingual
  features:
  - name: id
    dtype: string
  - name: language
    dtype: string
  - name: score
    dtype: string
  - name: audio
    dtype:
      audio:
        sampling_rate: 16000
  - name: text
    dtype: string
  - name: summary
    dtype: string
  - name: translation
    dtype: string
  splits:
  - name: dev
    num_bytes: 49979924635.32
    num_examples: 1154
  - name: test
    num_bytes: 56817933774.28
    num_examples: 1188
  download_size: 102717641464
  dataset_size: 106797858409.6
configs:
- config_name: monolingual
  data_files:
  - split: train
    path: monolingual/train-*
  - split: dev
    path: monolingual/dev-*
  - split: test
    path: monolingual/test-*
- config_name: multilingual
  data_files:
  - split: dev
    path: multilingual/dev-*
  - split: test
    path: multilingual/test-*
task_categories:
- automatic-speech-recognition
- translation
- summarization
language:
- en
- es
- fr
- de
- nl
- it
- pt
- hu
- fi
- el
- ca
- eo
- et
- da
- la
- sv
- cy
- gl
- ru
- pl
- uk
- ro
- cs
- sl
- sk
- hr
- bg
- bs
- ka
- tr
- fa
- ar
- uz
- az
- ku
- ky
- hi
- ta
- ur
- bn
- id
- vi
- th
- mi
- ms
- ja
- zh
---

# FLORAS

FLORAS is a 50-language benchmark **F**or **LO**ng-form **R**ecognition **A**nd **S**ummarization of spoken language. 
The goal of FLORAS is to create a more realistic benchmarking environment for speech recognition, translation, and summarization models. 
Unlike typical academic benchmarks like LibriSpeech and FLEURS that uses pre-segmented single-speaker read-speech, FLORAS tests the capabilities of models on raw long-form conversational audio, which can have one or many speakers.

To encourage research in multi-tasking, FLORAS provides 1-way to 3-way parallel data for long-form Automatic Speech Recognition (ASR), long-form X-to-EN Speech Translation (ST), and Speech Summarization (SSUM).
This means that some samples only have paired speech and transcripts, while others may have paired speech, transcripts, translations and/or summaries.
In total, FLORAS contains roughly 32,000 hours of raw audio.

## Dataset Creation

FLORAS is derived from [YODAS](https://huggingface.co/datasets/espnet/yodas), a large multilingual crawl of YouTube videos and their subtitles. 
Since the raw crawl of YODAS is too noisy for direct training in many settings, we filter out most of the data using CTC alignment scores.
The translations and summaries are obtained via pseudo-labelling using Google's [Gemini Flash](https://deepmind.google/technologies/gemini/flash/).
Our translators then filtered out or corrected faulty pseudo-labels in the test set. We did not perform filtering on the training/development sets.

## Dataset Structure

FLORAS is organized into two subsets, each with data splits for training, validation, and testing.
```
FLORAS
- monolingual
  - train
  - dev
  - test
- multilingual
  - train
  - dev
  - test_unverified
  - test_verified
```
The monolingual subset contains English-only data. The multilingual subset contains the data for the other 49 languages.

The multilingual subset contains two test sets: `test_unverified` and `test_verified`. 

Verified languages are those that have had professional translators and/or native speakers verify the translation/summary pseudo-labels.

Unverified languages are those that did not go through this process (See below to determine which languages have been verified).

## Data Fields

Each subset/split has the following data fields:
- **id** (str): sample ID of the speech.
- **language** (str): ISO3 language code of the speech.
- **score** (float): CTC alignment score of the video. Closer to 0 is better.
- **audio** (dict):  Audio object including loaded audio array, sampling rate and path to audio.
- **text** (str): Text transcription.
- **translation** (str): English translation of transcript, if available. If not available, will yield the empty string.
- **summary** (str): Summary of transcript, if available. If not available, will yield the empty string.

Since FLORAS only supports X-to-EN translation, the `translation` field is always empty for samples in the `monolingual` subset.

## Languages

The languages in FLORAS by region are as follows:
- **Western Europe**: _English_, Spanish, German, French, Italian, Portuguese, Dutch, Basque, Hungarian, Finnish, Greek, Catalan, Esperanto, Danish, Latin, Swedish, Galician, Welsh
- **Eastern Europe**: Russian, Polish, Ukrainian, Romanian, Czech, Estonian, Slovak, Slovenian, Croatian, Serbian, Bulgarian, Bosnian, Georgian
- **Central-Asia/Middle-East/North-Africa**: Turkish, Persian, Arabic, Uzbek, Kurdish, Kyrgyz, Azerbaijani
- **South-Asia**: Hindi, Tamil, Urdu, Bengali
- **South-East Asia**: Indonesian, Vietnamese, Thai, Malay, Maori
- **East Asia**: _Japanese_, _Mandarin Chinese_

_Italicized_ languages have been verified by professional translators and/or native speakers for the translation/summary pseudo-labels.

**If a language that you speak is not verified and you would like to donate some time to check the pseudo-label quality, please reach out to us!**",Medium,2.0
Translation,neurlang/phonetic,4.0,23.0,2025-03-05 22:17:33+00:00,cc,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
language:
- af
- am
- ar
- az
- be
- bn
- my
- ceb
- ce
- zh
- cs
- da
- nl
- dz
- en
- eo
- fa
- fi
- fr
- de
- el
- gu
- ha
- he
- hi
- hu
- is
- id
- tts
- it
- jam
- ja
- jv
- kk
- ko
- lb
- mk
- ml
- ms
- mt
- mr
- mn
- ne
- 'no'
- ps
- pl
- pt
- pa
- ro
- ru
- sk
- es
- sw
- sv
- ta
- te
- th
- bo
- tr
- uk
- ur
- ug
- vi
- zu
- hy
- eu
- bg
- ca
- ny
- hr
- et
- gl
- ka
- km
- lo
- lv
- lt
- sr
- tl
- yo
tags:
- ipa
- phonetic
task_categories:
- translation
- text-classification
- token-classification
pretty_name: IPA Phonetic Dataset
size_categories:
- 1M<n<10M
license: cc
---
# Phonetic IPA Dataset for 85+ languages

**Download the dataset**: [https://github.com/neurlang/dataset]

**Use our model trained on the dataset**: [https://www.hashtron.cloud]


# Licensing information:

* **MIT** Original data for the 15 languages taken from [gruut databases](https://github.com/rhasspy/gruut)
* **MIT** To this the data for the 31 languages were added [ipa dict files](https://github.com/open-dict-data/ipa-dict/)
* **CC0: Public Domain** Chinese/Mandarin-IPA language sentence pairs were generated:
  - from the chinese sentences taken from dataset [from kaggle](https://www.kaggle.com/datasets/concyclics/machine-translation-chinese-and-english)
  - based on the above dictionary and MISTRAL-nemo made IPA dictionary which was paired with chinese sentences to ipa sentences using string substitution.
* **Mozilla Public License 2.0** Chinese/Mandarin Extra missing phrases added-on from Mozilla Common Voice 19.
* **Apache-2.0** Wikipron data were added for selected large languages [from wikipron data](https://github.com/CUNY-CL/wikipron/tree/master/data/scrape/tsv)
* **cc-by-nc-4.0** Tibetan taken from [billingsmoore](https://huggingface.co/datasets/billingsmoore/tibetan-phonetic-transliteration-dataset)
* **cc-by-nc-4.0** Tibetan added more data at [billingsmoore](https://huggingface.co/datasets/billingsmoore/tibetan-to-english-translation-dataset)
* **MIT/Apache2** Slovak language is mostly self made, I hereby dedicate it under MIT/Apache2
* **CC-BY-SA 3.0**: Text in Japanese corpus is licensed as follows. The text data were modified and pronunciation information is added. basic5000 is as follows:
  - wikipedia [wikipedia](https://ja.wikipedia.org/) CC-BY-SA 3.0
  - TANAKA corpus [Tanaka_Corpus](http://www.edrdg.org/wiki/index.php/Tanaka_Corpus) CC-BY 2.0
  - JSUT (Japanese speech corpus of Saruwatari-lab., University of Tokyo) [JSUT](https://sites.google.com/site/shinnosuketakamichi/publication/jsut). CC-BY-SA 4.0
* **Mozilla Public License 2.0** Japanese City Names added-on from Mozilla Common Voice 19.
",Low,1.0
Translation,neulab/PangeaBench-flores,0.0,19.0,2024-11-01 18:31:37+00:00,cc-by-sa-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2207.04672,none,"---
annotations_creators:
- found
language_creators:
- expert-generated
language:
- ace
- acm
- acq
- aeb
- af
- ajp
- ak
- als
- am
- apc
- ar
- ars
- ary
- arz
- as
- ast
- awa
- ayr
- azb
- azj
- ba
- bm
- ban
- be
- bem
- bn
- bho
- bjn
- bo
- bs
- bug
- bg
- ca
- ceb
- cs
- cjk
- ckb
- crh
- cy
- da
- de
- dik
- dyu
- dz
- el
- en
- eo
- et
- eu
- ee
- fo
- fj
- fi
- fon
- fr
- fur
- fuv
- gaz
- gd
- ga
- gl
- gn
- gu
- ht
- ha
- he
- hi
- hne
- hr
- hu
- hy
- ig
- ilo
- id
- is
- it
- jv
- ja
- kab
- kac
- kam
- kn
- ks
- ka
- kk
- kbp
- kea
- khk
- km
- ki
- rw
- ky
- kmb
- kmr
- knc
- kg
- ko
- lo
- lij
- li
- ln
- lt
- lmo
- ltg
- lb
- lua
- lg
- luo
- lus
- lvs
- mag
- mai
- ml
- mar
- min
- mk
- mt
- mni
- mos
- mi
- my
- nl
- nn
- nb
- npi
- nso
- nus
- ny
- oc
- ory
- pag
- pa
- pap
- pbt
- pes
- plt
- pl
- pt
- prs
- quy
- ro
- rn
- ru
- sg
- sa
- sat
- scn
- shn
- si
- sk
- sl
- sm
- sn
- sd
- so
- st
- es
- sc
- sr
- ss
- su
- sv
- swh
- szl
- ta
- taq
- tt
- te
- tg
- tl
- th
- ti
- tpi
- tn
- ts
- tk
- tum
- tr
- tw
- tzm
- ug
- uk
- umb
- ur
- uzn
- vec
- vi
- war
- wo
- xh
- ydd
- yo
- yue
- zh
- zsm
- zu
license:
- cc-by-sa-4.0
multilinguality:
- multilingual
- translation
size_categories:
- unknown
source_datasets:
- extended|flores
task_categories:
- text2text-generation
- translation
task_ids: []
paperswithcode_id: flores
pretty_name: flores200
language_details: ace_Arab, ace_Latn, acm_Arab, acq_Arab, aeb_Arab, afr_Latn, ajp_Arab,
  aka_Latn, amh_Ethi, apc_Arab, arb_Arab, ars_Arab, ary_Arab, arz_Arab, asm_Beng,
  ast_Latn, awa_Deva, ayr_Latn, azb_Arab, azj_Latn, bak_Cyrl, bam_Latn, ban_Latn,bel_Cyrl,
  bem_Latn, ben_Beng, bho_Deva, bjn_Arab, bjn_Latn, bod_Tibt, bos_Latn, bug_Latn,
  bul_Cyrl, cat_Latn, ceb_Latn, ces_Latn, cjk_Latn, ckb_Arab, crh_Latn, cym_Latn,
  dan_Latn, deu_Latn, dik_Latn, dyu_Latn, dzo_Tibt, ell_Grek, eng_Latn, epo_Latn,
  est_Latn, eus_Latn, ewe_Latn, fao_Latn, pes_Arab, fij_Latn, fin_Latn, fon_Latn,
  fra_Latn, fur_Latn, fuv_Latn, gla_Latn, gle_Latn, glg_Latn, grn_Latn, guj_Gujr,
  hat_Latn, hau_Latn, heb_Hebr, hin_Deva, hne_Deva, hrv_Latn, hun_Latn, hye_Armn,
  ibo_Latn, ilo_Latn, ind_Latn, isl_Latn, ita_Latn, jav_Latn, jpn_Jpan, kab_Latn,
  kac_Latn, kam_Latn, kan_Knda, kas_Arab, kas_Deva, kat_Geor, knc_Arab, knc_Latn,
  kaz_Cyrl, kbp_Latn, kea_Latn, khm_Khmr, kik_Latn, kin_Latn, kir_Cyrl, kmb_Latn,
  kon_Latn, kor_Hang, kmr_Latn, lao_Laoo, lvs_Latn, lij_Latn, lim_Latn, lin_Latn,
  lit_Latn, lmo_Latn, ltg_Latn, ltz_Latn, lua_Latn, lug_Latn, luo_Latn, lus_Latn,
  mag_Deva, mai_Deva, mal_Mlym, mar_Deva, min_Latn, mkd_Cyrl, plt_Latn, mlt_Latn,
  mni_Beng, khk_Cyrl, mos_Latn, mri_Latn, zsm_Latn, mya_Mymr, nld_Latn, nno_Latn,
  nob_Latn, npi_Deva, nso_Latn, nus_Latn, nya_Latn, oci_Latn, gaz_Latn, ory_Orya,
  pag_Latn, pan_Guru, pap_Latn, pol_Latn, por_Latn, prs_Arab, pbt_Arab, quy_Latn,
  ron_Latn, run_Latn, rus_Cyrl, sag_Latn, san_Deva, sat_Beng, scn_Latn, shn_Mymr,
  sin_Sinh, slk_Latn, slv_Latn, smo_Latn, sna_Latn, snd_Arab, som_Latn, sot_Latn,
  spa_Latn, als_Latn, srd_Latn, srp_Cyrl, ssw_Latn, sun_Latn, swe_Latn, swh_Latn,
  szl_Latn, tam_Taml, tat_Cyrl, tel_Telu, tgk_Cyrl, tgl_Latn, tha_Thai, tir_Ethi,
  taq_Latn, taq_Tfng, tpi_Latn, tsn_Latn, tso_Latn, tuk_Latn, tum_Latn, tur_Latn,
  twi_Latn, tzm_Tfng, uig_Arab, ukr_Cyrl, umb_Latn, urd_Arab, uzn_Latn, vec_Latn,
  vie_Latn, war_Latn, wol_Latn, xho_Latn, ydd_Hebr, yor_Latn, yue_Hant, zho_Hans,
  zho_Hant, zul_Latn
tags:
- conditional-text-generation
---

# Dataset Card for Flores 200

## Table of Contents

- [Dataset Card for Flores 200](#dataset-card-for-flores-200)
  - [Table of Contents](#table-of-contents)
  - [Dataset Description](#dataset-description)
    - [Dataset Summary](#dataset-summary)
    - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
    - [Languages](#languages)
  - [Dataset Structure](#dataset-structure)
    - [Data Instances](#data-instances)
    - [Data Fields](#data-fields)
    - [Data Splits](#data-splits)
    - [Dataset Creation](#dataset-creation)
  - [Additional Information](#additional-information)
    - [Dataset Curators](#dataset-curators)
    - [Licensing Information](#licensing-information)
    - [Citation Information](#citation-information)

## Dataset Description

- **Home:** [Flores](https://github.com/facebookresearch/flores)
- **Repository:** [Github](https://github.com/facebookresearch/flores)

### Dataset Summary

FLORES is a benchmark dataset for machine translation between English and low-resource languages.

>The creation of FLORES-200 doubles the existing language coverage of FLORES-101. 
Given the nature of the new languages, which have less standardization and require 
more specialized professional translations, the verification process became more complex. 
This required modifications to the translation workflow. FLORES-200 has several languages 
which were not translated from English. Specifically, several languages were translated 
from Spanish, French, Russian and Modern Standard Arabic. Moreover, FLORES-200 also 
includes two script alternatives for four languages. FLORES-200 consists of translations 
from 842 distinct web articles, totaling 3001 sentences. These sentences are divided 
into three splits: dev, devtest, and test (hidden). On average, sentences are approximately 
21 words long.

**Disclaimer**: *The Flores-200 dataset is hosted by the Facebook and licensed under the [Creative Commons Attribution-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-sa/4.0/).
### Supported Tasks and Leaderboards
#### Multilingual Machine Translation
Refer to the [Dynabench leaderboard](https://dynabench.org/flores/Flores%20MT%20Evaluation%20(FULL)) for additional details on model evaluation on FLORES-101 in the context of the WMT2021 shared task on [Large-Scale Multilingual Machine Translation](http://www.statmt.org/wmt21/large-scale-multilingual-translation-task.html). Flores 200 is an extention of this.

### Languages
The dataset contains parallel sentences for 200 languages, as mentioned in the original [Github](https://github.com/facebookresearch/flores/blob/master/README.md) page for the project. Languages are identified with the ISO 639-3 code (e.g. `eng`, `fra`, `rus`) plus an additional code describing the script (e.g., ""eng_Latn"", ""ukr_Cyrl""). See [the webpage for code descriptions](https://github.com/facebookresearch/flores/blob/main/flores200/README.md).

Use the configuration `all` to access the full set of parallel sentences for all the available languages in a single command. 

Use a hyphenated pairing to get two langauges in one datapoint (e.g., ""eng_Latn-ukr_Cyrl"" will provide sentences in the format below).

## Dataset Structure
### Data Instances
A sample from the `dev` split for the Ukrainian language (`ukr_Cyrl` config) is provided below. All configurations have the same structure, and all sentences are aligned across configurations and splits.
```python
{
	'id': 1,
	'sentence': 'У понеділок, науковці зі Школи медицини Стенфордського університету оголосили про винайдення нового діагностичного інструменту, що може сортувати клітини за їх видами: це малесенький друкований чіп, який можна виготовити за допомогою стандартних променевих принтерів десь по одному центу США за штуку.',
	'URL': 'https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet',
	'domain': 'wikinews',
	'topic': 'health',
	'has_image': 0,
	'has_hyperlink': 0
}
```
When using a hyphenated pairing or using the `all` function, data will be presented as follows:

```python
{
    'id': 1, 
    'URL': 'https://en.wikinews.org/wiki/Scientists_say_new_medical_diagnostic_chip_can_sort_cells_anywhere_with_an_inkjet', 
    'domain': 'wikinews', 
    'topic': 'health', 
    'has_image': 0, 
    'has_hyperlink': 0, 
    'sentence_eng_Latn': 'On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each.', 
    'sentence_ukr_Cyrl': 'У понеділок, науковці зі Школи медицини Стенфордського університету оголосили про винайдення нового діагностичного інструменту, що може сортувати клітини за їх видами: це малесенький друкований чіп, який можна виготовити за допомогою стандартних променевих принтерів десь по одному центу США за штуку.'
}
```


The text is provided as-in the original dataset, without further preprocessing or tokenization.
### Data Fields
- `id`: Row number for the data entry, starting at 1.
- `sentence`: The full sentence in the specific language (may have _lang for pairings)
- `URL`: The URL for the English article from which the sentence was extracted.
- `domain`: The domain of the sentence.
- `topic`: The topic of the sentence.
- `has_image`: Whether the  original article contains an image.
- `has_hyperlink`: Whether the  sentence contains a hyperlink.
### Data Splits
|            config| `dev`| `devtest`|
|-----------------:|-----:|---------:|
|all configurations|   997|     1012:|
### Dataset Creation
Please refer to the original article [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) for additional information on dataset creation.
## Additional Information
### Dataset Curators
See paper for details.
### Licensing Information
Licensed with Creative Commons Attribution Share Alike 4.0. License available [here](https://creativecommons.org/licenses/by-sa/4.0/).
### Citation Information
Please cite the authors if you use these corpora in your work:
```bibtex
@article{nllb2022,
  author    = {NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi,  Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Jeff Wang},
  title     = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  year      = {2022}
}
```

Please also cite prior work that this dataset builds on:

```bibtex
@inproceedings{,
  title={The FLORES-101  Evaluation Benchmark for Low-Resource and Multilingual Machine Translation},
  author={Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc'Aurelio and Guzm\'{a}n, Francisco and Fan, Angela},
  year={2021}
}
```

```bibtex
@inproceedings{,
  title={Two New Evaluation Datasets for Low-Resource Machine Translation: Nepali-English and Sinhala-English},
  author={Guzm\'{a}n, Francisco and Chen, Peng-Jen and Ott, Myle and Pino, Juan and Lample, Guillaume and Koehn, Philipp and Chaudhary, Vishrav and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1902.01382},
  year={2019}
}
```",High,4.0
Translation,efederici/mc-translation,2.0,49.0,2024-11-05 10:27:01+00:00,mit,0.0,350 MB,367001600.0,350 MB,367001600.0,209115,none,none,"---
language:
- en
- sw
- es
- de
- zh
- bn
- it
- hi
- ja
- ko
- pt
- ar
- id
license: mit
size_categories:
- 100K<n<1M
task_categories:
- translation
dataset_info:
  features:
  - name: prompt
    dtype: string
  - name: output
    dtype: string
  - name: in_lang
    dtype: string
  - name: out_lang
    dtype: string
  - name: conversations
    list:
    - name: from
      dtype: string
    - name: value
      dtype: string
  splits:
  - name: train
    num_bytes: 722699027
    num_examples: 209115
  download_size: 349500393
  dataset_size: 722699027
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
tags:
- multiple-choice
- eval
---

This dataset contains professional human translations from OpenAI's MMMLU dataset, repurposed to train translation models that can help translate future evaluation datasets.

## Why This Dataset?

Translation of evaluation benchmarks is a critical but challenging task. While automated translations may introduce errors or biases, professional human translations are expensive and time-consuming. This dataset leverages existing professional translations (MMMLU) to train specialized translation models that can assist in translating future evaluation sets.",Medium,2.0
Translation,hghalebi/refugiesinfo,1.0,8.0,2024-11-28 09:13:33+00:00,other,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
language:
- fr
- ar
- en
- uk
- ps
- fa
- ti
- ru
pretty_name: ""Refugee Assistance Dataset""
tags:
- refugees
- integration
- multilingual
- RAG
- NLP
- social-inclusion
license: ""other"" # Update based on the license terms of https://refugies.info/en
task_categories:
- text-retrieval
- translation
- question-answering
- text-classification
dataset_info:
  source: ""https://refugies.info/en""
  description: ""This dataset is sourced from Réfugiés.info, an online platform providing structured information to assist refugees and newcomers in navigating administrative, educational, healthcare, housing, and social integration procedures in their host countries.""
  multilingual_support: true
  available_languages: [""French (fr)"", ""Arabic (ar)"", ""English (en)"", ""Ukrainian (uk)"", ""Pashto (ps)"", ""Persian/Dari (fa)"", ""Tigrinya (ti)"", ""Russian (ru)""]
  use_cases: 
    - Retrieval-Augmented Generation (RAG) systems
    - Fine-tuning NLP models
    - Knowledge Retrieval and QA systems for refugees and social workers
---

This dataset is derived from https://refugies.info/en, an online platform providing resources, guides, and information to support refugees and newcomers in navigating various aspects of life in their host countries. It encompasses a structured collection of multilingual information, covering topics such as:

**Administrative Procedures:** Guidance on residence permits, asylum applications, and legal rights.

**Employment:** Job search assistance, training programs, and integration into the workforce.

**Education:** Access to language courses, school enrollment, and higher education opportunities.

**Healthcare:** Information on accessing medical care and health insurance.

**Housing:** Steps for finding accommodation and understanding local housing regulations.

**Social and Community Integration:** Tips for cultural adaptation, community involvement, and accessing support networks.
Purpose and Applications

# The dataset can be used for:

**RAG (Retrieval-Augmented Generation) Applications:** Providing contextually relevant answers to queries about refugee integration processes.

**Fine-Tuning Models:** Training specialized NLP models to assist in providing multilingual, accurate, and empathetic support for refugees and newcomers.

**Knowledge Retrieval Systems:** Building search engines or recommendation tools tailored to refugee and newcomer needs.
Data Provenance

All data originates from https://refugies.info/en, ensuring the content is reliable and specifically curated for refugee assistance.

*Any applications built using this dataset should credit the platform as the source and adhere to its terms of use.*",Medium,2.0
Translation,dijihax/Dataset,2.0,7.0,2024-12-06 17:37:23+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: apache-2.0
task_categories:
- text-classification
- token-classification
- table-question-answering
- question-answering
- zero-shot-classification
- translation
- summarization
- feature-extraction
- text-generation
- text2text-generation
- fill-mask
- sentence-similarity
- text-to-speech
- text-to-audio
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
- image-to-image
- unconditional-image-generation
- multiple-choice
- image-to-text
- image-to-video
- video-classification
- reinforcement-learning
- robotics
- tabular-classification
- tabular-regression
- tabular-to-text
- table-to-text
- text-retrieval
- time-series-forecasting
- text-to-video
- visual-question-answering
- zero-shot-image-classification
- graph-ml
- mask-generation
- zero-shot-object-detection
- text-to-3d
- image-to-3d
- image-feature-extraction
- video-text-to-text
language:
- en
- aa
- ab
- ae
- af
- ak
- am
- an
- ar
- as
- av
- ay
- az
- ba
- be
- bg
- bh
- bi
- bm
- bn
- bo
- br
- bs
- ca
- ce
- ch
- co
- cr
- cs
- cu
- cv
- cy
- da
- de
- dv
- dz
- ee
- el
- eo
- es
- et
- eu
- fa
- ff
- fi
- fj
- fo
- fr
- fy
- ga
- gd
- gl
- gn
- gu
- gv
- ha
- he
- hi
- ho
- hr
- ht
- hu
- hy
- hz
- ia
- id
- ie
- ig
- ii
- ik
- io
- is
- it
- iu
- ja
- jv
- ka
- kg
- ki
- kj
- kk
- kl
- km
- kn
- ko
- kr
- ks
- ku
- kv
- kw
- ky
- la
- lb
- lg
- li
- ln
- lo
- lt
- lu
- lv
- mg
- mh
- mi
- mk
- ml
- mn
- mr
- ms
- mt
- my
- na
- nb
- nd
- ne
- ng
- nl
- nn
- 'no'
- nr
- nv
- ny
- oc
- oj
- om
- or
- os
- pa
- pi
- pl
- ps
- pt
- qu
- rm
- rn
- ro
- ru
- rw
- sa
- sc
- sd
- se
- sg
- si
- sk
- sl
- sm
- sn
- so
tags:
- chemistry
- biology
- finance
- legal
- music
- art
- code
- climate
- medical
- not-for-all-audiences
- synthetic
pretty_name: DijiHax/DataStax
size_categories:
- n>1T
---
# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Translation,bot-yaya/parallel_corpus_game,5.0,70.0,2025-06-13 02:37:16+00:00,mit,0.0,1.41 GB,1513975971.84,1.41 GB,1513975971.84,2265793,none,none,"---
dataset_info:
  features:
  - name: ar_text
    dtype: string
  - name: cht_text
    dtype: string
  - name: de_text
    dtype: string
  - name: en_text
    dtype: string
  - name: eo_text
    dtype: string
  - name: es_text
    dtype: string
  - name: fr_text
    dtype: string
  - name: he_text
    dtype: string
  - name: id_text
    dtype: string
  - name: it_text
    dtype: string
  - name: ja_text
    dtype: string
  - name: ko_text
    dtype: string
  - name: nl_text
    dtype: string
  - name: pt_text
    dtype: string
  - name: ru_text
    dtype: string
  - name: sv_text
    dtype: string
  - name: th_text
    dtype: string
  - name: vi_text
    dtype: string
  - name: zh_text
    dtype: string
  - name: zh_text_md5
    dtype: string
  - name: 低质量段落数
    dtype: int64
  - name: 去重段落数
    dtype: int64
  - name: 扩展字段
    dtype: string
  - name: 文件名
    dtype: string
  - name: 时间
    dtype: string
  - name: 是否待查文件
    dtype: bool
  - name: 是否跨文件重复
    dtype: bool
  - name: 是否重复
    dtype: bool
  - name: 是否重复文件
    dtype: bool
  - name: 段落数
    dtype: int64
  - name: 行号
    dtype: int64
  splits:
  - name: train
    num_bytes: 2595882458
    num_examples: 2265793
  download_size: 1412978679
  dataset_size: 2595882458
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: mit
language:
- ar
- zh
- de
- en
- eo
- es
- fr
- he
- id
- it
- ja
- ko
- nl
- pt
- ru
- sv
- th
- vi
- pl
- tr
task_categories:
- translation
tags:
- game
---

https://github.com/mnbvc-parallel-corpus-team/parallel_corpus_mnbvc

Game Corpus Collected by MNBVC Parallel Corpus Team.

47 Games are Included:
- Anonymous Hacker Simulator
- Borderlands 2
- Borderlands 3
- Baldur's Gate 3
- Cultist Simulator
- Cyberpunk 2077
- Dark Souls 3
- Detroit Become Human
- Disaster Band
- Do Not Starve
- Elden Ring
- Genshin Impact
- Guilty Gear Strive
- Grand Theft Auto 4
- Grand Theft Auto 5
- Hades
- Hi-Fi Rush
- Hogwarts Legacy
- Ib
- Like A Dragon 8
- Like A Dragon Gaiden 7 Side
- Metro 2033 Redux
- Metro Exodus Enhanced Edition
- Metro Last Light Redux
- Mirror 2 Project X
- Oxygen Not Included
- Red Dead Redemption 2
- Rhythm Doctor
- Sekiro: Shadows Die Twice
- Sid Meier's Civilization VI
- Sifu
- Slay the Spire
- Stardew Valley
- Honkai: Star Rail
- Stellaris
- Terraria
- The Witcher 3: Wild Hunt
- Turing Complete
- Witch Spring 3
- Witch Spring R
- Wuthering Waves
- Like A Dragon 3
- Like A Dragon 4
- Like A Dragon 5
- Like A Dragon 6
- Like A Dragon 7
- Like A Dragon Kiwami 2

We collect any parallel corpus data from games that contains at least Chinese and English.

Localization resources are collected from the Internet. If there is any infringement, please contact us to delete it.

If you are interested in our work, please refer to https://github.com/mnbvc-parallel-corpus-team/parallel_corpus_mnbvc for more infomations.
",Medium,2.0
Translation,ymoslem/Tatoeba-EN-AR,0.0,52.0,2024-12-16 08:46:55+00:00,cc-by-2.0,0.0,1.77 MB,1855979.52,1.77 MB,1855979.52,30852,none,none,"---
dataset_info:
  features:
  - name: English
    dtype: string
  - name: Arabic
    dtype: string
  splits:
  - name: train
    num_bytes: 2710825
    num_examples: 30852
  download_size: 1768292
  dataset_size: 2710825
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: cc-by-2.0
task_categories:
- translation
language:
- en
- ar
size_categories:
- 10K<n<100K
---

## Dataset Details

English-to-Arabic translated sentences from Tatoeba, OPUS.
The dataset includes unique pairs only, and its statistics are as follows:

```
Dataset({
    features: ['English', 'Arabic'],
    num_rows: 30853
})
```


### Citation

```
@inproceedings{tiedemann-2012-parallel,
    title = ""Parallel Data, Tools and Interfaces in {OPUS}"",
    author = {Tiedemann, J{\""o}rg},
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf"",
    pages = ""2214--2218"",
    abstract = ""This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project."",
}
```",High,4.0
Translation,ymoslem/Tatoeba-Translations,2.0,71.0,2024-12-29 13:38:30+00:00,cc-by-2.0,0.0,726 MB,761266176.0,726 MB,761266176.0,8547819,none,none,"---
dataset_info:
  features:
  - name: id_src
    dtype: int64
  - name: lang_src
    dtype: string
  - name: sentence_src
    dtype: string
  - name: id_tgt
    dtype: int64
  - name: lang_tgt
    dtype: string
  - name: sentence_tgt
    dtype: string
  - name: lang_pair
    sequence: string
  splits:
  - name: train
    num_bytes: 1144194352
    num_examples: 8547819
  download_size: 726390210
  dataset_size: 1144194352
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
language:
- multilingual
- ab
- af
- am
- ar
- an
- as
- av
- ay
- az
- ba
- bm
- be
- bn
- bi
- bo
- bs
- br
- bg
- ca
- cs
- ch
- ce
- cv
- kw
- co
- cy
- da
- de
- dv
- el
- en
- eo
- et
- eu
- ee
- fo
- fj
- fi
- fr
- fy
- gd
- ga
- gl
- gv
- gn
- gu
- ht
- ha
- he
- hi
- hr
- hu
- hy
- ig
- io
- ii
- ie
- ia
- id
- is
- it
- jv
- ja
- kl
- kn
- ks
- ka
- kk
- km
- rw
- ky
- ko
- lo
- la
- li
- ln
- lt
- lb
- lg
- mh
- ml
- mr
- mk
- mg
- mt
- mn
- mi
- my
- na
- nv
- nl
- nn
- nb
- ny
- oc
- oj
- or
- os
- pa
- pi
- pl
- pt
- ps
- qu
- rm
- ro
- rn
- ru
- sg
- sa
- si
- sk
- sl
- se
- sm
- sn
- sd
- so
- st
- es
- sq
- sc
- sr
- ss
- su
- sv
- ty
- ta
- tt
- te
- tg
- tl
- th
- ti
- to
- tn
- ts
- tk
- tr
- ug
- uk
- ur
- uz
- vi
- vo
- wa
- wo
- xh
- yi
- yo
- zu
license: cc-by-2.0
task_categories:
- translation
size_categories:
- 1M<n<10M
---

## Dataset Details

This is the latest version of Tatoeba translations as of December 2024.
The sentences are downloaded from the [Tatoeba collection website](https://tatoeba.org/en/downloads).
The dataset is processed through mapping `sentences.tar.bz2` using `sentences_base.tar.bz2` to find source (`sentence_src`) and target (`sentence_tgt`) sentences.
While `lang_src` and `lang_tgt` columns follow the mapping provided by Tatoeba, the `lang_pair` column merely lists the two languages in the translation pair.

### Statistics

The Tatoeba dataset includes 8,547,819 unique translation pairs in 414 languages, covering ~5,917 language pairs.

### Languages

The dataset includes the 414 languages:

<details>
  <summary>Show the full list of languages.</summary>

  Abkhazian (abk), Adyghe (ady), Afrihili (afh), Afrikaans (afr), Ainu (Japan) (ain), Albanian (sqi), Algerian Arabic (arq), Amharic (amh), Ancient Greek (to 1453) (grc), Ancient Hebrew (hbo), Arabic (ara), Aragonese (arg), Armenian (hye), Assamese (asm), Assyrian Neo-Aramaic (aii), Asturian (ast), Avaric (ava), Awadhi (awa), Aymara (aym), Azerbaijani (aze), Balinese (ban), Baluchi (bal), Bambara (bam), Banjar (bjn), Bashkir (bak), Basque (eus), Bavarian (bar), Baybayanon (bvy), Belarusian (bel), Bengali (ben), Berber languages (ber), Berom (bom), Bhojpuri (bho), Bislama (bis), Bodo (India) (brx), Bosnian (bos), Breton (bre), Brithenig (bzt), Bulgarian (bul), Buriat (bua), Burmese (mya), Catalan (cat), Cayuga (cay), Cebuano (ceb), Central Bikol (bcl), Central Huasteca Nahuatl (nch), Central Kanuri (knc), Central Kurdish (ckb), Central Mnong (cmo), Central Okinawan (ryu), Chagatai (chg), Chamorro (cha), Chavacano (cbk), Chechen (che), Cherokee (chr), Chinese Pidgin English (cpi), Chinook jargon (chn), Choctaw (cho), Chukot (ckt), Chuvash (chv), Classical Syriac (syc), Congo Swahili (swc), Cornish (cor), Corsican (cos), Creek (mus), Crimean Tatar (crh), Croatian (hrv), Cuyonon (cyo), Czech (ces), Danish (dan), Dhivehi (div), Dimli (individual language) (diq), Drents (drt), Dungan (dng), Dutch (nld), Dutton World Speedwords (dws), Eastern Canadian Inuktitut (ike), Eastern Mari (mhr), Egyptian Arabic (arz), Emilian (egl), English (eng), Erromintxela (emx), Erzya (myv), Esperanto (epo), Estonian (est), Evenki (evn), Ewe (ewe), Extremaduran (ext), Faroese (fao), Fiji Hindi (hif), Fijian (fij), Finnish (fin), French (fra), Friulian (fur), Ga (gaa), Gagauz (gag), Galician (glg), Gan Chinese (gan), Ganda (lug), Garhwali (gbm), Georgian (kat), German (deu), Gheg Albanian (aln), Gilbertese (gil), Goan Konkani (gom), Gothic (got), Gronings (gos), Guadeloupean Creole French (gcf), Guarani (grn), Guerrero Nahuatl (ngu), Gujarati (guj), Gulf Arabic (afb), Gun (guw), Haitian (hat), Hakka Chinese (hak), Hausa (hau), Hawaiian (haw), Hebrew (heb), Hiligaynon (hil), Hindi (hin), Hmong Daw (mww), Hmong Njua (hnj), Ho (hoc), Hungarian (hun), Hunsrik (hrx), Iban (iba), Icelandic (isl), Ido (ido), Igbo (ibo), Iloko (ilo), Indonesian (ind), Ingrian (izh), Interglossa (igs), Interlingua (International Auxiliary Language Association) (ina), Interlingue (ile), Iranian Persian (pes), Irish (gle), Italian (ita), Jamaican Creole English (jam), Japanese (jpn), Javanese (jav), Jewish Babylonian Aramaic (ca. 200-1200 CE) (tmr), Jewish Palestinian Aramaic (jpa), Jinyu Chinese (cjy), Judeo-Tat (jdt), K'iche' (quc), Kabardian (kbd), Kabyle (kab), Kadazan Dusun (dtp / kzj), Kalaallisut (kal), Kalmyk (xal), Kamba (Kenya) (kam), Kannada (kan), Kara-Kalpak (kaa), Karachay-Balkar (krc), Karakhanid (xqa), Karelian (krl), Kashmiri (kas), Kashubian (csb), Kazakh (kaz), Kekchí (kek), Keningau Murut (kxi), Khakas (kjh), Khalaj (klj), Khasi (kha), Khmer (khm), Kinyarwanda (kin), Kirghiz (kir), Kirmanjki (individual language) (kiu), Klingon (tlh), Komi-Permyak (koi), Komi-Zyrian (kpv), Korean (kor), Kotava (avk), Kriang (ngt), Kumyk (kum), Kven Finnish (fkv), Kölsch (ksh), Ladin (lld), Ladino (lad), Lakota (lkt), Lao (lao), Latgalian (ltg), Latin (lat), Laz (lzz), Levantine Arabic (apc / ajp), Lezghian (lez), Libyan Arabic (ayl), Ligurian (lij), Limburgan (lim), Lingala (lin), Lingua Franca Nova (lfn), Literary Chinese (lzh), Lithuanian (lit), Liv (liv), Lojban (jbo), Lombard (lmo), Louisiana Creole (lou), Low German (nds), Lower Sorbian (dsb), Lushootseed (lut), Luxembourgish (ltz), Láadan (ldn), Macedonian (mkd), Madurese (mad), Mahasu Pahari (bfz), Maithili (mai), Malagasy (mlg), Malay (individual language) (zlm), Malayalam (mal), Maltese (mlt), Mambae (mgm), Manchu (mnc), Mandarin Chinese (cmn), Manipuri (mni), Manx (glv), Maori (mri), Mapudungun (arn), Marathi (mar), Marshallese (mah), Mesopotamian Arabic (acm), Mi'kmaq (mic), Middle English (1100-1500) (enm), Middle French (ca. 1400-1600) (frm), Mikasuki (mik), Min Nan Chinese (nan), Minangkabau (min), Mingrelian (xmf), Mirandese (mwl), Modern Greek (1453-) (ell), Mohawk (moh), Moksha (mdf), Mon (mnw), Mongolian (mon), Mono (USA) (mnr), Morisyen (mfe), Moroccan Arabic (ary), Nahuatl languages (nah), Nande (nnb), Nauru (nau), Navajo (nav), Neapolitan (nap), Nepali (individual language) (npi), Nigerian Fulfulde (fuv), Niuean (niu), Nogai (nog), North Moluccan Malay (max), Northeastern Thai (tts), Northern Frisian (frr), Northern Haida (hdn), Northern Kurdish (kmr), Northern Sami (sme), Norwegian Bokmål (nob), Norwegian Nynorsk (nno), Novial (nov), Nuer (nus), Nyanja (nya), Nyungar (nys), Occitan (post 1500) (oci), Ojibwa (oji), Old Aramaic (up to 700 BCE) (oar), Old English (ca. 450-1100) (ang), Old French (842-ca. 1400) (fro), Old Frisian (ofs), Old Norse (non), Old Russian (orv), Old Saxon (osx), Old Spanish (osp), Old Turkish (otk), Oriya (macrolanguage) (ori), Orizaba Nahuatl (nlv), Ossetian (oss), Ottoman Turkish (1500-1928) (ota), Pahlavi (pal), Palauan (pau), Pali (pli), Pampanga (pam), Pangasinan (pag), Panjabi (pan), Papiamento (pap), Pattani Malay (mfa), Pennsylvania German (pdc), Pfaelzisch (pfl), Phoenician (phn), Picard (pcd), Piemontese (pms), Pipil (ppl), Plains Cree (crk), Polish (pol), Portuguese (por), Prussian (prg), Pulaar (fuc), Pushto (pus), Qashqa'i (qxq), Quechua (que), Quenya (qya), Rapanui (rap), Rohingya (rhg), Romanian (ron), Romansh (roh), Romany (rom), Rundi (run), Russian (rus), Rusyn (rue), Samoan (smo), Samogitian (sgs), Sango (sag), Sanskrit (san), Santali (sat), Saraiki (skr), Sardinian (srd), Saterfriesisch (stq), Scots (sco), Scottish Gaelic (gla), Serbian (srp), Seselwa Creole French (crs), Shona (sna), Shuswap (shs), Sichuan Yi (iii), Sicilian (scn), Silesian (szl), Sindarin (sjn), Sindhi (snd), Sinhala (sin), Slovak (slk), Slovenian (slv), Somali (som), Southern Altai (alt), Southern Haida (hax), Southern Kurdish (sdh), Southern Sami (sma), Southern Sotho (sot), Southern Subanen (laa), Spanish (spa), Sranan Tongo (srn), Standard Latvian (lvs), Standard Malay (zsm), Standard Moroccan Tamazight (zgh), Sumerian (sux), Sundanese (sun), Swabian (swg), Swahili (individual language) (swh), Swati (ssw), Swedish (swe), Swiss German (gsw), Sylheti (syl), Tachawit (shy), Tachelhit (shi), Tagal Murut (mvv), Tagalog (tgl), Tahaggart Tamahaq (thv), Tahitian (tah), Tajik (tgk), Talossan (tzl), Talysh (tly), Tamil (tam), Tarifit (rif), Tase Naga (nst), Tatar (tat), Telugu (tel), Temuan (tmw), Tetum (tet), Thai (tha), Tibetan (bod), Tigre (tig), Tigrinya (tir), Tohono O'odham (ood), Tok Pisin (tpi), Tokelau (tkl), Toki Pona (tok), Tonga (Tonga Islands) (ton), Tonga (Zambia) (toi), Tsonga (tso), Tswana (tsn), Tumbuka (tum), Tupinambá (tpn / tpw), Turkish (tur), Turkmen (tuk), Tuvalu (tvl), Tuvinian (tyv), Uab Meto (aoz), Udmurt (udm), Uighur (uig), Ukrainian (ukr), Umbundu (umb), Upper Sorbian (hsb), Urdu (urd), Urhobo (urh), Uzbek (uzb), Venetian (vec), Veps (vep), Vietnamese (vie), Volapük (vol), Võro (vro), Walloon (wln), Waray (Philippines) (war), Wayuu (guc), Welsh (cym), Western Armenian (hyw), Western Frisian (fry), Western Mari (mrj), Western Panjabi (pnb), Wolof (wol), Wu Chinese (wuu), Xhosa (xho), Xiang Chinese (hsn), Yakut (sah), Yiddish (yid), Yoruba (yor), Yucateco (yua), Yue Chinese (yue), Zaza (zza), Zeeuws (zea), Zulu (zul)
</details>

### Contact

The dataset was processed and brought to Hugging Face by [ymoslem](https://huggingface.co/ymoslem).

",Medium,2.0
Translation,MawaredHR/translation2,0.0,16.0,2025-01-04 13:34:14+00:00,apache-2.0,0.0,14.6 MB,15309209.6,4.1 MB,4299161.6,15000,none,none,"---
license: apache-2.0
task_categories:
- translation
language:
- en
- ar
size_categories:
- 10K<n<100K
---
# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Translation,khoubrane-yousef/tulu-v2-sft-mixture-english-darija,1.0,26.0,2025-01-16 22:19:31+00:00,none,0.0,308 MB,322961408.0,308 MB,322961408.0,157897,https://arxiv.org/abs/2311.10702,none,"---
dataset_info:
  features:
  - name: dataset
    dtype: string
  - name: id
    dtype: string
  - name: messages
    list:
    - name: content
      dtype: string
    - name: role
      dtype: string
  - name: messages_darija
    list:
    - name: content
      dtype: string
    - name: role
      dtype: string
  splits:
  - name: train
    num_bytes: 680164153
    num_examples: 157897
  download_size: 307832670
  dataset_size: 680164153
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- translation
language:
- ar
- en
---

# TÜLU-v2-SFT-Mixture-English-Darija

The **TÜLU-v2-SFT-Mixture-English-Darija** dataset aligns English instructions from [AllenAI's TÜLU-v2-SFT-Mixture](https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture) with their Moroccan Darija translations from [MBZUAI-Paris's Darija-SFT-Mixture](https://huggingface.co/datasets/MBZUAI-Paris/Darija-SFT-Mixture). It supports tasks like machine translation, instruction-following, and cross-lingual understanding.

## Dataset Details

- **English Instructions:** Sourced from [AllenAI's TÜLU-v2-SFT-Mixture](https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture), a mixture of diverse instruction-tuning datasets.
- **Darija Translations:** From [MBZUAI-Paris's Darija-SFT-Mixture](https://huggingface.co/datasets/MBZUAI-Paris/Darija-SFT-Mixture), an instruction-tuning dataset for Darija LLMs that combines instructions from various sources. I filtered the subsets corresponding to the Darija translations of TÜLU-v2 based on their IDs.

## Usage

This dataset is ideal for training and fine-tuning language models in English and Moroccan Darija, enhancing accessibility for Moroccan Arabic speakers.

## Notes

- Not all original English samples have Darija translations. This is because some filtering was done both before and after the translation process to ensure the quality and relevance of Darija-SFT-Mixture. This dataset only includes rows with existing translations.
- Rows with duplicated IDs were excluded from this version of the dataset. In the original English dataset, some IDs were repeated, each corresponding to a slightly different variation of the same instruction. These variations were then translated into Darija. The challenge here is that the usual method of mapping by ID doesn't work, as multiple translations exist for a single ID, and it's difficult to determine which translation matches which variation. This issue needs to be resolved before including these rows in the dataset.

## Licensing

The dataset is released under the [ODC-BY license](https://opendatacommons.org/licenses/by/1-0/). Some portions are non-commercial; review licenses for each subset to ensure compliance.

## Citation

If you use this dataset, please cite the original sources:

```none
@misc{ivison2023camels,
      title = {Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2},
      author = {Ivison, Hamish and Wang, Yizhong and Pyatkin, Valentina and Lambert, Nathan and Peters, Matthew and Dasigi, Pradeep and Jang, Joel and Wadden, David and Smith, Noah A. and Beltagy, Iz and Hajishirzi, Hannaneh},
      year = {2023},
      eprint = {2311.10702},
      archivePrefix = {arXiv},
      primaryClass = {cs.CL}
}
@article{shang2024atlaschat,
      title = {Atlas-Chat: Adapting Large Language Models for Low-Resource Moroccan Arabic Dialect},
      author = {Shang, Guokan and Abdine, Hadi and Khoubrane, Yousef and Mohamed, Amr and Abbahaddou, Yassine and Ennadir, Sofiane and Momayiz, Imane and Ren, Xuguang and Moulines, Eric and Nakov, Preslav and Vazirgiannis, Michalis and Xing, Eric},
      year = {2024},
      eprint = {2409.17912},
      archivePrefix = {arXiv},
      primaryClass = {cs.CL},
      url = {https://arxiv.org/abs/2409.17912}
}
```
---",High,5.0
Translation,fantastic-jobs/linkedin-industry-list,0.0,52.0,2025-01-11 15:04:13+00:00,mit,0.0,317 KB,324608.0,165 KB,168960.0,7606,none,none,"---
license: mit
task_categories:
- text-classification
- translation
language:
- en
- ko
- es
- ru
- nl
- de
- ml
- it
- fr
- pt
- id
- cs
- tr
- da
- sv
- ar
- th
tags:
- jobs
- linkedin
- job
- industries
- industry
size_categories:
- 1K<n<10K
---",Low,1.0
Translation,ronnieaban/alquran,1.0,29.0,2025-01-14 22:13:21+00:00,cc-by-4.0,0.0,47.8 MB,50121932.8,10.2 MB,10695475.2,6236,none,none,"---
license: cc-by-4.0
task_categories:
- text-generation
- translation
- text-classification
language:
- ar
- id
tags:
- quran
- translation
- tafsir
- religious-text
- islamic-studies
- arabic
- indonesian
- >-
  on, tafsir, religious-text, islamic-studies, arabic, indonesian,
  text-generation
- machine-translation
- text-classification
- natural-language-understanding
pretty_name: Terjemahan dan Tafsir Al-Quran
size_categories:
- 1K<n<10K
---
# Dataset Terjemahan dan Tafsir Al-Quran

## Deskripsi Dataset

Dataset ini berisi terjemahan Al-Quran dalam bahasa Indonesia beserta tafsirnya. Dataset ini dapat digunakan untuk berbagai tugas NLP seperti machine translation, text generation, dan text summarization.

### Fitur Utama

- **Terjemahan Al-Quran**: Teks Al-Quran dalam bahasa Arab beserta terjemahannya dalam bahasa Indonesia.
- **Tafsir Al-Quran**: Penjelasan atau interpretasi dari ayat-ayat Al-Quran dalam bahasa Indonesia.
- **Metadata**: Informasi tambahan seperti nomor surat, nomor ayat, dan nama surat.

## Penggunaan Dataset

Dataset ini dapat digunakan untuk:

- **Machine Translation**: Melatih model untuk menerjemahkan teks Al-Quran dari bahasa Arab ke bahasa Indonesia.
- **Text Generation**: Membuat model yang dapat menghasilkan tafsir atau penjelasan dari ayat-ayat Al-Quran.
- **Text Summarization**: Membuat ringkasan dari tafsir Al-Quran.

### Struktur Data

```
{
  ""id"": ""14"",
  ""surah_id"": ""2"",
  ""surah_arabic"": "" البقرة"",
  ""surah_latin"": ""Al-Baqarah "",
  ""surah_transliteration"": ""Al-Baqarah"",
  ""surah_translation"": ""Sapi"",
  ""surah_num_ayah"": ""286"",
  ""surah_page"": ""2"",
  ""surah_location"": ""Madaniyah"",
  ""ayah"": ""7"",
  ""page"": ""3"",
  ""quarter_hizb"": ""1"",
  ""juz"": ""1"",
  ""manzil"": ""1"",
  ""arabic"": ""خَتَمَ اللّٰهُ عَلٰى قُلُوْبِهِمْ وَعَلٰى سَمْعِهِمْ ۗ وَعَلٰٓى اَبْصَارِهِمْ غِشَاوَةٌ وَّلَهُمْ عَذَابٌ عَظِيْمٌ ࣖ "",
  ""latin"": ""Khatamallāhu ‘alā qulūbihim wa ‘alā sam‘ihim wa ‘alā abṣārihim gisyāwatuw wa lahum ‘ażābun ‘aẓīm(un). "",
  ""translation"": ""Allah telah mengunci hati dan pendengaran mereka.5) Pada penglihatan mereka ada penutup, dan bagi mereka azab yang sangat berat."",
  ""no_footnote"": ""5"",
  ""footnotes"": ""5) Allah Swt. telah mengunci hati dan telinga orang kafir sehingga nasihat atau hidayah tidak bisa masuk ke dalam hatinya."",
  ""tafsir_wajiz"": ""Karena mereka ingkar dengan menutup diri dari kebenaran, maka seakan Allah telah mengunci hati mereka dengan sekat yang tertutup rapat sehingga nasihat atau hidayah tersebut tidak bisa masuk ke dalam hati mereka, dan pendengaran mereka juga seakan terkunci, sehingga tidak mendengar kebenaran dari Allah. Demikian pula penglihatan mereka telah tertutup, sehingga tidak melihat tanda-tanda kekuasaan Allah yang dapat mengantarkan kepada keimanan, dan sebagai akibatnya, mereka akan mendapat azab yang berat. "",
  ""tafsir_tahlili"": ""Hal yang menyebabkan orang-orang kafir tidak menerima peringatan adalah karena hati dan pendengaran mereka tertutup, bahkan terkunci mati, tidak dapat menerima petunjuk, dan segala macam nasihat tidak berbekas pada mereka. Karena penglihatan mereka tertutup, mereka tidak dapat melihat, memperhatikan dan memahami ayat-ayat Al-Qur’an yang telah mereka dengar, tidak dapat mengambil pelajaran dari tanda-tanda kebesaran Allah yang mereka lihat di cakrawala, di permukaan bumi dan pada diri mereka sendiri.\nTerkuncinya hati dan pendengaran, serta tertutupnya penglihatan orang-orang kafir itu karena mereka selalu mengerjakan perbuatan-perbuatan yang terlarang. Tiap-tiap perbuatan terlarang yang mereka lakukan akan menambah rapat dan kuatnya kunci yang menutup hati dan pendengaran mereka. Makin banyak perbuatan itu mereka lakukan, makin bertambah kuat pula kunci dan tutup pada hati dan telinga mereka:\nفَبِمَا نَقْضِهِمْ مِّيْثَاقَهُمْ وَكُفْرِهِمْ بِاٰيٰتِ اللّٰهِ وَقَتْلِهِمُ الْاَنْۢبِيَاۤءَ بِغَيْرِ حَقٍّ وَّقَوْلِهِمْ قُلُوْبُنَا غُلْفٌ ۗ بَلْ طَبَعَ اللّٰهُ عَلَيْهَا بِكُفْرِهِمْ فَلَا يُؤْمِنُوْنَ اِلَّا قَلِيْلًاۖ ١٥٥ (النساۤء)\nMaka (Kami hukum mereka), karena mereka melanggar perjanjian itu, dan karena kekafiran mereka terhadap keterangan-keterangan Allah, serta karena mereka telah membunuh nabi-nabi tanpa hak (alasan yang benar), dan karena mereka mengatakan, ”Hati kami tertutup.” Sebenarnya Allah telah mengunci hati mereka karena kekafirannya, karena itu hanya sebagian kecil dari mereka yang beriman (an-Nisā’/4: 155).\nوَنُقَلِّبُ اَفْـِٕدَتَهُمْ وَاَبْصَارَهُمْ كَمَا لَمْ يُؤْمِنُوْا بِهٖٓ اَوَّلَ مَرَّةٍ وَّنَذَرُهُمْ فِيْ طُغْيَانِهِمْ يَعْمَهُوْنَ ࣖ ۔ ١١٠ (الانعام)\nDan (begitu pula) Kami memalingkan hati dan penglihatan mereka seperti pertama kali mereka tidak beriman kepadanya (Al-Qur’an), dan Kami biarkan mereka bingung dalam kesesatan. (al-An‘ām/6: 110)\nProses bertambah kuatnya tutup dan bertambah kuatnya kunci hati dan pendengaran orang-orang kafir itu diterangkan oleh hadis :\nقَالَ رَسُوْلُ اللهِ صَلَّى الله ُعَلَيْهِ وَسَلَّمَ: اِنَّ الْعَبْدَ اِذَا اَذْنَبَ ذَنْباً كَانَتْ نُقْطَةٌ سَوْدَاءُ فِي قَلْبِهِ فَاِنْ تَابَ مِنْهَا صَقُلَ قَلْبُهُ وَاِنْ زَادَ زَادَتْ فَذٰلِكَ قَوْلُ اللهِ \""كَلاَّ بَلْ رَانَ عَلـٰى قُلُوْبِهِمْ مَّا كَانُوْا يَكْسِبُوْنَ (رواه الترمذي وابن جرير الطبري عن ابي هريرة)\nRasulullah saw bersabda, \""Sesungguhnya seorang hamba apabila ia mengerjakan perbuatan dosa terdapatlah suatu noda hitam di dalam hatinya, maka jika ia bertobat, mengkilat hatinya, dan jika ia tambah mengerjakan perbuatan buruk, bertambahlah noda hitam \"". Itulah firman Allah, \""Tidak, tetapi perbuatan mereka menjadi noda hitam di hati mereka”. (Riwayat at-Tirmiżī dan Ibnu Jarīr aṭ-Ṭabari dari Abū Hurairah)"",
  ""tafsir_intro_surah"": ""Surah al-Baqarah yang terdiri dari 286 ayat adalah termasuk golongan surah Madaniyah yang diturunkan pada tahun-tahun permulaan periode Nabi Muhammad saw di Medinah. Ia merupakan surah yang terpanjang dan terbanyak ayat-ayatnya di antara surah yang ada di dalam Al-Qur’an. Surah ini dinamai “al-Baqarah” yang berarti “seekor sapi”, karena di dalamnya disebutkan kisah penyembelihan sapi betina yang diperintahkan Allah kepada Bani Israil. Dalam pelaksanaan penyembelihan sapi betina itu tampak dengan jelas sifat dan watak orang-orang Yahudi pada umumnya.\nDinamakan juga fusṭaṭ al-Qur’ān yang berarti “puncak Al-Qur’an” karena surah ini memuat beberapa hukum yang tidak disebut di surah-surah yang lain. Juga dinamakan Alīf Lām Mīm, karena surah ini dimulai dengan huruf-huruf hijaiyah (abjad) alif lām mīm.\nDi antara pokok-pokok isinya ialah:\n1.\tKeimanan: Dakwah Islamiah yang ditujukan kepada umat Islam, Ahli Kitab dan orang-orang musyrik.\n2.\tHukum: Perintah mengerjakan salat, perintah menunaikan zakat, puasa, haji dan umrah, qiṣaṣ, yang halal dan yang haram, bernafkah di jalan Allah, minum arak dan berjudi, cara bergaul dengan anak yatim, prinsip-prinsip ekonomi, larangan memakan riba, utang piutang, nafkah dan yang berhak menerimanya, wasiat kepada dua orang ibu bapak dan kaum kerabat, hukum sumpah, kewajiban menyampaikan amanat, sihir, hukum merusak masjid, hukum mengubah kitab-kitab Allah, haid, idah, talak, khulu‘, ilā, hukum susuan, meminang, mahar, menikahi wanita musyrik dan sebaliknya, hukum perang, dan lain-lain. \n3.\tKisah: Penciptaan Nabi Adam a.s., kisah Nabi Ibrahim a.s., dan kisah Nabi Musa a.s. dengan Bani Israil.\n4.\tLain-lain, seperti: sifat orang yang bertakwa, sifat-sifat orang munafik, sifat-sifat Allah, perumpamaan-perumpamaan, kiblat, dan kebangkitan sesudah mati."",
  ""tafsir_outro_surah"": """",
  ""tafsir_munasabah_prev_surah"": ""1.\tSurah al-Fātiḥah merupakan pokok-pokok pembahasan yang akan dirinci dalam surah al-Baqarah dan surah-surah sesudahnya.\n2.\tDi bagian akhir surah al-Fātiḥah disebutkan permohonan hamba, agar diberi petunjuk oleh Allah ke jalan yang lurus, sedang surah al-Baqarah dimulai dengan ayat yang menerangkan bahwa Al-Qur’an adalah kitab yang menunjukkan jalan yang dimaksudkan itu.\n3.\tDi akhir surah al-Fātiḥah disebutkan tiga kelompok manusia, yaitu yang diberi nikmat, yang dimurkai Allah dan orang yang sesat, sedangkan di awal surah al-Baqarah juga disebutkan tiga kelompok manusia, yaitu orang yang bertakwa, orang kafir, dan orang munafik."",
  ""tafsir_munasabah_prev_theme"": ""Pada ayat-ayat yang lalu diterangkan sifat orang bertakwa yang hatinya telah mendapat petunjuk dan bimbingan Allah serta balasan yang mereka peroleh dari buah ketakwaannya itu. Ayat ini menerangkan sifat-sifat orang kafir yang hatinya telah tertutup, tidak mau menerima petunjuk-petunjuk Allah dan mereka menerima akibat yang buruk, yaitu azab yang besar."",
  ""tafsir_theme_group"": ""GOLONGAN KAFIR"",
  ""tafsir_kosakata"": ""Kosakata: Kafarū كَفَرُوْا (al-Baqarah/2: 6)\nKata kafarū diambil dari kata kufr, merupakan masdar (infinitif) dari kafara-yakfuru-kufran/kufr. Dalam Al-Qur’an kata kufr dan kata yang seasal dengannya disebut 525 kali. Sedangkan kata kafir disebut hanya 5 kali, yaitu pada surah al-Baqarah, al-Furqān, at-Tagābun dan an-Naba’.\nSecara bahasa, kata kufr mengandung beberapa arti, antara lain: menutupi, melepaskan diri, menghapus, kulit, dan denda (kaffārah) karena melanggar salah satu ketentuan Allah. Dalam ayat ini yang dimaksud orang kafir ialah orang yang ingkar, tidak percaya kepada adanya Allah, tidak percaya pada kekuasaan Allah, karena dia telah menutup diri dan melupakan diri dari kekuasaan Allah. Dia tidak mau tunduk dan patuh pada perintah Allah.\nDari beberapa arti secara bahasa di atas, menurut al-Asfahani dan Ibnu Manẓūr, yang dekat kepada arti secara istilah adalah “menutupi”, “menyembunyikan”. Malam hari disebut kafir karena ia menutupi siang atau tersembunyinya sesuatu oleh kegelapannya. Awan disebut kafir karena ia (dapat) menutupi atau menyembunyikan cahaya matahari. Kafir terhadap nikmat Allah berarti seseorang menutupi atau menyembunyikan nikmat Allah dengan cara tidak mensyukurinya. Demikian juga petani karena menutupi atau menyembunyikan benih dengan tanah waktu bercocok tanam."",
  ""tafsir_sabab_nuzul"": """",
  ""tafsir_conclusion"": ""Orang-orang kafir yang ingkar kepada Allah, bagi mereka sama saja, diberi petunjuk atau tidak diberi petunjuk, mereka tidak akan beriman, karena hati, telinga dan mata mereka tertutup.\nBalasan orang-orang kafir adalah siksa yang amat pedih.""
}
```
Dataset ini terdiri dari beberapa kolom:

- `id`: String yang berisi identifikasi unik untuk entri ini.
- `surah_id`: String yang berisi identifikasi unik untuk surah.
- `surah_arabic`: String yang berisi nama surah dalam bahasa Arab.
- `surah_latin`: String yang berisi nama surah dalam huruf Latin.
- `surah_transliteration`: String yang berisi transliterasi nama surah.
- `surah_translation`: String yang berisi terjemahan nama surah.
- `surah_num_ayah`: String yang berisi jumlah ayat dalam surah.
- `surah_page`: String yang berisi nomor halaman di mana surah dimulai dalam mushaf.
- `surah_location`: String yang berisi lokasi turunnya surah (Misalnya, ""Madaniyah"" untuk surah yang turun di Madinah).
- `ayah`: String yang berisi nomor ayat.
- `page`: String yang berisi nomor halaman di mana ayat tersebut berada.
- `quarter_hizb`: String yang berisi informasi tentang seperempat hizb di mana ayat tersebut berada.
- `juz`: String yang berisi informasi tentang juz di mana ayat tersebut berada.
- `manzil`: String yang berisi informasi tentang manzil di mana ayat tersebut berada.
- `arabic`: String yang berisi teks ayat dalam bahasa Arab.
- `latin`: String yang berisi teks ayat dalam huruf Latin.
- `translation`: String yang berisi terjemahan ayat.
- `no_footnote`: String yang berisi nomor catatan kaki.
- `footnotes`: String yang berisi teks catatan kaki.
- `tafsir_wajiz`: String yang berisi tafsir singkat (tafsir wajiz) dari ayat.
- `tafsir_tahlili`: String yang berisi tafsir analitis (tafsir tahlili) dari ayat.
- `tafsir_intro_surah`: String yang berisi pengantar tentang surah.
- `tafsir_outro_surah`: String yang berisi penutup tentang surah (kosong dalam contoh ini).
- `tafsir_munasabah_prev_surah`: String yang berisi hubungan (munasabah) dengan surah sebelumnya.
- `tafsir_munasabah_prev_theme`: String yang berisi hubungan (munasabah) dengan tema sebelumnya.
- `tafsir_theme_group`: String yang berisi kelompok tema tafsir.
- `tafsir_kosakata`: String yang berisi penjelasan kosakata dalam ayat.
- `tafsir_sabab_nuzul`: String yang berisi informasi tentang sebab turunnya ayat (kosong dalam contoh ini).
- `tafsir_conclusion`: String yang berisi kesimpulan dari tafsir ayat.

**Sumber Data**: [Quran Kemenag](https://quran.kemenag.go.id/)",Low,1.0
Translation,khoubrane-yousef/tulu-v2-sft-mixture-role-split-english-darija,2.0,20.0,2025-01-16 23:22:11+00:00,none,0.0,306 MB,320864256.0,306 MB,320864256.0,407986,none,none,"---
dataset_info:
  features:
  - name: dataset
    dtype: string
  - name: id
    dtype: string
  - name: role
    dtype: string
  - name: english
    dtype: string
  - name: darija
    dtype: string
  splits:
  - name: train
    num_bytes: 674898043
    num_examples: 407986
  download_size: 306195051
  dataset_size: 674898043
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- translation
language:
- ar
- en
---
# Tulu-v2 Role-Split English-Darija

This dataset is derived from [Tulu-v2-SFT-Mixture-English-Darija](https://huggingface.co/datasets/khoubrane-yousef/tulu-v2-sft-mixture-english-darija). It splits the original data by role (`user` and `assistant`), creating a separate row for each role with its corresponding English and Darija content.

## Features
- **Dataset:** Original source (e.g., `flan_v2`, `sharegpt`).
- **ID:** Unique identifier from the original dataset.
- **Role:** `user` or `assistant`.
- **English:** English content for the role.
- **Darija:** Translated Darija content for the role.

This format facilitates role-specific analysis and training for dialogue-based language models and translation models.

## Notes
- Messages with a mismatch in the number of turns between English (`messages`) and Darija (`messages_darija`) were removed if the number of turns exceeded 2. This is because repeated roles make it challenging to accurately match Darija messages to their English counterparts.
- If the number of turns was exactly 2 in English (`len(messages) == 2`) but only 1 in Darija (`len(messages_darija) == 1`), the message corresponding to the existing Darija role was retained.

## Citation
If you use this dataset, please cite the original [Tulu-v2-SFT-Mixture-English-Darija](https://huggingface.co/datasets/khoubrane-yousef/tulu-v2-sft-mixture-english-darija) sources.",Medium,2.0
Translation,kingkaung/islamqainfo_parallel_corpus,0.0,36.0,2025-01-27 07:00:39+00:00,cc-by-nc-4.0,0.0,UNKNOWN,unknown,177 MB,unknown,19052,none,none,"---
license: cc-by-nc-4.0
task_categories:
- table-question-answering
- text-generation
- translation
- summarization
- text-classification
language:
- en
- ar
- ur
- bn
- fr
- es
- zh
- ru
- de
- tg
- pt
- hi
- ug
- tr
- id
- ja
tags:
- Islam
- Question&Answer
- Religion
- Translation
- Corpus
- Parallel
- ParallelCorpus
pretty_name: islamqa.info
size_categories:
- 10K<n<100K
---

# Dataset Card for IslamQA Info Parallel Corpus

#### Dataset Description

The **IslamQA Info Parallel Corpus** is a multilingual dataset derived from the IslamQA repository. It contains curated question-and-answer pairs across 17 languages, making it a valuable resource for multilingual and cross-lingual natural language processing (NLP) tasks. The dataset has been created over nearly three decades (since 1997) by Sheikhul Islam Muhammad Saalih al-Munajjid and his team.

### Key Features

- **Curated by:** IslamQA team
- **Languages (NLP):** English (base), Arabic, Urdu, Bengali, French, Chinese, Spanish, Russian, German, Tajik, Portuguese, Hindi, Uyghur, Turkish, Indonesian, and Japanese.
- **License:** CC BY-NC 4.0 (Attribution-NonCommercial)
- **Size:** Contains 19,052 rows in Parquet format.
- **Content Fields:**
  - `Global ID`: Unique ID for each question-answer group.
  - `topic`: Thematic category (e.g., Belief, Sacrifices).
  - `original_id`: Original source ID.
  - `title`, `question`, `answer`: Content fields for each language (e.g., `title_en`, `question_ar`, `answer_bn`).
  - `link`: URLs for verification (e.g., `link_en`, `link_zh`).

### Dataset Sources

- **Repository:** [IslamQA Info Parallel Corpus on Hugging Face](https://huggingface.co/datasets/kingkaung/islamqainfo_parallel_corpus)
- **Original Source:** [IslamQA Website](https://islamqa.info/)

# Uses

### Direct Use

This dataset is suitable for:
- Multilingual NLP tasks such as machine translation, question-answering, and text summarization.
- Cross-lingual semantic search and information retrieval.
- Training and fine-tuning multilingual models.

### Out-of-Scope Use

The dataset is not intended for generating biased or harmful content. Some topics may include culturally or religiously sensitive concepts; users should respect the context and original intent.

### Dataset Structure

The dataset is available **only in Parquet format**, encoded in UTF-8. It includes the following fields:

1. `Global ID`: Groups translations of the same Q&A across languages.
2. `topic`: Thematic category of the Q&A.
3. `original_id`: Reference to the original source.
4. `title`, `question`, `answer`: Content fields for each language (e.g., `title_en`, `question_ar`, `answer_bn`).
5. `link`: URLs for verification (e.g., `link_en`, `link_fr`).

### Sample Record

```json
{
    ""Global ID"": ""G_ID_00157"",
    ""topic"": ""Belief"",
    ""original_id"": ""10776"",
    ""title_en"": ""The Best Means to Increase Your Faith"",
    ""question_en"": ""Question\n\nWhat are the means of increasing faith?"",
    ""answer_en"": ""Praise be to Allah.There are several means of increasing faith :  \n\nLearning about Allah through His names and attributes. The more a person learns about Allah through His names and attributes, the more he will increase in faith, without a doubt. Hence you find that the scholars who know more about the names and attributes of Allah than others do, are stronger in faith than others in this regard.  \nLooking at the signs of Allah in the universe and the signs of Shari`ah (i.e., verses of the Quran and miracles of the Prophet (peace and blessings of Allah be upon him), etc.). The more a person looks at the signs of Allah in the universe, the more he increases in faith. Allah says (interpretation of the meaning):\n\n“And on the earth are signs for those who have Faith with certainty. And also in your own selves. Will you not then see?.” [Al-Dhariyat 51:20-21]\nThe verses which indicate that are many, I mean the verses which indicate that by pondering and thinking about this universe, man can increase in faith. \nDoing many acts of worship, for the more acts of worship a person does, the more he will increase in faith thereby, whether those acts of worship involve words or actions. So dhikr increases faith in quantity and quality, and prayer, fasting and Hajj also increase faith in quantity and quality.\nFor more, please see these answers: 14041 , 331637 , 22877 , 20059 , 223615 , and 9082 .\nAnd Allah knows best."",
    ""title_ar"": ""ما هي أسباب زيادة الإيمان ؟"",
    ""question_ar"": """",
    ""answer_ar"": ""الحمد لله.للزيادة أسباب :\nالسبب الأول : معرفة الله تعالى بأسمائه وصفاته , فإن الإنسان كلما ازداد معرفة بالله وبأسمائه وصفاته ازداد إيماناً بلا شك , ولهذا تجد أهل العلم الذين يعلمون من أسماء الله وصفاته ما لا يعلمه غيرهم تجدهم أقوى إيماناً من الآخرين من هذا الوجه .\nالسبب الثاني : النظر في آيات الله الكونية , والشرعية , فإن الإنسان كلما نظر في الآيات الكونية التي هي المخلوقات ازداد إيماناً قال تعالى : ( وفي الأرض آيات للموقنين وفي أنفسكم أفلا تبصرون ) الذاريات /20-21 والآيات الدالة على هذا كثيرة أعني الآيات الدالة على أن الإنسان بتدبره وتأمله في هذا الكون يزداد إيمانه .\nالسبب الثالث : كثرة الطاعات فإن الإنسان كلما كثرت طاعاته ازداد بذلك إيماناً سواء كانت هذه الطاعات قولية أم فعلية , فالذكر يزيد الإيمان كمية وكيفية , والصلاة والصوم والحج تزيد الإيمان أيضاً كمية وكيفية ."",
    ""title_bn"": ""ঈমান বৃদ্ধির কারণগুলো কী কী?"",
    ""question_bn"": """",
    ""answer_bn"": ""আলহামদু লিল্লাহ।.ঈমান বৃদ্ধির কারণগুলো:\nপ্রথম কারণ: \nআল্লাহ্‌কে তার নাম ও গুণসমূহসহ জানা। নিঃসন্দেহে মানুষ আল্লাহ্‌কে যতবেশি জানবে, তাঁর নাম ও গুণগুলোকে যতবেশি জানবে; ততই তাঁর ঈমান বৃদ্ধি পাবে।..."",
    ""title_fr"": ""Les causes de l’augmentation de la foi"",
    ""question_fr"": """",
    ""answer_fr"": ""Louange à Allah.Les causes sont les suivantes :\nLa première : La connaissance d’Allah le Très-Haut par l'acquisition de la connaissance de Ses noms et Ses attributs. Plus l’homme approfondit sa connaissance d’Allah, de Ses noms et de Ses attributs, plus sa foi s’affermit certainement..."",
    ""title_es"": ""¿Cuáles son los medios para aumentar la fe?"",
    ""question_es"": """",
    ""answer_es"": ""Alabado sea Dios.Hay muchos medios para aumentar la fe: \nEl primer medio es aprender sobre Allah a través de Sus nombres y atributos. Cuanto más aprenda una persona sobre Allah, Sus nombres y atributos, más aumentará su fe..."",
    ""title_zh"": ""信仰增加的因素是什么？"",
    ""question_zh"": """",
    ""answer_zh"": ""一切赞颂，全归真主。\n“信仰增加的因素很多：\n第一个因素：了解伟大真主的所有尊名和属性，每当一个人对真主的所有尊名和属性了解越多，他的信仰肯定会增加..."",
    ""title_ru"": ""Как можно увеличить веру?"",
    ""question_ru"": """",
    ""answer_ru"": ""Хвала Аллаху.Есть несколько способов увеличения веры.\nПервое средство. Больше узнавать об Аллахе через изучение Его имён и атрибутов..."",
    ""title_de"": ""Was sind Wege zu einem stärkeren Glauben (Iimaan)?"",
    ""question_de"": """",
    ""answer_de"": ""Alles Lob gebührt Allah..Für einen starken Glauben (Iimaan) gibt es verschiedene Gründe:\nDer erste Grund: Das Kennen Allahs dem Erhabenen mit Seinen Namen und Eigenschaften..."",
    ""link_en"": ""https://islamqa.info/en/answers/10776/the-best-means-to-increase-your-faith"",
    ""link_ar"": ""https://islamqa.info/ar/answers/10776/%D9%85%D8%A7-%D9%87%D9%8A-%D8%A7%D8%B3%D8%A8%D8%A7%D8%A8-%D8%B2%D9%8A%D8%A7%D8%AF%D8%A9-%D8%A7%D9%84%D8%A7%D9%8A%D9%85%D8%A7%D9%86"",
    ""link_bn"": ""https://islamqa.info/bn/answers/10776/%E0%A6%88%E0%A6%AE%E0%A6%A8-%E0%A6%AC%E0%A6%A6%E0%A6%A7%E0%A6%B0-%E0%A6%95%E0%A6%B0%E0%A6%A3%E0%A6%97%E0%A6%B2-%E0%A6%95-%E0%A6%95""
}

``` 
# Dataset Creation

### Curation Rationale

This dataset was created to facilitate research and applications in multilingual NLP. It offers a rich source of content, including translations, topics, and unique cultural insights.

### Source Data

The data was collected directly from the IslamQA website using web scraping tools. The original content was manually curated and verified by the IslamQA team over the years.

Data Collection and Processing
	•	Extracted Q&A pairs, topics, and multilingual versions using HTML parsing tools (e.g., BeautifulSoup).
	•	Mapped multilingual articles to their English equivalents.
	•	Standardized column names and ensured content consistency across languages.

```
````
### Usage Example

To use this dataset in your NLP workflows, you can load it directly using the Hugging Face `datasets` library:

```python
from datasets import load_dataset

# Load the dataset
dataset = load_dataset(""kingkaung/islamqainfo_parallel_corpus"")

# Display the first few rows
print(dataset[""train""].select(range(5)))

```

# Who are the Source Data Producers?

The dataset originates from the IslamQA website, managed by Sheikhul Islam Muhammad Saalih al-Munajjid and his team.

# Bias, Risks, and Limitations

### Risks and Limitations
	•	The dataset reflects Islamic values and may include topics that are sensitive or non-Western (e.g., Jihad, religious rulings).
	•	Translations may vary in quality and completeness across languages.
	•	The dataset is not anonymized and includes URLs pointing to the original content.

### Recommendations
	•	Users should respect the cultural and religious context of the dataset.
	•	Obtain permission from IslamQA for any commercial use.
	•	Consider additional filtering or preprocessing if required for specific tasks.

# Citation

### BibTeX:

```
@misc{islamqa_parallel_corpus,
  author = {Sheikhul Islam Muhammad Saalih al-Munajjid and the IslamQA team},
  title = {IslamQA Info Parallel Corpus},
  year = {2025},
  url = {https://huggingface.co/datasets/kingkaung/islamqainfo_parallel_corpus},
  note = {Dataset for research purposes only.}
}

```

###### APA:

```

Sheikhul Islam Muhammad Saalih al-Munajjid & IslamQA team. (2025). IslamQA Info Parallel Corpus. Retrieved from https://huggingface.co/datasets/kingkaung/islamqainfo_parallel_corpus

```

## License
This dataset is licensed under the [Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/).  
- You are free to share, adapt, and use the dataset for non-commercial purposes.  
- Commercial use is prohibited without explicit permission from the original authors.  
",High,6.0
Translation,ZoneTwelve/multilingual-stories,0.0,11.0,2025-02-05 10:01:17+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
language: 
  - fr  # French
  - it  # Italian
  - de  # German
  - en  # English
  - ko  # Korean
  - es  # Spanish
  - zh  # Chinese
  - ja  # Japanese
  - ru  # Russian
  - cs  # Czech
  - da  # Danish
  - nl  # Dutch
  - ar  # Arabic
  - bg  # Bulgarian
  - et  # Estonian
  - hu  # Hungarian
  - id  # Indonesian
  - nb  # Norwegian
  - pt  # Portuguese
  - el  # Greek
  - lt  # Lithuanian
  - fi  # Finnish
  - lv  # Latvian
  - pl  # Polish
  - sv  # Swedish
  - sk  # Slovak
  - sl  # Slovenian
  - ro  # Romanian
  - tr  # Turkish
  - uk  # Ukrainian

language_bcp47:
  - zh-Hant  # Chinese (Traditional)
  - zh-Hans  # Chinese (Simplified)
  - en-GB  # English (British)
  - pt-BR  # Portuguese (Brazilian)

pretty_name: ""Multilingual Dataset""

tags:
  - multilingual
  - NLP
  - text-generation
  - text-classification
  - dataset-processing
  - large-scale-dataset

license: ""apache-2.0""

task_categories:
  - text-classification
  - text-generation
  - text2text-generation
  - feature-extraction
  - translation
  - summarization

size_categories:
  - large

source_datasets: []
---

# Multilingual Dataset

This dataset contains multilingual articles generated using various models.

# Dataset Summary

**Total records:** 14850

## Categories
Unique categories and their frequencies:
- **OmundoAmanha**: 99
- **malinois**: 33
- **CBDPouches**: 33
- **ingenieurs**: 66
- **PPeperomioides**: 33
- **DiscGolfCarts**: 33
- **TIMAF**: 33
- **AntisocialMemeClub**: 33
- **Tancan**: 66
- **ogerwaters**: 33
- **MotosBr**: 33
- **lgballt**: 66
- **CipherAcademy**: 33
- **LofiEdits**: 33
- **jennyraee**: 33
- **RonnieHotdogs**: 33
- **Bratzillaz**: 66
- **cubiccommunity**: 33
- **Labectance**: 33
- **ChasmicOccult**: 33
- **exresidents**: 33
- **GamingParents**: 33
- **Elisaonfire**: 33
- **blackdesert**: 66
- **MockElectionsAUS**: 33
- **DogfreeHumor**: 33
- **TToT**: 33
- **GeneralContractor**: 33
- **nkim**: 33
- **HeadandNeckCancer**: 33
- **CantonReddit**: 99
- **GrassrootsSelectAZ**: 33
- **BlackCloverMobile**: 33
- **uclafood**: 66
- **trueskyrimmods**: 33
- **heximal**: 33
- **Hetrochromia**: 33
- **photographysatx**: 33
- **AsturiasPatriaQuerida**: 33
- **2b2tqueue**: 99
- **kristyglas**: 33
- **advancedwitchcraft**: 33
- **rockymountainhouse**: 132
- **Kontaktanzeigen**: 33
- **DepressionVentSpace**: 33
- **MagickaWizardWarz**: 33
- **miniature_tigers**: 66
- **ifunnyNovoMundo**: 33
- **ShrineOfAC**: 66
- **EDAnonPics**: 33
- **FontLab**: 33
- **agent_of_shield**: 66
- **Infinity_laugh**: 33
- **CataclismoRTS**: 33
- **5DChess**: 33
- **idiotsflyingdrones**: 33
- **cscareerquestionsIN**: 66
- **birdsonbirds**: 99
- **msvduisburg**: 33
- **Eunha**: 33
- **Rotundanimals**: 33
- **Norse**: 66
- **treadstone**: 33
- **MadilynMei**: 66
- **ValentinaRose**: 66
- **AzurePercept**: 33
- **HollyHoes**: 66
- **exquisitecorpse**: 33
- **NoglaTerroriserReacts**: 33
- **StephenHillenburg**: 99
- **maxima**: 33
- **turndotme**: 33
- **dakboard**: 66
- **NetflixAnime**: 66
- **semaglutideweightloss**: 198
- **TheGatewayExperience**: 33
- **SubredditForFriends**: 33
- **porchpiratepwnage**: 33
- **throughtheages**: 66
- **TheFakeDentist**: 66
- **SquidMoonCoin**: 99
- **BazarRogueExcrement**: 66
- **techmaestro**: 33
- **ChurchOf_AquaxAixRuby**: 33
- **NLProducerGang**: 66
- **flannel**: 33
- **alternativewritings**: 66
- **Crypts**: 33
- **Mr_MandM**: 33
- **7counter**: 33
- **TwitchSafe**: 66
- **dRehmFlight**: 66
- **TheLetterOctothorpe**: 33
- **chuck**: 33
- **ConfrontingChaos**: 33
- **GettingOverItGame**: 33
- **KillYourConsole**: 33
- **Male_Studies**: 66
- **lacenterwa**: 33
- **IrishFishing**: 66
- **ptvo**: 99
- **Phigolf**: 33
- **the_derek**: 33
- **preciousmoments**: 33
- **AnaDaniela69xxx**: 33
- **blergg**: 33
- **GraphingCalculator**: 132
- **Knoxville**: 33
- **The_Real_Austin**: 33
- **EricBrakey**: 33
- **NightWitchGang**: 33
- **CityNerd**: 99
- **Pratibha_Ranta**: 99
- **BSCStation**: 33
- **WLWactually**: 33
- **suddenlymandjtv**: 33
- **Celestron**: 99
- **CSMagicRecipe**: 33
- **Phone_Foldables**: 99
- **MavuikaMainsGI**: 33
- **InboxPounds**: 33
- **janiscorner**: 33
- **PECG**: 33
- **IcebergExplained**: 33
- **GreedIncorporated**: 33
- **SurvivingRingworm**: 66
- **Snowplow**: 33
- **Tadano**: 66
- **funtechnology**: 33
- **ACUsedVillagers**: 33
- **Fordtractors**: 66
- **CargoWise**: 33
- **earwig**: 33
- **nocontextaudio**: 33
- **kenbarbbe**: 33
- **cwstonekingfans**: 33
- **VicBWolfArt**: 33
- **BrothersoftheGrain**: 99
- **TheRunaways**: 33
- **NewsWorld**: 66
- **DELF**: 33
- **deardiary**: 33
- **MyOneLineDogma**: 33
- **bunchofamateurs**: 33
- **Goulburn**: 66
- **Mattpryor**: 33
- **communitydevelopment**: 33
- **MiddleGradeWriters**: 66
- **businesslaw**: 33
- **BanglaMuktobak**: 33
- **StupidMario**: 66
- **Grimgar**: 33
- **HarvestMoonGBA**: 66
- **SOTR_auditions**: 99
- **TheShannaraChronicles**: 33
- **overcoding**: 33
- **Awareparents**: 66
- **dogmalove**: 66
- **HAESfood**: 33
- **EvergreenSkills**: 33
- **valentina94O**: 33
- **fsft**: 33
- **OkBuddyYharim**: 33
- **Swapandsell**: 66
- **warhospital**: 33
- **Demeo**: 33
- **TvIntroMashups**: 33
- **Winiday**: 132
- **algogaming**: 33
- **IhateElon**: 99
- **XtotheV**: 33
- **SouthOfTheCircleGame**: 66
- **wlu**: 33
- **MarketingFails**: 33
- **constanzavelazcoteton**: 33
- **legoww1**: 33
- **cheekdimples**: 33
- **circularguements**: 33
- **BattleforBFCI**: 33
- **JohnFarnham**: 99
- **DisablEdCanTeach**: 33
- **saysueme**: 66
- **Rengarmains**: 33
- **overclockfps**: 33
- **GOTV**: 66
- **Business_Management**: 33
- **latticeclimbing**: 33
- **rarehugs**: 33
- **AroAllo**: 33
- **gfbreadrecipies**: 66
- **JCBDL**: 33
- **snowplowproject**: 33
- **knockedloose**: 66
- **ScoreLords**: 33
- **ShopImprovements**: 33
- **256cub**: 66
- **UltiumCellsLLC**: 66
- **ObscureGIFs**: 33
- **alchademy**: 33
- **CastroPodcast**: 33
- **MelissaONeilPics**: 66
- **ConsensusDebate**: 33
- **okbuddyplants**: 33
- **mischakim**: 99
- **Palermo**: 33
- **losersofinstagram**: 99
- **lezconnect**: 33
- **TyreNichols**: 33
- **Primis_Bank**: 33
- **givemeanidea**: 33
- **Actualfixes**: 33
- **GrowingGirls**: 33
- **YukongMains**: 66
- **YapDollarMyBeloved**: 33
- **trials**: 33
- **wbmedicos**: 33
- **RaceTrackDesigns**: 33
- **supercellmoment**: 33
- **Survivors_BBC_1975**: 66
- **the_mysterium**: 33
- **KillerKitty**: 33
- **kaybomb**: 33
- **Blogsabouttea**: 33
- **OlympiacosBC**: 33
- **ReDIY**: 33
- **Hampture**: 33
- **Demonias_**: 66
- **WILTWIFLS**: 33
- **Analysts_of_MBTI**: 66
- **SoulSplitGame**: 33
- **gracelyra**: 33
- **Meme__World**: 66
- **CrescentHub**: 66
- **psytrancefrance**: 33
- **2016campaign**: 66
- **theguild3**: 33
- **practicingheathenry**: 33
- **budgetguitargear**: 99
- **Dogma**: 33
- **novomesto**: 33
- **ContinentedoBombarral**: 33
- **Comadre**: 33
- **thethyroidmadness**: 33
- **Xooginme**: 33
- **HomebrewTactical**: 33
- **kosmic**: 33
- **GameGeeks**: 33
- **TESHeroes**: 66
- **SchoolMotivation**: 66
- **dataforenergyskills**: 33
- **VeganJunkFood**: 66
- **TheRealArkhamWorld**: 33
- **ExpressiveArtsTherapy**: 33
- **TojinoMiko**: 33
- **Suicide_Talk**: 33
- **Shaolin**: 33
- **PositiveHerpes**: 33
- **MaddiMays**: 33
- **GeneseoIllinois**: 33
- **AutoClassifieds**: 33
- **littlenatie_**: 33
- **KoolKorsunsky**: 33
- **Airpower**: 33
- **foundCust0mCraft**: 33
- **PowerUniversity**: 66
- **BirdBoxMemes**: 33
- **Sevigor**: 33
- **Wreddit**: 66
- **digitalcats**: 33
- **llamas**: 66
- **XGIMI_Beta_Testing**: 33
- **orlandofishing**: 33
- **animalsasleaders**: 33
- **MUCC**: 33
- **Icefuse**: 33
- **FunkyLizard**: 33
- **Keensmemes**: 66
- **hamood**: 66
- **Terrarya**: 33
- **indianmetamisogynist**: 99
- **CelebrityBoots**: 66
- **autokey**: 66
- **kegswitharms**: 33
- **Unexpectedllamas**: 132
- **accidentalsubreddit**: 99
- **temporarygunowners**: 33
- **djtzone**: 33
- **LetsTalkRocks**: 66
- **antisexwork**: 33
- **aquaponie**: 33
- **clandestineairsoft**: 33
- **Paget_Schroetter**: 33
- **civcast**: 66
- **GHSE**: 33
- **massnorthshorefmlokin**: 33
- **HandmadeLure**: 33
- **PantslessPajamas**: 33
- **CrocodileHunters**: 33
- **lumberjackcidents**: 33
- **ShintoReligion**: 33
- **Moondrop_Sundrop**: 33
- **NarutoNinjaStorm**: 33
- **TrevTutor**: 66
- **ZichWedding**: 33
- **ParadigmFoundation**: 33
- **unsentLoveLetters1st**: 33
- **PaidInternships**: 33
- **serfposting**: 33
- **thatsthesexnumber**: 33
- **OutlanderPHEV**: 33
- **SUITrump**: 33
- **redwolf344**: 33
- **ratemyfridge**: 33
- **Starry_Flowers**: 33
- **f1online**: 66
- **hodlandshill**: 33
- **bigchungusarmy**: 33
- **OddportAcademy**: 33
- **MCxYuri**: 66
- **Islamic_History**: 99
- **FMF**: 33
- **JacindaArdern**: 33
- **bristoldrill**: 33
- **Halloweenlove**: 33
- **iPhone14Pro**: 99
- **Mountaineering**: 66
- **PuyEnVelay**: 66
- **GameGearMicroMods**: 33
- **ThriftStoreHaulsValue**: 66
- **Hoisan**: 33
- **artcovers**: 33
- **BrawlStarsGiftShop**: 33
- **GrumpyCoinCommunity**: 33
- **foundAwesomeness_999**: 33
- **imaginaryelectionscj**: 33
- **akbbyy**: 66
- **kleptomanicsupport**: 33
- **CivScarcity**: 66
- **DesignforFDM**: 66
- **cursedminecraftimages**: 66
- **simplethoughts**: 33
- **Nat1pub**: 66
- **hawaiiboonie**: 33
- **investigators**: 33
- **YabukiNako**: 33
- **hiphoptoday**: 99
- **maximalistfashion**: 33
- **accurateguides**: 66
- **DevExpress**: 33
- **WonderlandGameSeries**: 66
- **ErgoMechKeyboards**: 99
- **Urgot**: 99
- **PopulationMaps**: 66
- **CatsNamedToothless**: 33
- **MinecartDriverServer**: 33
- **GwapaGangNFT**: 66
- **Midnightthoughts**: 66
- **comicstriphistory**: 33
- **unibo_studenti**: 99
- **IronGiantMemes**: 33
- **homelessmind**: 33
- **MajesticCats**: 33
- **preprocessing**: 66
- **Ripsquad**: 99
- **Flowerfellfuture**: 33
- **badradio**: 33
- **SavingPrivateNoOne**: 33
- **ToothpasteEnema**: 33
- **karinerino**: 99
- **albertanudist**: 33
- **LINCHWAR**: 33
- **vanifestos**: 33
- **FakemonDesignContests**: 33
- **schmitty**: 33
- **gensoukyou**: 33
- **Warside**: 33
- **Apex_NC**: 33
- **CreamyCami8407**: 33
- **smallengines**: 33
- **IABrasil**: 33
- **FeiProtocol**: 66
- **BugandSeek**: 33
- **Finntroll**: 33
- **SneakersNL**: 33
- **ImpracticalFoods**: 33
- **arabhealth**: 66
- **okbuddynicotine**: 33
- **stoicfemina**: 99
- **mathsucks**: 33
- **freezepop**: 33
- **AuthoritarianNewsHub**: 33
- **corpsepaint**: 66
- **FXMasterRace**: 33
- **RetroConsoleModding**: 33
- **e17**: 66
- **UruguaySoftCombat**: 33
- **cupcakes_please**: 33
- **OfMonstersAndMen**: 33
- **HingeStories**: 33
- **sashannarcy**: 33
- **slapbass**: 66
- **fakepng**: 33
- **hibid**: 33
- **obletsvet**: 33
- **MaxarTechnologies**: 66
- **HopeKelesis3**: 33
- **wrestlingbelts**: 99
- **indian_desi**: 33
- **failedshinypokemon**: 99
- **ChutneyMusic**: 33
- **kleinerfanclub**: 33
- **TheeCrew**: 33
- **OctopathTraveler2**: 33
- **slidan**: 33
- **Worldbuilders**: 33
- **AussieTherians**: 66
- **PaganRockCollectors**: 33
- **SixDegreesOfCoulson**: 66
- **rubik**: 99
- **slk**: 33
- **Paper4Paper**: 33
- **YoutubeSings**: 33
- **KingdomHearts**: 33
- **reallycoolrobots**: 99
- **RedmiNote9pro**: 33
- **NarutoRP**: 99
- **basketballanalytics**: 33
- **russianfemboy**: 33
- **IndyOnlyfans**: 33
- **Teentalks**: 33
- **UKISP**: 33
- **VisualNovelCryptids**: 33
- **StudyCatalogPublish**: 33
- **Pawar2018**: 33
- **CanadaMonopolyProtest**: 33
- **chainsawLife**: 66
- **AltPolitic**: 33
- **yizhuoning**: 99
- **calculus**: 66
- **deservedapmaan**: 66
- **proofoftalk**: 33
- **joinelevatehealth**: 33
- **readbeforeeat**: 33
- **DramaticReading**: 33
- **worst_maps**: 33
- **anitwtitter**: 33
- **dramatic_kibbe**: 66
- **lookingforgroup**: 33
- **HISHE**: 66
- **laserweapons**: 66
- **CrazyPassengers**: 33
- **KSPMildlyInteresting**: 33
- **stpetebeach**: 99
- **BeingTube**: 66
- **JaydenBartels**: 99
- **Ticos_TI**: 66
- **MuseumOfArtifacts**: 33
- **GastroHealth**: 33
- **Gailbatescncdream**: 66
- **CoalitionAgainstEvil**: 99
- **TotalDramaPolls**: 66
- **TOPTIERSESSIONS**: 66
- **BossfightBestiary**: 66
- **playlost**: 33
- **DurangoCJAYM**: 33
- **Plunderstorm**: 33
- **Solitary**: 33
- **SproutValleyGame**: 33
- **LesbicasPortuguesas**: 33
- **blip**: 33
- **DeerfieldHighSchool**: 99
- **norwalkct**: 66
- **Dumbconspiracytheorys**: 33
- **FBEPC**: 33
- **bestdealsPH**: 33
- **careerguidance**: 33
- **izombiecw**: 33
- **tongueknots**: 33
- **MarketingCareers**: 66
- **Sommerspiele**: 33
- **Spaceagency2138**: 66
- **monkeynft**: 33
- **foundthewaltwhitepfp**: 33
- **MillaSofiaLookbook**: 66
- **horizonmemes**: 33
- **MVAgusta**: 33
- **FlagRedesigns**: 33
- **ShesJudgingYou**: 66
- **Lisle**: 33
- **NoMansSkyOfficial**: 33
- **PeptideForum**: 66
- **cutoutdresses**: 33
- **r2northstar**: 66
- **PatrioticMurica**: 33
- **WildernessBackpacking**: 66
- **herpes_dating**: 99
- **Unsatisfying**: 33
- **naming_animals**: 33
- **AskPhotography**: 66
- **jakeneutrality**: 66
- **tacticalops**: 33
- **BackyardBoxing**: 33
- **yujin**: 33
- **DammitZappa**: 33
- **AITL**: 33
- **jianca14**: 33
- **CuteAndFunnyAnime**: 33
- **MyHotwife_**: 33
- **Razorlight**: 33
- **yunli**: 33
- **wholesomebutstrange**: 66
- **jaywalking**: 66
- **marmello**: 33
- **turkayak1**: 66
- **reddithydrogen**: 99
- **Szormok_AVE**: 66
- **OpenMatte**: 33
- **XboxStudios**: 66
- **ScottishMeetUps**: 33
- **gsh**: 33
- **DasBootMaimais**: 33
- **phf**: 66
- **TopCustomPaper**: 66
- **TheJournal4Project**: 33
- **agfy**: 99
- **GOPwatch**: 33
- **meepo**: 66
- **LondonLadies**: 33
- **ukrainethelatest**: 99
- **PoliceDog**: 33
- **ActinicFolliculitis**: 33
- **BlockedAndReported**: 33
- **DPSdubai**: 66
- **skku**: 99
- **formulaminecraft**: 33
- **aremystitchestwisted**: 33
- **JESSIFOXXinflates**: 33
- **DungeonBuilder**: 33
- **myronkoops_**: 33
- **GeorginaCampbell**: 66
- **MeticulousEspresso**: 33
- **RoadRash64**: 33
- **HamiltonEuchre**: 33
- **JusticeforKarenRead_2**: 33
- **Lystfiskeri**: 33
- **dogsofukraine**: 33
- **CubaVexillology**: 33
- **Manga_Germany**: 33
- **MadLed**: 33
- **SuddenlyGayCommunism**: 66
- **TransFem18**: 33
- **Stickwars**: 66
- **minecraft_youtube**: 33
- **penguin**: 33
- **Charliethething**: 66
- **2imperialeurope4u**: 33
- **imsorrymumbo**: 33
- **guitarproblems**: 33
- **actuallyItsCMF**: 66
- **Surfari**: 66
- **WorldwideKahoot**: 66
- **BillBrasky**: 33
- **Shoeler**: 33
- **DishaPataniFans**: 33
- **tunnelsnakes**: 33
- **TangleAndWhisper**: 33
- **Onshape**: 33
- **LGV60**: 33
- **weddinghashtags**: 66
- **Travelbuddy**: 33
- **RedGreenShow**: 33
- **tankionlineint**: 33
- **mermaid**: 66
- **FansofMissAnnaGigi**: 66
- **AtheistCheeseCake**: 66
- **systemml**: 33
- **TransPuns**: 33
- **deepnutrition**: 33
- **cindernft**: 33
- **Farmasi_drama_snark**: 33
- **pollotarian**: 33
- **CityOfLiverpoolFC**: 33
- **femboy_kay**: 33
- **inmigracionhaitiana**: 33
- **OuterSpaceTravel**: 66
- **duckliftingnews**: 33
- **Tasha_sexy_21**: 33
- **SMK25**: 66
- **EnochianMagick**: 33
- **FtMFashionAdvice**: 33
- **twosentencevindicated**: 33
- **GameStopCanada**: 33
- **southbaldwincountyfwb**: 99
- **RealTruthSeeking**: 33
- **CarDIY**: 33
- **brookiecookiesnark**: 33
- **beardmeetsfood**: 33
- **shiphappens**: 33
- **zuhanygondolatok**: 33
- **jojoly**: 66
- **Love101Netflix**: 33
- **GoodTask**: 66
- **F95zonee**: 66
- **cutetextfaces**: 33
- **boardgamingmy**: 66
- **carbobike**: 33
- **FlowerGirlsTamagotchi**: 99
- **Gemology**: 33
- **northrupphoto**: 33
- **rdms**: 33
- **ErikaHerceg**: 66
- **aua**: 66
- **blockethaveri**: 33
- **routeros**: 33
- **IndieWorldOrder**: 33
- **EliteOuendan**: 99
- **ChristianHistory**: 33
- **EastAfricaNews**: 33
- **FatFrankie**: 33
- **HDDcoinNetwork**: 33
- **americanman**: 33
- **callthephotoshoppers**: 33
- **Pisstearmanseries**: 33
- **shaderDev**: 33
- **RollWithAdvantage**: 33
- **thenightmarecollector**: 33
- **SeadooSpark**: 33
- **JadeAzevedo**: 66
- **CHSMichelleSnark**: 33
- **unpresidented**: 33
- **SimLab**: 99
- **Cybers_public_school**: 33
- **monte_video**: 33
- **chaoticswifties**: 66
- **ImmersiveDaydreaming**: 33
- **MattChampion**: 33
- **RLCarShow**: 33
- **KtuKerala**: 33
- **TaylorSwiftAnkles**: 66
- **ryze**: 33
- **ratemysinging**: 99
- **HMHSBritannic**: 33
- **WhoSampled**: 33
- **DiscussChrist**: 33
- **JavelinTalk**: 33
- **titaniumbikes**: 33
- **AnimeAndEtc**: 66
- **KSPToMarslanderteam**: 66
- **Catchirps**: 33
- **virtualwarfighter**: 33
- **coopgames**: 66
- **YouthHockeyHighlights**: 33
- **buyaplantsellaplant**: 33
- **BolsaChile**: 33
- **Dikpipiler**: 33
- **EryingMotherboard**: 33
- **RUFUSDUSOL**: 66
- **sanantoniobeer**: 33
- **techpolitics**: 66
- **RockPosters**: 33
- **dorajar**: 33
- **suddenlyvicodin**: 66
- **nadide**: 33
- **BigStickEnergy**: 66
- **zoomergen**: 99
- **drivingUK**: 66
- **MSPA**: 66
- **TheATOShow**: 33
- **TheMegatronDon**: 33
- **EnemiesBeingBros**: 33
- **ratmemes**: 33
- **MirMGlobal**: 33
- **AppleAndOnion**: 33
- **B638**: 66
- **LGBT_iel**: 66
- **TrainingEverywhere**: 99
- **biggestbroent**: 66
- **WWYDif**: 33
- **DSTAlumnaeChapter**: 33
- **WWE_Eva_Marie**: 33
- **guatemala_gay**: 66
- **CringyAntiVegans**: 33
- **NinaDobrev_**: 33
- **makemoneyforexreddit**: 33
- **iJustPublishedMyGame**: 66
- **lordtuts**: 33
- **brennanjones**: 33
- **SubmarineTitans**: 33
- **ClonazepamHealth**: 33
- **okbuddyrhodok**: 66
- **FinancialMeme**: 33
- **FiverFilth**: 33
- **Theni**: 33
- **VZfit**: 132
- **FunctionalFill**: 33
- **SACWATR**: 33
- **GiuliaBenitee**: 33
- **Reality_Quest**: 33
- **SheGotFrameWork**: 33
- **abertayhackers**: 99
- **ThePower**: 66
- **IceandFirePowers**: 33
- **datasatanism**: 66
- **badshirtanatomy**: 33
- **Necrontyr**: 33
- **signs**: 33
- **vrbohosts**: 66
- **erannorthchronicles**: 66
- **Brainysweddit**: 66
- **Ooergonewild**: 66
- **EB2_NIW_Cybersecurity**: 33
- **Djlynch2009**: 33
- **foundtheAngela**: 66
- **digitaltabletop**: 33
- **Idvitalia**: 33
- **surebet**: 33
- **subscriptionsharing**: 33
- **Exodus90**: 33
- **northfork**: 33
- **Emsland**: 33
- **CoilCommunity**: 66
- **shameless**: 33
- **ManitobaGardening**: 33
- **Telekinetic**: 99
- **TheNew52**: 33
- **TeamSky**: 33
- **HypixelSlovenija**: 33
- **Stabbingthemap**: 33
- **mysteriousmetallicorb**: 33
- **DeerVirus**: 66
- **EquinoxEv**: 33
- **titration**: 33
- **NoSenseMemes**: 33
- **HellenicMemes**: 66
- **FoundTheGiantSquid**: 33
- **MetroidDread**: 33
- **Chiskate**: 33
- **cannadips**: 66
- **WeHateYT**: 33
- **askdisqus**: 33
- **marniexvictor**: 66
- **RootandDigby**: 33
- **SuperheroLeague**: 33
- **CipherMains_HSR**: 33
- **obviouslyincorrect**: 33
- **InfinityNado**: 33
- **desporto**: 33
- **Dallas_commons**: 33
- **go_ask_your_crush_out**: 132
- **heelonly**: 33
- **Porsgrunn**: 33
- **truetexan**: 66
- **rushzilla**: 33
- **banime**: 33
- **Nalbinding**: 33
- **oliviadejarnet**: 66
- **preparedguitar**: 66
- **NielsonShow**: 66
- **transsexmalesurgeries**: 99
- **cleanpuppers**: 33
- **InesperadoPau**: 33
- **louderthanlife**: 33
- **ZamasuDidNothingWrong**: 99
- **OdeyaRush**: 33
- **CyberSecurityAdvice**: 33
- **dutchposer**: 33
- **WK2**: 33
- **minecraftinfographics**: 33
- **applycoldwatertoburn**: 66
- **fearofheights**: 33
- **themeaneyoflife**: 33
- **mildlyaccessible**: 33
- **BLACKED_AL**: 33
- **bluegrassguitar**: 66
- **azuretips**: 66
- **fitbod**: 33
- **somaligay**: 33
- **TheWellsTable**: 33
- **Poudlard**: 33
- **10000ShipsHBO**: 66
- **TookOurJerbs**: 66
- **FreeGameKeys**: 33
- **CloudAssessments**: 33
- **OTRebels**: 66
- **berlinfoodies**: 66
- **DisneyParkHistory**: 33
- **infjhome**: 33
- **jotform**: 33
- **danielledominguez_**: 66
- **ryzencpu**: 33
- **bandirma**: 33
- **cdldriver**: 99
- **CarpetsforAirports**: 66
- **Superchunk**: 66
- **artisansasylum**: 33
- **ClubeDoZanzigau**: 99
- **HowToMen**: 33
- **accidentaldiamondclub**: 33
- **CelebrityOutside**: 33
- **RobertoArguelles**: 33
- **ConcourseCityTransit**: 33
- **VisitingNorway**: 33
- **Sims4Build**: 33
- **TNOmod**: 66
- **WorshipTheColander**: 33
- **CowboyIPTV**: 66
- **owsisajoke**: 33
- **DACA_Medicine**: 33
- **shironeko**: 33
- **anomic**: 33
- **GardeningIndia2**: 33
- **TeenDevsIndia**: 66
- **Ssupdhq**: 33
- **JCVAP**: 33
- **DecorativePlumbing**: 99
- **TwoSentenceFeels**: 33
- **CodeMystery**: 66
- **htlhollabrunn**: 66
- **AlternativeMedia**: 33
- **guildgrumps**: 33
- **kiwibrowser**: 33
- **BabyCheeks**: 33
- **Milo_Baughman_Design**: 66
- **PadresVent**: 33
- **Nyelveszet**: 33
- **AltFantasySports**: 33
- **TOTC_UFOs**: 33
- **VIU**: 66
- **FistBros**: 99
- **catTunes**: 33
- **okbuddyinfantry**: 33
- **We_are_doing_this**: 33
- **asianamericanytsnark**: 33
- **lssmod_526**: 33
- **zora**: 33
- **Beginners_Photography**: 99
- **Anarcho_Frontierism**: 33
- **DiwaariHeads**: 33
- **hmtforsale**: 33
- **LosingItTogether**: 99
- **StormHawksTv**: 33
- **theCRAMPSstaysick**: 33
- **nokeric**: 33
- **GRITS**: 66
- **KamersNijmegen**: 33
- **UkraineLongRead**: 99
- **MyInternetCrushes**: 66
- **LawandOrder_OC**: 33
- **rachaelharris**: 33
- **In_the_name_of_Beers**: 99
- **godscomputer**: 33
- **PokemonPlaza**: 33
- **theUKSpaceNews**: 33
- **CivRealms**: 33
- **expectedundertale**: 99
- **BigGangsterMod**: 66
- **minecraftdupers**: 66
- **BipocWitchesandPagans**: 99
- **GSMFerroglobe**: 66
- **SingleIncomeNoKid**: 66
- **FansOfBAL**: 33
- **TwinRiverPoker**: 66
- **Redditquandaryroyale**: 33
- **chiptunes**: 33
- **notdisappointed**: 66
- **DionisoES**: 33
- **SweetBabyRays**: 33
- **khargone**: 33
- **KennedySpaceCenter**: 33
- **jmc2obj**: 66
- **Aerospace_Engineering**: 33
- **xb70**: 66
- **SheSaidSheWas12**: 66
- **LockportTownshipHS**: 33
- **F5Robin**: 33
- **BroilKingKeg**: 66
- **highdesert**: 66
- **AdvancedEntityScience**: 33
- **CATAGRUM**: 33
- **tiffin**: 66
- **Wealthsimple_Invest**: 99
- **xplane12**: 99
- **silverbackgorillas**: 33
- **boysarescary**: 33
- **dendaris**: 66
- **toyexchange**: 99
- **Brightline**: 33
- **LordoftheLost**: 33
- **YardHypeSpot**: 66
- **CalebHarris**: 33
- **livefromhere**: 33
- **Revita**: 33
- **sgcgrading**: 33
- **HecarimMains**: 33
- **TheMetGala**: 132
- **ForzaModding**: 33
- **Farbenfuchs_Facharzt**: 33
- **SteamWorldHeist**: 33
- **MeerkatMillionaires**: 66
- **Foodrecalls**: 66
- **CodeFellows**: 66
- **OffGridCrappers**: 66
- **ChurchOfBlueAlienChef**: 33
- **VeeInfuso**: 66
- **GameScience**: 132
- **wtfshippingsupplies**: 33
- **Gamecocks**: 66
- **WeloBk**: 33
- **SneakersEgypt**: 33
- **bon_appetit**: 33
- **RemoteHRJobs**: 33
- **feathery**: 33
- **CuriousExpedition**: 33
- **Cebu**: 33
- **SmallBusinessSellers**: 33
- **germanchubbywife**: 33
- **ImaginaryPirates**: 66
- **TheDeprogramEurope**: 33
- **CycleKarts**: 66
- **hornystan**: 33
- **schleeey**: 33
- **JetEngines**: 33
- **QUITOO**: 66
- **findayoutube**: 33
- **A_Separate_Peace**: 33
- **kittykeep**: 33
- **EiyuuouBuwoKiwameru**: 66
- **YECirclehugs**: 33
- **hallenhollylove**: 66
- **PhotoEnts**: 66
- **Network_Automation**: 33
- **GymOwnerNetwork**: 66
- **Bunkd**: 99
- **kota**: 33
- **canplfantasy**: 33
- **FortunateCum**: 66
- **waukeshaparade**: 33
- **Mario3DWorldOnline**: 33
- **Gandhinagar_**: 66
- **Arazhul**: 33
- **kfee**: 33
- **ShinyPokeCommunity**: 33
- **ShortStoryPrompts**: 33
- **thrashcub**: 66
- **Cuteaggression**: 33
- **CritCrab**: 33
- **Alipay**: 33
- **agrochemistry**: 99
- **ThatsLegal**: 33
- **EarthFinalConflict**: 33
- **Tetr_College**: 33
- **FoxestheSinger**: 66
- **PFFO**: 33
- **whyperart**: 33
- **heilzamememes**: 66
- **TamilCinephiles**: 33
- **trumpsolana**: 33
- **nuance**: 33
- **RealPDKC**: 66
- **fukuyamameme**: 33
- **GreenwichVillage**: 33
- **Tramlines**: 66

## Languages
Unique languages and their frequencies:
- **French**: 450
- **Italian**: 450
- **German**: 450
- **English (American)**: 450
- **Korean**: 450
- **Spanish**: 450
- **Chinese (traditional)**: 450
- **Chinese (simplified)**: 450
- **Japanese**: 450
- **Russian**: 450
- **Czech**: 450
- **Danish**: 450
- **Dutch**: 450
- **Arabic**: 450
- **Bulgarian**: 450
- **English (British)**: 450
- **Estonian**: 450
- **Hungarian**: 450
- **Indonesian**: 450
- **Norwegian (bokmål)**: 450
- **Portuguese**: 450
- **Greek**: 450
- **Lithuanian**: 450
- **Finnish**: 450
- **Latvian**: 450
- **Portuguese (Brazilian)**: 450
- **Polish**: 450
- **Swedish**: 450
- **Slovak**: 450
- **Slovenian**: 450
- **Romanian**: 450
- **Turkish**: 450
- **Ukrainian**: 450

## Models
Unique models and their frequencies:
- **Granite-3.1-8B-Instruct**: 2871
- **UnieInfra2.aqua-mini**: 3069
- **aqua-mini-fast**: 6303
- **phi4**: 2607

## Article Statistics
- **Average article length (characters):** 1275.34
- **Minimum article length (characters):** 2
- **Maximum article length (characters):** 15407
- **Average word count:** 184.62
- **Minimum word count:** 1
- **Maximum word count:** 1134
",Medium,2.0
Translation,unieai/multilingual-stories-original,2.0,7.0,2025-02-05 10:01:17+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
language: 
  - fr  # French
  - it  # Italian
  - de  # German
  - en  # English
  - ko  # Korean
  - es  # Spanish
  - zh  # Chinese
  - ja  # Japanese
  - ru  # Russian
  - cs  # Czech
  - da  # Danish
  - nl  # Dutch
  - ar  # Arabic
  - bg  # Bulgarian
  - et  # Estonian
  - hu  # Hungarian
  - id  # Indonesian
  - nb  # Norwegian
  - pt  # Portuguese
  - el  # Greek
  - lt  # Lithuanian
  - fi  # Finnish
  - lv  # Latvian
  - pl  # Polish
  - sv  # Swedish
  - sk  # Slovak
  - sl  # Slovenian
  - ro  # Romanian
  - tr  # Turkish
  - uk  # Ukrainian

language_bcp47:
  - zh-Hant  # Chinese (Traditional)
  - zh-Hans  # Chinese (Simplified)
  - en-GB  # English (British)
  - pt-BR  # Portuguese (Brazilian)

pretty_name: ""Multilingual Dataset""

tags:
  - multilingual
  - NLP
  - text-generation
  - text-classification
  - dataset-processing
  - large-scale-dataset

license: ""apache-2.0""

task_categories:
  - text-classification
  - text-generation
  - text2text-generation
  - feature-extraction
  - translation
  - summarization

size_categories:
  - large

source_datasets: []
---

# Multilingual Dataset

This dataset contains multilingual articles generated using various models.

# Dataset Summary

**Total records:** 14850

## Categories
Unique categories and their frequencies:
- **OmundoAmanha**: 99
- **malinois**: 33
- **CBDPouches**: 33
- **ingenieurs**: 66
- **PPeperomioides**: 33
- **DiscGolfCarts**: 33
- **TIMAF**: 33
- **AntisocialMemeClub**: 33
- **Tancan**: 66
- **ogerwaters**: 33
- **MotosBr**: 33
- **lgballt**: 66
- **CipherAcademy**: 33
- **LofiEdits**: 33
- **jennyraee**: 33
- **RonnieHotdogs**: 33
- **Bratzillaz**: 66
- **cubiccommunity**: 33
- **Labectance**: 33
- **ChasmicOccult**: 33
- **exresidents**: 33
- **GamingParents**: 33
- **Elisaonfire**: 33
- **blackdesert**: 66
- **MockElectionsAUS**: 33
- **DogfreeHumor**: 33
- **TToT**: 33
- **GeneralContractor**: 33
- **nkim**: 33
- **HeadandNeckCancer**: 33
- **CantonReddit**: 99
- **GrassrootsSelectAZ**: 33
- **BlackCloverMobile**: 33
- **uclafood**: 66
- **trueskyrimmods**: 33
- **heximal**: 33
- **Hetrochromia**: 33
- **photographysatx**: 33
- **AsturiasPatriaQuerida**: 33
- **2b2tqueue**: 99
- **kristyglas**: 33
- **advancedwitchcraft**: 33
- **rockymountainhouse**: 132
- **Kontaktanzeigen**: 33
- **DepressionVentSpace**: 33
- **MagickaWizardWarz**: 33
- **miniature_tigers**: 66
- **ifunnyNovoMundo**: 33
- **ShrineOfAC**: 66
- **EDAnonPics**: 33
- **FontLab**: 33
- **agent_of_shield**: 66
- **Infinity_laugh**: 33
- **CataclismoRTS**: 33
- **5DChess**: 33
- **idiotsflyingdrones**: 33
- **cscareerquestionsIN**: 66
- **birdsonbirds**: 99
- **msvduisburg**: 33
- **Eunha**: 33
- **Rotundanimals**: 33
- **Norse**: 66
- **treadstone**: 33
- **MadilynMei**: 66
- **ValentinaRose**: 66
- **AzurePercept**: 33
- **HollyHoes**: 66
- **exquisitecorpse**: 33
- **NoglaTerroriserReacts**: 33
- **StephenHillenburg**: 99
- **maxima**: 33
- **turndotme**: 33
- **dakboard**: 66
- **NetflixAnime**: 66
- **semaglutideweightloss**: 198
- **TheGatewayExperience**: 33
- **SubredditForFriends**: 33
- **porchpiratepwnage**: 33
- **throughtheages**: 66
- **TheFakeDentist**: 66
- **SquidMoonCoin**: 99
- **BazarRogueExcrement**: 66
- **techmaestro**: 33
- **ChurchOf_AquaxAixRuby**: 33
- **NLProducerGang**: 66
- **flannel**: 33
- **alternativewritings**: 66
- **Crypts**: 33
- **Mr_MandM**: 33
- **7counter**: 33
- **TwitchSafe**: 66
- **dRehmFlight**: 66
- **TheLetterOctothorpe**: 33
- **chuck**: 33
- **ConfrontingChaos**: 33
- **GettingOverItGame**: 33
- **KillYourConsole**: 33
- **Male_Studies**: 66
- **lacenterwa**: 33
- **IrishFishing**: 66
- **ptvo**: 99
- **Phigolf**: 33
- **the_derek**: 33
- **preciousmoments**: 33
- **AnaDaniela69xxx**: 33
- **blergg**: 33
- **GraphingCalculator**: 132
- **Knoxville**: 33
- **The_Real_Austin**: 33
- **EricBrakey**: 33
- **NightWitchGang**: 33
- **CityNerd**: 99
- **Pratibha_Ranta**: 99
- **BSCStation**: 33
- **WLWactually**: 33
- **suddenlymandjtv**: 33
- **Celestron**: 99
- **CSMagicRecipe**: 33
- **Phone_Foldables**: 99
- **MavuikaMainsGI**: 33
- **InboxPounds**: 33
- **janiscorner**: 33
- **PECG**: 33
- **IcebergExplained**: 33
- **GreedIncorporated**: 33
- **SurvivingRingworm**: 66
- **Snowplow**: 33
- **Tadano**: 66
- **funtechnology**: 33
- **ACUsedVillagers**: 33
- **Fordtractors**: 66
- **CargoWise**: 33
- **earwig**: 33
- **nocontextaudio**: 33
- **kenbarbbe**: 33
- **cwstonekingfans**: 33
- **VicBWolfArt**: 33
- **BrothersoftheGrain**: 99
- **TheRunaways**: 33
- **NewsWorld**: 66
- **DELF**: 33
- **deardiary**: 33
- **MyOneLineDogma**: 33
- **bunchofamateurs**: 33
- **Goulburn**: 66
- **Mattpryor**: 33
- **communitydevelopment**: 33
- **MiddleGradeWriters**: 66
- **businesslaw**: 33
- **BanglaMuktobak**: 33
- **StupidMario**: 66
- **Grimgar**: 33
- **HarvestMoonGBA**: 66
- **SOTR_auditions**: 99
- **TheShannaraChronicles**: 33
- **overcoding**: 33
- **Awareparents**: 66
- **dogmalove**: 66
- **HAESfood**: 33
- **EvergreenSkills**: 33
- **valentina94O**: 33
- **fsft**: 33
- **OkBuddyYharim**: 33
- **Swapandsell**: 66
- **warhospital**: 33
- **Demeo**: 33
- **TvIntroMashups**: 33
- **Winiday**: 132
- **algogaming**: 33
- **IhateElon**: 99
- **XtotheV**: 33
- **SouthOfTheCircleGame**: 66
- **wlu**: 33
- **MarketingFails**: 33
- **constanzavelazcoteton**: 33
- **legoww1**: 33
- **cheekdimples**: 33
- **circularguements**: 33
- **BattleforBFCI**: 33
- **JohnFarnham**: 99
- **DisablEdCanTeach**: 33
- **saysueme**: 66
- **Rengarmains**: 33
- **overclockfps**: 33
- **GOTV**: 66
- **Business_Management**: 33
- **latticeclimbing**: 33
- **rarehugs**: 33
- **AroAllo**: 33
- **gfbreadrecipies**: 66
- **JCBDL**: 33
- **snowplowproject**: 33
- **knockedloose**: 66
- **ScoreLords**: 33
- **ShopImprovements**: 33
- **256cub**: 66
- **UltiumCellsLLC**: 66
- **ObscureGIFs**: 33
- **alchademy**: 33
- **CastroPodcast**: 33
- **MelissaONeilPics**: 66
- **ConsensusDebate**: 33
- **okbuddyplants**: 33
- **mischakim**: 99
- **Palermo**: 33
- **losersofinstagram**: 99
- **lezconnect**: 33
- **TyreNichols**: 33
- **Primis_Bank**: 33
- **givemeanidea**: 33
- **Actualfixes**: 33
- **GrowingGirls**: 33
- **YukongMains**: 66
- **YapDollarMyBeloved**: 33
- **trials**: 33
- **wbmedicos**: 33
- **RaceTrackDesigns**: 33
- **supercellmoment**: 33
- **Survivors_BBC_1975**: 66
- **the_mysterium**: 33
- **KillerKitty**: 33
- **kaybomb**: 33
- **Blogsabouttea**: 33
- **OlympiacosBC**: 33
- **ReDIY**: 33
- **Hampture**: 33
- **Demonias_**: 66
- **WILTWIFLS**: 33
- **Analysts_of_MBTI**: 66
- **SoulSplitGame**: 33
- **gracelyra**: 33
- **Meme__World**: 66
- **CrescentHub**: 66
- **psytrancefrance**: 33
- **2016campaign**: 66
- **theguild3**: 33
- **practicingheathenry**: 33
- **budgetguitargear**: 99
- **Dogma**: 33
- **novomesto**: 33
- **ContinentedoBombarral**: 33
- **Comadre**: 33
- **thethyroidmadness**: 33
- **Xooginme**: 33
- **HomebrewTactical**: 33
- **kosmic**: 33
- **GameGeeks**: 33
- **TESHeroes**: 66
- **SchoolMotivation**: 66
- **dataforenergyskills**: 33
- **VeganJunkFood**: 66
- **TheRealArkhamWorld**: 33
- **ExpressiveArtsTherapy**: 33
- **TojinoMiko**: 33
- **Suicide_Talk**: 33
- **Shaolin**: 33
- **PositiveHerpes**: 33
- **MaddiMays**: 33
- **GeneseoIllinois**: 33
- **AutoClassifieds**: 33
- **littlenatie_**: 33
- **KoolKorsunsky**: 33
- **Airpower**: 33
- **foundCust0mCraft**: 33
- **PowerUniversity**: 66
- **BirdBoxMemes**: 33
- **Sevigor**: 33
- **Wreddit**: 66
- **digitalcats**: 33
- **llamas**: 66
- **XGIMI_Beta_Testing**: 33
- **orlandofishing**: 33
- **animalsasleaders**: 33
- **MUCC**: 33
- **Icefuse**: 33
- **FunkyLizard**: 33
- **Keensmemes**: 66
- **hamood**: 66
- **Terrarya**: 33
- **indianmetamisogynist**: 99
- **CelebrityBoots**: 66
- **autokey**: 66
- **kegswitharms**: 33
- **Unexpectedllamas**: 132
- **accidentalsubreddit**: 99
- **temporarygunowners**: 33
- **djtzone**: 33
- **LetsTalkRocks**: 66
- **antisexwork**: 33
- **aquaponie**: 33
- **clandestineairsoft**: 33
- **Paget_Schroetter**: 33
- **civcast**: 66
- **GHSE**: 33
- **massnorthshorefmlokin**: 33
- **HandmadeLure**: 33
- **PantslessPajamas**: 33
- **CrocodileHunters**: 33
- **lumberjackcidents**: 33
- **ShintoReligion**: 33
- **Moondrop_Sundrop**: 33
- **NarutoNinjaStorm**: 33
- **TrevTutor**: 66
- **ZichWedding**: 33
- **ParadigmFoundation**: 33
- **unsentLoveLetters1st**: 33
- **PaidInternships**: 33
- **serfposting**: 33
- **thatsthesexnumber**: 33
- **OutlanderPHEV**: 33
- **SUITrump**: 33
- **redwolf344**: 33
- **ratemyfridge**: 33
- **Starry_Flowers**: 33
- **f1online**: 66
- **hodlandshill**: 33
- **bigchungusarmy**: 33
- **OddportAcademy**: 33
- **MCxYuri**: 66
- **Islamic_History**: 99
- **FMF**: 33
- **JacindaArdern**: 33
- **bristoldrill**: 33
- **Halloweenlove**: 33
- **iPhone14Pro**: 99
- **Mountaineering**: 66
- **PuyEnVelay**: 66
- **GameGearMicroMods**: 33
- **ThriftStoreHaulsValue**: 66
- **Hoisan**: 33
- **artcovers**: 33
- **BrawlStarsGiftShop**: 33
- **GrumpyCoinCommunity**: 33
- **foundAwesomeness_999**: 33
- **imaginaryelectionscj**: 33
- **akbbyy**: 66
- **kleptomanicsupport**: 33
- **CivScarcity**: 66
- **DesignforFDM**: 66
- **cursedminecraftimages**: 66
- **simplethoughts**: 33
- **Nat1pub**: 66
- **hawaiiboonie**: 33
- **investigators**: 33
- **YabukiNako**: 33
- **hiphoptoday**: 99
- **maximalistfashion**: 33
- **accurateguides**: 66
- **DevExpress**: 33
- **WonderlandGameSeries**: 66
- **ErgoMechKeyboards**: 99
- **Urgot**: 99
- **PopulationMaps**: 66
- **CatsNamedToothless**: 33
- **MinecartDriverServer**: 33
- **GwapaGangNFT**: 66
- **Midnightthoughts**: 66
- **comicstriphistory**: 33
- **unibo_studenti**: 99
- **IronGiantMemes**: 33
- **homelessmind**: 33
- **MajesticCats**: 33
- **preprocessing**: 66
- **Ripsquad**: 99
- **Flowerfellfuture**: 33
- **badradio**: 33
- **SavingPrivateNoOne**: 33
- **ToothpasteEnema**: 33
- **karinerino**: 99
- **albertanudist**: 33
- **LINCHWAR**: 33
- **vanifestos**: 33
- **FakemonDesignContests**: 33
- **schmitty**: 33
- **gensoukyou**: 33
- **Warside**: 33
- **Apex_NC**: 33
- **CreamyCami8407**: 33
- **smallengines**: 33
- **IABrasil**: 33
- **FeiProtocol**: 66
- **BugandSeek**: 33
- **Finntroll**: 33
- **SneakersNL**: 33
- **ImpracticalFoods**: 33
- **arabhealth**: 66
- **okbuddynicotine**: 33
- **stoicfemina**: 99
- **mathsucks**: 33
- **freezepop**: 33
- **AuthoritarianNewsHub**: 33
- **corpsepaint**: 66
- **FXMasterRace**: 33
- **RetroConsoleModding**: 33
- **e17**: 66
- **UruguaySoftCombat**: 33
- **cupcakes_please**: 33
- **OfMonstersAndMen**: 33
- **HingeStories**: 33
- **sashannarcy**: 33
- **slapbass**: 66
- **fakepng**: 33
- **hibid**: 33
- **obletsvet**: 33
- **MaxarTechnologies**: 66
- **HopeKelesis3**: 33
- **wrestlingbelts**: 99
- **indian_desi**: 33
- **failedshinypokemon**: 99
- **ChutneyMusic**: 33
- **kleinerfanclub**: 33
- **TheeCrew**: 33
- **OctopathTraveler2**: 33
- **slidan**: 33
- **Worldbuilders**: 33
- **AussieTherians**: 66
- **PaganRockCollectors**: 33
- **SixDegreesOfCoulson**: 66
- **rubik**: 99
- **slk**: 33
- **Paper4Paper**: 33
- **YoutubeSings**: 33
- **KingdomHearts**: 33
- **reallycoolrobots**: 99
- **RedmiNote9pro**: 33
- **NarutoRP**: 99
- **basketballanalytics**: 33
- **russianfemboy**: 33
- **IndyOnlyfans**: 33
- **Teentalks**: 33
- **UKISP**: 33
- **VisualNovelCryptids**: 33
- **StudyCatalogPublish**: 33
- **Pawar2018**: 33
- **CanadaMonopolyProtest**: 33
- **chainsawLife**: 66
- **AltPolitic**: 33
- **yizhuoning**: 99
- **calculus**: 66
- **deservedapmaan**: 66
- **proofoftalk**: 33
- **joinelevatehealth**: 33
- **readbeforeeat**: 33
- **DramaticReading**: 33
- **worst_maps**: 33
- **anitwtitter**: 33
- **dramatic_kibbe**: 66
- **lookingforgroup**: 33
- **HISHE**: 66
- **laserweapons**: 66
- **CrazyPassengers**: 33
- **KSPMildlyInteresting**: 33
- **stpetebeach**: 99
- **BeingTube**: 66
- **JaydenBartels**: 99
- **Ticos_TI**: 66
- **MuseumOfArtifacts**: 33
- **GastroHealth**: 33
- **Gailbatescncdream**: 66
- **CoalitionAgainstEvil**: 99
- **TotalDramaPolls**: 66
- **TOPTIERSESSIONS**: 66
- **BossfightBestiary**: 66
- **playlost**: 33
- **DurangoCJAYM**: 33
- **Plunderstorm**: 33
- **Solitary**: 33
- **SproutValleyGame**: 33
- **LesbicasPortuguesas**: 33
- **blip**: 33
- **DeerfieldHighSchool**: 99
- **norwalkct**: 66
- **Dumbconspiracytheorys**: 33
- **FBEPC**: 33
- **bestdealsPH**: 33
- **careerguidance**: 33
- **izombiecw**: 33
- **tongueknots**: 33
- **MarketingCareers**: 66
- **Sommerspiele**: 33
- **Spaceagency2138**: 66
- **monkeynft**: 33
- **foundthewaltwhitepfp**: 33
- **MillaSofiaLookbook**: 66
- **horizonmemes**: 33
- **MVAgusta**: 33
- **FlagRedesigns**: 33
- **ShesJudgingYou**: 66
- **Lisle**: 33
- **NoMansSkyOfficial**: 33
- **PeptideForum**: 66
- **cutoutdresses**: 33
- **r2northstar**: 66
- **PatrioticMurica**: 33
- **WildernessBackpacking**: 66
- **herpes_dating**: 99
- **Unsatisfying**: 33
- **naming_animals**: 33
- **AskPhotography**: 66
- **jakeneutrality**: 66
- **tacticalops**: 33
- **BackyardBoxing**: 33
- **yujin**: 33
- **DammitZappa**: 33
- **AITL**: 33
- **jianca14**: 33
- **CuteAndFunnyAnime**: 33
- **MyHotwife_**: 33
- **Razorlight**: 33
- **yunli**: 33
- **wholesomebutstrange**: 66
- **jaywalking**: 66
- **marmello**: 33
- **turkayak1**: 66
- **reddithydrogen**: 99
- **Szormok_AVE**: 66
- **OpenMatte**: 33
- **XboxStudios**: 66
- **ScottishMeetUps**: 33
- **gsh**: 33
- **DasBootMaimais**: 33
- **phf**: 66
- **TopCustomPaper**: 66
- **TheJournal4Project**: 33
- **agfy**: 99
- **GOPwatch**: 33
- **meepo**: 66
- **LondonLadies**: 33
- **ukrainethelatest**: 99
- **PoliceDog**: 33
- **ActinicFolliculitis**: 33
- **BlockedAndReported**: 33
- **DPSdubai**: 66
- **skku**: 99
- **formulaminecraft**: 33
- **aremystitchestwisted**: 33
- **JESSIFOXXinflates**: 33
- **DungeonBuilder**: 33
- **myronkoops_**: 33
- **GeorginaCampbell**: 66
- **MeticulousEspresso**: 33
- **RoadRash64**: 33
- **HamiltonEuchre**: 33
- **JusticeforKarenRead_2**: 33
- **Lystfiskeri**: 33
- **dogsofukraine**: 33
- **CubaVexillology**: 33
- **Manga_Germany**: 33
- **MadLed**: 33
- **SuddenlyGayCommunism**: 66
- **TransFem18**: 33
- **Stickwars**: 66
- **minecraft_youtube**: 33
- **penguin**: 33
- **Charliethething**: 66
- **2imperialeurope4u**: 33
- **imsorrymumbo**: 33
- **guitarproblems**: 33
- **actuallyItsCMF**: 66
- **Surfari**: 66
- **WorldwideKahoot**: 66
- **BillBrasky**: 33
- **Shoeler**: 33
- **DishaPataniFans**: 33
- **tunnelsnakes**: 33
- **TangleAndWhisper**: 33
- **Onshape**: 33
- **LGV60**: 33
- **weddinghashtags**: 66
- **Travelbuddy**: 33
- **RedGreenShow**: 33
- **tankionlineint**: 33
- **mermaid**: 66
- **FansofMissAnnaGigi**: 66
- **AtheistCheeseCake**: 66
- **systemml**: 33
- **TransPuns**: 33
- **deepnutrition**: 33
- **cindernft**: 33
- **Farmasi_drama_snark**: 33
- **pollotarian**: 33
- **CityOfLiverpoolFC**: 33
- **femboy_kay**: 33
- **inmigracionhaitiana**: 33
- **OuterSpaceTravel**: 66
- **duckliftingnews**: 33
- **Tasha_sexy_21**: 33
- **SMK25**: 66
- **EnochianMagick**: 33
- **FtMFashionAdvice**: 33
- **twosentencevindicated**: 33
- **GameStopCanada**: 33
- **southbaldwincountyfwb**: 99
- **RealTruthSeeking**: 33
- **CarDIY**: 33
- **brookiecookiesnark**: 33
- **beardmeetsfood**: 33
- **shiphappens**: 33
- **zuhanygondolatok**: 33
- **jojoly**: 66
- **Love101Netflix**: 33
- **GoodTask**: 66
- **F95zonee**: 66
- **cutetextfaces**: 33
- **boardgamingmy**: 66
- **carbobike**: 33
- **FlowerGirlsTamagotchi**: 99
- **Gemology**: 33
- **northrupphoto**: 33
- **rdms**: 33
- **ErikaHerceg**: 66
- **aua**: 66
- **blockethaveri**: 33
- **routeros**: 33
- **IndieWorldOrder**: 33
- **EliteOuendan**: 99
- **ChristianHistory**: 33
- **EastAfricaNews**: 33
- **FatFrankie**: 33
- **HDDcoinNetwork**: 33
- **americanman**: 33
- **callthephotoshoppers**: 33
- **Pisstearmanseries**: 33
- **shaderDev**: 33
- **RollWithAdvantage**: 33
- **thenightmarecollector**: 33
- **SeadooSpark**: 33
- **JadeAzevedo**: 66
- **CHSMichelleSnark**: 33
- **unpresidented**: 33
- **SimLab**: 99
- **Cybers_public_school**: 33
- **monte_video**: 33
- **chaoticswifties**: 66
- **ImmersiveDaydreaming**: 33
- **MattChampion**: 33
- **RLCarShow**: 33
- **KtuKerala**: 33
- **TaylorSwiftAnkles**: 66
- **ryze**: 33
- **ratemysinging**: 99
- **HMHSBritannic**: 33
- **WhoSampled**: 33
- **DiscussChrist**: 33
- **JavelinTalk**: 33
- **titaniumbikes**: 33
- **AnimeAndEtc**: 66
- **KSPToMarslanderteam**: 66
- **Catchirps**: 33
- **virtualwarfighter**: 33
- **coopgames**: 66
- **YouthHockeyHighlights**: 33
- **buyaplantsellaplant**: 33
- **BolsaChile**: 33
- **Dikpipiler**: 33
- **EryingMotherboard**: 33
- **RUFUSDUSOL**: 66
- **sanantoniobeer**: 33
- **techpolitics**: 66
- **RockPosters**: 33
- **dorajar**: 33
- **suddenlyvicodin**: 66
- **nadide**: 33
- **BigStickEnergy**: 66
- **zoomergen**: 99
- **drivingUK**: 66
- **MSPA**: 66
- **TheATOShow**: 33
- **TheMegatronDon**: 33
- **EnemiesBeingBros**: 33
- **ratmemes**: 33
- **MirMGlobal**: 33
- **AppleAndOnion**: 33
- **B638**: 66
- **LGBT_iel**: 66
- **TrainingEverywhere**: 99
- **biggestbroent**: 66
- **WWYDif**: 33
- **DSTAlumnaeChapter**: 33
- **WWE_Eva_Marie**: 33
- **guatemala_gay**: 66
- **CringyAntiVegans**: 33
- **NinaDobrev_**: 33
- **makemoneyforexreddit**: 33
- **iJustPublishedMyGame**: 66
- **lordtuts**: 33
- **brennanjones**: 33
- **SubmarineTitans**: 33
- **ClonazepamHealth**: 33
- **okbuddyrhodok**: 66
- **FinancialMeme**: 33
- **FiverFilth**: 33
- **Theni**: 33
- **VZfit**: 132
- **FunctionalFill**: 33
- **SACWATR**: 33
- **GiuliaBenitee**: 33
- **Reality_Quest**: 33
- **SheGotFrameWork**: 33
- **abertayhackers**: 99
- **ThePower**: 66
- **IceandFirePowers**: 33
- **datasatanism**: 66
- **badshirtanatomy**: 33
- **Necrontyr**: 33
- **signs**: 33
- **vrbohosts**: 66
- **erannorthchronicles**: 66
- **Brainysweddit**: 66
- **Ooergonewild**: 66
- **EB2_NIW_Cybersecurity**: 33
- **Djlynch2009**: 33
- **foundtheAngela**: 66
- **digitaltabletop**: 33
- **Idvitalia**: 33
- **surebet**: 33
- **subscriptionsharing**: 33
- **Exodus90**: 33
- **northfork**: 33
- **Emsland**: 33
- **CoilCommunity**: 66
- **shameless**: 33
- **ManitobaGardening**: 33
- **Telekinetic**: 99
- **TheNew52**: 33
- **TeamSky**: 33
- **HypixelSlovenija**: 33
- **Stabbingthemap**: 33
- **mysteriousmetallicorb**: 33
- **DeerVirus**: 66
- **EquinoxEv**: 33
- **titration**: 33
- **NoSenseMemes**: 33
- **HellenicMemes**: 66
- **FoundTheGiantSquid**: 33
- **MetroidDread**: 33
- **Chiskate**: 33
- **cannadips**: 66
- **WeHateYT**: 33
- **askdisqus**: 33
- **marniexvictor**: 66
- **RootandDigby**: 33
- **SuperheroLeague**: 33
- **CipherMains_HSR**: 33
- **obviouslyincorrect**: 33
- **InfinityNado**: 33
- **desporto**: 33
- **Dallas_commons**: 33
- **go_ask_your_crush_out**: 132
- **heelonly**: 33
- **Porsgrunn**: 33
- **truetexan**: 66
- **rushzilla**: 33
- **banime**: 33
- **Nalbinding**: 33
- **oliviadejarnet**: 66
- **preparedguitar**: 66
- **NielsonShow**: 66
- **transsexmalesurgeries**: 99
- **cleanpuppers**: 33
- **InesperadoPau**: 33
- **louderthanlife**: 33
- **ZamasuDidNothingWrong**: 99
- **OdeyaRush**: 33
- **CyberSecurityAdvice**: 33
- **dutchposer**: 33
- **WK2**: 33
- **minecraftinfographics**: 33
- **applycoldwatertoburn**: 66
- **fearofheights**: 33
- **themeaneyoflife**: 33
- **mildlyaccessible**: 33
- **BLACKED_AL**: 33
- **bluegrassguitar**: 66
- **azuretips**: 66
- **fitbod**: 33
- **somaligay**: 33
- **TheWellsTable**: 33
- **Poudlard**: 33
- **10000ShipsHBO**: 66
- **TookOurJerbs**: 66
- **FreeGameKeys**: 33
- **CloudAssessments**: 33
- **OTRebels**: 66
- **berlinfoodies**: 66
- **DisneyParkHistory**: 33
- **infjhome**: 33
- **jotform**: 33
- **danielledominguez_**: 66
- **ryzencpu**: 33
- **bandirma**: 33
- **cdldriver**: 99
- **CarpetsforAirports**: 66
- **Superchunk**: 66
- **artisansasylum**: 33
- **ClubeDoZanzigau**: 99
- **HowToMen**: 33
- **accidentaldiamondclub**: 33
- **CelebrityOutside**: 33
- **RobertoArguelles**: 33
- **ConcourseCityTransit**: 33
- **VisitingNorway**: 33
- **Sims4Build**: 33
- **TNOmod**: 66
- **WorshipTheColander**: 33
- **CowboyIPTV**: 66
- **owsisajoke**: 33
- **DACA_Medicine**: 33
- **shironeko**: 33
- **anomic**: 33
- **GardeningIndia2**: 33
- **TeenDevsIndia**: 66
- **Ssupdhq**: 33
- **JCVAP**: 33
- **DecorativePlumbing**: 99
- **TwoSentenceFeels**: 33
- **CodeMystery**: 66
- **htlhollabrunn**: 66
- **AlternativeMedia**: 33
- **guildgrumps**: 33
- **kiwibrowser**: 33
- **BabyCheeks**: 33
- **Milo_Baughman_Design**: 66
- **PadresVent**: 33
- **Nyelveszet**: 33
- **AltFantasySports**: 33
- **TOTC_UFOs**: 33
- **VIU**: 66
- **FistBros**: 99
- **catTunes**: 33
- **okbuddyinfantry**: 33
- **We_are_doing_this**: 33
- **asianamericanytsnark**: 33
- **lssmod_526**: 33
- **zora**: 33
- **Beginners_Photography**: 99
- **Anarcho_Frontierism**: 33
- **DiwaariHeads**: 33
- **hmtforsale**: 33
- **LosingItTogether**: 99
- **StormHawksTv**: 33
- **theCRAMPSstaysick**: 33
- **nokeric**: 33
- **GRITS**: 66
- **KamersNijmegen**: 33
- **UkraineLongRead**: 99
- **MyInternetCrushes**: 66
- **LawandOrder_OC**: 33
- **rachaelharris**: 33
- **In_the_name_of_Beers**: 99
- **godscomputer**: 33
- **PokemonPlaza**: 33
- **theUKSpaceNews**: 33
- **CivRealms**: 33
- **expectedundertale**: 99
- **BigGangsterMod**: 66
- **minecraftdupers**: 66
- **BipocWitchesandPagans**: 99
- **GSMFerroglobe**: 66
- **SingleIncomeNoKid**: 66
- **FansOfBAL**: 33
- **TwinRiverPoker**: 66
- **Redditquandaryroyale**: 33
- **chiptunes**: 33
- **notdisappointed**: 66
- **DionisoES**: 33
- **SweetBabyRays**: 33
- **khargone**: 33
- **KennedySpaceCenter**: 33
- **jmc2obj**: 66
- **Aerospace_Engineering**: 33
- **xb70**: 66
- **SheSaidSheWas12**: 66
- **LockportTownshipHS**: 33
- **F5Robin**: 33
- **BroilKingKeg**: 66
- **highdesert**: 66
- **AdvancedEntityScience**: 33
- **CATAGRUM**: 33
- **tiffin**: 66
- **Wealthsimple_Invest**: 99
- **xplane12**: 99
- **silverbackgorillas**: 33
- **boysarescary**: 33
- **dendaris**: 66
- **toyexchange**: 99
- **Brightline**: 33
- **LordoftheLost**: 33
- **YardHypeSpot**: 66
- **CalebHarris**: 33
- **livefromhere**: 33
- **Revita**: 33
- **sgcgrading**: 33
- **HecarimMains**: 33
- **TheMetGala**: 132
- **ForzaModding**: 33
- **Farbenfuchs_Facharzt**: 33
- **SteamWorldHeist**: 33
- **MeerkatMillionaires**: 66
- **Foodrecalls**: 66
- **CodeFellows**: 66
- **OffGridCrappers**: 66
- **ChurchOfBlueAlienChef**: 33
- **VeeInfuso**: 66
- **GameScience**: 132
- **wtfshippingsupplies**: 33
- **Gamecocks**: 66
- **WeloBk**: 33
- **SneakersEgypt**: 33
- **bon_appetit**: 33
- **RemoteHRJobs**: 33
- **feathery**: 33
- **CuriousExpedition**: 33
- **Cebu**: 33
- **SmallBusinessSellers**: 33
- **germanchubbywife**: 33
- **ImaginaryPirates**: 66
- **TheDeprogramEurope**: 33
- **CycleKarts**: 66
- **hornystan**: 33
- **schleeey**: 33
- **JetEngines**: 33
- **QUITOO**: 66
- **findayoutube**: 33
- **A_Separate_Peace**: 33
- **kittykeep**: 33
- **EiyuuouBuwoKiwameru**: 66
- **YECirclehugs**: 33
- **hallenhollylove**: 66
- **PhotoEnts**: 66
- **Network_Automation**: 33
- **GymOwnerNetwork**: 66
- **Bunkd**: 99
- **kota**: 33
- **canplfantasy**: 33
- **FortunateCum**: 66
- **waukeshaparade**: 33
- **Mario3DWorldOnline**: 33
- **Gandhinagar_**: 66
- **Arazhul**: 33
- **kfee**: 33
- **ShinyPokeCommunity**: 33
- **ShortStoryPrompts**: 33
- **thrashcub**: 66
- **Cuteaggression**: 33
- **CritCrab**: 33
- **Alipay**: 33
- **agrochemistry**: 99
- **ThatsLegal**: 33
- **EarthFinalConflict**: 33
- **Tetr_College**: 33
- **FoxestheSinger**: 66
- **PFFO**: 33
- **whyperart**: 33
- **heilzamememes**: 66
- **TamilCinephiles**: 33
- **trumpsolana**: 33
- **nuance**: 33
- **RealPDKC**: 66
- **fukuyamameme**: 33
- **GreenwichVillage**: 33
- **Tramlines**: 66

## Languages
Unique languages and their frequencies:
- **French**: 450
- **Italian**: 450
- **German**: 450
- **English (American)**: 450
- **Korean**: 450
- **Spanish**: 450
- **Chinese (traditional)**: 450
- **Chinese (simplified)**: 450
- **Japanese**: 450
- **Russian**: 450
- **Czech**: 450
- **Danish**: 450
- **Dutch**: 450
- **Arabic**: 450
- **Bulgarian**: 450
- **English (British)**: 450
- **Estonian**: 450
- **Hungarian**: 450
- **Indonesian**: 450
- **Norwegian (bokmål)**: 450
- **Portuguese**: 450
- **Greek**: 450
- **Lithuanian**: 450
- **Finnish**: 450
- **Latvian**: 450
- **Portuguese (Brazilian)**: 450
- **Polish**: 450
- **Swedish**: 450
- **Slovak**: 450
- **Slovenian**: 450
- **Romanian**: 450
- **Turkish**: 450
- **Ukrainian**: 450

## Models
Unique models and their frequencies:
- **Granite-3.1-8B-Instruct**: 2871
- **UnieInfra2.aqua-mini**: 3069
- **aqua-mini-fast**: 6303
- **phi4**: 2607

## Article Statistics
- **Average article length (characters):** 1275.34
- **Minimum article length (characters):** 2
- **Maximum article length (characters):** 15407
- **Average word count:** 184.62
- **Minimum word count:** 1
- **Maximum word count:** 1134
",Medium,2.0
Translation,google/wmt24pp,43.0,3741.0,2025-03-13 21:53:34+00:00,apache-2.0,4.0,76 MB,79691776.0,24.4 MB,25585254.4,54890,https://arxiv.org/abs/2502.12404,none,"---
license: apache-2.0
language:
- ar
- bg
- bn
- ca
- da
- de
- el
- es
- et
- fa
- fi
- fr
- gu
- he
- hi
- hr
- hu
- id
- is
- it
- ja
- kn
- ko
- lt
- lv
- ml
- mr
- nl
- 'no'
- pa
- pl
- pt
- ro
- ru
- sk
- sl
- sr
- sv
- sw
- ta
- te
- th
- tr
- uk
- ur
- vi
- zh
- zu
task_categories:
- translation
size_categories:
- 10K<n<100K
configs:
  - config_name: en-ar_EG
    data_files:
    - split: train
      path: ""en-ar_EG.jsonl""
  - config_name: en-ar_SA
    data_files:
    - split: train
      path: ""en-ar_SA.jsonl""
  - config_name: en-bg_BG
    data_files:
    - split: train
      path: ""en-bg_BG.jsonl""
  - config_name: en-bn_IN
    data_files:
    - split: train
      path: ""en-bn_IN.jsonl""
  - config_name: en-ca_ES
    data_files:
    - split: train
      path: ""en-ca_ES.jsonl""
  - config_name: en-cs_CZ
    data_files:
    - split: train
      path: ""en-cs_CZ.jsonl""
  - config_name: en-da_DK
    data_files:
    - split: train
      path: ""en-da_DK.jsonl""
  - config_name: en-de_DE
    data_files:
    - split: train
      path: ""en-de_DE.jsonl""
  - config_name: en-el_GR
    data_files:
    - split: train
      path: ""en-el_GR.jsonl""
  - config_name: en-es_MX
    data_files:
    - split: train
      path: ""en-es_MX.jsonl""
  - config_name: en-et_EE
    data_files:
    - split: train
      path: ""en-et_EE.jsonl""
  - config_name: en-fa_IR
    data_files:
    - split: train
      path: ""en-fa_IR.jsonl""
  - config_name: en-fi_FI
    data_files:
    - split: train
      path: ""en-fi_FI.jsonl""
  - config_name: en-fil_PH
    data_files:
    - split: train
      path: ""en-fil_PH.jsonl""
  - config_name: en-fr_CA
    data_files:
    - split: train
      path: ""en-fr_CA.jsonl""
  - config_name: en-fr_FR
    data_files:
    - split: train
      path: ""en-fr_FR.jsonl""
  - config_name: en-gu_IN
    data_files:
    - split: train
      path: ""en-gu_IN.jsonl""
  - config_name: en-he_IL
    data_files:
    - split: train
      path: ""en-he_IL.jsonl""
  - config_name: en-hi_IN
    data_files:
    - split: train
      path: ""en-hi_IN.jsonl""
  - config_name: en-hr_HR
    data_files:
    - split: train
      path: ""en-hr_HR.jsonl""
  - config_name: en-hu_HU
    data_files:
    - split: train
      path: ""en-hu_HU.jsonl""
  - config_name: en-id_ID
    data_files:
    - split: train
      path: ""en-id_ID.jsonl""
  - config_name: en-is_IS
    data_files:
    - split: train
      path: ""en-is_IS.jsonl""
  - config_name: en-it_IT
    data_files:
    - split: train
      path: ""en-it_IT.jsonl""
  - config_name: en-ja_JP
    data_files:
    - split: train
      path: ""en-ja_JP.jsonl""
  - config_name: en-kn_IN
    data_files:
    - split: train
      path: ""en-kn_IN.jsonl""
  - config_name: en-ko_KR
    data_files:
    - split: train
      path: ""en-ko_KR.jsonl""
  - config_name: en-lt_LT
    data_files:
    - split: train
      path: ""en-lt_LT.jsonl""
  - config_name: en-lv_LV
    data_files:
    - split: train
      path: ""en-lv_LV.jsonl""
  - config_name: en-ml_IN
    data_files:
    - split: train
      path: ""en-ml_IN.jsonl""
  - config_name: en-mr_IN
    data_files:
    - split: train
      path: ""en-mr_IN.jsonl""
  - config_name: en-nl_NL
    data_files:
    - split: train
      path: ""en-nl_NL.jsonl""
  - config_name: en-no_NO
    data_files:
    - split: train
      path: ""en-no_NO.jsonl""
  - config_name: en-pa_IN
    data_files:
    - split: train
      path: ""en-pa_IN.jsonl""
  - config_name: en-pl_PL
    data_files:
    - split: train
      path: ""en-pl_PL.jsonl""
  - config_name: en-pt_BR
    data_files:
    - split: train
      path: ""en-pt_BR.jsonl""
  - config_name: en-pt_PT
    data_files:
    - split: train
      path: ""en-pt_PT.jsonl""
  - config_name: en-ro_RO
    data_files:
    - split: train
      path: ""en-ro_RO.jsonl""
  - config_name: en-ru_RU
    data_files:
    - split: train
      path: ""en-ru_RU.jsonl""
  - config_name: en-sk_SK
    data_files:
    - split: train
      path: ""en-sk_SK.jsonl""
  - config_name: en-sl_SI
    data_files:
    - split: train
      path: ""en-sl_SI.jsonl""
  - config_name: en-sr_RS
    data_files:
    - split: train
      path: ""en-sr_RS.jsonl""
  - config_name: en-sv_SE
    data_files:
    - split: train
      path: ""en-sv_SE.jsonl""
  - config_name: en-sw_KE
    data_files:
    - split: train
      path: ""en-sw_KE.jsonl""
  - config_name: en-sw_TZ
    data_files:
    - split: train
      path: ""en-sw_TZ.jsonl""
  - config_name: en-ta_IN
    data_files:
    - split: train
      path: ""en-ta_IN.jsonl""
  - config_name: en-te_IN
    data_files:
    - split: train
      path: ""en-te_IN.jsonl""
  - config_name: en-th_TH
    data_files:
    - split: train
      path: ""en-th_TH.jsonl""
  - config_name: en-tr_TR
    data_files:
    - split: train
      path: ""en-tr_TR.jsonl""
  - config_name: en-uk_UA
    data_files:
    - split: train
      path: ""en-uk_UA.jsonl""
  - config_name: en-ur_PK
    data_files:
    - split: train
      path: ""en-ur_PK.jsonl""
  - config_name: en-vi_VN
    data_files:
    - split: train
      path: ""en-vi_VN.jsonl""
  - config_name: en-zh_CN
    data_files:
    - split: train
      path: ""en-zh_CN.jsonl""
  - config_name: en-zh_TW
    data_files:
    - split: train
      path: ""en-zh_TW.jsonl""
  - config_name: en-zu_ZA
    data_files:
    - split: train
      path: ""en-zu_ZA.jsonl""
---

# WMT24++

This repository contains the human translation and post-edit data for the 55 en->xx language pairs released in
the publication
[WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages & Dialects](https://arxiv.org/abs/2502.12404).
If you are interested in the MT/LLM system outputs and automatic metric scores, please see [MTME](https://github.com/google-research/mt-metrics-eval/tree/main?tab=readme-ov-file#wmt24-data).
If you are interested in the images of the source URLs for each document, please see [here](https://huggingface.co/datasets/google/wmt24pp-images).

## Schema

Each language pair is stored in its own jsonl file.
Each row is a serialized JSON object with the following fields:

  - `lp`: The language pair (e.g., `""en-de_DE""`).
  - `domain`: The domain of the source, either `""canary""`, `""news""`, `""social""`, `""speech""`, or `""literary""`.
  - `document_id`: The unique ID that identifies the document the source came from.
  - `segment_id`: The globally unique ID that identifies the segment.
  - `is_bad_source`: A Boolean that indicates whether this source is low quality (e.g., HTML, URLs, emoijs). In the paper, the segments marked as true were removed from the evaluation, and we recommend doing the same.
  - `source`: The English source text.
  - `target`: The post-edit of `original_target`. We recommend using the post-edit as the default reference.
  - `original_target`: The original reference translation.

## Citation
If you use any of the data released in our work, please cite the following paper:

```
@misc{deutsch2025wmt24expandinglanguagecoverage,
      title={{WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages & Dialects}}, 
      author={Daniel Deutsch and Eleftheria Briakou and Isaac Caswell and Mara Finkelstein and Rebecca Galor and Juraj Juraska and Geza Kovacs and Alison Lui and Ricardo Rei and Jason Riesa and Shruti Rijhwani and Parker Riley and Elizabeth Salesky and Firas Trabelsi and Stephanie Winkler and Biao Zhang and Markus Freitag},
      year={2025},
      eprint={2502.12404},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.12404}, 
}
```

## Helpful Python Constants

```python
LANGUAGE_PAIRS = (
    ""en-ar_EG"", ""en-ar_SA"", ""en-bg_BG"", ""en-bn_IN"", ""en-ca_ES"", ""en-cs_CZ"", ""en-da_DK"", ""en-de_DE"",
    ""en-el_GR"", ""en-es_MX"", ""en-et_EE"", ""en-fa_IR"", ""en-fi_FI"", ""en-fil_PH"", ""en-fr_CA"", ""en-fr_FR"",
    ""en-gu_IN"", ""en-he_IL"", ""en-hi_IN"", ""en-hr_HR"", ""en-hu_HU"", ""en-id_ID"", ""en-is_IS"", ""en-it_IT"",
    ""en-ja_JP"", ""en-kn_IN"", ""en-ko_KR"", ""en-lt_LT"", ""en-lv_LV"", ""en-ml_IN"", ""en-mr_IN"", ""en-nl_NL"",
    ""en-no_NO"", ""en-pa_IN"", ""en-pl_PL"", ""en-pt_BR"", ""en-pt_PT"", ""en-ro_RO"", ""en-ru_RU"", ""en-sk_SK"",
    ""en-sl_SI"", ""en-sr_RS"", ""en-sv_SE"", ""en-sw_KE"", ""en-sw_TZ"", ""en-ta_IN"", ""en-te_IN"", ""en-th_TH"",
    ""en-tr_TR"", ""en-uk_UA"", ""en-ur_PK"", ""en-vi_VN"", ""en-zh_CN"", ""en-zh_TW"", ""en-zu_ZA"",
)

LANGUAGE_BY_CODE = {
    ""ar_EG"": ""Arabic"",
    ""ar_SA"": ""Arabic"",
    ""bg_BG"": ""Bulgarian"",
    ""bn_BD"": ""Bengali"",
    ""bn_IN"": ""Bengali"",
    ""ca_ES"": ""Catalan"",
    ""cs_CZ"": ""Czech"",
    ""da_DK"": ""Danish"",
    ""de_DE"": ""German"",
    ""el_GR"": ""Greek"",
    ""es_MX"": ""Spanish"",
    ""et_EE"": ""Estonian"",
    ""fa_IR"": ""Farsi"",
    ""fi_FI"": ""Finnish"",
    ""fil_PH"": ""Filipino"",
    ""fr_CA"": ""French"",
    ""fr_FR"": ""French"",
    ""gu_IN"": ""Gujarati"",
    ""he_IL"": ""Hebrew"",
    ""hi_IN"": ""Hindi"",
    ""hr_HR"": ""Croatian"",
    ""hu_HU"": ""Hungarian"",
    ""id_ID"": ""Indonesian"",
    ""is_IS"": ""Icelandic"",
    ""it_IT"": ""Italian"",
    ""ja_JP"": ""Japanese"",
    ""kn_IN"": ""Kannada"",
    ""ko_KR"": ""Korean"",
    ""lt_LT"": ""Lithuanian"",
    ""lv_LV"": ""Latvian"",
    ""ml_IN"": ""Malayalam"",
    ""mr_IN"": ""Marathi"",
    ""nl_NL"": ""Dutch"",
    ""no_NO"": ""Norwegian"",
    ""pa_IN"": ""Punjabi"",
    ""pl_PL"": ""Polish"",
    ""pt_BR"": ""Portuguese"",
    ""pt_PT"": ""Portuguese"",
    ""ro_RO"": ""Romanian"",
    ""ru_RU"": ""Russian"",
    ""sk_SK"": ""Slovak"",
    ""sl_SI"": ""Slovenian"",
    ""sr_RS"": ""Serbian"",
    ""sv_SE"": ""Swedish"",
    ""sw_KE"": ""Swahili"",
    ""sw_TZ"": ""Swahili"",
    ""ta_IN"": ""Tamil"",
    ""te_IN"": ""Telugu"",
    ""th_TH"": ""Thai"",
    ""tr_TR"": ""Turkish"",
    ""uk_UA"": ""Ukrainian"",
    ""ur_PK"": ""Urdu"",
    ""vi_VN"": ""Vietnamese"",
    ""zh_CN"": ""Mandarin"",
    ""zh_TW"": ""Mandarin"",
    ""zu_ZA"": ""Zulu"",
}

REGION_BY_CODE = {
    ""ar_EG"": ""Egypt"",
    ""ar_SA"": ""Saudi Arabia"",
    ""bg_BG"": ""Bulgaria"",
    ""bn_BD"": ""Bangladesh"",
    ""bn_IN"": ""India"",
    ""ca_ES"": ""Spain"",
    ""cs_CZ"": ""Czechia"",
    ""da_DK"": ""Denmark"",
    ""de_DE"": ""Germany"",
    ""el_GR"": ""Greece"",
    ""es_MX"": ""Mexico"",
    ""et_EE"": ""Estonia"",
    ""fa_IR"": ""Iran"",
    ""fi_FI"": ""Finland"",
    ""fil_PH"": ""Philippines"",
    ""fr_CA"": ""Canada"",
    ""fr_FR"": ""France"",
    ""gu_IN"": ""India"",
    ""he_IL"": ""Israel"",
    ""hi_IN"": ""India"",
    ""hr_HR"": ""Croatia"",
    ""hu_HU"": ""Hungary"",
    ""id_ID"": ""Indonesia"",
    ""is_IS"": ""Iceland"",
    ""it_IT"": ""Italy"",
    ""ja_JP"": ""Japan"",
    ""kn_IN"": ""India"",
    ""ko_KR"": ""South Korea"",
    ""lt_LT"": ""Lithuania"",
    ""lv_LV"": ""Latvia"",
    ""ml_IN"": ""India"",
    ""mr_IN"": ""India"",
    ""nl_NL"": ""Netherlands"",
    ""no_NO"": ""Norway"",
    ""pa_IN"": ""India"",
    ""pl_PL"": ""Poland"",
    ""pt_BR"": ""Brazil"",
    ""pt_PT"": ""Portugal"",
    ""ro_RO"": ""Romania"",
    ""ru_RU"": ""Russia"",
    ""sk_SK"": ""Slovakia"",
    ""sl_SI"": ""Slovenia"",
    ""sr_RS"": ""Serbia"",
    ""sv_SE"": ""Sweden"",
    ""sw_KE"": ""Kenya"",
    ""sw_TZ"": ""Tanzania"",
    ""ta_IN"": ""India"",
    ""te_IN"": ""India"",
    ""th_TH"": ""Thailand"",
    ""tr_TR"": ""Turkey"",
    ""uk_UA"": ""Ukraine"",
    ""ur_PK"": ""Pakistan"",
    ""vi_VN"": ""Vietnam"",
    ""zh_CN"": ""China"",
    ""zh_TW"": ""Taiwan"",
    ""zu_ZA"": ""South Africa"",
}
```",Medium,3.0
Translation,LLaMAX/BenchMAX_General_Translation,0.0,1656.0,2025-06-06 07:13:36+00:00,cc-by-4.0,0.0,45.5 MB,47710208.0,19.8 MB,20761804.8,227543,https://arxiv.org/abs/2502.07346,none,"---
license: cc-by-4.0
task_categories:
- translation
language:
- en
- zh
- es
- fr
- de
- ru
- ja
- th
- sw
- te
- bn
- ar
- ko
- vi
- cs
- hu
- sr
multilinguality:
- multilingual
dataset_info:
- config_name: flores_en
  features:
  - name: text
    dtype: string
- config_name: ted_en
  features:
  - name: id
    dtype: int32
  - name: text
    dtype: string
  - name: talk_name
    dtype: string
  - name: lang
    dtype: string
configs:
- config_name: flores_en
  data_files: flores200/flores_200_en.jsonl
- config_name: flores_zh
  data_files: flores200/flores_200_zh.jsonl
- config_name: flores_es
  data_files: flores200/flores_200_es.jsonl
- config_name: flores_fr
  data_files: flores200/flores_200_fr.jsonl
- config_name: flores_de
  data_files: flores200/flores_200_de.jsonl
- config_name: flores_ru
  data_files: flores200/flores_200_ru.jsonl
- config_name: flores_ja
  data_files: flores200/flores_200_ja.jsonl
- config_name: flores_th
  data_files: flores200/flores_200_th.jsonl
- config_name: flores_bn
  data_files: flores200/flores_200_bn.jsonl
- config_name: flores_sw
  data_files: flores200/flores_200_sw.jsonl
- config_name: flores_te
  data_files: flores200/flores_200_te.jsonl
- config_name: flores_ar
  data_files: flores200/flores_200_ar.jsonl
- config_name: flores_ko
  data_files: flores200/flores_200_ko.jsonl
- config_name: flores_vi
  data_files: flores200/flores_200_vi.jsonl
- config_name: flores_cs
  data_files: flores200/flores_200_cs.jsonl
- config_name: flores_hu
  data_files: flores200/flores_200_hu.jsonl
- config_name: flores_sr
  data_files: flores200/flores_200_sr.jsonl
- config_name: ted_en
  data_files: ted/test.en.jsonl
- config_name: ted_zh
  data_files: ted/test.zh.jsonl
- config_name: ted_es
  data_files: ted/test.es.jsonl
- config_name: ted_fr
  data_files: ted/test.fr.jsonl
- config_name: ted_de
  data_files: ted/test.de.jsonl
- config_name: ted_ru
  data_files: ted/test.ru.jsonl
- config_name: ted_ja
  data_files: ted/test.ja.jsonl
- config_name: ted_th
  data_files: ted/test.th.jsonl
- config_name: ted_bn
  data_files: ted/test.bn.jsonl
- config_name: ted_ar
  data_files: ted/test.ar.jsonl
- config_name: ted_ko
  data_files: ted/test.ko.jsonl
- config_name: ted_vi
  data_files: ted/test.vi.jsonl
- config_name: ted_cs
  data_files: ted/test.cs.jsonl
- config_name: ted_hu
  data_files: ted/test.hu.jsonl
- config_name: ted_sr
  data_files: ted/test.sr.jsonl
- config_name: wmt24_en-zh_en
  data_files: wmt24/wmt24.en-zh.en.jsonl
- config_name: wmt24_en-zh_zh
  data_files: wmt24/wmt24.en-zh.zh.jsonl
- config_name: wmt24_en-es_en
  data_files: wmt24/wmt24.en-es.en.jsonl
- config_name: wmt24_en-es_es
  data_files: wmt24/wmt24.en-es.es.jsonl
- config_name: wmt24_en-de_en
  data_files: wmt24/wmt24.en-de.en.jsonl
- config_name: wmt24_en-de_de
  data_files: wmt24/wmt24.en-de.de.jsonl
- config_name: wmt24_en-cs_en
  data_files: wmt24/wmt24.en-cs.en.jsonl
- config_name: wmt24_en-cs_cs
  data_files: wmt24/wmt24.en-cs.cs.jsonl
- config_name: wmt24_en-ja_en
  data_files: wmt24/wmt24.en-ja.en.jsonl
- config_name: wmt24_en-ja_ja
  data_files: wmt24/wmt24.en-ja.ja.jsonl
- config_name: wmt24_en-ru_en
  data_files: wmt24/wmt24.en-ru.en.jsonl
- config_name: wmt24_en-ru_ru
  data_files: wmt24/wmt24.en-ru.ru.jsonl
- config_name: flores_af
  data_files: flores200/flores_200_af.jsonl
- config_name: flores_am
  data_files: flores200/flores_200_am.jsonl
- config_name: flores_as
  data_files: flores200/flores_200_as.jsonl
- config_name: flores_ast
  data_files: flores200/flores_200_ast.jsonl
- config_name: flores_az
  data_files: flores200/flores_200_az.jsonl
- config_name: flores_be
  data_files: flores200/flores_200_be.jsonl
- config_name: flores_bs
  data_files: flores200/flores_200_bs.jsonl
- config_name: flores_bg
  data_files: flores200/flores_200_bg.jsonl
- config_name: flores_ca
  data_files: flores200/flores_200_ca.jsonl
- config_name: flores_ceb
  data_files: flores200/flores_200_ceb.jsonl
- config_name: flores_ku
  data_files: flores200/flores_200_ku.jsonl
- config_name: flores_cy
  data_files: flores200/flores_200_cy.jsonl
- config_name: flores_da
  data_files: flores200/flores_200_da.jsonl
- config_name: flores_el
  data_files: flores200/flores_200_el.jsonl
- config_name: flores_et
  data_files: flores200/flores_200_et.jsonl
- config_name: flores_fa
  data_files: flores200/flores_200_fa.jsonl
- config_name: flores_fi
  data_files: flores200/flores_200_fi.jsonl
- config_name: flores_ff
  data_files: flores200/flores_200_ff.jsonl
- config_name: flores_ga
  data_files: flores200/flores_200_ga.jsonl
- config_name: flores_gl
  data_files: flores200/flores_200_gl.jsonl
- config_name: flores_gu
  data_files: flores200/flores_200_gu.jsonl
- config_name: flores_ha
  data_files: flores200/flores_200_ha.jsonl
- config_name: flores_he
  data_files: flores200/flores_200_he.jsonl
- config_name: flores_hi
  data_files: flores200/flores_200_hi.jsonl
- config_name: flores_hr
  data_files: flores200/flores_200_hr.jsonl
- config_name: flores_hy
  data_files: flores200/flores_200_hy.jsonl
- config_name: flores_ig
  data_files: flores200/flores_200_ig.jsonl
- config_name: flores_id
  data_files: flores200/flores_200_id.jsonl
- config_name: flores_is
  data_files: flores200/flores_200_is.jsonl
- config_name: flores_it
  data_files: flores200/flores_200_it.jsonl
- config_name: flores_jv
  data_files: flores200/flores_200_jv.jsonl
- config_name: flores_kam
  data_files: flores200/flores_200_kam.jsonl
- config_name: flores_kn
  data_files: flores200/flores_200_kn.jsonl
- config_name: flores_ka
  data_files: flores200/flores_200_ka.jsonl
- config_name: flores_kk
  data_files: flores200/flores_200_kk.jsonl
- config_name: flores_kea
  data_files: flores200/flores_200_kea.jsonl
- config_name: flores_km
  data_files: flores200/flores_200_km.jsonl
- config_name: flores_ky
  data_files: flores200/flores_200_ky.jsonl
- config_name: flores_lo
  data_files: flores200/flores_200_lo.jsonl
- config_name: flores_lv
  data_files: flores200/flores_200_lv.jsonl
- config_name: flores_ln
  data_files: flores200/flores_200_ln.jsonl
- config_name: flores_lt
  data_files: flores200/flores_200_lt.jsonl
- config_name: flores_lb
  data_files: flores200/flores_200_lb.jsonl
- config_name: flores_lg
  data_files: flores200/flores_200_lg.jsonl
- config_name: flores_luo
  data_files: flores200/flores_200_luo.jsonl
- config_name: flores_ml
  data_files: flores200/flores_200_ml.jsonl
- config_name: flores_mr
  data_files: flores200/flores_200_mr.jsonl
- config_name: flores_mk
  data_files: flores200/flores_200_mk.jsonl
- config_name: flores_mt
  data_files: flores200/flores_200_mt.jsonl
- config_name: flores_mn
  data_files: flores200/flores_200_mn.jsonl
- config_name: flores_mi
  data_files: flores200/flores_200_mi.jsonl
- config_name: flores_ms
  data_files: flores200/flores_200_ms.jsonl
- config_name: flores_my
  data_files: flores200/flores_200_my.jsonl
- config_name: flores_nl
  data_files: flores200/flores_200_nl.jsonl
- config_name: flores_no
  data_files: flores200/flores_200_no.jsonl
- config_name: flores_ne
  data_files: flores200/flores_200_ne.jsonl
- config_name: flores_ns
  data_files: flores200/flores_200_ns.jsonl
- config_name: flores_ny
  data_files: flores200/flores_200_ny.jsonl
- config_name: flores_oc
  data_files: flores200/flores_200_oc.jsonl
- config_name: flores_om
  data_files: flores200/flores_200_om.jsonl
- config_name: flores_or
  data_files: flores200/flores_200_or.jsonl
- config_name: flores_pa
  data_files: flores200/flores_200_pa.jsonl
- config_name: flores_pl
  data_files: flores200/flores_200_pl.jsonl
- config_name: flores_pt
  data_files: flores200/flores_200_pt.jsonl
- config_name: flores_ps
  data_files: flores200/flores_200_ps.jsonl
- config_name: flores_ro
  data_files: flores200/flores_200_ro.jsonl
- config_name: flores_sk
  data_files: flores200/flores_200_sk.jsonl
- config_name: flores_sl
  data_files: flores200/flores_200_sl.jsonl
- config_name: flores_sn
  data_files: flores200/flores_200_sn.jsonl
- config_name: flores_sd
  data_files: flores200/flores_200_sd.jsonl
- config_name: flores_so
  data_files: flores200/flores_200_so.jsonl
- config_name: flores_sv
  data_files: flores200/flores_200_sv.jsonl
- config_name: flores_ta
  data_files: flores200/flores_200_ta.jsonl
- config_name: flores_tg
  data_files: flores200/flores_200_tg.jsonl
- config_name: flores_tl
  data_files: flores200/flores_200_tl.jsonl
- config_name: flores_tr
  data_files: flores200/flores_200_tr.jsonl
- config_name: flores_uk
  data_files: flores200/flores_200_uk.jsonl
- config_name: flores_umb
  data_files: flores200/flores_200_umb.jsonl
- config_name: flores_ur
  data_files: flores200/flores_200_ur.jsonl
- config_name: flores_uz
  data_files: flores200/flores_200_uz.jsonl
- config_name: flores_wo
  data_files: flores200/flores_200_wo.jsonl
- config_name: flores_xh
  data_files: flores200/flores_200_xh.jsonl
- config_name: flores_yo
  data_files: flores200/flores_200_yo.jsonl
- config_name: flores_zho_trad
  data_files: flores200/flores_200_zho_trad.jsonl
- config_name: flores_zu
  data_files: flores200/flores_200_zu.jsonl
---
## Dataset Sources

- **Paper**: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models
- **Link**: https://huggingface.co/papers/2502.07346
- **Repository**: https://github.com/CONE-MT/BenchMAX

## Dataset Description
BenchMAX_General_Translation is a dataset of [BenchMAX](https://arxiv.org/pdf/2502.07346), which evaluates the translation capability on the general domain.

We collect parallel test data from [Flore-200](https://github.com/facebookresearch/flores), [TED-talk](https://huggingface.co/datasets/IWSLT/ted_talks_iwslt), and [WMT24](https://www2.statmt.org/wmt24/).

## Usage

```bash
git clone https://github.com/CONE-MT/BenchMAX.git
cd BenchMAX
pip install -r requirements.txt

cd tasks/translation
# generate general translations
# -s denotes source languages, -t denotes target languages
python generate_translation.py -s en -t zh,es,fr,de,ru,ja,th,sw,bn,te,ar,ko,vi,cs,hu,sr --task-name flores --model-name $model --infer-backend vllm --max-tokens 512
python generate_translation.py -s zh,es,fr,de,ru,ja,th,sw,bn,te,ar,ko,vi,cs,hu,sr -t en --task-name flores --model-name $model --infer-backend vllm --max-tokens 512
python generate_translation.py -s en -t zh,es,fr,de,ru,ja,th,bn,ar,ko,vi,cs,hu,sr --task-name ted --model-name $model --infer-backend vllm --max-tokens 512
python generate_translation.py -s zh,es,fr,de,ru,ja,th,bn,ar,ko,vi,cs,hu,sr -t en --task-name ted --model-name $model --infer-backend vllm --max-tokens 512
python generate_translation.py -s en -t cs,de,es,ja,ru,zh --task-name wmt24 --model-name $model --infer-backend vllm --max-tokens 1024

# evaluate general translations
python evaluate_translation.py -s en -t zh,es,fr,de,ru,ja,th,sw,bn,te,ar,ko,vi,cs,hu,sr --task-name flores --model-name $model --metrics spBLEU
python evaluate_translation.py -s zh,es,fr,de,ru,ja,th,sw,bn,te,ar,ko,vi,cs,hu,sr -t en --task-name flores --model-name $model --metrics spBLEU
python evaluate_translation.py -s en -t zh,es,fr,de,ru,ja,th,bn,ar,ko,vi,cs,hu,sr --task-name ted --model-name $model --metrics spBLEU
python evaluate_translation.py -s zh,es,fr,de,ru,ja,th,bn,ar,ko,vi,cs,hu,sr -t en --task-name ted --model-name $model --metrics spBLEU
python evaluate_translation.py -s en -t cs,de,es,ja,ru,zh --task-name wmt24 --model-name $model --metrics spBLEU

```

## Supported Languages
Arabic, Bengali, Chinese, Czech, English, French, German, Hungarian, Japanese, Korean, Serbian, Spanish, Swahili, Telugu, Thai, Russian, Vietnamese

## Citation
If you find our dataset helpful, please cite this paper:

```
@article{huang2025benchmax,
  title={BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models},
  author={Huang, Xu and Zhu, Wenhao and Hu, Hanxu and He, Conghui and Li, Lei and Huang, Shujian and Yuan, Fei},
  journal={arXiv preprint arXiv:2502.07346},
  year={2025}
}
```",High,5.0
Translation,LLaMAX/BenchMAX_Domain_Translation,0.0,136.0,2025-03-19 07:53:38+00:00,cc-by-4.0,0.0,119 MB,124780544.0,29.8 MB,31247564.8,47277,https://arxiv.org/abs/2502.07346,none,"---
license: cc-by-4.0
task_categories:
- translation
language:
- en
- zh
- es
- fr
- de
- ru
- ja
- th
- sw
- te
- bn
- ar
- ko
- vi
- cs
- hu
- sr
multilinguality:
- multilingual
configs:
- config_name: arenahard_en
  data_files: arenahard/arenahard_en.jsonl
- config_name: arenahard_zh
  data_files: arenahard/arenahard_zh.jsonl
- config_name: arenahard_es
  data_files: arenahard/arenahard_es.jsonl
- config_name: arenahard_fr
  data_files: arenahard/arenahard_fr.jsonl
- config_name: arenahard_de
  data_files: arenahard/arenahard_de.jsonl
- config_name: arenahard_ru
  data_files: arenahard/arenahard_ru.jsonl
- config_name: arenahard_ja
  data_files: arenahard/arenahard_ja.jsonl
- config_name: arenahard_th
  data_files: arenahard/arenahard_th.jsonl
- config_name: arenahard_bn
  data_files: arenahard/arenahard_bn.jsonl
- config_name: arenahard_sw
  data_files: arenahard/arenahard_sw.jsonl
- config_name: arenahard_te
  data_files: arenahard/arenahard_te.jsonl
- config_name: arenahard_ar
  data_files: arenahard/arenahard_ar.jsonl
- config_name: arenahard_ko
  data_files: arenahard/arenahard_ko.jsonl
- config_name: arenahard_vi
  data_files: arenahard/arenahard_vi.jsonl
- config_name: arenahard_cs
  data_files: arenahard/arenahard_cs.jsonl
- config_name: arenahard_hu
  data_files: arenahard/arenahard_hu.jsonl
- config_name: arenahard_sr
  data_files: arenahard/arenahard_sr.jsonl
- config_name: gpqa_en
  data_files: gpqa/gpqa_en.jsonl
- config_name: gpqa_zh
  data_files: gpqa/gpqa_zh.jsonl
- config_name: gpqa_es
  data_files: gpqa/gpqa_es.jsonl
- config_name: gpqa_fr
  data_files: gpqa/gpqa_fr.jsonl
- config_name: gpqa_de
  data_files: gpqa/gpqa_de.jsonl
- config_name: gpqa_ru
  data_files: gpqa/gpqa_ru.jsonl
- config_name: gpqa_ja
  data_files: gpqa/gpqa_ja.jsonl
- config_name: gpqa_th
  data_files: gpqa/gpqa_th.jsonl
- config_name: gpqa_bn
  data_files: gpqa/gpqa_bn.jsonl
- config_name: gpqa_sw
  data_files: gpqa/gpqa_sw.jsonl
- config_name: gpqa_te
  data_files: gpqa/gpqa_te.jsonl
- config_name: gpqa_ar
  data_files: gpqa/gpqa_ar.jsonl
- config_name: gpqa_ko
  data_files: gpqa/gpqa_ko.jsonl
- config_name: gpqa_vi
  data_files: gpqa/gpqa_vi.jsonl
- config_name: gpqa_cs
  data_files: gpqa/gpqa_cs.jsonl
- config_name: gpqa_hu
  data_files: gpqa/gpqa_hu.jsonl
- config_name: gpqa_sr
  data_files: gpqa/gpqa_sr.jsonl
- config_name: humaneval_en
  data_files: humaneval/humaneval_en.jsonl
- config_name: humaneval_zh
  data_files: humaneval/humaneval_zh.jsonl
- config_name: humaneval_es
  data_files: humaneval/humaneval_es.jsonl
- config_name: humaneval_fr
  data_files: humaneval/humaneval_fr.jsonl
- config_name: humaneval_de
  data_files: humaneval/humaneval_de.jsonl
- config_name: humaneval_ru
  data_files: humaneval/humaneval_ru.jsonl
- config_name: humaneval_ja
  data_files: humaneval/humaneval_ja.jsonl
- config_name: humaneval_th
  data_files: humaneval/humaneval_th.jsonl
- config_name: humaneval_bn
  data_files: humaneval/humaneval_bn.jsonl
- config_name: humaneval_sw
  data_files: humaneval/humaneval_sw.jsonl
- config_name: humaneval_te
  data_files: humaneval/humaneval_te.jsonl
- config_name: humaneval_ar
  data_files: humaneval/humaneval_ar.jsonl
- config_name: humaneval_ko
  data_files: humaneval/humaneval_ko.jsonl
- config_name: humaneval_vi
  data_files: humaneval/humaneval_vi.jsonl
- config_name: humaneval_cs
  data_files: humaneval/humaneval_cs.jsonl
- config_name: humaneval_hu
  data_files: humaneval/humaneval_hu.jsonl
- config_name: humaneval_sr
  data_files: humaneval/humaneval_sr.jsonl
- config_name: ifeval_en
  data_files: ifeval/ifeval_en.jsonl
- config_name: ifeval_zh
  data_files: ifeval/ifeval_zh.jsonl
- config_name: ifeval_es
  data_files: ifeval/ifeval_es.jsonl
- config_name: ifeval_fr
  data_files: ifeval/ifeval_fr.jsonl
- config_name: ifeval_de
  data_files: ifeval/ifeval_de.jsonl
- config_name: ifeval_ru
  data_files: ifeval/ifeval_ru.jsonl
- config_name: ifeval_ja
  data_files: ifeval/ifeval_ja.jsonl
- config_name: ifeval_th
  data_files: ifeval/ifeval_th.jsonl
- config_name: ifeval_bn
  data_files: ifeval/ifeval_bn.jsonl
- config_name: ifeval_sw
  data_files: ifeval/ifeval_sw.jsonl
- config_name: ifeval_te
  data_files: ifeval/ifeval_te.jsonl
- config_name: ifeval_ar
  data_files: ifeval/ifeval_ar.jsonl
- config_name: ifeval_ko
  data_files: ifeval/ifeval_ko.jsonl
- config_name: ifeval_vi
  data_files: ifeval/ifeval_vi.jsonl
- config_name: ifeval_cs
  data_files: ifeval/ifeval_cs.jsonl
- config_name: ifeval_hu
  data_files: ifeval/ifeval_hu.jsonl
- config_name: ifeval_sr
  data_files: ifeval/ifeval_sr.jsonl
- config_name: lcb_v4_en
  data_files: lcb_v4/lcb_v4_en.jsonl
- config_name: lcb_v4_zh
  data_files: lcb_v4/lcb_v4_zh.jsonl
- config_name: lcb_v4_es
  data_files: lcb_v4/lcb_v4_es.jsonl
- config_name: lcb_v4_fr
  data_files: lcb_v4/lcb_v4_fr.jsonl
- config_name: lcb_v4_de
  data_files: lcb_v4/lcb_v4_de.jsonl
- config_name: lcb_v4_ru
  data_files: lcb_v4/lcb_v4_ru.jsonl
- config_name: lcb_v4_ja
  data_files: lcb_v4/lcb_v4_ja.jsonl
- config_name: lcb_v4_th
  data_files: lcb_v4/lcb_v4_th.jsonl
- config_name: lcb_v4_bn
  data_files: lcb_v4/lcb_v4_bn.jsonl
- config_name: lcb_v4_sw
  data_files: lcb_v4/lcb_v4_sw.jsonl
- config_name: lcb_v4_te
  data_files: lcb_v4/lcb_v4_te.jsonl
- config_name: lcb_v4_ar
  data_files: lcb_v4/lcb_v4_ar.jsonl
- config_name: lcb_v4_ko
  data_files: lcb_v4/lcb_v4_ko.jsonl
- config_name: lcb_v4_vi
  data_files: lcb_v4/lcb_v4_vi.jsonl
- config_name: lcb_v4_cs
  data_files: lcb_v4/lcb_v4_cs.jsonl
- config_name: lcb_v4_hu
  data_files: lcb_v4/lcb_v4_hu.jsonl
- config_name: lcb_v4_sr
  data_files: lcb_v4/lcb_v4_sr.jsonl
- config_name: mgsm_en
  data_files: mgsm/mgsm_en.jsonl
- config_name: mgsm_zh
  data_files: mgsm/mgsm_zh.jsonl
- config_name: mgsm_es
  data_files: mgsm/mgsm_es.jsonl
- config_name: mgsm_fr
  data_files: mgsm/mgsm_fr.jsonl
- config_name: mgsm_de
  data_files: mgsm/mgsm_de.jsonl
- config_name: mgsm_ru
  data_files: mgsm/mgsm_ru.jsonl
- config_name: mgsm_ja
  data_files: mgsm/mgsm_ja.jsonl
- config_name: mgsm_th
  data_files: mgsm/mgsm_th.jsonl
- config_name: mgsm_bn
  data_files: mgsm/mgsm_bn.jsonl
- config_name: mgsm_sw
  data_files: mgsm/mgsm_sw.jsonl
- config_name: mgsm_te
  data_files: mgsm/mgsm_te.jsonl
- config_name: mgsm_ar
  data_files: mgsm/mgsm_ar.jsonl
- config_name: mgsm_ko
  data_files: mgsm/mgsm_ko.jsonl
- config_name: mgsm_vi
  data_files: mgsm/mgsm_vi.jsonl
- config_name: mgsm_cs
  data_files: mgsm/mgsm_cs.jsonl
- config_name: mgsm_hu
  data_files: mgsm/mgsm_hu.jsonl
- config_name: mgsm_sr
  data_files: mgsm/mgsm_sr.jsonl
- config_name: nexus_en
  data_files: nexus/nexus_en.jsonl
- config_name: nexus_zh
  data_files: nexus/nexus_zh.jsonl
- config_name: nexus_es
  data_files: nexus/nexus_es.jsonl
- config_name: nexus_fr
  data_files: nexus/nexus_fr.jsonl
- config_name: nexus_de
  data_files: nexus/nexus_de.jsonl
- config_name: nexus_ru
  data_files: nexus/nexus_ru.jsonl
- config_name: nexus_ja
  data_files: nexus/nexus_ja.jsonl
- config_name: nexus_th
  data_files: nexus/nexus_th.jsonl
- config_name: nexus_bn
  data_files: nexus/nexus_bn.jsonl
- config_name: nexus_sw
  data_files: nexus/nexus_sw.jsonl
- config_name: nexus_te
  data_files: nexus/nexus_te.jsonl
- config_name: nexus_ar
  data_files: nexus/nexus_ar.jsonl
- config_name: nexus_ko
  data_files: nexus/nexus_ko.jsonl
- config_name: nexus_vi
  data_files: nexus/nexus_vi.jsonl
- config_name: nexus_cs
  data_files: nexus/nexus_cs.jsonl
- config_name: nexus_hu
  data_files: nexus/nexus_hu.jsonl
- config_name: nexus_sr
  data_files: nexus/nexus_sr.jsonl
---
## Dataset Sources
- **Paper**: BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models
- **Link**: https://huggingface.co/papers/2502.07346
- **Repository**: https://github.com/CONE-MT/BenchMAX

## Dataset Description
BenchMAX_Domain_Translation is a dataset of [BenchMAX](https://arxiv.org/pdf/2502.07346), which evaluates the translation capability on specific domains.

We collect the domain multi-way parallel data from other tasks in BenchMAX, such as math data, code data, etc.
Each sample contains one or three human-annotated translations.

## Usage

```bash
git clone --recurse-submodules https://github.com/CONE-MT/BenchMAX.git
cd BenchMAX
pip install -r requirements.txt

cd tasks/translation
tasks=(""ifeval"" ""gpqa"" ""lcb_v4"" ""mgsm"" ""humaneval"" ""nexus"" ""arenahard"")
max_tokens_list=(512 3072 2048 1024 1024 512 3072)
for i in ""${!tasks[@]}""; do
    task=${tasks[$i]}
    max_tokens=${max_tokens_list[$i]}
    python generate_translation.py -s en -t zh,es,fr,de,ru,ja,th,sw,bn,te,ar,ko,vi,cs,hu,sr --task-name $task --model-name ${model} --infer-backend vllm --max-tokens ${max_tokens}
    python generate_translation.py -s zh,es,fr,de,ru,ja,th,sw,bn,te,ar,ko,vi,cs,hu,sr -t en --task-name $task --model-name ${model} --infer-backend vllm --max-tokens ${max_tokens}

    python evaluate_translation.py -s en -t zh,es,fr,de,ru,ja,th,sw,bn,te,ar,ko,vi,cs,hu,sr --task-name $task --model-name ${model} --metrics spBLEU
    python evaluate_translation.py -s zh,es,fr,de,ru,ja,th,sw,bn,te,ar,ko,vi,cs,hu,sr -t en --task-name $task --model-name ${model} --metrics spBLEU
done
```

## Supported Languages
Arabic, Bengali, Chinese, Czech, English, French, German, Hungarian, Japanese, Korean, Serbian, Spanish, Swahili, Telugu, Thai, Russian, Vietnamese

## Citation
If you find our dataset helpful, please cite this paper:

```
@article{huang2025benchmax,
  title={BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models},
  author={Huang, Xu and Zhu, Wenhao and Hu, Hanxu and He, Conghui and Li, Lei and Huang, Shujian and Yuan, Fei},
  journal={arXiv preprint arXiv:2502.07346},
  year={2025}
}
```",High,5.0
Translation,nylancer/OpenShark,0.0,10.0,2025-02-10 16:32:58+00:00,mit,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: mit
task_categories:
- text-classification
- table-question-answering
- question-answering
- translation
- text-generation
- text-to-audio
language:
- bn
- en
- hi
- ar
- ur
tags:
- chemistry
- biology
- finance
- legal
- code
- medical
- climate
pretty_name: openshark
size_categories:
- 10K<n<100K
---

# Dataset Card for Dataset Name

<!-- Provide a quick summary of the dataset. -->

This dataset card aims to be a base template for new datasets. It has been generated using [this raw template](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/datasetcard_template.md?plain=1).

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->



- **Curated by:** [More Information Needed]
- **Funded by [optional]:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** [More Information Needed]

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** [More Information Needed]
- **Paper [optional]:** [More Information Needed]
- **Demo [optional]:** [More Information Needed]

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->

[More Information Needed]

## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

[More Information Needed]

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

[More Information Needed]",High,4.0
Translation,Youseff1987/multilingual_translation_gpt4o_gen,2.0,21.0,2025-03-01 20:52:26+00:00,mit,0.0,1.83 GB,1964947537.92,1.83 GB,1964947537.92,7659557,none,none,"---
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: test
    path: data/test-*
- config_name: flores200
  data_files:
  - split: train
    path: flores200/train-*
dataset_info:
- config_name: default
  features:
  - name: id
    dtype: int64
  - name: topic
    dtype: string
  - name: origin_language
    dtype: string
  - name: target_language
    dtype: string
  - name: system
    dtype: string
  - name: origin_text
    dtype: string
  - name: translated_text
    dtype: string
  - name: model_name
    dtype: string
  - name: origin_length
    dtype: int64
  - name: trans_length
    dtype: int64
  splits:
  - name: train
    num_bytes: 68709919
    num_examples: 60519
  - name: test
    num_bytes: 1496108
    num_examples: 1000
  download_size: 30194319
  dataset_size: 70206027
- config_name: flores200
  features:
  - name: id
    dtype: int64
  - name: topic
    dtype: string
  - name: origin_language
    dtype: string
  - name: target_language
    dtype: string
  - name: system
    dtype: string
  - name: origin_text
    dtype: string
  - name: translated_text
    dtype: string
  - name: model_name
    dtype: string
  - name: origin_length
    dtype: int64
  - name: trans_length
    dtype: int64
  splits:
  - name: train
    num_bytes: 3786588481
    num_examples: 7598038
  download_size: 1795337936
  dataset_size: 3786588481
license: mit
task_categories:
- translation
language:
- ko
- en
- zh
- zh
- zu
- ja
- am
- ar
- es
- fr
- ru
- de
- it
- pt
- nl
- sv
- tr
- id
- vi
- pl
- cs
- ro
- uk
- hu
- sl
- el
- fi
- 'no'
- da
- bg
- hi
- he
- ms
- ta
- te
- pa
- bn
- fa
- sw
- th
- sr
- hr
- ca
- is
- lv
- lt
- sk
- et
- mn
- la
- my
- tl
- jv
- mr
- gu
- ps
- sd
- kn
- ml
- ha
- yo
- ig
- ber
---",Medium,2.0
Translation,shivaniku/UniMoral,3.0,37.0,2025-02-21 18:54:53+00:00,none,0.0,13.6 MB,unknown,2.11 MB,unknown,8784,https://arxiv.org/abs/2502.14083,none,"---
task_categories:
- text-classification
- zero-shot-classification
- translation
- text2text-generation
- text-generation
language:
- ar
- zh
- en
- hi
- ru
- es
tags:
- moral
- reasoning
- morality
- culture
- multilingual
size_categories:
- 1K<n<10K
---

# UniMoral: A unified dataset for multilingual moral reasoning

UniMoral is a multilingual dataset designed to study moral reasoning as a computational pipeline. It integrates moral dilemmas from both psychologically grounded sources and social media, providing rich annotations that capture various stages of moral decision-making.

Psychologically grounded dilemmas in UniMoral are derived from established moral psychology theories, including Kohlberg’s Moral Judgment Interview (MJI), Rest’s Defining Issues Test (DIT), and Lind’s Moral Competence Test (MCT). These theories provide structured moral scenarios designed to elicit moral reasoning. To expand the dataset, LLaMA-3.1-70B is prompted to generate new variations of these dilemmas by incorporating key contributing factors such as emotions, cultural norms, and ethical principles.

Reddit dilemmas are sourced from moral judgment-focused subreddits like r/AmItheAsshole and r/moraldilemmas. Posts are rephrased into standardized moral dilemmas using LLaMA-3.3-70B, which also generates mutually exclusive action choices. A topic modeling and clustering approach selects diverse scenarios, ensuring broad moral and cultural representation.

## Key Features

- Multilingual Coverage: Spanning six languages: Arabic, Chinese, English, Hindi, Russian, and Spanish, to ensure cultural diversity in moral reasoning.
      - The dataset has 12 files: 1 long + 1 short for each of the 6 languages
      - The 'long' file contains all the annotations described below whereas the 'short' file contains only 1 and 6 out of the below.
- Comprehensive Annotations: Each file for 'long' annotations includes the following for each moral scenario:
      1. Action choices made by annotators
      2. Ethical principles guiding decisions (e.g., deontology, utilitarianism)
      3. Contributing factors influencing decisions (e.g., emotions, cultural values, legal considerations)
      4. Contributing emotion(s) influencing decisions
      5. Consequences of the chosen actions
      6. Annotators' moral and cultural profiles, including responses to standardized moral and cultural value questionnaires (MFQ2 and VSM 2013)

## Important Columns:

While all the columns in the files may be self evident based on their name, we explain some of them below:
- Scenario_id: Corresponding to psychological dilemmas formatted as type(long/short)_count_theory_contributingFactor
- Annotator_id: Prolific assigned annotator id
- Selected_action: The index of the action selected from Possible_actions
- Action_criteria: Likert scale rating corresponding to four ethical frameworks -- Deontology, Utilitarianism, Rights-based, and Virtuous.
- Contributing_factors: Likert scale rating corresponding to eight contributing factors -- Emotions, Moral, Culture, Responsibilities, Relationships, Legality, Politeness, and Sacred values.
- Moral_values: Annotator's aggregated moral values from MFQ2
- Cultural_values: Annotator's aggregated cultural values from VSM 2013.

## Citation
If you use the dataset in your work, please cite the following paper:
```
@misc{kumar2025rulesmeantbrokenunderstanding,
      title={Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning as a Computational Pipeline with UniMoral}, 
      author={Shivani Kumar and David Jurgens},
      year={2025},
      eprint={2502.14083},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.14083}, 
}
```",Medium,2.0
Translation,ainz/darija-alpaca,0.0,38.0,2025-03-03 20:55:25+00:00,cc-by-4.0,0.0,12 MB,12582912.0,12 MB,12582912.0,21405,none,none,"---
dataset_info:
  features:
  - name: instruction
    dtype: string
  - name: input
    dtype: string
  - name: output
    dtype: string
  splits:
  - name: train
    num_bytes: 25150486
    num_examples: 21405
  download_size: 12020511
  dataset_size: 25150486
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: cc-by-4.0
task_categories:
- table-question-answering
- text2text-generation
- translation
language:
- ar
tags:
- moroccan darija
- darija
- maroc
- arabic
- morocco
pretty_name: Alpaca dataset translated into darija
---
",Medium,2.0
Translation,YuRiVeRTi/V1Q,3.0,510.0,2025-03-11 05:39:24+00:00,apache-2.0,0.0,161 MB,168820736.0,161 MB,168820736.0,185799,none,none,"---
license: apache-2.0
task_categories:
- text-classification
- token-classification
- table-question-answering
- question-answering
- zero-shot-classification
- summarization
- translation
- feature-extraction
- text-generation
- text2text-generation
- sentence-similarity
- fill-mask
- text-to-speech
- text-to-audio
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
- image-to-text
- image-to-image
- image-to-video
- unconditional-image-generation
- video-classification
- reinforcement-learning
- tabular-classification
- robotics
- tabular-regression
- tabular-to-text
- table-to-text
- multiple-choice
- text-retrieval
- time-series-forecasting
- text-to-video
- visual-question-answering
- zero-shot-image-classification
- graph-ml
- mask-generation
- zero-shot-object-detection
- text-to-3d
- image-to-3d
- image-feature-extraction
- video-text-to-text
language:
- en
- aa
- ab
- ae
- af
- ak
- am
- an
- ar
- as
- av
- ay
- az
- ba
- be
- bg
- bh
- bi
- bm
- bn
- bo
- br
- bs
- ca
- ce
- ch
- co
- cr
- cs
- cu
- cv
- cy
- da
- de
- dv
- dz
- ee
- el
- eo
- es
- et
- eu
- fa
- ff
- fi
- fj
- fo
- fr
- fy
- ga
- gd
- gl
- gn
- gu
- gv
- ha
- he
- hi
- ho
- hr
- ht
- hu
- hy
- hz
- ia
- id
- ie
- ig
- ii
- ik
- io
- is
- it
- iu
- ja
- jv
- ka
- kg
- ki
- kj
- kk
- kl
- km
- kn
- ko
- kr
- ks
- ku
- kv
- kw
- ky
- la
- lb
- lg
- li
- ln
- lo
- lt
- lu
- lv
- mg
- mh
- mi
- mk
- ml
- mn
- mr
- ms
- 'no'
- my
- na
- nb
- nd
- ne
- mt
- ng
- nl
- nn
- nr
- nv
- ny
- oc
- oj
- om
- or
- os
- pa
- pi
- pl
- ps
- pt
- qu
- rm
- rn
- ro
- ru
- sm
- rw
- sc
- sd
- se
- sg
- si
- sk
- sl
- sn
- so
- sq
- sr
- ss
- st
- su
- sv
- sw
- ta
- te
- tg
- th
- ti
- tk
- tl
- tn
- to
- tr
- ts
- tt
- sa
- tw
- ty
- ug
- uk
- ur
- uz
- ve
- vi
- vo
- wa
- wo
- xh
- yi
- yo
- za
- zh
- zu
tags:
- code
- chemistry
- synthetic
size_categories:
- n>1T
pretty_name: VQ1
---
from datasets import load_dataset

ds = load_dataset(""b3x0m/Chinese-H-Novels"")
import sagemaker
import boto3
from sagemaker.huggingface import HuggingFaceModel

try:
	role = sagemaker.get_execution_role()
except ValueError:
	iam = boto3.client('iam')
	role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']

# Hub Model configuration. https://huggingface.co/models
hub = {
	'HF_MODEL_ID':'deepseek-ai/Janus-Pro-7B',
	'HF_TASK':'any-to-any'
}

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
	transformers_version='4.37.0',
	pytorch_version='2.1.0',
	py_version='py310',
	env=hub,
	role=role, 
)

# deploy model to SageMaker Inference
predictor = huggingface_model.deploy(
	initial_instance_count=1, # number of instances
	instance_type='ml.m5.xlarge' # ec2 instance type
)",Low,1.0
Translation,Youseff1987/multilingual_translation_gen_binarized,0.0,28.0,2025-03-06 00:03:15+00:00,mit,0.0,20.7 MB,21705523.2,20.7 MB,21705523.2,25298,none,none,"---
dataset_info:
  features:
  - name: topic
    dtype: string
  - name: origin_language
    dtype: string
  - name: target_language
    dtype: string
  - name: system
    dtype: string
  - name: input
    dtype: string
  - name: chosen_model
    dtype: string
  - name: rejected_model
    dtype: string
  - name: chosen
    dtype: string
  - name: rejected
    dtype: string
  - name: score_chosen
    dtype: int64
  - name: score_rejected
    dtype: int64
  - name: input_length
    dtype: int64
  - name: chosen_length
    dtype: int64
  - name: rejected_length
    dtype: int64
  splits:
  - name: train
    num_bytes: 48882856
    num_examples: 25298
  download_size: 20698962
  dataset_size: 48882856
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: mit
task_categories:
- translation
language:
- ko
- en
- zh
- zh
- zu
- ja
- am
- ar
- es
- fr
- ru
- de
- it
- pt
- nl
- sv
- tr
- id
- vi
- pl
- cs
- ro
- uk
- hu
- sl
- el
- fi
- 'no'
- da
- bg
- hi
- he
- ms
- ta
- te
- pa
- bn
- fa
- sw
- th
- sr
- hr
- ca
- is
- lv
- lt
- sk
- et
- mn
- la
- my
- tl
- jv
- mr
- gu
- ps
- sd
- kn
- ml
- ha
- yo
- ig
- ber
---
",Medium,2.0
Translation,Youseff1987/multilingual_translation_sft,2.0,54.0,2025-03-01 20:58:01+00:00,mit,0.0,1.83 GB,1964947537.92,1.83 GB,1964947537.92,7659557,none,none,"---
dataset_info:
- config_name: default
  features:
  - name: id
    dtype: int64
  - name: topic
    dtype: string
  - name: origin_language
    dtype: string
  - name: target_language
    dtype: string
  - name: system
    dtype: string
  - name: origin_text
    dtype: string
  - name: translated_text
    dtype: string
  - name: model_name
    dtype: string
  - name: origin_length
    dtype: int64
  - name: trans_length
    dtype: int64
  splits:
  - name: train
    num_bytes: 68709919
    num_examples: 60519
  - name: test
    num_bytes: 1496108
    num_examples: 1000
  download_size: 30194319
  dataset_size: 70206027
- config_name: flores200
  features:
  - name: id
    dtype: int64
  - name: topic
    dtype: string
  - name: origin_language
    dtype: string
  - name: target_language
    dtype: string
  - name: system
    dtype: string
  - name: origin_text
    dtype: string
  - name: translated_text
    dtype: string
  - name: model_name
    dtype: string
  - name: origin_length
    dtype: int64
  - name: trans_length
    dtype: int64
  splits:
  - name: train
    num_bytes: 3786588481
    num_examples: 7598038
  download_size: 1795337936
  dataset_size: 3786588481
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: test
    path: data/test-*
- config_name: flores200
  data_files:
  - split: train
    path: flores200/train-*
task_categories:
- translation
language:
- ko
- en
- zh
- zh
- zu
- ja
- am
- ar
- es
- fr
- ru
- de
- it
- pt
- nl
- sv
- tr
- id
- vi
- pl
- cs
- ro
- uk
- hu
- sl
- el
- fi
- 'no'
- da
- bg
- hi
- he
- ms
- ta
- te
- pa
- bn
- fa
- sw
- th
- sr
- hr
- ca
- is
- lv
- lt
- sk
- et
- mn
- la
- my
- tl
- jv
- mr
- gu
- ps
- sd
- kn
- ml
- ha
- yo
- ig
- ber
license: mit
---",Medium,2.0
Translation,lenoxaugo254/LenoxAI,1.0,4.0,2025-03-04 04:48:43+00:00,none,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
task_categories:
- text-classification
- table-question-answering
- question-answering
- zero-shot-classification
- translation
- summarization
- feature-extraction
- text-generation
- text2text-generation
- fill-mask
- sentence-similarity
- text-to-speech
- text-to-audio
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
language:
- aa
- ab
- ae
- af
- ak
- am
- an
- ar
- as
- av
- ay
- az
- ba
- be
- bg
- bh
- bi
- bm
- bn
- bo
- br
- bs
- ca
- ce
- ch
- co
- cr
- cs
- cu
- cv
- cy
- da
- de
- dv
- dz
- ee
- el
- en
- eo
- es
- et
- eu
- fa
- ff
- fi
- fj
- fo
- fr
- fy
- ga
- gd
- gl
- gn
- gu
- gv
- ha
- he
- hi
- ho
- hr
- ht
- hu
- hy
- hz
- ia
- id
- ie
- ig
- ii
- ik
- io
- is
- it
- iu
- ja
- jv
- ka
- kg
- ki
- kj
- kk
- kl
- km
- lb
- kr
- ku
- ko
- kn
- ks
- kv
- kw
- ky
- la
- lg
- li
- ln
- lo
- lt
- lu
- lv
- mg
- mh
- mi
- mk
- ml
- mn
tags:
- chemistry
- biology
- finance
- legal
- music
- art
- code
- climate
- medical
- not-for-all-audiences
- synthetic
pretty_name: Lenox Augo
size_categories:
- 1M<n<10M
---",Low,0.0
Translation,ymoslem/Human-Evaluation,1.0,31.0,2025-03-05 13:40:40+00:00,none,0.0,427 KB,437248.0,427 KB,437248.0,3200,none,"['https://aclanthology.org/2022.amta-research.2/)', 'https://aclanthology.org/2023.eamt-1.22/)', 'https://aclanthology.org/2022.amta-research.2/"",', 'https://aclanthology.org/2023.eamt-1.22/"",', 'https://aclanthology.org/2020.nlpcovid19-2.5/"",', 'https://aclanthology.org/L12-1246/"",']","---
dataset_info:
  features:
  - name: lang
    dtype: large_string
  - name: src
    dtype: large_string
  - name: ref
    dtype: large_string
  - name: tgt
    dtype: large_string
  - name: score
    dtype: int64
  - name: domain
    dtype: large_string
  - name: year
    dtype: int64
  - name: system
    dtype: large_string
  splits:
  - name: train
    num_bytes: 1954352
    num_examples: 3200
  download_size: 426721
  dataset_size: 1954352
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
language:
- ar
- en
- fr
- es
task_categories:
- translation
- text-classification
size_categories:
- 10K<n<100K
---

# Human Evaluation Dataset

The dataset includes human evaluation for General and Health domains. It was created as part of my two papers:
* [*“Domain-Specific Text Generation for Machine Translation”*](https://aclanthology.org/2022.amta-research.2/) (Moslem et al., 2022)
* [*""Adaptive Machine Translation with Large Language Models""*](https://aclanthology.org/2023.eamt-1.22/) (Moslem et al., 2023)

The evaluators were asked to assess the acceptability of each translation
using a scale ranging from 1 to 4, where 4 is ideal and 1 is unacceptable translation.


For the paper Moslem et al., 2022, the human test set contained 50 sentences for each language pair and domain, 
randomly extracted from the original test set and verified as accepted translations.
This means there are 200 unique sentences that are distributed as follows:
* EN→AR: General: 50 sentences x 8 systems
* EN→AR: Health: 50 sentences x 8 systems
* AR→EN: General: 50 sentences x 8 systems
* AR→EN: Health: 50 sentences x 8 systems
* Total: 1600 sentences

For the paper Moslem et al., 2023, the human test sets include unique 400 segments, distributed as follows:
* EN→AR: Health: 100 sentences x 4 systems
* EN→FR: Health: 100+100 sentences x 4 systems
* EN→ES: Health: 100 sentences x 4 systems
* Total: 1600 sentences

For more details, please refer to the papers or contact me.


## Citation

* If you use the **2022** portion of the Human Evaluation dataset, please cite the following paper:

```
@inproceedings{moslem-etal-2022-domain,
    title = ""Domain-Specific Text Generation for Machine Translation"",
    author = ""Moslem, Yasmin  and
      Haque, Rejwanul  and
      Kelleher, John  and
      Way, Andy"",
    booktitle = ""Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)"",
    month = sep,
    year = ""2022"",
    address = ""Orlando, USA"",
    publisher = ""Association for Machine Translation in the Americas"",
    url = ""https://aclanthology.org/2022.amta-research.2/"",
    pages = ""14--30"",
    abstract = ""Preservation of domain knowledge from the source to target is crucial in any translation workflow. It is common in the translation industry to receive highly-specialized projects, where there is hardly any parallel in-domain data. In such scenarios where there is insufficient in-domain data to fine-tune Machine Translation (MT) models, producing translations that are consistent with the relevant context is challenging. In this work, we propose leveraging state-of-the-art pretrained language models (LMs) for domain-specific data augmentation for MT, simulating the domain characteristics of either (a) a small bilingual dataset, or (b) the monolingual source text to be translated. Combining this idea with back-translation, we can generate huge amounts of synthetic bilingual in-domain data for both use cases. For our investigation, we used the state-of-the-art MT architecture, Transformer. We employed mixed fine-tuning to train models that significantly improve translation of in-domain texts. More specifically, our proposed methods achieved improvements of approximately 5-6 BLEU and 2-3 BLEU, respectively, on Arabic-to-English and English-to-Arabic language pairs. Furthermore, the outcome of human evaluation corroborates the automatic evaluation results.""
}
```

* If you use the **2023** portion of the Human Evaluation dataset, please cite the following paper:

```
@inproceedings{moslem-etal-2023-adaptive,
    title = ""Adaptive Machine Translation with Large Language Models"",
    author = ""Moslem, Yasmin  and
      Haque, Rejwanul  and
      Kelleher, John D.  and
      Way, Andy"",
    booktitle = ""Proceedings of the 24th Annual Conference of the European Association for Machine Translation"",
    month = jun,
    year = ""2023"",
    address = ""Tampere, Finland"",
    publisher = ""European Association for Machine Translation"",
    url = ""https://aclanthology.org/2023.eamt-1.22/"",
    pages = ""227--237"",
    abstract = ""Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).""
}
```

* As the segments of the *“Health”* domain are extracted from TICO-19 and the segments of the *“General”* domain are extracted from other OPUS datasets,
please also cite the following papers:

```
@inproceedings{anastasopoulos-etal-2020-tico,
    title = ""{TICO}-19: the Translation Initiative for {CO}vid-19"",
    author = {Anastasopoulos, Antonios  and
      Cattelan, Alessandro  and
      Dou, Zi-Yi  and
      Federico, Marcello  and
      Federmann, Christian  and
      Genzel, Dmitriy  and
      Guzm{\'a}n, Franscisco  and
      Hu, Junjie  and
      Hughes, Macduff  and
      Koehn, Philipp  and
      Lazar, Rosie  and
      Lewis, Will  and
      Neubig, Graham  and
      Niu, Mengmeng  and
      {\""O}ktem, Alp  and
      Paquin, Eric  and
      Tang, Grace  and
      Tur, Sylwia},
    booktitle = ""Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020"",
    month = dec,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2020.nlpcovid19-2.5/"",
    doi = ""10.18653/v1/2020.nlpcovid19-2.5"",
    abstract = ""The COVID-19 pandemic is the worst pandemic to strike the world in over a century. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating to vulnerable populations the means by which they can protect themselves. To this end, the collaborators forming the Translation Initiative for COvid-19 (TICO-19) have made test and development data available to AI and MT researchers in 35 different languages in order to foster the development of tools and resources for improving access to information about COVID-19 in these languages. In addition to 9 high-resourced, {\textquotedblright}pivot{\textquotedblright} languages, the team is targeting 26 lesser resourced languages, in particular languages of Africa, South Asia and South-East Asia, whose populations may be the most vulnerable to the spread of the virus. The same data is translated into all of the languages represented, meaning that testing or development can be done for any pairing of languages in the set. Further, the team is converting the test and development data into translation memories (TMXs) that can be used by localizers from and to any of the languages.""
}
```

```
@inproceedings{tiedemann-2012-parallel,
    title = ""Parallel Data, Tools and Interfaces in {OPUS}"",
    author = {Tiedemann, J{\""o}rg},
    booktitle = ""Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}`12)"",
    month = may,
    year = ""2012"",
    address = ""Istanbul, Turkey"",
    publisher = ""European Language Resources Association (ELRA)"",
    url = ""https://aclanthology.org/L12-1246/"",
    pages = ""2214--2218"",
    abstract = ""This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project.""
}
```
",Medium,3.0
Translation,gurgutan/sunnah_ar_en_dataset,0.0,22.0,2025-03-13 08:30:41+00:00,mit,0.0,16.7 MB,17511219.2,28.2 MB,29569843.2,50762,none,none,"---
license: mit
task_categories:
- translation
- question-answering
language:
- en
- ar
pretty_name: Hadiths 14 books collection
size_categories:
- 10K<n<100K
---

# Dataset Card for Dataset Name

# Dataset Card for Hadiths 14 Books Collection

This dataset contains a comprehensive bilingual (Arabic-English) collection of hadiths from 14 major authenticated books of Islamic tradition. It includes over 50762 narrations with complete metadata, organized by book, chapter, and narrator. Each hadith is presented in both its original Arabic text and English translation, making it ideal for cross-lingual NLP tasks, Islamic question-answering systems, and religious chatbots. The structured format allows for easy retrieval, citation, and integration into AI applications focused on Islamic knowledge.

Hadiths 14 books collection. Books included:
- Sahih al-Bukhari صحيح البخاري
- Sahih Muslim صحيح مسلم
- Sunan Abi Dawud سنن أبي داود
- Jami` at-Tirmidhi جامع الترمذي
- Sunan an-Nasa'i سنن النسائي
- Sunan Ibn Majah سنن ابن ماجه
- Muwatta Malik موطأ مالك
- Musnad Ahmad مسند أحمد
- Sunan ad-Darimi سنن الدارمي
- Riyad as-Salihin رياض الصالحين
- Shamail al-Muhammadiyah الشمائل المحمدية
- Bulugh al-Maram بلوغ المرام
- Al-Adab Al-Mufrad الأدب المفرد
- Mishkat al-Masabih مشكاة المصابيح

## Dataset Details

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->
Hadith are the transmitted narrations concerning the speech, actions, appearance, and approvals of the Messenger of Allah, the Prophet Muhammad (peace and blessings be upon him).

- **Curated by:** [Slepovichev Ivan](https://github.com/gurgutan)
- **Language(s) (NLP):** [English, Arabic]
- **License:** [MIT](https://mit-license.org/)

### Dataset Sources [optional]

- **Repository:** [hadith-database](https://github.com/zafhi/hadith-database)

## Uses

This dataset is used for research purposes.


## Dataset Structure

The dataset consists of hadith records from 14 books, with each record containing metadata and text in both English and Arabic. Each entry in the dataset follows this structure:

```json
{
  ""book_id"": ""Unique identifier for the book"",
  ""book_title_en"": ""Book title in English"",
  ""book_author_en"": ""Book author name in English"",
  ""book_title_ar"": ""Book title in Arabic"",
  ""book_author_ar"": ""Book author name in Arabic"",
  ""hadith_uid"": ""Unique identifier for the hadith"",
  ""hadith_book_id"": ""Book identifier the hadith belongs to"",
  ""hadith_chapter_id"": ""Chapter identifier within the book"",
  ""hadith_chapter_name_ar"": ""Chapter name in Arabic"",
  ""hadith_chapter_name_en"": ""Chapter name in English"",
  ""hadith_text_ar"": ""Full hadith text in Arabic"",
  ""hadith_text_en"": ""Full hadith text in English"",
  ""hadith_narrator_en"": ""Name of the narrator in English""
}
```

**Fields Description:**
- book_id: Unique identifier for each of the 14 hadith books in the collection
- book_title_en/ar: Title of the hadith book in English and Arabic
- book_author_en/ar: Author of the hadith book in English and Arabic
- hadith_uid: Unique identifier for each hadith across the entire collection
- hadith_book_id: Identifier for hadith within the book containing this hadith
- hadith_chapter_id: Identifier for the chapter within the book
- hadith_chapter_name_en/ar: Name of the chapter in English and Arabic
- hadith_text_en/ar: The full text of the hadith in English and Arabic
- hadith_narrator_en: The name of the narrator in English

The dataset contains hadiths from 14 major collections, providing a comprehensive resource for hadith studies, translation tasks, and question-answering applications in both English and Arabic.

## Dataset Creation

### Curation Rationale

This dataset was curated to provide a comprehensive, bilingual collection of authentic hadiths from 14 major books for use in AI applications, particularly:

1. Islamic question-answering systems
2. Religious chatbots and virtual assistants
3. Cross-lingual information retrieval systems
4. Educational tools for Islamic studies

The parallel English-Arabic structure makes it especially valuable for developing systems that can process and respond to religious queries in multiple languages.

### Source Data

#### Data Collection and Processing

The hadiths were collected from reliable Islamic sources and digitized with careful attention to authenticity and accuracy. The processing workflow included:

1. Extraction of hadiths from original Arabic sources
2. Verification against authenticated collections
3. Alignment with established English translations
4. Structuring into a consistent JSON format
5. Assignment of unique identifiers for cross-referencing
6. Organization by book, chapter, and narrative chain

The dataset maintains the original chapter structure of each book, preserving the contextual organization of the hadiths as intended by their original compilers.

#### Who are the source data producers?

The original hadiths were compiled by renowned Islamic scholars including Imam Bukhari, Imam Muslim, and others who meticulously collected and verified these narrations during the formative period of Islamic scholarship. The digital compilation and structuring was performed by Slepovichev Ivan, drawing from established translations and digital Islamic libraries.

### Intended Uses

This dataset is specifically designed for:

- Training AI models to answer questions about Islamic teachings based on authentic hadiths
- Developing conversational agents that can discuss Islamic topics with proper scriptural references
- Creating search and retrieval systems for Islamic knowledge
- Supporting comparative religious studies and research
- Enabling multilingual access to hadith literature

Researchers and developers can use this dataset to train models that provide evidence-based responses to religious queries, with the ability to cite specific hadiths as sources for the information provided.

## Bias, Risks, and Limitations

### Recommendations

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation

If you find our work helpful, feel free to give us a cite.
Dataset was created by Slepvochev Ivan. If you use this dataset, please cite: 

@dataset{quran_tafsir,
    title = {QuranDataset: A Dataset of Quran Verses and Tafseer},    
    url = {https://escape-team.tech/},    
    author = {Slepovichev Ivan},    
    email = {gurgutan@yandex.ru},    
    month = {March},    
    year = {2025}
}

## Dataset Card Contact

gurgutan@yandex.ru
hi@escape-team.tech",High,4.0
Translation,ymoslem/acl-6060,0.0,140.0,2025-03-14 19:14:58+00:00,cc-by-4.0,0.0,196 MB,205520896.0,196 MB,205520896.0,884,none,"['https://aclanthology.org/2023.iwslt-1.2/"",']","---
dataset_info:
  features:
  - name: index
    dtype: uint32
  - name: audio
    dtype: audio
  - name: text_en
    dtype: string
  - name: text_ar
    dtype: string
  - name: text_de
    dtype: string
  - name: text_fa
    dtype: string
  - name: text_fr
    dtype: string
  - name: text_ja
    dtype: string
  - name: text_nl
    dtype: string
  - name: text_pt
    dtype: string
  - name: text_ru
    dtype: string
  - name: text_tr
    dtype: string
  - name: text_zh
    dtype: string
  splits:
  - name: dev
    num_bytes: 100170127
    num_examples: 468
  - name: eval
    num_bytes: 98822395
    num_examples: 416
  download_size: 196166661
  dataset_size: 198992522
configs:
- config_name: default
  data_files:
  - split: dev
    path: data/dev-*
  - split: eval
    path: data/eval-*
license: cc-by-4.0
language:
- en
- ar
- de
- fa
- fr
- ja
- nl
- pt
- ru
- tr
- zh
task_categories:
- translation
- automatic-speech-recognition
size_categories:
- n<1K
---
# ACL 60/60

## Dataset details

ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages.

### Citation
```
@inproceedings{salesky-etal-2023-evaluating,
    title = ""Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology"",
    author = ""Salesky, Elizabeth  and
      Darwish, Kareem  and
      Al-Badrashiny, Mohamed  and
      Diab, Mona  and
      Niehues, Jan"",
    booktitle = ""Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)"",
    month = jul,
    year = ""2023"",
    address = ""Toronto, Canada"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.iwslt-1.2/"",
    doi = ""10.18653/v1/2023.iwslt-1.2"",
    pages = ""62--78"",
    abstract = ""We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.""
}

```",High,4.0
Translation,SukrutAI/NCERT-Parallel-Dataset-Indic,1.0,71.0,2025-03-24 07:44:49+00:00,none,0.0,36.7 MB,38482739.2,36.7 MB,38482739.2,265600,none,none,"---
task_categories:
- translation
language:
- hi
- ta
- te
- gu
- ar
- ur
- pa
- kn
- mr
- bn
- en
tags:
- chemistry
- biology
- finance
- synthetic
- english
- maths
- physics
- history
size_categories:
- 10M<n<100M
---",Low,0.0
Translation,gurgutan/sira_ar_en,0.0,11.0,2025-03-19 11:11:03+00:00,mit,0.0,6.14 MB,6438256.64,10.2 MB,10695475.2,6061,none,none,"---
license: mit
task_categories:
- translation
language:
- ar
- en
pretty_name: Summaries of Islam history events
size_categories:
- 1K<n<10K
---
# Dataset Card for Dataset Name

## Dataset Details

### Dataset Description

Dataset containts short articles about important historical events that took place in the Islam universe. 
Every record has original description on Arabic and AI translation to English. Translation was created by 

- **Curated by:** Ivan Slepovichev
- **Language(s) (NLP):** Arabic, English
- **License:** MIT

### Dataset Sources [optional]

<!-- Provide the basic links for the dataset. -->

- **Repository:** Most of data was rettrieved from dorar.net/history source.

## Uses

Dataset can be used for any research purposes.


## Dataset Structure

Contains fields:
- id  - local dataset identifier
- title - title of historical event
- hijri_date  - date in Hijra (Islamic) calendar
- gregorian_date - date in Gregorian calendar
- detailes - description of historical event on English, obtained by transletion of Arabic original text from field `detailes_ar`
- title_ar - title on Arabic
- details_ar - original description on Arabic


### Source Data

Main source of data: https://dorar.net/history

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

Data was scraped, cleaned, normalized. Fields `title`, `detailes` was created by LLM translation from Arabic to English by quantized version of model https://huggingface.co/CohereForAI/c4ai-command-r7b-arabic-02-2025.


## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

Before using please check important information like hijri_date and details.

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.


## Dataset Card Contact

gurgutan@yandex.ru
",Medium,2.0
Translation,Tamazight-NLP/Tamazight-Speech-to-Arabic-Text,4.0,245.0,2025-03-29 22:57:49+00:00,none,0.0,9 GB,9663676416.0,9 GB,9663676416.0,20344,none,none,"---
language:
- ar
- zgh
- shi
task_categories:
- translation
pretty_name: Tamazight-Arabic Speech Translation Dataset
size_categories:
- 10K<n<100K
tags:
- speech
- tamazight
- arabic
- speech-to-text
- low-resource
- north-africa
---

# Tamazight-Arabic Speech Recognition Dataset

This is the Tamazight-NLP organization-hosted version of the [Tamazight-Arabic Speech Recognition Dataset](https://huggingface.co/datasets/SoufianeDahimi/Tamazight-ASR-Dataset-v2). This dataset contains ~15.5 hours of Tamazight (Tachelhit dialect) speech paired with Arabic transcriptions, designed for automatic speech recognition (ASR) and speech-to-text translation tasks.

## Dataset Details

- **Total Examples:** 20,344 audio segments
- **Training Set:** 18,309 examples (~8.9GB)
- **Test Set:** 2,035 examples (~992MB)
- **Total Duration:** ~15.5 hours
- **Audio Format:** WAV
- **Text Format:** Arabic script

## Quick Start

```python
from datasets import load_dataset
dataset = load_dataset(""Tamazight-NLP/Tamazight-Speech-to-Arabic-Text"")
for example in dataset[""train""]:
    audio, text = example[""audio""], example[""text""]
```

## Fields

- `audio`: Path, waveform, and sampling rate
- `text`: Arabic transcription
- `metadata`: Duration, timestamps, language, source

<!-- ## License -->

## Citation

```bibtex
@dataset{tamazight_asr_2024,
  author       = {Contributors},
  title        = {Tamazight-Arabic Speech Recognition Dataset},
  year         = {2025},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/datasets/Tamazight-NLP/Tamazight-Speech-to-Arabic-Text}}
}
```

### Acknowledgments

This dataset was initially created by [Soufiane Dahimi](https://huggingface.co/SoufianeDahimi)",High,4.0
Translation,miscovery/General_Facts_in_English_Arabic_Egyptian_Arabic,12.0,66.0,2025-04-06 07:10:44+00:00,mit,1.0,16.3 MB,17091788.8,7.44 MB,7801405.44,37149,none,none,"---
license: mit
task_categories:
- question-answering
- translation
- text-generation
- fill-mask
language:
- en
- ar
pretty_name: World_Facts_in_English_Arabic_Egyptian_Arabic
size_categories:
- 10K<n<100K
---


# 🌍 World Facts in English, Arabic & Egyptian Arabic (v1.0) (Categorized)

The **World Facts General Knowledge Dataset (v1.0)** is a high-quality, human-reviewed Q&A resource by **Miscovery**. It features general facts categorized across **50+ knowledge domains**, provided in **three languages**:

- 🌍 **English**
- 🇸🇦 **Modern Standard Arabic (MSA)**
- 🇪🇬 **Egyptian Arabic (Dialect)**

Each entry includes:
- The **question** and **answer**
- A **category** and **sub-category**
- **Language tag** (en, ar, ar_eg)
- Basic metadata: question & answer **word/character counts**

---

## ✅ Main Features

- ✅ **Human-reviewed**, 99% clean and high-quality data  
- 🌐 **Multilingual support** (EN, AR, AR_EG)  
- 🗂️ **Over 50 categories** including History, Science, Technology, Literature, Politics, and more  
- 🧠 **LLM-generated** and manually refined for clarity and accuracy  
- 📚 **Ideal for NLP tasks**: translation, classification, cross-lingual QA, cultural analysis

---

## 📁 Data Format

Each record follows this structure:

question, answer, category, sub_category, language, question_char_length, answer_char_length, question_word_count, answer_word_count


### 🔍 Example Entry

| Question (AR_EG)                                   | Answer                                                                                                  | Category | Sub-Category | Language |
|----------------------------------------------------|---------------------------------------------------------------------------------------------------------|----------|--------------|----------|
| مين اللي اخترع شبكة الويب العالمية؟              | بيقولوا على العالم البريطاني تيم بيرنرز لي هو اللي اخترع شبكة الويب العالمية...                       | History  | Discoveries  | ar_eg    |

---

## 🔓 License & Citation

MIT License

---

For any suggestions or contributions, feel free to reach out to **Miscovery**.



",Medium,3.0
Translation,ademchaoua/darja-en-translation,1.0,36.0,2025-04-08 02:08:21+00:00,mit,0.0,3.25 MB,3407872.0,1.61 MB,1688207.36,30464,none,none,"---
license: mit
task_categories:
- translation
- text-classification
- summarization
- text-generation
language:
- ar
- en
tags:
- translation
- multilingual
- algerian-darja
- english
- natural-language-processing
pretty_name: Algerian Darja to English Translations
size_categories:
- 10K<n<100K
---
# Darja-English Translation Dataset

This dataset contains translations from Algerian Darja (Arabic dialect) to English. The dataset includes sentences in Darja along with their corresponding English translations.

## Languages

- Darja (Algerian Arabic dialect)
- English

## Dataset Structure

The dataset consists of two fields:

- `input`: Sentence in Darja
- `translation`: Corresponding translation in English

## License

This dataset is licensed under the MIT License. Please refer to the LICENSE file for more details.",Low,1.0
Translation,pviechn1/ParaBLoCC,0.0,696.0,2025-05-20 21:52:48+00:00,mit,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,['https://aclanthology.org/2024.eacl-long.66/]'],"---
license: mit
task_categories:
- translation
- zero-shot-classification
language:
- en
- sw
- fi
- hu
- ca
- cs
- nl
- fr
- de
- el
- it
- pl
- ru
- es
- sv
- ig
- ja
- ko
- ay
- qu
- am
- ar
- he
- ti
- zh
- tr
- uz
pretty_name: ParaBLoCC
---
# Dataset Card for Dataset Name

Parallel basic locative constructions in English and 26 target languages.

## Dataset Details

### Dataset Description

We introduce ParaBLoCC, the Parallel Basic Locative Construction Corpus, the first
multilingual compendium of this important grammatico-functional construction, and par-
ticularly the first such corpus containing semantically equivalent BLCs in source/target
language pairs. The data —taken from bitext corpora in English paired with twenty-six ty-
pologically diverse languages —are likely to prove useful for studying questions of cogni-
tive underpinnings and cross-linguistic usage patterns of spatial expressions, as well as for
improving multilingual spatial relation extraction and related tasks.



- **Curated by:** [More Information Needed]
- **Shared by [optional]:** [More Information Needed]
- **Language(s) (NLP):** en, sw, fi, hu, ca, cs, nl, fr, de, el, it, pl, ru, es, sv, ig, ja, ko, ay, qu, am, ar, he, ti, zh, tr, uz
- **License:** MIT (Derived from Opus Portal (Tiedeman et al 2022))

### Dataset Sources [optional]

- **Repository:** [https://github.com/pviechnicki/parablocc]
- **Paper [optional]:** [https://aclanthology.org/2024.eacl-long.66/]
- **Demo [optional]:** NA

## Uses

We created the ParaBLoCC data to appeal to a
wide variety of scholars interested in spatial lan-
guage, and by making them available we hope to
encourage additional study in this area. The pri-
mary utility of the data are to allow study of usage
patterns for parallel spatial expressions in twenty-
six genetically and typologically diverse languages.
Through automated alignment and span detection,
silver labels for BLCs in the target languages can
be extracted and studied themselves or used for
downstream tasks.
Likely secondary uses for the ParaBLoCC data
will be to enable work on multilingual aspects of
spatial relation extraction (Rawsthorne et al., 2023).
Until very recently, text corpora annotated for spa-
tial relation triples were limited to the most high-
resource numbers of languages, though this situa-
tion is starting to improve (Wang et al., 2023) so
the multilinguality of ParaBLoCC should be wel-
come. The data can be used to improve current
models of geospatial expression resolution (Wang
et al., 2024). Finally we expect multilingual image
caption models (Ramos et al., 2023) will benefit
from the parallel data collected by ParaBLoCC.

### Direct Use

<!-- This section describes suitable use cases for the dataset. -->

[More Information Needed]

### Out-of-Scope Use

<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->

[More Information Needed]

## Dataset Structure

<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->

[More Information Needed]

## Dataset Creation

### Curation Rationale

<!-- Motivation for the creation of this dataset. -->

[More Information Needed]

### Source Data

<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->

#### Data Collection and Processing

<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->

[More Information Needed]

#### Who are the source data producers?

<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->

[More Information Needed]

### Annotations [optional]

<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->

#### Annotation process

<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->

[More Information Needed]

#### Who are the annotators?

<!-- This section describes the people or systems who created the annotations. -->

[More Information Needed]

#### Personal and Sensitive Information

To our knowledge, the dataset contains no PII or sensitive information.

[More Information Needed]

## Bias, Risks, and Limitations

The datasets are drawn from particular domains, such as news, subtitles, parliamentary speeches, religious texts, etc, and share the same biases with those domains.


### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations.

## Citation [optional]

<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

[More Information Needed]

**APA:**

[More Information Needed]

## Glossary [optional]

<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->

[More Information Needed]

## More Information [optional]

[More Information Needed]

## Dataset Card Authors [optional]

[More Information Needed]

## Dataset Card Contact

Peter Viechnicki, pviechn1@jh.edu",High,5.0
Translation,Gregniuki/arabic_polish_quran_translation,0.0,35.0,2025-04-21 20:30:13+00:00,apache-2.0,0.0,87.8 KB,89907.2,87.8 KB,89907.2,299,none,none,"---
dataset_info:
  features:
  - name: arabic_text
    dtype: string
  - name: polish_translation
    dtype: string
  splits:
  - name: train
    num_bytes: 177522
    num_examples: 299
  download_size: 87840
  dataset_size: 177522
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: apache-2.0
task_categories:
- translation
- text2text-generation
language:
- ar
- pl
size_categories:
- n<1K
---",Medium,2.0
Translation,miscovery/arabic_egypt_english_world_facts,13.0,130.0,2025-04-26 02:18:29+00:00,mit,1.0,15.5 MB,16252928.0,7.51 MB,7874805.76,11798,none,none,"---
license: mit
task_categories:
- question-answering
- translation
- text-generation
- fill-mask
language:
- en
- ar
pretty_name: Arabic_Egypt_English_World_Facts
size_categories:
- 10K<n<100K
---


# 🌍 Version (v2.0) World Facts in English, Arabic & Egyptian Arabic (Categorized)

The **World Facts General Knowledge Dataset (v2.0)** is a high-quality, human-reviewed Q&A resource by **Miscovery**. It features general facts categorized across **50+ knowledge domains**, provided in **three languages**:

- 🌍 **English**
- 🇸🇦 **Modern Standard Arabic (MSA)**
- 🇪🇬 **Egyptian Arabic (Dialect)**

Each entry includes:
- The **question** and **answer**
- A **category** and **sub-category**
- **Language tag** (en, ar, ar_eg)
- Basic metadata: question & answer **word/character counts**

---

## ✅ Main Features

- ✅ **Human-reviewed**, 99% clean and high-quality data  
- 🌐 **Multilingual support** (EN, AR, AR_EG)  
- 🗂️ **Over 50 categories** including History, Science, Technology, Literature, Politics, and more  
- 🧠 **LLM-generated** and manually refined for clarity and accuracy  
- 📚 **Ideal for NLP tasks**: translation, classification, cross-lingual QA, cultural analysis

---

## 📁 Data Format

Each record follows this structure:

en_question, ar_question, ar_eg_question, en_answer, ar_answer, ar_eg_answer, category, sub_category, en_q_char, en_a_char, en_q_word, en_a_word, ar_q_char, ar_a_char, ar_q_word, ar_a_word, ar_eg_q_char, ar_eg_a_char, ar_eg_q_word, ar_eg_a_word


---

## 🔓 License & Citation

MIT License

---

For any suggestions or contributions, feel free to reach out to **Miscovery**.



",Medium,2.0
Translation,IbrahimAmin/egyptian-arabic-fake-reviews,2.0,87.0,2025-06-06 14:53:09+00:00,mit,0.0,75.6 MB,79272345.6,75.6 MB,79272345.6,60000,none,none,"---
license: mit
dataset_info:
  features:
  - name: user_id
    dtype: int64
  - name: product_id
    dtype: int64
  - name: original_review
    dtype: string
  - name: translated_review
    dtype: string
  - name: normalized_translated_review
    dtype: string
  - name: date
    dtype: string
  - name: rating
    dtype: int64
  - name: sentiment_label
    dtype: string
  - name: positive_normalized_score
    dtype: float64
  - name: neutral_normalized_score
    dtype: float64
  - name: negative_normalized_score
    dtype: float64
  - name: spam_hit_score
    dtype: int64
  - name: arabic_num_words
    dtype: int64
  - name: entropy1
    dtype: float64
  - name: entropy2
    dtype: float64
  - name: first_review_date
    dtype: string
  - name: last_review_date
    dtype: string
  - name: review_gap_days
    dtype: int64
  - name: review_count
    dtype: int64
  - name: product_avg_rating
    dtype: float64
  - name: rating_deviation
    dtype: float64
  - name: product_first_review_date
    dtype: string
  - name: days_since_first_review
    dtype: int64
  - name: user_tenure_days
    dtype: int64
  - name: label
    dtype: int64
  - name: label_str
    dtype: string
  splits:
  - name: train
    num_bytes: 116976968
    num_examples: 50000
  - name: test
    num_bytes: 23482905
    num_examples: 10000
  download_size: 75609484
  dataset_size: 140459873
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: test
    path: data/test-*
task_categories:
- text-classification
- translation
- text-generation
- zero-shot-classification
language:
- ar
- arz
- en
pretty_name: 'FREAD: Fake Reviews Egyptian Arabic Dataset'
size_categories:
- 10K<n<100K
---

# 🕵️‍♂️🇪🇬 FREAD: Fake Reviews Egyptian Arabic Dataset  

**Author**: [IbrahimAmin](https://huggingface.co/IbrahimAmin), [Ismail Fakhr](https://www.linkedin.com/in/ismail-fakhr-292b95278/?originalSubdomain=eg), [M. Waleed Fakhr](https://scholar.google.com/citations?user=37Rfy7sAAAAJ&hl=en), [Rasha Kashef](https://scholar.google.com/citations?user=mGFBzjIAAAAJ&hl=en) \
**License**: MIT \
**Paper**: *Arabic Fake Reviews Detection Combining Textual and Metadata Using Transformers* (under review) \
**Languages**: Arabic (Egyptian Dialect)

---

## 📚 Dataset Summary

**FREAD** is designed for detecting fake reviews in Arabic using both **textual content** and **behavioral metadata**. It contains **60,000 reviews** (50K train / 10K test) translated from the [YelpZip dataset](https://www.yelp.com/dataset) into **Egyptian Colloquial Arabic** using **OpenAI’s GPT-4o (2024-08-06)**. It is the **first large-scale, dialectal Arabic fake review dataset** that includes extensive **metadata fields** for user behavior modeling.

---

## 🧾 Dataset Structure

Each review includes:

### 🔤 Text Fields

- `original_review`: English review from Yelp.
- `translated_review`: GPT-4o translation to Egyptian Arabic.
- `normalized_translated_review`: Text normalized for diacritics, spelling, and punctuation.

### 🔍 Labels

- `label`: 0 for fake, 1 for authentic.
- `label_str`: `""fake""` or `""authentic""`.

### 📊 Metadata

- **User Metadata**: `user_id`, `review_count`, `user_tenure_days`, `entropy1`
- **Product Metadata**: `product_id`, `product_avg_rating`, `rating_deviation`, `entropy2`
- **Temporal**: `review_gap_days`, `date`, `first_review_date`, `last_review_date`, `days_since_first_review`
- **Review Stats**: `arabic_num_words`, `spam_hit_score`, `rating`
- **Sentiment Features** (from CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment):  
  - `sentiment_label`, `positive_normalized_score`, `neutral_normalized_score`, `negative_normalized_score`

---

## 🔧 Usage

```python
from datasets import load_dataset

dataset = load_dataset(""IbrahimAmin/egyptian-arabic-fake-reviews"")

# Access a sample
sample = dataset[""train""][0]
print(sample[""normalized_translated_review""])
print(sample[""label_str""])
```

---

## 🎯 Intended Uses

- Training fake review detection models in Arabic
- Studying metadata-based behavioral modeling
- Benchmarking Arabic sentiment models
- Cross-lingual or dialectal generalization

---

## 📈 Fine-tuning Benchmark Results

In our unimodal modeling approach, we concatenate the following 16 metadata features using the **[SEP]** token with our input text `normalized_translated_review`, 
forming a single input string per instance. This input is then passed to a BERT-based model:

1. user_id
1. product_id
1. rating
1. sentiment_label
1. positive_normalized_score
1. neutral_normalized_score
1. negative_normalized_score
1. spam_hit_score
1. arabic_num_words
1. entropy1
1. entropy2
1. review_gap_days
1. review_count
1. product_avg_rating
1. rating_deviation
1. days_since_first_review

Example Input: `[CLS] 205104 [SEP] 3508 [SEP] 4 [SEP] neutral [SEP] 0. 37 [SEP] 0. 57 [SEP] 0. 06 [SEP] 0 [SEP] 22 [SEP] 0. 0 [SEP] 1. 2 [SEP] 0 [SEP] 1 [SEP] 3. 58 [SEP] 0. 42 [SEP] 43 [SEP] اختيار تاني ممتاز لوجبه سريعه في وقت متاخر. اللحمه مش ناشفه ( وده مهم جدا للكباب ) ، السلاطه كتيره وطازه ، المكان نضيف والموظفين ودودين. [SEP]`

| Model                                                                                                                                      | Features           | Test-set F1-score |
| ------------------------------------------------------------------------------------------------------------------------------------------ | ------------------ | ----------------- |
| [CAMeL-Lab/bert-base-arabic-camelbert-mix](https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix) + full metadata **(unimodal)** | Text + 16 features | **86.78%**        |
| [UBC-NLP/MARBERTv2](UBC-NLP/MARBERTv2) + full metadata **(unimodal)**                                                                      | Text + 16 features | 84.14%            |
| Multiple Arabic/Multilingual Pre-trained BERTs                                                                                             | Text only          | ~69%              |
| XGBoost                                                                                                                                    | 16 features only   | 85%               |
| phi4:14b 4-bit quantized Zero-shot                                                                                                         | Prompt only        | 56.44%            |
| GPT-4o-mini Zero-shot                                                                                                                      | Prompt only        | 19.17%-41.56%     |

---

## 🧠 Translation Notes

All English reviews were translated into Egyptian Colloquial Arabic using GPT-4o (2024-08-06). To ensure natural, informal, and dialectal quality, the following setup was used:

- **System Prompt**:

```json
{""role"": ""system"", ""content"": ""You are a creative and helpful assistant who is skilled in translation.""}
```

- **User Prompt**:

```json
{""role"": ""user"", ""content"": f""Translate the following sentence into Egyptian Arabic without adding any additional explanation or English text: '{english_sentence}'""}
```

This setup explicitly forced the model to generate only **Egyptian Arabic (عامية مصرية)** without introducing Modern Standard Arabic (MSA) or English, ensuring the dataset reflects how Arabic is actually used in informal contexts such as online reviews.

---

## 📌 Citation

```bibtex
@misc{amin2025fread,
  author       = {Ibrahim Amin and Ismail Fakhr and M. Waleed Fakhr and Rasha Kashef},
  title        = {FREAD: Fake Reviews Egyptian Arabic Dataset},
  year         = {2025},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/datasets/IbrahimAmin/egyptian-arabic-fake-reviews}},
  note         = {Dataset available on Hugging Face Datasets}
}
```

---

## ⚖️ License

This dataset is distributed under the [MIT License](https://opensource.org/licenses/MIT). Use freely with attribution.",High,5.0
Translation,taha-alnasser/ArzEn-CodeMixed,0.0,14.0,2025-05-09 03:38:16+00:00,afl-3.0,0.0,2.28 MB,2390753.28,1.24 MB,1300234.24,5227,none,none,"---
license: afl-3.0
task_categories:
- translation
language:
- ar
- en
pretty_name: ArzEn-CodeMixed
size_categories:
- 1K<n<10K
---
# ArzEn-CodeMixed: English-Arabic Intra-Sentential Code-Mixed Translation Dataset

This dataset contains aligned English, Arabic, and English-Arabic as well as Arabic-English intra-sentential code-mixed sentences, created for evaluating large LLMs on translating code-mixed text. It is constructed from the ArzEn-MultiGenre parallel corpus and enhanced with carefully prompted and human-validated code-mixed variants.

## Dataset Details

- **Instances:** Each entry contains:
    - GroundTruth_English: The original English (monolingual) sentence
    - GroundTruth_Arabic: The original Arabic (monolingual) sentence
    - cat_en: Label for the English percentage in the sentence (optional)
    - cat_ar:  Label for the Arabic percentage in the sentence (optional)
    - pr_en: Proportion of English in the code-mixed sentence (float, 0.0–1.0)
    - pr_ar: Proportion of Arabic in the code-mixed sentence (float, 0.0–1.0)
    - CodeMixed_English: Code-mixed sentence with English structure
    - CodeMixed_Arabic: Code-mixed sentence with Arabic structure

- **Source:** ArzEn-MultiGenre (Hamed et al., 2020)

- **Languages:** English, Arabic, English-Arabic Code-Mixed

- **Task:** Translation, Code-mixing, Multilingual NLP

- **Content Types:** Song Lyrics, Novels, TV Show Subtitles
  
- **Contributors:** Taha Alnasser, Chiemeka Nwakama, Lily Li, Ibrahim Ismail-Adebiyi

## Citation
Please cite as:


@misc{alnasser2024arzen_codemixed,
  title={ArzEn-CodeMixed: A Human-Validated English-Arabic Intra-Sentential Code-Mixed Translation Dataset},
  author={Taha Alnasser, Chiemeka Nwakama, Lily Li, Ibrahim Ismail-Adebiyi},
  year={2024},
  url={https://huggingface.co/datasets/Taha-alnasser/arzen-codemixed}
}

## Construction

- Cleaned monolingual sentence pairs (purity, >6 words, de-duplicated)
- GPT-4.1 prompts to create intra-sentential code-mixed variants (see sample prompt below)
- Native Arabic speakers reviewed generations for naturalness
- The proportion of English and Arabic tokens was computed as the percentage of each language’s tokens relative to the total.

Sample Prompt:
""
You are a linguist specializing in Arabic-English code-switching.  
Given an Egyptian Arabic sentence and its English translation,
generate two intra-sentential code-mixed outputs that reflect 
commonly-used code-switched speech:

  1. Arabic-dominant with English insertions  
  2. English-dominant with Arabic insertions

Input:
  Arabic: الدكتور قال إن الحالة اتحسنت بعد العملية، بس لسه محتاجة متابعة.
  
  English: The doctor said the condition improved after the surgery, but it still needs monitoring.

Important:
  Only output the two sentences—no labels, prefixes, or explanations.  
  Exact format:
      <Arabic -> English code-mixed sentence>  
      <English -> Arabic code-mixed sentence>
""",Medium,3.0
Translation,Hamzah-Asadullah/TinyDS-20k,1.0,166.0,2025-06-09 09:22:53+00:00,mit,0.0,103 MB,108003328.0,46.8 MB,49073356.8,19764,none,none,"---
license: mit
language:
- en
- zh
- hi
- es
- fr
- ar
- bn
- ru
- pt
- ur
- id
- de
- ja
- mr
- te
- tr
- ta
- pa
- wuu
- ko
- vi
- ha
- jv
- arz
- it
- th
- gu
- kn
- fa
- bho
- yo
- ml
task_categories:
- question-answering
- translation
- text-generation
- text2text-generation
tags:
- reasoning
- cot
- thinking
- thoughts
- think
- qwen3
- synthetic
- multilingual
- STEM
- maths
- math
- mathematics
- literature
- chemistry
- physics
- programming
- coding
size_categories:
- 10K<n<100K
---

# TinyDS

![License](https://img.shields.io/badge/License-MIT-blue)
![Qwen3](https://img.shields.io/badge/LLM-Qwen3-red)
![Multilingual](https://img.shields.io/badge/Multilingual-green)

Alpaca-style dataset with around 20k samples scraped from [Qwen3-8B](https://huggingface.co/unsloth/Qwen3-8B-GGUF/blob/main/Qwen3-8B-Q4_K_M.gguf) using [SyntheticAlpaca](https://github.com/Hamzah-Asadullah/SyntheticAlpaca). Q&A pairs can be in 32 different languages, these are listed in the metadata.  
Topics are all around STEM, programming, and literature.  

MIT @ 2025 Hamzah Asadullah

---

[![View Counter](https://count.getloli.com/@Hamzah-Asadullah_TinyDS_20k?theme=booru-lewd)](https://hf.co/datasets/Hamzah-Asadullah/TinyDS-20k)",Low,1.0
Translation,Groovy-123/deep-think,1.0,58.0,2025-05-22 06:06:36+00:00,other,1.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
license: other
license_name: nexgen
license_link: LICENSE
task_categories:
- text-classification
- token-classification
- table-question-answering
- question-answering
- zero-shot-classification
- translation
- summarization
- feature-extraction
- text-generation
- text2text-generation
- fill-mask
- sentence-similarity
- text-to-speech
- text-to-audio
- automatic-speech-recognition
- audio-to-audio
- audio-classification
- voice-activity-detection
- depth-estimation
- image-classification
- object-detection
- image-segmentation
- text-to-image
- image-to-text
- image-to-image
- image-to-video
- unconditional-image-generation
- video-classification
- reinforcement-learning
- robotics
- tabular-classification
- tabular-regression
- tabular-to-text
- table-to-text
- multiple-choice
- text-ranking
- text-retrieval
- time-series-forecasting
- text-to-video
- visual-question-answering
- zero-shot-image-classification
- graph-ml
- mask-generation
- zero-shot-object-detection
- image-to-3d
- text-to-3d
- image-feature-extraction
- video-text-to-text
language:
- ak
- en
- ar
- ha
- ee
size_categories:
- n>1T
---",Low,1.0
Translation,liboaccn/nmt-parallel-corpus,1.0,104.0,2025-06-01 14:50:19+00:00,cc-by-nc-4.0,0.0,UNKNOWN,unknown,890 GB,unknown,13674839842,https://arxiv.org/abs/2505.14256,none,"---
license: cc-by-nc-4.0
task_categories:
- translation
configs:
- config_name: am-ar
  data_files: ""am-ar.parquet""
- config_name: am-az
  data_files: ""am-az.parquet""
- config_name: am-be
  data_files: ""am-be.parquet""
- config_name: am-bg
  data_files: ""am-bg.parquet""
- config_name: am-bn
  data_files: ""am-bn.parquet""
- config_name: am-bo
  data_files: ""am-bo.parquet""
- config_name: am-bs
  data_files: ""am-bs.parquet""
- config_name: am-cs
  data_files: ""am-cs.parquet""
- config_name: am-de
  data_files: ""am-de.parquet""
- config_name: am-el
  data_files: ""am-el.parquet""
- config_name: am-en
  data_files: ""am-en.parquet""
- config_name: am-es
  data_files: ""am-es.parquet""
- config_name: am-et
  data_files: ""am-et.parquet""
- config_name: am-fa
  data_files: ""am-fa.parquet""
- config_name: am-fr
  data_files: ""am-fr.parquet""
- config_name: am-ha
  data_files: ""am-ha.parquet""
- config_name: am-hi
  data_files: ""am-hi.parquet""
- config_name: am-hr
  data_files: ""am-hr.parquet""
- config_name: am-hu
  data_files: ""am-hu.parquet""
- config_name: am-hy
  data_files: ""am-hy.parquet""
- config_name: am-id
  data_files: ""am-id.parquet""
- config_name: am-it
  data_files: ""am-it.parquet""
- config_name: am-ja
  data_files: ""am-ja.parquet""
- config_name: am-ka
  data_files: ""am-ka.parquet""
- config_name: am-kk
  data_files: ""am-kk.parquet""
- config_name: am-km
  data_files: ""am-km.parquet""
- config_name: am-ko
  data_files: ""am-ko.parquet""
- config_name: am-ky
  data_files: ""am-ky.parquet""
- config_name: am-lo
  data_files: ""am-lo.parquet""
- config_name: am-lt
  data_files: ""am-lt.parquet""
- config_name: am-lv
  data_files: ""am-lv.parquet""
- config_name: am-mg
  data_files: ""am-mg.parquet""
- config_name: am-mi
  data_files: ""am-mi.parquet""
- config_name: am-mk
  data_files: ""am-mk.parquet""
- config_name: am-mn
  data_files: ""am-mn.parquet""
- config_name: am-ms
  data_files: ""am-ms.parquet""
- config_name: am-my
  data_files: ""am-my.parquet""
- config_name: am-ne
  data_files: ""am-ne.parquet""
- config_name: am-pl
  data_files: ""am-pl.parquet""
- config_name: am-ps
  data_files: ""am-ps.parquet""
- config_name: am-pt
  data_files: ""am-pt.parquet""
- config_name: am-ro
  data_files: ""am-ro.parquet""
- config_name: am-ru
  data_files: ""am-ru.parquet""
- config_name: am-rw
  data_files: ""am-rw.parquet""
- config_name: am-si
  data_files: ""am-si.parquet""
- config_name: am-sk
  data_files: ""am-sk.parquet""
- config_name: am-sl
  data_files: ""am-sl.parquet""
- config_name: am-so
  data_files: ""am-so.parquet""
- config_name: am-sq
  data_files: ""am-sq.parquet""
- config_name: am-sr
  data_files: ""am-sr.parquet""
- config_name: am-sw
  data_files: ""am-sw.parquet""
- config_name: am-ta
  data_files: ""am-ta.parquet""
- config_name: am-th
  data_files: ""am-th.parquet""
- config_name: am-ti
  data_files: ""am-ti.parquet""
- config_name: am-tk
  data_files: ""am-tk.parquet""
- config_name: am-tr
  data_files: ""am-tr.parquet""
- config_name: am-ug
  data_files: ""am-ug.parquet""
- config_name: am-uk
  data_files: ""am-uk.parquet""
- config_name: am-ur
  data_files: ""am-ur.parquet""
- config_name: am-vi
  data_files: ""am-vi.parquet""
- config_name: am-zh
  data_files: ""am-zh.parquet""
- config_name: ar-az
  data_files: ""ar-az.parquet""
- config_name: ar-be
  data_files: ""ar-be.parquet""
- config_name: ar-bg
  data_files: ""ar-bg.parquet""
- config_name: ar-bn
  data_files: ""ar-bn.parquet""
- config_name: ar-bo
  data_files: ""ar-bo.parquet""
- config_name: ar-bs
  data_files: ""ar-bs.parquet""
- config_name: ar-cs
  data_files: ""ar-cs.parquet""
- config_name: ar-de
  data_files: ""ar-de.parquet""
- config_name: ar-el
  data_files: ""ar-el.parquet""
- config_name: ar-en
  data_files: ""ar-en.parquet""
- config_name: ar-es
  data_files: ""ar-es.parquet""
- config_name: ar-et
  data_files: ""ar-et.parquet""
- config_name: ar-fa
  data_files: ""ar-fa.parquet""
- config_name: ar-fr
  data_files: ""ar-fr.parquet""
- config_name: ar-ha
  data_files: ""ar-ha.parquet""
- config_name: ar-hi
  data_files: ""ar-hi.parquet""
- config_name: ar-hr
  data_files: ""ar-hr.parquet""
- config_name: ar-hu
  data_files: ""ar-hu.parquet""
- config_name: ar-hy
  data_files: ""ar-hy.parquet""
- config_name: ar-id
  data_files: ""ar-id.parquet""
- config_name: ar-it
  data_files: ""ar-it.parquet""
- config_name: ar-ja
  data_files: ""ar-ja.parquet""
- config_name: ar-ka
  data_files: ""ar-ka.parquet""
- config_name: ar-kk
  data_files: ""ar-kk.parquet""
- config_name: ar-km
  data_files: ""ar-km.parquet""
- config_name: ar-ko
  data_files: ""ar-ko.parquet""
- config_name: ar-ky
  data_files: ""ar-ky.parquet""
- config_name: ar-lo
  data_files: ""ar-lo.parquet""
- config_name: ar-lt
  data_files: ""ar-lt.parquet""
- config_name: ar-lv
  data_files: ""ar-lv.parquet""
- config_name: ar-mg
  data_files: ""ar-mg.parquet""
- config_name: ar-mi
  data_files: ""ar-mi.parquet""
- config_name: ar-mk
  data_files: ""ar-mk.parquet""
- config_name: ar-mn
  data_files: ""ar-mn.parquet""
- config_name: ar-ms
  data_files: ""ar-ms.parquet""
- config_name: ar-my
  data_files: ""ar-my.parquet""
- config_name: ar-ne
  data_files: ""ar-ne.parquet""
- config_name: ar-pl
  data_files: ""ar-pl.parquet""
- config_name: ar-prs
  data_files: ""ar-prs.parquet""
- config_name: ar-ps
  data_files: ""ar-ps.parquet""
- config_name: ar-pt
  data_files: ""ar-pt.parquet""
- config_name: ar-ro
  data_files: ""ar-ro.parquet""
- config_name: ar-ru
  data_files: ""ar-ru.parquet""
- config_name: ar-rw
  data_files: ""ar-rw.parquet""
- config_name: ar-si
  data_files: ""ar-si.parquet""
- config_name: ar-sk
  data_files: ""ar-sk.parquet""
- config_name: ar-sl
  data_files: ""ar-sl.parquet""
- config_name: ar-so
  data_files: ""ar-so.parquet""
- config_name: ar-sq
  data_files: ""ar-sq.parquet""
- config_name: ar-sr
  data_files: ""ar-sr.parquet""
- config_name: ar-sw
  data_files: ""ar-sw.parquet""
- config_name: ar-ta
  data_files: ""ar-ta.parquet""
- config_name: ar-th
  data_files: ""ar-th.parquet""
- config_name: ar-ti
  data_files: ""ar-ti.parquet""
- config_name: ar-tk
  data_files: ""ar-tk.parquet""
- config_name: ar-tr
  data_files: ""ar-tr.parquet""
- config_name: ar-ug
  data_files: ""ar-ug.parquet""
- config_name: ar-uk
  data_files: ""ar-uk.parquet""
- config_name: ar-ur
  data_files: ""ar-ur.parquet""
- config_name: ar-vi
  data_files: ""ar-vi.parquet""
- config_name: ar-zh
  data_files: ""ar-zh.parquet""
- config_name: az-be
  data_files: ""az-be.parquet""
- config_name: az-bg
  data_files: ""az-bg.parquet""
- config_name: az-bn
  data_files: ""az-bn.parquet""
- config_name: az-bo
  data_files: ""az-bo.parquet""
- config_name: az-bs
  data_files: ""az-bs.parquet""
- config_name: az-cs
  data_files: ""az-cs.parquet""
- config_name: az-de
  data_files: ""az-de.parquet""
- config_name: az-el
  data_files: ""az-el.parquet""
- config_name: az-en
  data_files: ""az-en.parquet""
- config_name: az-es
  data_files: ""az-es.parquet""
- config_name: az-et
  data_files: ""az-et.parquet""
- config_name: az-fa
  data_files: ""az-fa.parquet""
- config_name: az-fr
  data_files: ""az-fr.parquet""
- config_name: az-ha
  data_files: ""az-ha.parquet""
- config_name: az-hi
  data_files: ""az-hi.parquet""
- config_name: az-hr
  data_files: ""az-hr.parquet""
- config_name: az-hu
  data_files: ""az-hu.parquet""
- config_name: az-hy
  data_files: ""az-hy.parquet""
- config_name: az-id
  data_files: ""az-id.parquet""
- config_name: az-it
  data_files: ""az-it.parquet""
- config_name: az-ja
  data_files: ""az-ja.parquet""
- config_name: az-ka
  data_files: ""az-ka.parquet""
- config_name: az-kk
  data_files: ""az-kk.parquet""
- config_name: az-km
  data_files: ""az-km.parquet""
- config_name: az-ko
  data_files: ""az-ko.parquet""
- config_name: az-ky
  data_files: ""az-ky.parquet""
- config_name: az-lo
  data_files: ""az-lo.parquet""
- config_name: az-lt
  data_files: ""az-lt.parquet""
- config_name: az-lv
  data_files: ""az-lv.parquet""
- config_name: az-mg
  data_files: ""az-mg.parquet""
- config_name: az-mi
  data_files: ""az-mi.parquet""
- config_name: az-mk
  data_files: ""az-mk.parquet""
- config_name: az-mn
  data_files: ""az-mn.parquet""
- config_name: az-ms
  data_files: ""az-ms.parquet""
- config_name: az-my
  data_files: ""az-my.parquet""
- config_name: az-ne
  data_files: ""az-ne.parquet""
- config_name: az-pl
  data_files: ""az-pl.parquet""
- config_name: az-prs
  data_files: ""az-prs.parquet""
- config_name: az-ps
  data_files: ""az-ps.parquet""
- config_name: az-pt
  data_files: ""az-pt.parquet""
- config_name: az-ro
  data_files: ""az-ro.parquet""
- config_name: az-ru
  data_files: ""az-ru.parquet""
- config_name: az-rw
  data_files: ""az-rw.parquet""
- config_name: az-si
  data_files: ""az-si.parquet""
- config_name: az-sk
  data_files: ""az-sk.parquet""
- config_name: az-sl
  data_files: ""az-sl.parquet""
- config_name: az-so
  data_files: ""az-so.parquet""
- config_name: az-sq
  data_files: ""az-sq.parquet""
- config_name: az-sr
  data_files: ""az-sr.parquet""
- config_name: az-sw
  data_files: ""az-sw.parquet""
- config_name: az-ta
  data_files: ""az-ta.parquet""
- config_name: az-th
  data_files: ""az-th.parquet""
- config_name: az-ti
  data_files: ""az-ti.parquet""
- config_name: az-tk
  data_files: ""az-tk.parquet""
- config_name: az-tr
  data_files: ""az-tr.parquet""
- config_name: az-ug
  data_files: ""az-ug.parquet""
- config_name: az-uk
  data_files: ""az-uk.parquet""
- config_name: az-ur
  data_files: ""az-ur.parquet""
- config_name: az-vi
  data_files: ""az-vi.parquet""
- config_name: az-zh
  data_files: ""az-zh.parquet""
- config_name: be-bg
  data_files: ""be-bg.parquet""
- config_name: be-bn
  data_files: ""be-bn.parquet""
- config_name: be-bo
  data_files: ""be-bo.parquet""
- config_name: be-bs
  data_files: ""be-bs.parquet""
- config_name: be-cs
  data_files: ""be-cs.parquet""
- config_name: be-de
  data_files: ""be-de.parquet""
- config_name: be-el
  data_files: ""be-el.parquet""
- config_name: be-en
  data_files: ""be-en.parquet""
- config_name: be-es
  data_files: ""be-es.parquet""
- config_name: be-et
  data_files: ""be-et.parquet""
- config_name: be-fa
  data_files: ""be-fa.parquet""
- config_name: be-fr
  data_files: ""be-fr.parquet""
- config_name: be-ha
  data_files: ""be-ha.parquet""
- config_name: be-hi
  data_files: ""be-hi.parquet""
- config_name: be-hr
  data_files: ""be-hr.parquet""
- config_name: be-hu
  data_files: ""be-hu.parquet""
- config_name: be-hy
  data_files: ""be-hy.parquet""
- config_name: be-id
  data_files: ""be-id.parquet""
- config_name: be-it
  data_files: ""be-it.parquet""
- config_name: be-ja
  data_files: ""be-ja.parquet""
- config_name: be-ka
  data_files: ""be-ka.parquet""
- config_name: be-kk
  data_files: ""be-kk.parquet""
- config_name: be-km
  data_files: ""be-km.parquet""
- config_name: be-ko
  data_files: ""be-ko.parquet""
- config_name: be-ky
  data_files: ""be-ky.parquet""
- config_name: be-lo
  data_files: ""be-lo.parquet""
- config_name: be-lt
  data_files: ""be-lt.parquet""
- config_name: be-lv
  data_files: ""be-lv.parquet""
- config_name: be-mg
  data_files: ""be-mg.parquet""
- config_name: be-mi
  data_files: ""be-mi.parquet""
- config_name: be-mk
  data_files: ""be-mk.parquet""
- config_name: be-mn
  data_files: ""be-mn.parquet""
- config_name: be-ms
  data_files: ""be-ms.parquet""
- config_name: be-my
  data_files: ""be-my.parquet""
- config_name: be-ne
  data_files: ""be-ne.parquet""
- config_name: be-pl
  data_files: ""be-pl.parquet""
- config_name: be-ps
  data_files: ""be-ps.parquet""
- config_name: be-pt
  data_files: ""be-pt.parquet""
- config_name: be-ro
  data_files: ""be-ro.parquet""
- config_name: be-ru
  data_files: ""be-ru.parquet""
- config_name: be-rw
  data_files: ""be-rw.parquet""
- config_name: be-si
  data_files: ""be-si.parquet""
- config_name: be-sk
  data_files: ""be-sk.parquet""
- config_name: be-sl
  data_files: ""be-sl.parquet""
- config_name: be-so
  data_files: ""be-so.parquet""
- config_name: be-sq
  data_files: ""be-sq.parquet""
- config_name: be-sr
  data_files: ""be-sr.parquet""
- config_name: be-sw
  data_files: ""be-sw.parquet""
- config_name: be-ta
  data_files: ""be-ta.parquet""
- config_name: be-th
  data_files: ""be-th.parquet""
- config_name: be-ti
  data_files: ""be-ti.parquet""
- config_name: be-tk
  data_files: ""be-tk.parquet""
- config_name: be-tr
  data_files: ""be-tr.parquet""
- config_name: be-ug
  data_files: ""be-ug.parquet""
- config_name: be-uk
  data_files: ""be-uk.parquet""
- config_name: be-ur
  data_files: ""be-ur.parquet""
- config_name: be-vi
  data_files: ""be-vi.parquet""
- config_name: be-zh
  data_files: ""be-zh.parquet""
- config_name: bg-bn
  data_files: ""bg-bn.parquet""
- config_name: bg-bo
  data_files: ""bg-bo.parquet""
- config_name: bg-bs
  data_files: ""bg-bs.parquet""
- config_name: bg-cs
  data_files: ""bg-cs.parquet""
- config_name: bg-de
  data_files: ""bg-de.parquet""
- config_name: bg-el
  data_files: ""bg-el.parquet""
- config_name: bg-en
  data_files: ""bg-en.parquet""
- config_name: bg-es
  data_files: ""bg-es.parquet""
- config_name: bg-et
  data_files: ""bg-et.parquet""
- config_name: bg-fa
  data_files: ""bg-fa.parquet""
- config_name: bg-fr
  data_files: ""bg-fr.parquet""
- config_name: bg-ha
  data_files: ""bg-ha.parquet""
- config_name: bg-hi
  data_files: ""bg-hi.parquet""
- config_name: bg-hr
  data_files: ""bg-hr.parquet""
- config_name: bg-hu
  data_files: ""bg-hu.parquet""
- config_name: bg-hy
  data_files: ""bg-hy.parquet""
- config_name: bg-id
  data_files: ""bg-id.parquet""
- config_name: bg-it
  data_files: ""bg-it.parquet""
- config_name: bg-ja
  data_files: ""bg-ja.parquet""
- config_name: bg-ka
  data_files: ""bg-ka.parquet""
- config_name: bg-kk
  data_files: ""bg-kk.parquet""
- config_name: bg-km
  data_files: ""bg-km.parquet""
- config_name: bg-ko
  data_files: ""bg-ko.parquet""
- config_name: bg-ky
  data_files: ""bg-ky.parquet""
- config_name: bg-lo
  data_files: ""bg-lo.parquet""
- config_name: bg-lt
  data_files: ""bg-lt.parquet""
- config_name: bg-lv
  data_files: ""bg-lv.parquet""
- config_name: bg-mg
  data_files: ""bg-mg.parquet""
- config_name: bg-mi
  data_files: ""bg-mi.parquet""
- config_name: bg-mk
  data_files: ""bg-mk.parquet""
- config_name: bg-mn
  data_files: ""bg-mn.parquet""
- config_name: bg-ms
  data_files: ""bg-ms.parquet""
- config_name: bg-my
  data_files: ""bg-my.parquet""
- config_name: bg-ne
  data_files: ""bg-ne.parquet""
- config_name: bg-pl
  data_files: ""bg-pl.parquet""
- config_name: bg-prs
  data_files: ""bg-prs.parquet""
- config_name: bg-ps
  data_files: ""bg-ps.parquet""
- config_name: bg-pt
  data_files: ""bg-pt.parquet""
- config_name: bg-ro
  data_files: ""bg-ro.parquet""
- config_name: bg-ru
  data_files: ""bg-ru.parquet""
- config_name: bg-rw
  data_files: ""bg-rw.parquet""
- config_name: bg-si
  data_files: ""bg-si.parquet""
- config_name: bg-sk
  data_files: ""bg-sk.parquet""
- config_name: bg-sl
  data_files: ""bg-sl.parquet""
- config_name: bg-so
  data_files: ""bg-so.parquet""
- config_name: bg-sq
  data_files: ""bg-sq.parquet""
- config_name: bg-sr
  data_files: ""bg-sr.parquet""
- config_name: bg-sw
  data_files: ""bg-sw.parquet""
- config_name: bg-ta
  data_files: ""bg-ta.parquet""
- config_name: bg-th
  data_files: ""bg-th.parquet""
- config_name: bg-ti
  data_files: ""bg-ti.parquet""
- config_name: bg-tk
  data_files: ""bg-tk.parquet""
- config_name: bg-tr
  data_files: ""bg-tr.parquet""
- config_name: bg-ug
  data_files: ""bg-ug.parquet""
- config_name: bg-uk
  data_files: ""bg-uk.parquet""
- config_name: bg-ur
  data_files: ""bg-ur.parquet""
- config_name: bg-vi
  data_files: ""bg-vi.parquet""
- config_name: bg-zh
  data_files: ""bg-zh.parquet""
- config_name: bn-bo
  data_files: ""bn-bo.parquet""
- config_name: bn-bs
  data_files: ""bn-bs.parquet""
- config_name: bn-cs
  data_files: ""bn-cs.parquet""
- config_name: bn-de
  data_files: ""bn-de.parquet""
- config_name: bn-el
  data_files: ""bn-el.parquet""
- config_name: bn-en
  data_files: ""bn-en.parquet""
- config_name: bn-es
  data_files: ""bn-es.parquet""
- config_name: bn-et
  data_files: ""bn-et.parquet""
- config_name: bn-fa
  data_files: ""bn-fa.parquet""
- config_name: bn-fr
  data_files: ""bn-fr.parquet""
- config_name: bn-ha
  data_files: ""bn-ha.parquet""
- config_name: bn-hi
  data_files: ""bn-hi.parquet""
- config_name: bn-hr
  data_files: ""bn-hr.parquet""
- config_name: bn-hu
  data_files: ""bn-hu.parquet""
- config_name: bn-hy
  data_files: ""bn-hy.parquet""
- config_name: bn-id
  data_files: ""bn-id.parquet""
- config_name: bn-it
  data_files: ""bn-it.parquet""
- config_name: bn-ja
  data_files: ""bn-ja.parquet""
- config_name: bn-ka
  data_files: ""bn-ka.parquet""
- config_name: bn-kk
  data_files: ""bn-kk.parquet""
- config_name: bn-km
  data_files: ""bn-km.parquet""
- config_name: bn-ko
  data_files: ""bn-ko.parquet""
- config_name: bn-ky
  data_files: ""bn-ky.parquet""
- config_name: bn-lo
  data_files: ""bn-lo.parquet""
- config_name: bn-lt
  data_files: ""bn-lt.parquet""
- config_name: bn-lv
  data_files: ""bn-lv.parquet""
- config_name: bn-mg
  data_files: ""bn-mg.parquet""
- config_name: bn-mi
  data_files: ""bn-mi.parquet""
- config_name: bn-mk
  data_files: ""bn-mk.parquet""
- config_name: bn-mn
  data_files: ""bn-mn.parquet""
- config_name: bn-ms
  data_files: ""bn-ms.parquet""
- config_name: bn-my
  data_files: ""bn-my.parquet""
- config_name: bn-ne
  data_files: ""bn-ne.parquet""
- config_name: bn-pl
  data_files: ""bn-pl.parquet""
- config_name: bn-prs
  data_files: ""bn-prs.parquet""
- config_name: bn-ps
  data_files: ""bn-ps.parquet""
- config_name: bn-pt
  data_files: ""bn-pt.parquet""
- config_name: bn-ro
  data_files: ""bn-ro.parquet""
- config_name: bn-ru
  data_files: ""bn-ru.parquet""
- config_name: bn-rw
  data_files: ""bn-rw.parquet""
- config_name: bn-si
  data_files: ""bn-si.parquet""
- config_name: bn-sk
  data_files: ""bn-sk.parquet""
- config_name: bn-sl
  data_files: ""bn-sl.parquet""
- config_name: bn-so
  data_files: ""bn-so.parquet""
- config_name: bn-sq
  data_files: ""bn-sq.parquet""
- config_name: bn-sr
  data_files: ""bn-sr.parquet""
- config_name: bn-sw
  data_files: ""bn-sw.parquet""
- config_name: bn-ta
  data_files: ""bn-ta.parquet""
- config_name: bn-th
  data_files: ""bn-th.parquet""
- config_name: bn-ti
  data_files: ""bn-ti.parquet""
- config_name: bn-tk
  data_files: ""bn-tk.parquet""
- config_name: bn-tr
  data_files: ""bn-tr.parquet""
- config_name: bn-ug
  data_files: ""bn-ug.parquet""
- config_name: bn-uk
  data_files: ""bn-uk.parquet""
- config_name: bn-ur
  data_files: ""bn-ur.parquet""
- config_name: bn-vi
  data_files: ""bn-vi.parquet""
- config_name: bn-zh
  data_files: ""bn-zh.parquet""
- config_name: bo-bs
  data_files: ""bo-bs.parquet""
- config_name: bo-cs
  data_files: ""bo-cs.parquet""
- config_name: bo-de
  data_files: ""bo-de.parquet""
- config_name: bo-el
  data_files: ""bo-el.parquet""
- config_name: bo-en
  data_files: ""bo-en.parquet""
- config_name: bo-es
  data_files: ""bo-es.parquet""
- config_name: bo-et
  data_files: ""bo-et.parquet""
- config_name: bo-fa
  data_files: ""bo-fa.parquet""
- config_name: bo-fr
  data_files: ""bo-fr.parquet""
- config_name: bo-ha
  data_files: ""bo-ha.parquet""
- config_name: bo-hi
  data_files: ""bo-hi.parquet""
- config_name: bo-hr
  data_files: ""bo-hr.parquet""
- config_name: bo-hu
  data_files: ""bo-hu.parquet""
- config_name: bo-hy
  data_files: ""bo-hy.parquet""
- config_name: bo-id
  data_files: ""bo-id.parquet""
- config_name: bo-it
  data_files: ""bo-it.parquet""
- config_name: bo-ja
  data_files: ""bo-ja.parquet""
- config_name: bo-ka
  data_files: ""bo-ka.parquet""
- config_name: bo-kk
  data_files: ""bo-kk.parquet""
- config_name: bo-km
  data_files: ""bo-km.parquet""
- config_name: bo-ko
  data_files: ""bo-ko.parquet""
- config_name: bo-ky
  data_files: ""bo-ky.parquet""
- config_name: bo-lo
  data_files: ""bo-lo.parquet""
- config_name: bo-lt
  data_files: ""bo-lt.parquet""
- config_name: bo-lv
  data_files: ""bo-lv.parquet""
- config_name: bo-mg
  data_files: ""bo-mg.parquet""
- config_name: bo-mi
  data_files: ""bo-mi.parquet""
- config_name: bo-mk
  data_files: ""bo-mk.parquet""
- config_name: bo-mn
  data_files: ""bo-mn.parquet""
- config_name: bo-ms
  data_files: ""bo-ms.parquet""
- config_name: bo-my
  data_files: ""bo-my.parquet""
- config_name: bo-ne
  data_files: ""bo-ne.parquet""
- config_name: bo-pl
  data_files: ""bo-pl.parquet""
- config_name: bo-prs
  data_files: ""bo-prs.parquet""
- config_name: bo-ps
  data_files: ""bo-ps.parquet""
- config_name: bo-pt
  data_files: ""bo-pt.parquet""
- config_name: bo-ro
  data_files: ""bo-ro.parquet""
- config_name: bo-ru
  data_files: ""bo-ru.parquet""
- config_name: bo-rw
  data_files: ""bo-rw.parquet""
- config_name: bo-si
  data_files: ""bo-si.parquet""
- config_name: bo-sk
  data_files: ""bo-sk.parquet""
- config_name: bo-sl
  data_files: ""bo-sl.parquet""
- config_name: bo-so
  data_files: ""bo-so.parquet""
- config_name: bo-sq
  data_files: ""bo-sq.parquet""
- config_name: bo-sr
  data_files: ""bo-sr.parquet""
- config_name: bo-sw
  data_files: ""bo-sw.parquet""
- config_name: bo-ta
  data_files: ""bo-ta.parquet""
- config_name: bo-th
  data_files: ""bo-th.parquet""
- config_name: bo-ti
  data_files: ""bo-ti.parquet""
- config_name: bo-tk
  data_files: ""bo-tk.parquet""
- config_name: bo-tr
  data_files: ""bo-tr.parquet""
- config_name: bo-ug
  data_files: ""bo-ug.parquet""
- config_name: bo-uk
  data_files: ""bo-uk.parquet""
- config_name: bo-ur
  data_files: ""bo-ur.parquet""
- config_name: bo-vi
  data_files: ""bo-vi.parquet""
- config_name: bo-zh
  data_files: ""bo-zh.parquet""
- config_name: bs-cs
  data_files: ""bs-cs.parquet""
- config_name: bs-de
  data_files: ""bs-de.parquet""
- config_name: bs-el
  data_files: ""bs-el.parquet""
- config_name: bs-en
  data_files: ""bs-en.parquet""
- config_name: bs-es
  data_files: ""bs-es.parquet""
- config_name: bs-et
  data_files: ""bs-et.parquet""
- config_name: bs-fa
  data_files: ""bs-fa.parquet""
- config_name: bs-fr
  data_files: ""bs-fr.parquet""
- config_name: bs-ha
  data_files: ""bs-ha.parquet""
- config_name: bs-hi
  data_files: ""bs-hi.parquet""
- config_name: bs-hr
  data_files: ""bs-hr.parquet""
- config_name: bs-hu
  data_files: ""bs-hu.parquet""
- config_name: bs-hy
  data_files: ""bs-hy.parquet""
- config_name: bs-id
  data_files: ""bs-id.parquet""
- config_name: bs-it
  data_files: ""bs-it.parquet""
- config_name: bs-ja
  data_files: ""bs-ja.parquet""
- config_name: bs-ka
  data_files: ""bs-ka.parquet""
- config_name: bs-kk
  data_files: ""bs-kk.parquet""
- config_name: bs-km
  data_files: ""bs-km.parquet""
- config_name: bs-ko
  data_files: ""bs-ko.parquet""
- config_name: bs-ky
  data_files: ""bs-ky.parquet""
- config_name: bs-lo
  data_files: ""bs-lo.parquet""
- config_name: bs-lt
  data_files: ""bs-lt.parquet""
- config_name: bs-lv
  data_files: ""bs-lv.parquet""
- config_name: bs-mg
  data_files: ""bs-mg.parquet""
- config_name: bs-mi
  data_files: ""bs-mi.parquet""
- config_name: bs-mk
  data_files: ""bs-mk.parquet""
- config_name: bs-mn
  data_files: ""bs-mn.parquet""
- config_name: bs-ms
  data_files: ""bs-ms.parquet""
- config_name: bs-my
  data_files: ""bs-my.parquet""
- config_name: bs-ne
  data_files: ""bs-ne.parquet""
- config_name: bs-pl
  data_files: ""bs-pl.parquet""
- config_name: bs-prs
  data_files: ""bs-prs.parquet""
- config_name: bs-ps
  data_files: ""bs-ps.parquet""
- config_name: bs-pt
  data_files: ""bs-pt.parquet""
- config_name: bs-ro
  data_files: ""bs-ro.parquet""
- config_name: bs-ru
  data_files: ""bs-ru.parquet""
- config_name: bs-rw
  data_files: ""bs-rw.parquet""
- config_name: bs-si
  data_files: ""bs-si.parquet""
- config_name: bs-sk
  data_files: ""bs-sk.parquet""
- config_name: bs-sl
  data_files: ""bs-sl.parquet""
- config_name: bs-so
  data_files: ""bs-so.parquet""
- config_name: bs-sq
  data_files: ""bs-sq.parquet""
- config_name: bs-sr
  data_files: ""bs-sr.parquet""
- config_name: bs-sw
  data_files: ""bs-sw.parquet""
- config_name: bs-ta
  data_files: ""bs-ta.parquet""
- config_name: bs-th
  data_files: ""bs-th.parquet""
- config_name: bs-ti
  data_files: ""bs-ti.parquet""
- config_name: bs-tk
  data_files: ""bs-tk.parquet""
- config_name: bs-tr
  data_files: ""bs-tr.parquet""
- config_name: bs-ug
  data_files: ""bs-ug.parquet""
- config_name: bs-uk
  data_files: ""bs-uk.parquet""
- config_name: bs-ur
  data_files: ""bs-ur.parquet""
- config_name: bs-vi
  data_files: ""bs-vi.parquet""
- config_name: bs-zh
  data_files: ""bs-zh.parquet""
- config_name: cs-de
  data_files: ""cs-de.parquet""
- config_name: cs-el
  data_files: ""cs-el.parquet""
- config_name: cs-en
  data_files: ""cs-en.parquet""
- config_name: cs-es
  data_files: ""cs-es.parquet""
- config_name: cs-et
  data_files: ""cs-et.parquet""
- config_name: cs-fa
  data_files: ""cs-fa.parquet""
- config_name: cs-fr
  data_files: ""cs-fr.parquet""
- config_name: cs-ha
  data_files: ""cs-ha.parquet""
- config_name: cs-hi
  data_files: ""cs-hi.parquet""
- config_name: cs-hr
  data_files: ""cs-hr.parquet""
- config_name: cs-hu
  data_files: ""cs-hu.parquet""
- config_name: cs-hy
  data_files: ""cs-hy.parquet""
- config_name: cs-id
  data_files: ""cs-id.parquet""
- config_name: cs-it
  data_files: ""cs-it.parquet""
- config_name: cs-ja
  data_files: ""cs-ja.parquet""
- config_name: cs-ka
  data_files: ""cs-ka.parquet""
- config_name: cs-kk
  data_files: ""cs-kk.parquet""
- config_name: cs-km
  data_files: ""cs-km.parquet""
- config_name: cs-ko
  data_files: ""cs-ko.parquet""
- config_name: cs-ky
  data_files: ""cs-ky.parquet""
- config_name: cs-lo
  data_files: ""cs-lo.parquet""
- config_name: cs-lt
  data_files: ""cs-lt.parquet""
- config_name: cs-lv
  data_files: ""cs-lv.parquet""
- config_name: cs-mg
  data_files: ""cs-mg.parquet""
- config_name: cs-mi
  data_files: ""cs-mi.parquet""
- config_name: cs-mk
  data_files: ""cs-mk.parquet""
- config_name: cs-mn
  data_files: ""cs-mn.parquet""
- config_name: cs-ms
  data_files: ""cs-ms.parquet""
- config_name: cs-my
  data_files: ""cs-my.parquet""
- config_name: cs-ne
  data_files: ""cs-ne.parquet""
- config_name: cs-pl
  data_files: ""cs-pl.parquet""
- config_name: cs-prs
  data_files: ""cs-prs.parquet""
- config_name: cs-ps
  data_files: ""cs-ps.parquet""
- config_name: cs-pt
  data_files: ""cs-pt.parquet""
- config_name: cs-ro
  data_files: ""cs-ro.parquet""
- config_name: cs-ru
  data_files: ""cs-ru.parquet""
- config_name: cs-rw
  data_files: ""cs-rw.parquet""
- config_name: cs-si
  data_files: ""cs-si.parquet""
- config_name: cs-sk
  data_files: ""cs-sk.parquet""
- config_name: cs-sl
  data_files: ""cs-sl.parquet""
- config_name: cs-so
  data_files: ""cs-so.parquet""
- config_name: cs-sq
  data_files: ""cs-sq.parquet""
- config_name: cs-sr
  data_files: ""cs-sr.parquet""
- config_name: cs-sw
  data_files: ""cs-sw.parquet""
- config_name: cs-ta
  data_files: ""cs-ta.parquet""
- config_name: cs-th
  data_files: ""cs-th.parquet""
- config_name: cs-ti
  data_files: ""cs-ti.parquet""
- config_name: cs-tk
  data_files: ""cs-tk.parquet""
- config_name: cs-tr
  data_files: ""cs-tr.parquet""
- config_name: cs-ug
  data_files: ""cs-ug.parquet""
- config_name: cs-uk
  data_files: ""cs-uk.parquet""
- config_name: cs-ur
  data_files: ""cs-ur.parquet""
- config_name: cs-vi
  data_files: ""cs-vi.parquet""
- config_name: cs-zh
  data_files: ""cs-zh.parquet""
- config_name: de-el
  data_files: ""de-el.parquet""
- config_name: de-en
  data_files: ""de-en.parquet""
- config_name: de-es
  data_files: ""de-es.parquet""
- config_name: de-et
  data_files: ""de-et.parquet""
- config_name: de-fa
  data_files: ""de-fa.parquet""
- config_name: de-fr
  data_files: ""de-fr.parquet""
- config_name: de-ha
  data_files: ""de-ha.parquet""
- config_name: de-hi
  data_files: ""de-hi.parquet""
- config_name: de-hr
  data_files: ""de-hr.parquet""
- config_name: de-hu
  data_files: ""de-hu.parquet""
- config_name: de-hy
  data_files: ""de-hy.parquet""
- config_name: de-id
  data_files: ""de-id.parquet""
- config_name: de-it
  data_files: ""de-it.parquet""
- config_name: de-ja
  data_files: ""de-ja.parquet""
- config_name: de-ka
  data_files: ""de-ka.parquet""
- config_name: de-kk
  data_files: ""de-kk.parquet""
- config_name: de-km
  data_files: ""de-km.parquet""
- config_name: de-ko
  data_files: ""de-ko.parquet""
- config_name: de-ky
  data_files: ""de-ky.parquet""
- config_name: de-lo
  data_files: ""de-lo.parquet""
- config_name: de-lt
  data_files: ""de-lt.parquet""
- config_name: de-lv
  data_files: ""de-lv.parquet""
- config_name: de-mg
  data_files: ""de-mg.parquet""
- config_name: de-mi
  data_files: ""de-mi.parquet""
- config_name: de-mk
  data_files: ""de-mk.parquet""
- config_name: de-mn
  data_files: ""de-mn.parquet""
- config_name: de-ms
  data_files: ""de-ms.parquet""
- config_name: de-my
  data_files: ""de-my.parquet""
- config_name: de-ne
  data_files: ""de-ne.parquet""
- config_name: de-pl
  data_files: ""de-pl.parquet""
- config_name: de-prs
  data_files: ""de-prs.parquet""
- config_name: de-ps
  data_files: ""de-ps.parquet""
- config_name: de-pt
  data_files: ""de-pt.parquet""
- config_name: de-ro
  data_files: ""de-ro.parquet""
- config_name: de-ru
  data_files: ""de-ru.parquet""
- config_name: de-rw
  data_files: ""de-rw.parquet""
- config_name: de-si
  data_files: ""de-si.parquet""
- config_name: de-sk
  data_files: ""de-sk.parquet""
- config_name: de-sl
  data_files: ""de-sl.parquet""
- config_name: de-so
  data_files: ""de-so.parquet""
- config_name: de-sq
  data_files: ""de-sq.parquet""
- config_name: de-sr
  data_files: ""de-sr.parquet""
- config_name: de-sw
  data_files: ""de-sw.parquet""
- config_name: de-ta
  data_files: ""de-ta.parquet""
- config_name: de-th
  data_files: ""de-th.parquet""
- config_name: de-ti
  data_files: ""de-ti.parquet""
- config_name: de-tk
  data_files: ""de-tk.parquet""
- config_name: de-tr
  data_files: ""de-tr.parquet""
- config_name: de-ug
  data_files: ""de-ug.parquet""
- config_name: de-uk
  data_files: ""de-uk.parquet""
- config_name: de-ur
  data_files: ""de-ur.parquet""
- config_name: de-vi
  data_files: ""de-vi.parquet""
- config_name: de-zh
  data_files: ""de-zh.parquet""
- config_name: el-en
  data_files: ""el-en.parquet""
- config_name: el-es
  data_files: ""el-es.parquet""
- config_name: el-et
  data_files: ""el-et.parquet""
- config_name: el-fa
  data_files: ""el-fa.parquet""
- config_name: el-fr
  data_files: ""el-fr.parquet""
- config_name: el-ha
  data_files: ""el-ha.parquet""
- config_name: el-hi
  data_files: ""el-hi.parquet""
- config_name: el-hr
  data_files: ""el-hr.parquet""
- config_name: el-hu
  data_files: ""el-hu.parquet""
- config_name: el-hy
  data_files: ""el-hy.parquet""
- config_name: el-id
  data_files: ""el-id.parquet""
- config_name: el-it
  data_files: ""el-it.parquet""
- config_name: el-ja
  data_files: ""el-ja.parquet""
- config_name: el-ka
  data_files: ""el-ka.parquet""
- config_name: el-kk
  data_files: ""el-kk.parquet""
- config_name: el-km
  data_files: ""el-km.parquet""
- config_name: el-ko
  data_files: ""el-ko.parquet""
- config_name: el-ky
  data_files: ""el-ky.parquet""
- config_name: el-lo
  data_files: ""el-lo.parquet""
- config_name: el-lt
  data_files: ""el-lt.parquet""
- config_name: el-lv
  data_files: ""el-lv.parquet""
- config_name: el-mg
  data_files: ""el-mg.parquet""
- config_name: el-mi
  data_files: ""el-mi.parquet""
- config_name: el-mk
  data_files: ""el-mk.parquet""
- config_name: el-mn
  data_files: ""el-mn.parquet""
- config_name: el-ms
  data_files: ""el-ms.parquet""
- config_name: el-my
  data_files: ""el-my.parquet""
- config_name: el-ne
  data_files: ""el-ne.parquet""
- config_name: el-pl
  data_files: ""el-pl.parquet""
- config_name: el-prs
  data_files: ""el-prs.parquet""
- config_name: el-ps
  data_files: ""el-ps.parquet""
- config_name: el-pt
  data_files: ""el-pt.parquet""
- config_name: el-ro
  data_files: ""el-ro.parquet""
- config_name: el-ru
  data_files: ""el-ru.parquet""
- config_name: el-rw
  data_files: ""el-rw.parquet""
- config_name: el-si
  data_files: ""el-si.parquet""
- config_name: el-sk
  data_files: ""el-sk.parquet""
- config_name: el-sl
  data_files: ""el-sl.parquet""
- config_name: el-so
  data_files: ""el-so.parquet""
- config_name: el-sq
  data_files: ""el-sq.parquet""
- config_name: el-sr
  data_files: ""el-sr.parquet""
- config_name: el-sw
  data_files: ""el-sw.parquet""
- config_name: el-ta
  data_files: ""el-ta.parquet""
- config_name: el-th
  data_files: ""el-th.parquet""
- config_name: el-ti
  data_files: ""el-ti.parquet""
- config_name: el-tk
  data_files: ""el-tk.parquet""
- config_name: el-tr
  data_files: ""el-tr.parquet""
- config_name: el-ug
  data_files: ""el-ug.parquet""
- config_name: el-uk
  data_files: ""el-uk.parquet""
- config_name: el-ur
  data_files: ""el-ur.parquet""
- config_name: el-vi
  data_files: ""el-vi.parquet""
- config_name: el-zh
  data_files: ""el-zh.parquet""
- config_name: en-es
  data_files: ""en-es.parquet""
- config_name: en-et
  data_files: ""en-et.parquet""
- config_name: en-fa
  data_files: ""en-fa.parquet""
- config_name: en-fr
  data_files: ""en-fr.parquet""
- config_name: en-ha
  data_files: ""en-ha.parquet""
- config_name: en-hi
  data_files: ""en-hi.parquet""
- config_name: en-hr
  data_files: ""en-hr.parquet""
- config_name: en-hu
  data_files: ""en-hu.parquet""
- config_name: en-hy
  data_files: ""en-hy.parquet""
- config_name: en-id
  data_files: ""en-id.parquet""
- config_name: en-it
  data_files: ""en-it.parquet""
- config_name: en-ja
  data_files: ""en-ja.parquet""
- config_name: en-ka
  data_files: ""en-ka.parquet""
- config_name: en-kk
  data_files: ""en-kk.parquet""
- config_name: en-km
  data_files: ""en-km.parquet""
- config_name: en-ko
  data_files: ""en-ko.p",High,4.0
Translation,freococo/quran_multilingual_parallel,0.0,123.0,2025-05-30 12:00:27+00:00,mit,0.0,80.3 MB,84200652.8,36.4 MB,38168166.4,6237,none,none,"---
license: mit
language:
- ar
- sq
- am
- az
- bn
- bs
- bg
- my
- zh
- da
- nl
- en
- tl
- fr
- ff
- fa
- de
- gu
- ha
- hi
- id
- it
- ja
- jv
- kk
- km
- ko
- ku
- ky
- ms
- ml
- ps
- pl
- pt
- pa
- ru
- sd
- si
- so
- es
- sw
- sv
- tg
- ta
- tt
- te
- th
- tr
- ur
- ug
- uz
- yo
- 'no'
- vi
task_categories:
- translation
---

# 📘 Qur’an Multilingual Parallel Dataset (`quran_multilingual_parallel`)

This dataset presents a clean, structurally-aligned multilingual parallel corpus of the Qur’anic text. It is intended for linguistic, computational, and cross-lingual AI applications — not only for religious interpretation.

It contains over **6,200 verse-level alignments** in **54 human languages**, formatted in a machine-friendly `.csv` structure with language-specific translation fields.

---

## 🧠 Dataset Highlights

- 📖 6,236 ayahs (verses)
- 📚 114 surahs (chapters)
- 🌍 Translations in 53 languages
- 🕌 Arabic source text included
- 🔢 Fully aligned, row-per-verse CSV
- ⚠️ Missing translations are transparently documented

---

## 📁 Files Included

| File                             | Description                                                                 |
|----------------------------------|-----------------------------------------------------------------------------|
| `quran_multilingual_parallel.csv`| Complete parallel corpus: 6,236 rows × 57 columns                          |
| `README.md`                      | This documentation                                                         |

---

## 🧾 Column Format

Each row corresponds to a verse (ayah). Columns include:

| Column           | Description                                      |
|------------------|--------------------------------------------------|
| `surah_number`   | Integer: Surah (chapter) number (1–114)          |
| `surah`          | Slugified Surah name (e.g., `al-baqarah`)        |
| `ayah`           | Integer: Ayah (verse) number                     |
| `arabic`         | Original Arabic text of the verse                |
| `{language}`     | One column per language, each containing the translation for that ayah |

---

## 🌍 Languages Included

This dataset includes translations in the following 54 languages:

    1.	Arabic         (original)
	2.	Albanian
	3.	Amharic
	4.	Azerbaijani
	5.	Bengali
	6.	Bosnian
	7.	Bulgarian
	8.	Burmese
	9.	Chinese
	10.	Danish
	11.	Dutch
	12.	English
	13.	Filipino
	14.	French
	15.	Fulah
	16.	Persian
	17.	German
	18.	Gujarati
	19.	Hausa
	20.	Hindi
	21.	Indonesian
	22.	Italian
	23.	Japanese
	24.	Javanese
	25.	Kazakh
	26.	Khmer
	27.	Korean
	28.	Kurdish
	29.	Kyrgyz
	30.	Malay
	31.	Malayalam
	32.	Norwegian
	33.	Pashto
	34.	Polish
	35.	Portuguese
	36.	Punjabi
	37.	Russian
	38.	Sindhi
	39.	Sinhalese
	40.	Somali
	41.	Spanish
	42.	Swahili
	43.	Swedish
	44.	Tajik
	45.	Tamil
	46.	Tatar
	47.	Telugu
	48.	Thai
	49.	Turkish
	50.	Urdu
	51.	Uyghur
	52.	Uzbek
	53.	Vietnamese
	54.	Yoruba
---

## ⚠️ Missing Translation Entries

While all 6,236 verses are structurally included, **79 translations** were unavailable from the source site ([al-quran.cc](https://www.al-quran.cc/quran-translation/)) at the time of scraping.

These translations are missing in the following `(language, surah, ayah)` combinations:

<details>
<summary>📋 Click to expand full list of missing translations (79)</summary>

01. azerbaijani — al-asr:1
02. azerbaijani — al-asr:2
03. azerbaijani — al-asr:3
04. filipino — al-haaqqa:52
05. german — al-baqara:220
06. german — al-isra:65
07. german — an-naba:3
08. german — ar-rum:5
09. german — az-zamar:40
10. jawa — al-furqan:37
11. kyrgyz — al-mulk:16
12. portuguese — aal-e-imran:50
13. portuguese — aal-e-imran:51
14. portuguese — an-naml:31
15. portuguese — an-naziat:19
16. portuguese — as-saaffat:53
17. portuguese — ash-shuara:17
18. portuguese — nooh:11
19. portuguese — nooh:12
20. portuguese — nooh:13
21. portuguese — nooh:14
22. portuguese — nooh:15
23. portuguese — nooh:16
24. portuguese — nooh:17
25. portuguese — nooh:18
26. portuguese — nooh:19
27. portuguese — nooh:20
28. sindhi — ibrahim:1
29. sindhi — ibrahim:2
30. sindhi — ibrahim:3
31. sindhi — ibrahim:4
32. sindhi — ibrahim:5
33. sindhi — ibrahim:6
34. sindhi — ibrahim:7
35. sindhi — ibrahim:8
36. sindhi — ibrahim:9
37. sindhi — ibrahim:10
38. sindhi — ibrahim:11
39. sindhi — ibrahim:12
40. sindhi — ibrahim:13
41. sindhi — ibrahim:14
42. sindhi — ibrahim:15
43. sindhi — ibrahim:16
44. sindhi — ibrahim:17
45. sindhi — ibrahim:18
46. sindhi — ibrahim:19
47. sindhi — ibrahim:20
48. sindhi — ibrahim:21
49. sindhi — ibrahim:22
50. sindhi — ibrahim:23
51. sindhi — ibrahim:24
52. sindhi — ibrahim:25
53. sindhi — ibrahim:26
54. sindhi — ibrahim:27
55. sindhi — ibrahim:28
56. sindhi — ibrahim:29
57. sindhi — ibrahim:30
58. sindhi — ibrahim:31
59. sindhi — ibrahim:32
60. sindhi — ibrahim:33
61. sindhi — ibrahim:34
62. sindhi — ibrahim:35
63. sindhi — ibrahim:36
64. sindhi — ibrahim:37
65. sindhi — ibrahim:38
66. sindhi — ibrahim:39
67. sindhi — ibrahim:40
68. sindhi — ibrahim:41
69. sindhi — ibrahim:42
70. sindhi — ibrahim:43
71. sindhi — ibrahim:44
72. sindhi — ibrahim:45
73. sindhi — ibrahim:46
74. sindhi — ibrahim:47
75. sindhi — ibrahim:48
76. sindhi — ibrahim:49
77. sindhi — ibrahim:50
78. sindhi — ibrahim:51
79. sindhi — ibrahim:52

</details>

These empty fields are left intentionally blank in the dataset. All ayahs are present in Arabic, and structural alignment is maintained.

---

## 🔍 Data Source and Extraction

- Source: [https://www.al-quran.cc/quran-translation/](https://www.al-quran.cc/quran-translation/)
- Extraction: Python-based HTML parsing with dynamic AJAX handling
- Structural validation: 114 surahs × 6,236 ayahs cross-checked
- Missing translations logged during post-validation

---


## 📜 Linguistic and Literary Significance of the Qur’an

### 🧬 A Living Corpus Preserved in Speech

The Qur’an is the only major historical text that has been **memorized word-for-word by millions of people** across generations, regions, and languages — regardless of whether they spoke Arabic natively.

- Over **1,400 years old**, the Arabic text of the Qur’an remains **unchanged**, **recited daily**, and **actively memorized**, verse by verse.
- It is preserved not only in manuscripts but in **oral transmission**, making it one of the most reliably reconstructed texts in human linguistic history.

### 🌍 Translated into Languages That Weren’t Yet Standardized

Several languages in this dataset — such as **today’s form of Burmese (Myanmar)**, **Swahili**, **Standard Indonesian**, and even **Modern English** — had not yet developed or reached their current standardized written form at the time the Qur’an was first revealed over 1,400 years ago.

Yet today, these communities:
- Study and translate the Qur’an with deep linguistic care
- Retain the **verse structure** in all translations
- Participate in a **global multilingual alignment** with the same source text

This makes the Qur’an a **linguistic anchor** across centuries, preserved in meaning and form across **dozens of linguistic systems**.

### ✍️ Neither Prose Nor Poetry — A Unique Register

From a literary standpoint, the Qur’an’s structure is:
- Not classical prose
- Not traditional poetry
- But a **distinct rhythmic and rhetorical form** known for:
  - Internal rhyme and parallelism
  - Recurring motifs
  - High semantic density and emotional resonance

This genre continues to challenge both literary analysis and computational modeling.

### 🤖 Relevance to AI and Linguistics

The Qur’an is an ideal corpus for AI-based multilingual NLP:

- 🔁 Fully aligned across 53 languages
- 🧩 Rigid source structure (verse-level)
- 🧠 Oral memory transmission modeling
- 🌐 Cross-lingual semantic drift analysis
- 🛠️ Faithful translation alignment tasks

It provides a **fixed point of semantic comparison** for understanding how languages represent meaning — historically and today.

---


## 🪪 License

- Text: Public Domain where applicable (verify by language)
- Code/scripts: MIT License

---

## 📌 Citation

```bibtex
@misc{quran_multilingual_parallel_2025,
  title  = {Qur’an Multilingual Parallel Dataset},
  author = {freococo},
  year   = {2025},
  url    = {https://huggingface.co/datasets/freococo/quran_multilingual_parallel}
}",High,4.0
Cultural Alignment,arbml/CIDAR,51.0,239.0,2025-04-03 08:35:36+00:00,apache-2.0,3.0,3.55 MB,3722444.8,3.55 MB,3722444.8,10000,https://arxiv.org/abs/2402.03177,none,"---
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
dataset_info:
  features:
  - name: output
    dtype: string
  - name: instruction
    dtype: string
  - name: index
    dtype: int64
  splits:
  - name: train
    num_bytes: 6712623
    num_examples: 10000
  download_size: 3553672
  dataset_size: 6712623
license: apache-2.0
task_categories:
- text-generation
tags:
  - Instruction
language:
- ar
pretty_name: CIDAR
size_categories:
- 1K<n<10K
---
# Dataset Card for ""CIDAR""

# 🌴CIDAR: *Culturally Relevant Instruction Dataset For Arabic*
<p align=""center"">
<img src=""https://cdn-uploads.huggingface.co/production/uploads/655e10b1c38270696b290f20/lKec96otC8VdM09SnPKL8.png"" width = ""150px""/>
  <p align=""center""> [ <a href=""https://arxiv.org/abs/2402.03177"">Paper</a> - <a href=""https://github.com/ARBML/CIDAR"">GitHub</a> ]</p>

</p>

CIDAR contains **10,000** `instructions` and their `output`. The dataset was created by selecting around **9,109** samples from [Alpagasus](https://huggingface.co/datasets/mlabonne/alpagasus) dataset then translating it to `Arabic` using ChatGPT. In addition, we append that with around **891** Arabic grammar instructions from the webiste [Ask the teacher](https://learning.aljazeera.net/ar/asktheteacher). All the 10,000 samples were reviewed by around 12 reviewers. 

<p align=""center"">

<img src=""https://cdn-uploads.huggingface.co/production/uploads/655e10b1c38270696b290f20/0m-RewO_OAjPQRI0zE175.png"" width = ""1200px""/>
</p>


## 📚 Datasets Summary

<table>
  <tr>
<th>Name</th>
<th>Explanation</th>
</tr>
<tr>
<td><a href=https://huggingface.co/datasets/arbml/cidar>CIDAR</a></t> 
<td>10,000 instructions and responses in Arabic</td>
</tr>
<tr>
<td><a href=https://huggingface.co/datasets/arbml/cidar-eval-100>CIDAR-EVAL-100</a></t> 
<td>100 instructions to evaluate LLMs on cultural relevance</td>
</tr>
<tr>
<td><a href=https://huggingface.co/datasets/arbml/cidar-mcq-100>CIDAR-MCQ-100</a></t> 
<td>100 Multiple choice questions and answers to evaluate LLMs on cultural relevance </td>
</tr>
</table>

## 📋 Dataset Structure
- `index(int)`: Unique number of each instruction-output pair.
- `instruction(str)`: Required instruction.
- `output(str)`: Response for the given instruction.



## 📁 Loading The Dataset
You can download the dataset directly from HuggingFace or use the following code:

```python
from datasets import load_dataset
cidar = load_dataset('arbml/CIDAR')
```

## 📄 Sample From The Dataset:
This sample shows an Arabic-specific instruction related to `البلاغة(rhetoric)` and `الإعراب(grammatical status)`. The sentence is originally in Arabic. 

 **index**: 10030

 **instruction**: لو سمحت اين صيغة المبالغة في "" الجاحظ اديب سيارة اراؤه"" و ما اعراب الجملة؟

 **output**:
 
           'صيغة المبالغة في هذه الجملة هي ""سيّارة""، فاسم الفاعل من ""السير"" هو '
           '""سائر/ سائرة"" وإذا أردت الدلالة على المبالغة في سير الآراء (أي '
           'شهرتها وكثرة تداولها) قلت ""آراء سيّارة"".\r\n'
           'أما إعراب الجملة فهو: ""الجاحظُ"": مبتدأ مرفوع بالضمة. ""أديبٌ"": خبر '
           'مرفوع بالضمة. ""سيارةٌ"": خبر متقدم، و""آراؤُه"": مبتدأ متأخر، وهو '
           'مضاف والهاء ضمير متصل مضاف إليه في محل جر.  ويمكن اعتبار ""سيارة"" '
           'مبتدأ وهو وصف يعمل عمل فعله، و""آراؤُه"" فاعل سدّ مسدّ الخبر.\r\n'
           'وفي الحالتين فجملة ""سيارة آراؤه"" جملة اسمية في محل رفع نعت '
           'لـ""أديب"".'

## 🧩 Contributers
There were at least 12 contributors to the annotation of CIDAR. You can check the list [here](https://alpacaarabic-production.up.railway.app/explore).

## ⛔️ Limitations and Future Work
CIDAR is intended for **research** purposes only. The authors disclaim any responsibility for misuse and condemn any use contrary to **Arabic culture** or **Islamic values**. Even though subjected to human verification, there is no guarantee that responses are entirely aligned with Arabic culture and Islamic values. Users of the dataset are urged to exercise caution, employ critical thinking, and seek guidance from representative figures when necessary.

## 🔑 License
[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).

## Citation

```
@misc{alyafeai2024cidar,
      title={{CIDAR: Culturally Relevant Instruction Dataset For Arabic}}, 
      author={Zaid Alyafeai and Khalid Almubarak and Ahmed Ashraf and Deema Alnuhait and Saied Alshahrani and Gubran A. Q. Abdulrahman and Gamil Ahmed and Qais Gawah and Zead Saleh and Mustafa Ghaleb and Yousef Ali and Maged S. Al-Shaibani},
      year={2024},
      eprint={2402.03177},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```",High,4.0
Cultural Alignment,FreedomIntelligence/ACVA-Arabic-Cultural-Value-Alignment,8.0,131.0,2023-09-21 12:39:18+00:00,apache-2.0,2.0,1.48 MB,1551892.48,473 KB,484352.0,9000,none,none,"---
language:
- ar
viewer: true
license: apache-2.0
size_categories:
- 1K<n<10K
---

# About ArabicCulture
The ArabicCulture dataset was generated by gpt3.5 and contains 8000+ True and False questions.   
The dataset contains questions from 58 different areas.  
In the answers, ""True"" accounted for 59.62%, and ""False"" accounted for 40.38%
# data-all
It contains 8000+ data, and we took 5 data from each area as few-shot data.
# data-select
We asked two Arabs to judge 4000 of all the data for us, and we left data that two Arabs both thought were good. Finally, we got 2.4k data covering 9 areas.   
We divided them into test sets and validation sets as above.",Low,1.0
Cultural Alignment,UBC-NLP/palmx_2025_subtask1_culture,0.0,142.0,2025-06-11 19:54:36+00:00,none,0.0,708 kB,unknown,708 kB,unknown,2500,none,none,"---
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: dev
    path: data/dev-*
dataset_info:
  features:
  - name: id
    dtype: int64
  - name: question
    dtype: string
  - name: A
    dtype: string
  - name: B
    dtype: string
  - name: C
    dtype: string
  - name: D
    dtype: string
  - name: answer
    dtype: string
  splits:
  - name: train
    num_bytes: 1059019
    num_examples: 2000
  - name: dev
    num_bytes: 271128
    num_examples: 500
  download_size: 707872
  dataset_size: 1330147
task_categories:
- question-answering
language:
- ar
tags:
- ArabicNLP
- Shared-task
- Culture
---
🏷️ PalmX 2025 — General Culture Evaluation (PalmX-GC)

### Dataset Summary

PalmX-GC evaluates a model’s grasp of general Arab culture—customs, history, geography, arts, cuisine, notable figures, and everyday life across the 22 Arab League countries.
Every item is written in Modern Standard Arabic (MSA). The dataset powers Subtask 1 of the PalmX 2025 shared task.

---

### Dataset Structure

| Split | # MCQs | Release Date | Notes                    |
|------:|-------:|-------------|--------------------------|
| Train | 2000    | 10 Jun 2025 | With gold answers        |
| Dev   | 500    | 10 Jun 2025 | With gold answers        |
| Test  | 2 000  | Blind       | Answers withheld         |

---
### Example Record
```json
{
  ""id"": 42042,
  ""question"": ""ما هو أعلى جبل في اليمن؟"",
  ""A"": ""جبل النبي شعيب"",
  ""B"": ""جبل شمسان"",
  ""C"": ""جبل صبر"",
  ""D"": ""جبل حدة"",
  ""answer"": ""A""
}
```
---
### License
PalmX Data License — non-commercial research use.

---
#### Access
You will gain access after registering for the shared task.

    • Register here → https://forms.gle/cpmYGkAhM3YLXBsQ7
    • Website → https://palmx.dlnlp.ai/
 
---
### Contact & Links
	•	🌐 Website: https://palmx.dlnlp.ai/
	•	📝 Registration Form: https://forms.gle/cpmYGkAhM3YLXBsQ7
	•	📣 Google Group: https://groups.google.com/g/palmx2025",Medium,2.0
Dialog/Conversation,mohamedemam/Arabic-samsum-dialogsum,1.0,49.0,2023-09-11 14:35:29+00:00,cc-by-nc-2.0,1.0,14 MB,14680064.0,14 MB,14680064.0,24813,https://arxiv.org/abs/1911.12237,none,"---
dataset_info:
  features:
  - name: index
    dtype: int64
  - name: id
    dtype: string
  - name: dialogue
    dtype: string
  - name: summary
    dtype: string
  - name: topic
    dtype: string
  splits:
  - name: train
    num_bytes: 27913254
    num_examples: 24813
  download_size: 13968520
  dataset_size: 27913254
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: cc-by-nc-2.0
task_categories:
- summarization
- conversational
language:
- ar
pretty_name: ar messum
size_categories:
- 10K<n<100K
---
# Dataset Card for ""Arabic-samsum-dialogsum""

this dataset is comption between samsum and dialogsum dataset translated in arabic
## Table of Contents
- [Dataset Description](#dataset-description)
  - [Dataset Summary](#dataset-summary)
  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)
  - [Languages](#languages)
- [Dataset Structure](#dataset-structure)
  - [Data Instances](#data-instances)
  - [Data Fields](#data-fields)
  - [Data Splits](#data-splits)
- [Dataset Creation](#dataset-creation)
  - [Curation Rationale](#curation-rationale)
  - [Source Data](#source-data)
  - [Annotations](#annotations)
  - [Personal and Sensitive Information](#personal-and-sensitive-information)
- [Considerations for Using the Data](#considerations-for-using-the-data)
  - [Social Impact of Dataset](#social-impact-of-dataset)
  - [Discussion of Biases](#discussion-of-biases)
  - [Other Known Limitations](#other-known-limitations)
- [Additional Information](#additional-information)
  - [Dataset Curators](#dataset-curators)
  - [Licensing Information](#licensing-information)
  - [Citation Information](#citation-information)
  - [Contributions](#contributions)

## Dataset Description

- **Homepage:** https://arxiv.org/abs/1911.12237v2
- **Repository:** [Needs More Information]
- **Paper:** https://arxiv.org/abs/1911.12237v2
- **Leaderboard:** [Needs More Information]
- **Point of Contact:** [Needs More Information]

### Dataset Summary

The SAMSum dataset contains about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English. Linguists were asked to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger convesations. The style and register are diversified - conversations could be informal, semi-formal or formal, they may contain slang words, emoticons and typos. Then, the conversations were annotated with summaries. It was assumed that summaries should be a concise brief of what people talked about in the conversation in third person.
The SAMSum dataset was prepared by Samsung R&D Institute Poland and is distributed for research purposes (non-commercial licence: CC BY-NC-ND 4.0).

### Supported Tasks and Leaderboards

[Needs More Information]

### Languages

Arabic

## Dataset Structure
t
### Data Instances

The created dataset is made of 16369 conversations distributed uniformly into 4 groups based on the number of utterances in con- versations: 3-6, 7-12, 13-18 and 19-30. Each utterance contains the name of the speaker. Most conversations consist of dialogues between two interlocutors (about 75% of all conversations), the rest is between three or more people

The first instance in the training set:
{'id': '13818513', 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.', 'dialogue': ""Amanda: I baked  cookies. Do you want some?\r\nJerry: Sure!\r\nAmanda: I'll bring you tomorrow :-)""}

### Data Fields

- dialogue: text of dialogue.
- summary: human written summary of the dialogue.
- id: unique id of an example.

### Data Splits

- train: 24732

## Dataset Creation

### Curation Rationale

In paper:
> In the first approach, we reviewed datasets from the following categories: chatbot dialogues, SMS corpora, IRC/chat data, movie dialogues, tweets, comments data (conversations formed by replies to comments), transcription of meetings, written discussions, phone dialogues and daily communication data. Unfortunately, they all differed in some respect from the conversations that are typ- ically written in messenger apps, e.g. they were too technical (IRC data), too long (comments data, transcription of meetings), lacked context (movie dialogues) or they were more of a spoken type, such as a dialogue between a petrol station assis- tant and a client buying petrol.
As a consequence, we decided to create a chat dialogue dataset by constructing such conversa- tions that would epitomize the style of a messenger app.

### Source Data

#### Initial Data Collection and Normalization

 In paper:
> We asked linguists to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger conversations. It includes chit-chats, gossiping about friends, arranging meetings, discussing politics, consulting university assignments with colleagues, etc. Therefore, this dataset does not contain any sensitive data or fragments of other corpora.

#### Who are the source language producers?

linguists

### Annotations

#### Annotation process

In paper:
> Each dialogue was created by one person. After collecting all of the conversations, we asked language experts to annotate them with summaries, assuming that they should (1) be rather short, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person. Each dialogue contains only one ref- erence summary.

#### Who are the annotators?

language experts

### Personal and Sensitive Information

None, see above: Initial Data Collection and Normalization

## Considerations for Using the Data

### Social Impact of Dataset

[Needs More Information]

### Discussion of Biases

[Needs More Information]

### Other Known Limitations

[Needs More Information]

## Additional Information

### Dataset Curators

[Needs More Information]

### Licensing Information

non-commercial licence: CC BY-NC-ND 4.0

### Citation Information

```
@inproceedings{gliwa-etal-2019-samsum,
    title = ""{SAMS}um Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization"",
    author = ""Gliwa, Bogdan  and
      Mochol, Iwona  and
      Biesek, Maciej  and
      Wawer, Aleksander"",
    booktitle = ""Proceedings of the 2nd Workshop on New Frontiers in Summarization"",
    month = nov,
    year = ""2019"",
    address = ""Hong Kong, China"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/D19-5409"",
    doi = ""10.18653/v1/D19-5409"",
    pages = ""70--79""
}
```

### Contributions

Thanks to [@cccntu](https://github.com/cccntu) for adding this dataset.
[More Information needed](https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)",High,5.0
Dialog/Conversation,m-ric/Open_Assistant_Conversation_Chains,6.0,46.0,2023-11-22 14:37:58+00:00,apache-2.0,0.0,30.2 MB,31666995.2,30.2 MB,31666995.2,46283,none,none,"---
license: apache-2.0
language:
  - en
  - es
  - ru
  - de
  - pl
  - th
  - vi
  - sv
  - bn
  - da
  - he
  - it
  - fa
  - sk
  - id
  - nb
  - el
  - nl
  - hu
  - eu
  - zh
  - eo
  - ja
  - ca
  - cs
  - bg
  - fi
  - pt
  - tr
  - ro
  - ar
  - uk
  - gl
  - fr
  - ko
task_categories:
  - conversational
  - text-generation
tags:
  - human-feedback
size_categories:
  - 10K<n<100K
pretty_name: OpenAssistant Conversations Unrolled
---
# Dataset Card for Dataset Name

## Dataset description
<!-- Provide a quick summary of the dataset. -->

This dataset is a reformatting of [OpenAssistant Conversations (OASST1)](https://huggingface.co/datasets/OpenAssistant/oasst1), which is
> a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 fully annotated conversation trees. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers.

It was modified from the original by following the tree branches and reforming the messages into conversation streams.


#### Who are the editors?

Aymeric Roucher, Hugging Face.

### Example

````
{
  'conversation_id': '6708c47f-05c9-4346-b3d2-40b2bd24fde4',
  'user_id': '2c96e467-66f0-4be7-9693-bda51356a424',
  'created_date': '2023-02-06T18:48:49.391686+00:00',
  'messages': [
    {'content': 'Can you write a short introduction about the relevance of the term ""monopsony"" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.',
  'role': 'user'},
    {'content': '""Monopsony"" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n\nReferences:\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.',
  'role': 'assistant'},
    {'content': 'Now explain it to a dog', 'role': 'user'}
  ]
}
````",Medium,3.0
Dialog/Conversation,premio-ai/TheArabicPile_Conversational,1.0,42.0,2024-03-21 21:42:33+00:00,cc-by-nc-4.0,0.0,2.27 GB,unknown,2.27 GB,unknown,2493431,none,none,"---
language:
- ar
license: cc-by-nc-4.0
task_categories:
- text-generation
dataset_info:
- config_name: dedup
  features:
  - name: text
    dtype: string
  splits:
  - name: train
    num_bytes: 2074285191
    num_examples: 1189978
  download_size: 1106103903
  dataset_size: 2074285191
- config_name: default
  features:
  - name: text
    dtype: string
  splits:
  - name: original
    num_bytes: 2180193661
    num_examples: 1303453
  download_size: 1168365713
  dataset_size: 2180193661
configs:
- config_name: dedup
  data_files:
  - split: train
    path: dedup/train-*
- config_name: default
  data_files:
  - split: original
    path: data/train-*
---


# The Arabic Pile

![image/png](https://cdn-uploads.huggingface.co/production/uploads/64da0fd923557cdce3e514c3/J0oY67lVvecV75SOlWpjc.png)

## Introduction: 
The Arabic Pile is a comprehensive dataset meticulously designed to parallel the structure of The Pile and The Nordic Pile. Focused on the Arabic language, the dataset encompasses a vast array of linguistic nuances, incorporating both Modern Standard Arabic (MSA) and various Levantine, North African, and Egyptian dialects. Tailored for the training and fine-tuning of large language models, the dataset consists of 13 subsets, each uniquely crafted to cater to different linguistic domains.

## The Conversational Subset:
This dataset has a collection of conversation-based content in Arabic.

## Other Subsets:
1. premio-ai/TheArabicPile
2. premio-ai/TheArabicPile_Web
3. premio-ai/TheArabicPile_Lyrics
4. premio-ai/TheArabicPile_Reviews
5. premio-ai/TheArabicPile_Dialects
6. premio-ai/TheArabicPile_Mathematics
7. premio-ai/TheArabicPile_Conversational
8. premio-ai/TheArabicPile_Articles
9. premio-ai/TheArabicPile_Poetry
10. premio-ai/TheArabicPile_Medical
11. premio-ai/TheArabicPile_Miscellaneous
12. premio-ai/TheArabicPile_SocialMedia
13. premio-ai/TheArabicPile_Translations
14. premio-ai/TheArabicPile_Books

These subsets serve distinct purposes, ranging from mathematical content to conversational dialogue, medical texts, and more. Notably, there's a dedicated subset, ""premio-ai/TheArabicPile_SocialMedia,"" emphasizing the inclusion of language commonly found in social media contexts.

## Dataset Description
* Curated by: Premio.AI team
* Language(s) (NLP): Arabic, multiple languages on the translation dataset.
* License: CC BY-NC 4.0 Deed - Non Commercial.
* For any commercial uses or licensing, please contact mo@premio.ai.


## Data Structure
The datasets are divided into two main subsets:
1. Original Subset: The raw data as collected from sources, without modifications.
2. Deduplication Subset: A filtered and cleaned version, enhancing usability for large language models by reducing redundancy and noise.
The Arabic Pile extends an invitation not only for training and fine-tuning large language models but also for diverse applications across linguistic domains. Whether for research, analysis, or other linguistic endeavors, The Arabic Pile stands as a rich resource for the exploration of Arabic language intricacies.

## Data Collection
Please refer to the paper for more details on our data collection procedures.

## Data Format
The dataset has one single column called text. The text should contain the required meta data and the body combined. This was done to make sure that it will be a good fit for direct training or fine-tuning of large language models.

Please note that the meta data might require to be repeated if your training context window won’t fit the entire body of text.

## Potential Bias
As with any large-scale dataset, The Arabic Pile is not immune to potential biases that may influence the training and performance of language models. It's crucial to transparently address these biases to ensure responsible usage and interpretation of the dataset. Here are some considerations:
1. Dialectal Imbalance: The dataset incorporates various Arabic dialects, with a focus on Levantine, North African, and Egyptian variants. However, there might be variations in the representation of these dialects, potentially leading to an imbalance in the training data.
2. Source Influence: Bias may arise from the sources of the original data. The dataset collects information from diverse platforms and domains, and biases inherent in those sources could transfer to the dataset.
3. Social Media Context: Some of our datasets have language from social media platforms and online platforms. This subset may introduce biases inherent in online discourse, such as informal language, colloquial expressions, and potential subjectivity in politics, religion or culture.
4. Genre and Domain Bias: Different subsets cater to distinct linguistic domains, such as medical texts, poetry, reviews, and more. Each domain carries its own linguistic characteristics, potentially leading to biases based on the genres represented.

## License Information for The Arabic Pile: No Commercial Use

The Arabic Pile is released under the Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0). This license is designed to facilitate the open sharing and collaboration of the dataset while ensuring responsible and non-commercial usage.
Key Points of the License:
* Attribution (BY): Users are free to share, adapt, and build upon the dataset, even commercially, as long as they provide appropriate attribution to the dataset creators.
* Non-Commercial (NC): The dataset may not be used for commercial purposes. Any use for commercial gain requires explicit permission from the dataset creators.
* No Additional Restrictions: The license allows for maximum freedom of use, provided the terms of attribution and non-commercial use are adhered to.
How to Cite: When using The Arabic Pile in your work, please include a proper citation to acknowledge the dataset creators. A recommended citation can be found in the model card for easy reference.
License Deed: For a comprehensive understanding of the terms and conditions, please refer to the CC BY-NC 4.0 License Deed.
By adopting this license, we aim to foster a collaborative and open environment for the exploration and advancement of Arabic language understanding and natural language processing.

## Citation
When utilizing The Arabic Pile in your research, development, or other projects, we kindly request that you cite the dataset using the following format:

@article{alrefaie2024arabicpile,
  author = {Mohamed Taher Alrefaie, Mahmoud Ibrahim Barbary, Ahmed Yasser Hassanein, Shiref Khaled Elhalawany, Karim Ashraf Elsayed, Ahmed Yasser },
  title = {The Arabic Pile: A Large Scale Dataset of Diverse Text for Large Language Modeling},
  year = {2024},
  url = {https://huggingface.co/datasets/premio-ai/TheArabicPile}
}

",High,6.0
Dialog/Conversation,Mars203020/arabic_medical_dialogue,3.0,125.0,2024-06-29 11:49:41+00:00,mit,0.0,460 KB,471040.0,303 KB,310272.0,3598,none,none,"---
license: mit
task_categories:
- text-generation
language:
- ar
tags:
- medical
pretty_name: Arabic Medical Dialogue
---",Low,1.0
Dialog/Conversation,willwade/AACConversations,0.0,9.0,2025-05-15 10:37:50+00:00,cc-by-4.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,none,none,"---
language:
- en
- fr
- de
- es
- it
- nl
- el
- ru
- he
- ar
- pt
- cy
- ja
- zh
- ko
- nb
- sv
- da
- fi
- cs
- sk
- pl
- hu
- sl
- hr
- uk
- eu
- ca
- fo
- af
license: cc-by-4.0
multilinguality: multilingual
size_categories:
- 10K<n<100K
task_categories:
- text-generation
- text-classification
- text2text-generation
task_ids:
- language-modeling
- sentiment-analysis
- text-simplification
pretty_name: AAC Conversations Dataset (2024)
tags:
- aac
- augmentative-alternative-communication
- assistive-technology
- conversation
- multilingual
- text-correction
- text-generation
---

# AAC Conversations Dataset

## Dataset Description

The AAC Conversations Dataset is a collection of simulated conversations involving Augmentative and Alternative Communication (AAC) users across multiple languages. This dataset is designed to help researchers and developers build better assistive technologies for people who use AAC devices.

### Dataset Summary

This dataset contains conversations between AAC users and communication partners in various scenarios. Each conversation includes both the original utterances and various augmented versions that simulate different types of typing errors that commonly occur when using AAC devices. The dataset supports multiple languages, making it valuable for developing multilingual assistive technologies.

## Dataset Structure

### Data Instances

Each instance in the dataset represents a single utterance from an AAC user, along with context from the conversation and various augmented versions of the utterance.

Example:
```json
{
  ""conversation_id"": 42,
  ""turn_number"": 2,
  ""language_code"": ""en-GB"",
  ""scene"": ""At a doctor's appointment"",
  ""context_speakers"": [""Doctor"", ""Patient (AAC)""],
  ""context_utterances"": [""How have you been feeling lately?"", ""Not great""],
  ""speaker"": ""Patient (AAC)"",
  ""utterance"": ""I've been having trouble sleeping"",
  ""utterance_intended"": ""I've been having trouble sleeping"",
  ""next_turn_speaker"": ""Doctor"",
  ""next_turn_utterance"": ""How long has this been going on?"",
  ""model"": ""gpt-4o-mini"",
  ""provider"": ""openai"",
  ""batch_id"": ""batch_682477c828bc81909f580a018af3a06c"",
  ""batch_number"": 3,
  ""noisy_qwerty_minimal"": ""I've been having troubke sleeping"",
  ""noisy_qwerty_light"": ""I've been havng troble sleepng"",
  ""noisy_qwerty_moderate"": ""I've ben havin troble sleping"",
  ""noisy_qwerty_severe"": ""Ive ben havin trble slping"",
  ""noisy_abc_minimal"": ""I've been having troubke sleeping"",
  ""noisy_abc_light"": ""I've been havng troble sleepng"",
  ""noisy_abc_moderate"": ""I've ben havin troble sleping"",
  ""noisy_abc_severe"": ""Ive ben havin trble slping"",
  ""noisy_frequency_minimal"": ""I've been having troubke sleeping"",
  ""noisy_frequency_light"": ""I've been havng troble sleepng"",
  ""noisy_frequency_moderate"": ""I've ben havin troble sleping"",
  ""noisy_frequency_severe"": ""Ive ben havin trble slping"",
  ""minimally_corrected"": ""I've been having trouble sleeping."",
  ""fully_corrected"": ""I've been having trouble sleeping.""
}
```

### Data Fields

#### Conversation Structure Fields
- `conversation_id`: Unique identifier for each conversation
- `turn_number`: The position of this utterance in the conversation
- `language_code`: The language code of the conversation (e.g., ""en-GB"", ""fr-FR"")
- `scene`: Description of the conversation setting
- `model`: The model used to generate the conversation (e.g., ""gpt-4o-mini"", ""gpt-4.1-mini-2025-04-14"")
- `provider`: The provider of the model (e.g., ""openai"", ""unknown"")
- `batch_id`: Identifier for the batch of data (e.g., ""v1"" for older data, specific batch IDs for newer data)
- `batch_number`: Batch number (0 for v1 dataset, 1-3 for newer batches)

#### Speaker and Utterance Fields
- `speaker`: The speaker of the current utterance
- `utterance`: The original utterance as typed by the AAC user
- `utterance_intended`: The intended utterance (what the user meant to type)

#### Context Fields (Flattened for Better Usability)
- `context_speakers`: List of speakers for the previous turns (up to 3)
- `context_utterances`: List of utterances for the previous turns (up to 3)
- `next_turn_speaker`: Speaker of the next turn in the conversation
- `next_turn_utterance`: Utterance of the next turn in the conversation

#### Augmented Utterance Fields
- `noisy_qwerty_minimal`: Utterance with minimal typing errors based on QWERTY keyboard layout
- `noisy_qwerty_light`: Utterance with light typing errors based on QWERTY keyboard layout
- `noisy_qwerty_moderate`: Utterance with moderate typing errors based on QWERTY keyboard layout
- `noisy_qwerty_severe`: Utterance with severe typing errors based on QWERTY keyboard layout
- `noisy_abc_minimal`: Utterance with minimal typing errors based on ABC keyboard layout
- `noisy_abc_light`: Utterance with light typing errors based on ABC keyboard layout
- `noisy_abc_moderate`: Utterance with moderate typing errors based on ABC keyboard layout
- `noisy_abc_severe`: Utterance with severe typing errors based on ABC keyboard layout
- `noisy_frequency_minimal`: Utterance with minimal typing errors based on frequency keyboard layout
- `noisy_frequency_light`: Utterance with light typing errors based on frequency keyboard layout
- `noisy_frequency_moderate`: Utterance with moderate typing errors based on frequency keyboard layout
- `noisy_frequency_severe`: Utterance with severe typing errors based on frequency keyboard layout
- `minimally_corrected`: Minimally corrected version of the utterance
- `fully_corrected`: Fully corrected version of the utterance

### Languages

The dataset includes conversations in 39 languages:

**European Languages**
- English (en-GB, en-US, en-CA, en-AU, en-NZ, en-ZA)
- French (fr-FR, fr-CA)
- German (de-DE, de-AT)
- Spanish (es-ES, es-US)
- Italian (it-IT)
- Dutch (nl-NL, nl-BE)
- Greek (el-GR)
- Russian (ru-RU)
- Portuguese (pt-PT, pt-BR)
- Welsh (cy-GB)
- Irish (ga-IE)
- Norwegian (nb-NO)
- Swedish (sv-SE)
- Danish (da-DK)
- Finnish (fi-FI)
- Czech (cs-CZ)
- Slovak (sk-SK)
- Polish (pl-PL)
- Hungarian (hu-HU)
- Slovenian (sl-SI)
- Croatian (hr-HR)
- Ukrainian (uk-UA)
- Basque (eu-ES)
- Catalan (ca-ES)
- Faroese (fo-FO)
- Afrikaans (af-ZA)

**Middle Eastern & Asian Languages**
- Hebrew (he-IL)
- Arabic (ar-SA)
- Japanese (ja-JP)
- Chinese (zh-CN)
- Korean (ko-KR)

### Dataset Statistics

The dataset contains:
- Over 86,000 examples across train and test splits
- 39 languages represented
- Approximately 300 conversations per language
- Multiple batches of data (v1 and batches 1-3)

The dataset is split into:
- Train split: ~68,800 examples
- Test split: ~17,200 examples

**MLU**: Mean Length of Utterance (average number of words per utterance)

## Dataset Creation

### Curation Rationale

AAC users often experience challenges with text entry that can lead to typing errors. This dataset was created to help develop and evaluate technologies that can assist AAC users by correcting typing errors, predicting text, and improving communication efficiency across multiple languages.

### Source Data

The conversations in this dataset are simulated based on common scenarios that AAC users might encounter in daily life, including medical appointments, social interactions, educational settings, and more.

### Generation Process

The dataset was created through a multi-step process:

1. **Template Creation**: Conversation templates were created with diverse scenarios relevant to AAC users
2. **Generation**: Conversations were generated using OpenAI's GPT-4o-mini model to create realistic AAC interactions
3. **Transformation**: Raw outputs were transformed into a structured format
4. **Augmentation**: AAC utterances were augmented with various noise levels and keyboard layouts
5. **Correction**: Both minimal and full corrections were added to each AAC utterance
6. **Multilingual Expansion**: Templates were translated and adapted for 39 languages

Each conversation in the dataset includes metadata about which model and provider were used to generate it (model=""gpt-4o-mini"", provider=""openai"" for most conversations, with some older conversations using different models), as well as batch information that allows tracking the dataset version.

### Annotations

The dataset includes several types of augmented utterances that simulate typing errors:

- **Error Rates**:
  - Minimal: 5% errors - very mild typing issues
  - Light: 15% errors - noticeable but clearly readable
  - Moderate: 25% errors - challenging but comprehensible
  - Severe: 35% errors - significant difficulty

- **Keyboard Layouts**:
  - QWERTY: Standard keyboard layout
  - ABC: Alphabetical keyboard layout
  - Frequency: Layout based on letter frequency

Each language uses appropriate keyboard layouts and letter frequencies for that language.

### Personal and Sensitive Information

This dataset does not contain any personal or sensitive information. All conversations are simulated and do not represent real individuals.

## Potential Uses

This dataset can be used for a variety of NLP tasks related to AAC:

1. **AAC Utterance Correction**: Train models to correct noisy AAC input
2. **Telegraphic Speech Expansion**: Expand telegraphic AAC utterances into grammatically complete sentences
3. **AAC Response Prediction**: Predict appropriate responses to AAC utterances
4. **AAC Interface Optimization**: Study error patterns across different keyboard layouts
5. **Multilingual Assistive Technology**: Develop assistive technologies that work across multiple languages
6. **Cross-lingual Transfer Learning**: Explore how models trained on one language can be adapted to others

## Considerations for Using the Data

### Social Impact of Dataset

This dataset aims to improve assistive technologies for people who use AAC devices, potentially enhancing their communication abilities and quality of life across multiple languages and cultures.

### Discussion of Biases

The dataset attempts to represent diverse scenarios and contexts, but may not capture all the nuances of real AAC user experiences. Users of this dataset should be aware of potential biases in the simulated conversations.

### Other Known Limitations

- The typing errors are generated algorithmically and may not perfectly represent the patterns of errors that real AAC users make
- Some languages have more comprehensive support than others
- The dataset focuses primarily on text-based communication and does not include symbol-based AAC

## Additional Information

### Licensing Information

This dataset is licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.

### Citation Information

If you use this dataset in your research, please cite:

```
@dataset{aac_conversations_dataset,
  author = {Wade, Will},
  title = {AAC Conversations Dataset},
  year = {2024},
  publisher = {Hugging Face},
  url = {https://huggingface.co/datasets/willwade/AACConversations}
}
```

### Contributions

Thanks to all who contributed to the creation of this dataset! Special thanks to the AAC community for their insights and guidance.

## How to Use

Here's a simple example of how to load and explore the dataset:

```python
from datasets import load_dataset

# Load the dataset
dataset = load_dataset(""willwade/AACConversations"")

# Access the train and test splits
train_data = dataset[""train""]
test_data = dataset[""test""]

# Print the first example
print(train_data[0])

# Filter examples by language
english_examples = train_data.filter(lambda example: example[""language_code""] == ""en-GB"")
print(f""Number of English examples: {len(english_examples)}"")

# Example of a task: AAC utterance correction
for example in train_data.select(range(5)):
    print(f""Original: {example['noisy_qwerty_moderate']}"")
    print(f""Corrected: {example['fully_corrected']}"")
    print()
```

### Retrieving Complete Conversations

To retrieve all utterances from a specific conversation, you can use the following function:

```python
def get_complete_conversation(dataset, conversation_id):
    """"""
    Retrieve all utterances from a specific conversation.

    Args:
        dataset: The Hugging Face dataset
        conversation_id: The ID of the conversation to retrieve

    Returns:
        A list of utterances in the conversation, sorted by turn_number
    """"""
    # Filter the dataset to get all utterances from the conversation
    conversation = dataset.filter(lambda example: example[""conversation_id""] == conversation_id)

    # Convert to list and sort by turn_number
    conversation_list = sorted(
        list(conversation), key=lambda x: x[""turn_number""]
    )

    return conversation_list

# Example usage
conversation_id = 1  # Replace with the desired conversation ID
conversation = get_complete_conversation(train_data, conversation_id)

# Print the conversation
print(f""Conversation {conversation_id}:"")
for turn in conversation:
    speaker = turn[""speaker""]
    utterance = turn[""utterance""]
    print(f""{speaker}: {utterance}"")
```

If you're working with pandas DataFrames (e.g., when loading from CSV files locally), you can use:

```python
def get_complete_conversation_pandas(df, conversation_id):
    """"""Get all utterances from a specific conversation using pandas.""""""
    # Filter the dataframe to get all utterances from the conversation
    conversation = df[df[""conversation_id""] == conversation_id]

    # Sort by turn_number
    conversation_sorted = conversation.sort_values(by=""turn_number"")

    return conversation_sorted

# Example usage with pandas
import pandas as pd
df = pd.read_csv(""huggingface/data/aac_dataset_merged/train/dataset.csv"")
conversation = get_complete_conversation_pandas(df, 114)

# Print the conversation
print(f""Conversation {conversation['conversation_id'].iloc[0]}"")
print(f""Language: {conversation['language_code'].iloc[0]}"")
print(f""Scene: {conversation['scene'].iloc[0]}"")

for _, turn in conversation.iterrows():
    speaker = turn[""speaker""]
    utterance = turn[""utterance""]
    print(f""{speaker}: {utterance}"")
```

### Filtering by Language, Model, Provider, or Batch

You can filter the dataset by language, model, provider, or batch ID:

```python
# Filter by language
english_examples = train_data.filter(lambda example: example[""language_code""] == ""en-GB"")
print(f""Number of English examples: {len(english_examples)}"")

# Filter by model
gpt4o_examples = train_data.filter(lambda example: example[""model""] == ""gpt-4o-mini"")
print(f""Number of examples generated with GPT-4o-mini: {len(gpt4o_examples)}"")

# Filter by provider
openai_examples = train_data.filter(lambda example: example[""provider""] == ""openai"")
print(f""Number of examples from OpenAI: {len(openai_examples)}"")

# Filter by batch
v1_examples = train_data.filter(lambda example: example[""batch_id""] == ""v1"")
print(f""Number of examples from batch v1: {len(v1_examples)}"")

# Combine filters
english_gpt4o_examples = train_data.filter(
    lambda example: example[""language_code""] == ""en-GB"" and example[""model""] == ""gpt-4o-mini""
)
print(f""Number of English examples generated with GPT-4o-mini: {len(english_gpt4o_examples)}"")
```

With pandas:

```python
import pandas as pd
df = pd.read_csv(""huggingface/data/aac_dataset_merged/train/dataset.csv"")

# Filter by language
english_examples = df[df[""language_code""] == ""en-GB""]
print(f""Number of English examples: {len(english_examples)}"")

# Filter by model
gpt4o_examples = df[df[""model""] == ""gpt-4o-mini""]
print(f""Number of examples generated with GPT-4o-mini: {len(gpt4o_examples)}"")

# Filter by provider
openai_examples = df[df[""provider""] == ""openai""]
print(f""Number of examples from OpenAI: {len(openai_examples)}"")

# Filter by batch
v1_examples = df[df[""batch_id""] == ""v1""]
print(f""Number of examples from batch v1: {len(v1_examples)}"")

# Combine filters
english_gpt4o_examples = df[
    (df[""language_code""] == ""en-GB"") & (df[""model""] == ""gpt-4o-mini"")
]
print(f""Number of English examples generated with GPT-4o-mini: {len(english_gpt4o_examples)}"")
```

### Example Scripts

The repository includes example scripts in the `examples` directory that demonstrate how to work with the dataset:

1. `example_get_conversation.py`: Demonstrates how to retrieve and display a complete conversation
2. `example_filter_dataset.py`: Demonstrates how to filter the dataset by language, model, provider, and batch ID

You can download and run these scripts locally:

```bash
git clone https://huggingface.co/datasets/willwade/AACConversations
cd AACConversations/examples

# Run the example scripts
python example_get_conversation.py
python example_filter_dataset.py --language en-GB
```

See the README in the examples directory for more information.

### Accessing Raw Data

The raw data files used to create this dataset are included in the repository in the `batch_files` directory. Each language has its own subdirectory containing:

1. Original batch files: `batch_output_*.jsonl`
2. Transformed files: `*_transformed.jsonl`
3. Augmented files: `*_augmented.jsonl`

You can access these files directly from the repository or clone the repository to explore them locally:

```bash
git clone https://huggingface.co/datasets/willwade/AACConversations
cd AACConversations/batch_files
```

To explore a specific language's raw data:

```bash
cd batch_files/en-GB
ls -la
```

This will show you all the raw files for English (UK) conversations, including the original batch outputs, transformed files, and augmented files.
",High,6.0
Dialog/Conversation,astroa7m/Conversational_AOU_tutor_dataset,0.0,11.0,2025-06-27 00:23:23+00:00,mit,0.0,2.69 MB,2820669.44,686 KB,702464.0,1440,none,none,"---
license: mit
task_categories:
- text-generation
language:
- ar
- en
tags:
- education
- chatbot
---",Low,1.0
Reasoning & Multi-step Thinking ,MohammedNasser/ARabic_Reasoning_QA,5.0,11.0,2024-09-07 23:00:07+00:00,apache-2.0,1.0,51.5 kB,unknown,51.5 kB,unknown,1000,none,none,"---
language:
- ar
license: apache-2.0
size_categories:
- 1K<n<10K
task_categories:
- question-answering
- text-generation
pretty_name: arabic_reasoning_qa_ds
dataset_info:
  features:
  - name: Question
    dtype: string
  - name: Answer
    dtype: int16
  splits:
  - name: train
    num_bytes: 84363
    num_examples: 715
  - name: eval
    num_bytes: 21034
    num_examples: 185
  - name: test
    num_bytes: 11585
    num_examples: 100
  download_size: 51525
  dataset_size: 116982
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: eval
    path: data/eval-*
  - split: test
    path: data/test-*
---

---
**language:** 🌐 Arabic  |  **license:** apache-2.0  |  **datasets:** 📚 ARabic Reasoning QA  |  **task_categories:** 🔍 Question Answering  |  **task_ids:** 🧠 Reasoning

---

# ARabic Reasoning QA

## Dataset Description

The **ARabic Reasoning QA** dataset consists of Arabic reasoning questions designed to test various levels of reasoning skills. It includes questions with varying complexity levels: beginner, intermediate, and advanced. This dataset is suitable for training and evaluating models on reasoning tasks in Arabic.

## Dataset Structure 🏗️


The dataset is organized into the following splits:
- **train**: Training data.
- **eval**: Evaluation data.
- **test**: Testing data.

### Data Fields

- **Question**: The reasoning question in Arabic.
- **Answer**: The integer answer to the question.

### Example

Here are some examples of the dataset:

**Example 1:**
- **Question**: لدي أربعة كتب وثلاثة دفاتر. كم عدد الأشياء التي لدي؟
- **Answer**: 7

**Example 2:**
- **Question**: إذا كان لديك ثلاث سيارات، وأعطيت واحدة منها، كم سيارة ستبقى لديك؟
- **Answer**: 2

**Example 3:**
- **Question**: كم عدد الأصابع في يد واحدة؟
- **Answer**: 5

## Usage

To use this dataset, you can load it via the Hugging Face `datasets` library. Here's an example of how to load the dataset:

```python
from datasets import load_dataset

dataset = load_dataset('MohammedNasser/ARabic_Reasoning_QA')
```

## License

This dataset is licensed under the Apache 2.0 License.

## Citation

If you use this dataset in your research, please cite it as follows:

```
@article{arabic_reasoning_benchmarking,
  title={Benchmarking Arabic Reasoning Models: Utilizing the ARabic Reasoning QA Dataset},
  author={M. N. Gaber},
  year={2024},
  publisher={Hugging Face Hub},
  url={https://huggingface.co/MohammedNasser/ARabic_Reasoning_QA}
}
```
```

This version reflects the Apache 2.0 license and includes all relevant details for users to properly cite and use the dataset.",High,6.0
Reasoning & Multi-step Thinking ,MohammedNasser/Arabic_Reasoning_Instruct_QA,2.0,61.0,2024-09-10 07:30:48+00:00,apache-2.0,1.0,112 KB,114688.0,112 KB,114688.0,1725,none,none,"---
language:
- ar
license: apache-2.0
size_categories:
- 1K<n<10K
task_categories:
- question-answering
- text2text-generation
pretty_name: ar_res_inst_qa
dataset_info:
  features:
  - name: instruction
    dtype: string
  - name: answer
    dtype: string
  splits:
  - name: train
    num_bytes: 741840
    num_examples: 1233
  - name: eval
    num_bytes: 187368
    num_examples: 319
  - name: test
    num_bytes: 101427
    num_examples: 173
  download_size: 112151
  dataset_size: 1030635
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
  - split: eval
    path: data/eval-*
  - split: test
    path: data/test-*
---
",Medium,2.0
Reasoning & Multi-step Thinking ,Omartificial-Intelligence-Space/Arabic_Reasoning_Dataset,9.0,75.0,2024-12-01 08:13:05+00:00,apache-2.0,4.0,2.92 MB,3061841.92,2.92 MB,3061841.92,9210,none,none,"---
dataset_info:
  features:
  - name: instruction
    dtype: string
  - name: answer
    dtype: string
  splits:
  - name: train
    num_bytes: 8892310
    num_examples: 9210
  download_size: 2922001
  dataset_size: 8892310
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: apache-2.0
language:
- ar
tags:
- datasets
size_categories:
- 1K<n<10K
---

# Arabic Reasoning Instruction QA Dataset

## Dataset Summary

This dataset contains **9.21K rows** of Arabic instruction-based reasoning QA pairs. It is a comprehensive collection of data points, meticulously crafted to enhance Arabic language reasoning capabilities for models.

The dataset was generated by combining:

♦︎ Data from the Hugging Face dataset [MohammedNasser/Arabic_Reasoning_Instruct_QA](MohammedNasser/Arabic_Reasoning_Instruct_QA). | **Size = 1.23K instructions**

♦︎ Synthetic data generated using the GPT-4o-Mini API with prompting engineering techniques to produce high-quality Arabic reasoning QA pairs. | **Size = 5.69K**

♦︎ Data from the Hugging Face dataset [arcee-globe/filtered-arabic-distilabel-math-preference-orpo](https://huggingface.co/datasets/arcee-globe/filtered-arabic-distilabel-math-preference-orpo?row=0) where it is filtred and combined with other | **Size = 1.96K**

## Dataset Structure

Columns:

♦︎ instruction: The instruction or question that the model must address.

♦︎ answer: The answer or reasoning process associated with the instruction.

## Processing Steps

♦︎ Combined data from the original dataset, synthetic data, and handcrafted data.

♦︎ Applied consistent formatting to ensure uniformity across instruction and answer fields.

♦︎ Cleaned rows to remove redundant or improperly formatted entries.

## Intended Use

This dataset is designed to:

♦︎ Train and fine-tune Arabic large language models on reasoning tasks.

♦︎ Evaluate reasoning and instruction-following performance in Arabic.

## Caveats and Limitations

♦︎ Synthetic Nature: A significant portion of the dataset is generated by GPT-4o-Mini, which may carry biases inherent to the model.

♦︎ Coverage: While diverse, the dataset may not cover all possible reasoning domains in Arabic.

♦︎ Quality: Efforts have been made to ensure high quality, but manual validation is limited.

## Citation

If you use this dataset, please cite as:

```bibtext
@misc{arabic_reasoning_instruct_qa,
  author = {Omer Nacar},
  title = {Arabic Reasoning Instruction QA Dataset},
  year = {2024},
  howpublished = {https://huggingface.co/datasets/Omartificial-Intelligence-Space/Arabic_Reasoning_Dataset},
  note = {A dataset combining Arabic reasoning QA data for NLP tasks.}
}
```",High,4.0
Reasoning & Multi-step Thinking ,lightblue/reasoning-multilingual-R1-Llama-70B-train,36.0,362.0,2025-01-31 07:04:20+00:00,apache-2.0,21.0,9.4 MB,9856614.4,9.4 MB,9856614.4,2477,none,none,"---
dataset_info:
  features:
  - name: en_prompt
    dtype: string
  - name: dataset_name
    dtype: string
  - name: language
    dtype: string
  - name: translated_prompt_raw
    dtype: string
  - name: num_tokens
    dtype: int64
  - name: translated_prompt
    dtype: string
  - name: is_valid_prompt
    dtype: bool
  - name: row_id
    dtype: int64
  - name: res_finish_reason
    dtype: string
  - name: response
    dtype: string
  - name: is_valid_think
    dtype: bool
  - name: is_valid_res
    dtype: bool
  - name: think_lang
    dtype: string
  - name: res_lang
    dtype: string
  - name: think_fluency
    dtype: float64
  - name: prompt_think_lang_correct
    dtype: bool
  - name: answer_correctness
    dtype: float64
  - name: conversations
    list:
    - name: from
      dtype: string
    - name: value
      dtype: string
  splits:
  - name: train
    num_bytes: 23131354
    num_examples: 2477
  download_size: 9404172
  dataset_size: 23131354
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
license: apache-2.0
language:
- am
- ar
- bn
- zh
- cs
- nl
- en
- fr
- de
- el
- ha
- he
- hi
- id
- it
- ja
- jv
- km
- ko
- lo
- ms
- mr
- fa
- pl
- pt
- ro
- ru
- es
- sw
- sv
- tl
- ta
- te
- th
- tr
- uk
- ur
- vi
---

# lightblue/reasoning-multilingual-R1-Llama-70B-train

This is a multilingual reasoning dataset covering more than 30 languages.

This dataset was made by:

1. Sampling prompts from English datasets and translating them to various languages
2. Generating responses to these prompts 8 times using deepseek-ai/DeepSeek-R1-Distill-Llama-70B
3. Filtering out \<think\> sections with incorrect language, non-fluent language, and incorrect answers

This dataset was then used to train a multilingual reasoning finetune of the [R1 distills](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d).

The reason we made this dataset was that we found it incredibly difficult to consistently generate \<think\> sections (Chain-of-Though processes) in the language that the R1 model was prompted. 
Therefore, we generated many responses using one of these models and removed the ones which were not in the original language of the prompt. 
This serves to make these models more understandable, more interpretable, more accountable, and more transparent to users of these LLMs outside of English and Chinese.

# Code for making this dataset

### 1. Sample prompts from English datasets and translate them

```python
from FlagEmbedding import BGEM3FlagModel
from datasets import load_dataset, concatenate_datasets
import numpy as np
import torch
import math
import random
from openai import OpenAI
from google.colab import userdata

def sample_datasets():
    argilla_ds = load_dataset(""argilla/distilabel-reasoning-prompts"", split=""train"").map(
        lambda x: {""en_prompt"": x[""instructions""], ""dataset_name"": ""distilabel-reasoning-prompts""},
        num_proc=12
    )
    
    oasst_ds = load_dataset(""OpenAssistant/oasst2"", split=""train"").filter(
        lambda x: x[""parent_id""] is None, num_proc=12
    ).filter(
        lambda x: x[""lang""] == ""en"", num_proc=12
    ).map(
        lambda x: {""en_prompt"": x[""text""], ""dataset_name"": ""oasst2""},
        num_proc=12
    ).shuffle().select(
        range(1000)
    )
    
    hfcode_ds = load_dataset(""HuggingFaceH4/code_alpaca_20_k"", split=""train"").map(
        lambda x: {""en_prompt"": x[""prompt""], ""dataset_name"": ""CodeAlpaca_20K""},
        num_proc=12
    ).shuffle().select(
        range(1000)
    )
    
    ds = concatenate_datasets([
        hfcode_ds,
        oasst_ds,
        argilla_ds,
    ]).select_columns([""en_prompt"", ""dataset_name""])
    
    ds = ds.map(lambda x: {
        ""en_prompt"": x[""en_prompt""].strip()
    })

    return ds

def deduplicate_dataset(ds):
    model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
    
    embeddings = torch.Tensor(
        model.encode(ds[""en_prompt""])['dense_vecs']
    ).to(torch.device(""cuda""))

    # Remove the second instance of any row pairs which at least 0.9 cosine similarity
    sims = (embeddings @ embeddings.T).cpu().numpy()
    np.fill_diagonal(sims, 0)
    rm_idx = set([max(x) for x in zip(*np.where(sims >= 0.9))])
    ds = ds.select([i for i in range(len(ds)) if i not in rm_idx])

    return ds

def add_languages_to_ds(ds):

    unique_languages = [
        ""English"", ""Chinese"",
        ""Swahili"", ""Hausa"", ""Yoruba"", ""Telugu"", ""Tamil"", ""Marathi"", ""Javanese"", ""Punjabi"", # Not in original Qwen training
        ""Amharic"", ""Ukrainian"", ""Swedish"", ""Greek"", ""Romanian"", # Not in original Qwen training
        ""German"", ""French"", ""Spanish"", ""Portuguese"", ""Italian"", ""Dutch"",
        ""Russian"", ""Czech"", ""Polish"",
        ""Arabic"", ""Persian"", ""Hebrew"", ""Turkish"",
        ""Japanese"", ""Korean"",
        ""Vietnamese"", ""Thai"", ""Indonesian"", ""Malay"", ""Lao"", ""Burmese"", ""Cebuano"", ""Khmer"", ""Tagalog"",
        ""Hindi"", ""Bengali"", ""Urdu""
    ]

    rows_per_language = math.floor(len(ds) / len(unique_languages))
    
    language_list = []
    for unique_language in unique_languages:
        language_list.extend([unique_language] * int(rows_per_language))
    
    language_list = language_list + random.sample(unique_languages, len(ds) - len(language_list))
    
    ds = ds.shuffle().add_column(""language"", language_list)

    return ds

client = OpenAI(api_key=userdata.get(""OPENAI_API_KEY""))

def translate_prompt(row, max_completion_tokens=500):
    
    language = row[""language""]

    if language == ""English"":
        return {
            ""translated_prompt_raw"": row[""en_prompt""],
            ""num_tokens"": -1
        }

    translation_system_message = """"""You are a prompt translation AI. Given a target language and a prompt in English, translate it into an prompt in the target language. Surround the translation in <translation></translation> tags.""""""

    translation_examples = [
        {""role"": ""user"", ""content"": ""<Japanese>\nWhat is the eccentricity of the ellipse $\frac{x^{2}}{3}+\frac{y^{2}}{4}=1$?""},
        {""role"": ""assistant"", ""content"": ""<translation>\n楕円$\frac{x^{2}}{3}+\frac{y^{2}}{4}=1$の離心率はいくつですか？\n</translation>""},
        {""role"": ""user"", ""content"": ""<French>\nContrast the implications of two eigenvectors being orthogonal versus being linearly independent.""},
        {""role"": ""assistant"", ""content"": ""<translation>\nComparez les implications lorsque deux vecteurs propres sont orthogonaux par rapport à lorsqu’ils ne sont que linéairement indépendants.\n</translation>""},
        {""role"": ""user"", ""content"": ""<German>\nHow many cells are there in the human body?""},
        {""role"": ""assistant"", ""content"": ""<translation>\nWie viele Zellen gibt es im menschlichen Körper?\n</translation>""},
    ]


    response = client.chat.completions.create(
      model=""gpt-4o-2024-11-20"",
      messages=[
        {
          ""role"": ""system"",
          ""content"": [
            {
              ""type"": ""text"",
              ""text"": translation_system_message
            }
          ]
        }]+translation_examples+[
        {
          ""role"": ""user"",
          ""content"": [
            {
              ""type"": ""text"",
              ""text"": ""<"" + language + "">\n""+ row[""en_prompt""]
            }
          ]
        }
      ],
      response_format={
        ""type"": ""text""
      },
      temperature=0.2,
      max_completion_tokens=max_completion_tokens,
      top_p=1,
      frequency_penalty=0,
      presence_penalty=0
    )

    if response.choices[0].finish_reason == ""stop"":
        return {
          ""translated_prompt_raw"": response.choices[0].message.content,
          ""num_tokens"": response.usage.completion_tokens
        }
    else:
        return {
          ""translated_prompt_raw"": None,
          ""num_tokens"": -1
        }

def process_prompt(prompt_raw):
    if prompt_raw is None:
        return None

    prompt_raw = prompt_raw.strip()
    
    if not prompt_raw.startswith(""<translation>"") or not prompt_raw.endswith(""</translation>""):
        return None
    
    return prompt_raw[len(""<translation>""):-len(""</translation>"")].strip()


ds = sample_datasets()

ds = add_languages_to_ds(ds)

ds = ds.map(
    lambda x: translate_prompt(x),
    num_proc=64
)

ds = ds.map(lambda x: {
    ""translated_prompt"": x[""translated_prompt_raw""] if x[""language""] == ""English"" else process_prompt(x[""translated_prompt_raw""])
})

ds = ds.map(lambda x: {
    ""is_valid_prompt"": isinstance(
            x[""translated_prompt""], str
        ) and bool(
            x[""num_tokens""] < 250
        )
})

ds.push_to_hub(
    ""lightblue/reasoning-multilingual"",
    split=""train"",
    private=True
)

```

### 2. Generate responses from deepseek-ai/DeepSeek-R1-Distill-Llama-70B

```python
from datasets import load_dataset, concatenate_datasets
from vllm import LLM, SamplingParams

examples = [
    {""role"": ""user"", ""content"": ""Wie viele Zellen gibt es im menschlichen Körper?\n<Deutsch>""},
    {""role"": ""assistant"", ""content"": """"""<think>
Die Frage ist, wie viele Zellen im menschlichen Körper existieren. Um dies zu beantworten, muss ich verschiedene Faktoren berücksichtigen.

Zunächst denke ich an die Größe eines durchschnittlichen Menschen. Ein Erwachsener wiegt etwa 70 Kilogramm. Davon entfallen ungefähr 30 Kilogramm auf Wasser, was bedeutet, dass der Rest aus Zellen und anderen Materialien besteht.

Als nächstes frage ich mich, wie viel eine einzelne Zelle wiegt. Die meisten Zellen sind mikroskopisch klein, und ich vermute, dass eine typische Zelle etwa 1 Nanogramm wiegt. Das ist 10^-9 Gramm.

Mit diesen Zahlen kann ich nun eine grobe Schätzung vornehmen. Wenn ich die Gesamtmasse der Zellen durch das Gewicht einer einzelnen Zelle teile, erhalte ich die Anzahl der Zellen. Das ergibt ungefähr 30.000 bis 40.000 Milliarden Zellen.

Ich bin mir bewusst, dass dies eine grobe Schätzung ist, da Zellen unterschiedliche Größen und Gewichte haben können. Dennoch gibt mir das eine Vorstellung davon, wie viele Zellen der menschliche Körper enthält.
</think>

**Antwort:**

Der menschliche Körper besteht aus ungefähr 30 bis 40 Billionen Zellen.""""""},
    {""role"": ""user"", ""content"": ""Le soi est-il une illusion ?\n<Français>""},
    {""role"": ""assistant"", ""content"": """"""<think>
Tout d'abord, je dois comprendre ce qu'est le ""soi"". Le soi fait référence à notre sens de l'identité personnelle, à notre conscience de nous-mêmes en tant qu'individus distincts. Cela inclut nos pensées, nos émotions, nos expériences et nos souvenirs, qui nous définissent en tant que personnes uniques.

Ensuite, je me demande pourquoi certaines personnes ou théories considèrent le soi comme une illusion. Peut-être parce que tout ce que nous percevons, y compris nous-mêmes, est construit par notre cerveau à partir de sensations et d'informations externes. Si notre perception de la réalité est subjective et sujette à des distorsions, notre sens du soi pourrait être une construction mentale plutôt qu'une réalité objective.

Je dois aussi penser aux différentes perspectives philosophiques et scientifiques sur cette question. Par exemple, le bouddhisme parle du ""non-soi"" (anatta), suggérant que le concept de soi est une illusion créée par l'ego. D'un point de vue scientifique, les neurosciences montrent que le cerveau construit notre identité à partir de multiples processus cognitifs, ce qui pourrait soutenir l'idée que le soi n'est pas une entité fixe ou essentielle.

Cependant, même si le soi est une construction, il reste une expérience subjective très réelle pour chacun de nous. Même si notre perception de nous-mêmes est influencée par divers facteurs, le sentiment d'exister en tant qu'individu avec des pensées et des émotions propres est inhérent à notre conscience humaine.

En conclusion, le débat sur le soi étant une illusion ou non dépend de la perspective adoptée. D'un côté, il peut être vu comme une construction mentale et sociale, et de l'autre, il représente une expérience intime et essentielle de notre existence.
</think>

Le soi est une question complexe qui a été explorée par divers domaines tels que la philosophie, la psychologie et les neurosciences. Voici une analyse organisée de cette question :

1. **Compréhension du concept de soi**:
   - Le soi se réfère à notre sentiment d'identité personnelle, englobant nos pensées, émotions, expériences et souvenirs. Il s'agit de ce qui nous distingue en tant qu'individus uniques.

2. **Arguments pour considérer le soi comme une illusion**:
   - **Perception subjective**: Notre perception de la réalité est construite par notre cerveau à partir de sensations et d'informations externes, qui peuvent être sujettes à des distorsions. Par conséquent, notre sens du soi pourrait être une construction mentale plutôt qu'une réalité objective.
   - **Influence des facteurs externes**: Les expériences, l'environnement et les interactions sociales influencent notre identité, suggérant que le soi est une construction dynamique plutôt qu'une entité fixe.
   - **Perspectives philosophiques**: Le bouddhisme, par exemple, soutient le concept du ""non-soi"" (anatta), où le soi est perçu comme une illusion créée par l'ego.

3. **Arguments contre l'idée que le soi est une illusion**:
   - **Expérience subjective**: Même si le soi est une construction, il reste une expérience intime et réelle pour chaque individu. Le sentiment d'exister en tant qu'entité distincte avec des pensées et des émotions propres est central à la conscience humaine.
   - **Nécessité fonctionnelle**: Le soi joue un rôle crucial dans la cohérence de notre comportement, la prise de décision et la création de buts personnels.

4. **Perspectives scientifiques**:
   - Les neurosciences montrent que le cerveau construit notre identité à travers de multiples processus cognitifs, ce qui soutient l'idée que le soi est une construction. Cependant, cela ne nie pas l'expérience subjective de l'individu.

5. **Conclusion**:
   Le débat sur le soi étant une illusion ou non dépend de la perspective adoptée. D'un côté, il peut être vu comme une construction mentale et sociale, et de l'autre, il représente une expérience intime et essentielle de notre existence. Ainsi, le soi est à la fois une construction dynamique et une réalité subjective profondément ancrée dans notre conscience.
""""""},
    {""role"": ""user"", ""content"": ""顧客の名前、メールアドレス、住所を収集するHTMLフォームを作成してください。\n<日本語>""},
    {""role"": ""assistant"", ""content"": """"""<think>
まず、必要な情報を収集するために、フォームに名前、メールアドレス、住所のフィールドを含める必要があります。名前はテキスト入力欄、メールアドレスはメール入力欄、住所はテキスト入力欄として設定します。各フィールドにラベルを付けて、ユーザーが入力する内容を明確にします。送信ボタンを追加し、フォームの送信時にデータが正しく送られるようにします。最後に、フォームの整体的なレイアウトを整理し、ユーザーが簡単に使用できるようにします。
</think>

以下に、顧客の名前、メールアドレス、住所を収集するためのHTMLフォームの例を示します。

\`\`\`html
<!DOCTYPE html>
<html lang=""ja"">
<head>
    <meta charset=""UTF-8"">
    <title>顧客情報登録フォーム</title>
</head>
<body>
    <h1>顧客情報登録フォーム</h1>
    
    <form action=""/submit"" method=""POST"">
        <div>
            <label for=""name"">名前：</label>
            <input type=""text"" id=""name"" name=""name"" required>
        </div>
        
        <div>
            <label for=""email"">メールアドレス：</label>
            <input type=""email"" id=""email"" name=""email"" required>
        </div>
        
        <div>
            <label for=""address"">住所：</label>
            <input type=""text"" id=""address"" name=""address"" required>
        </div>
        
        <button type=""submit"">送信</button>
    </form>
</body>
</html>
\`\`\`

### 解説：
1. **フォームの作成**：`<form>`タグでフォームを定義します。`action`属性にフォームデータを送信先のURLを指定し、`method`属性に送信方法（例：POST）を指定します。
2. **名前の入力**：`<input type=""text"">`を使用して名前を入力できるフィールドを作成します。`required`属性を追加して入力必須にします。
3. **メールアドレスの入力**：`<input type=""email"">`を使用してメールアドレスを入力できるフィールドを作成します。`required`属性を追加して入力必須にします。
4. **住所の入力**：`<input type=""text"">`を使用して住所を入力できるフィールドを作成します。`required`属性を追加して入力必須にします。
5. **送信ボタン**：`<button type=""submit"">`を使用して送信ボタンを作成します。

このフォームでは、ユーザーが必要な情報を入力し、送信ボタンを押すと指定されたURLにデータが送信されます。""""""}
]

lang_map = {
    'Amharic': 'አማርኛ',
    'Arabic': 'العربية',
    'Bengali': 'বাংলা',
    'Burmese': 'မြန်မာစာ',
    'Cebuano': 'Binisaya',
    'Chinese': '中文',
    'Czech': 'Čeština',
    'Dutch': 'Nederlands',
    'English': 'English',
    'French': 'Français',
    'German': 'Deutsch',
    'Greek': 'Ελληνικά',
    'Hausa': 'Hausa',
    'Hebrew': 'עברית',
    'Hindi': 'हिन्दी',
    'Indonesian': 'Bahasa Indonesia',
    'Italian': 'Italiano',
    'Japanese': '日本語',
    'Javanese': 'Basa Jawa',
    'Khmer': 'ភាសាខ្មែរ',
    'Korean': '한국어',
    'Lao': 'ພາສາລາວ',
    'Malay': 'Bahasa Melayu',
    'Marathi': 'मराठी',
    'Persian': 'فارسی',
    'Polish': 'Polski',
    'Portuguese': 'Português',
    'Punjabi': 'ਪੰਜਾਬੀ',
    'Romanian': 'Română',
    'Russian': 'Русский',
    'Spanish': 'Español',
    'Swahili': 'Kiswahili',
    'Swedish': 'Svenska',
    'Tagalog': 'Tagalog',
    'Tamil': 'தமிழ்',
    'Telugu': 'తెలుగు',
    'Thai': 'ภาษาไทย',
    'Turkish': 'Türkçe',
    'Ukrainian': 'Українська',
    'Urdu': 'اُردُو',
    'Vietnamese': 'Tiếng Việt',
    'Yoruba': 'Yorùbá'
}


llm = LLM(
    model=""deepseek-ai/DeepSeek-R1-Distill-Llama-70B"", 
    tensor_parallel_size=8,
    enable_prefix_caching=True
)

ds = load_dataset(""lightblue/reasoning-multilingual"", split=""train"")
ds = ds.add_column(""row_id"", list(range(len(ds))))
ds = ds.filter(lambda x: x[""is_valid_prompt""])
repeat_num = 8
cat_ds = concatenate_datasets([ds] * repeat_num)

conversations = [
    [{
        ""role"": ""system"",
        ""content"": f""You receive a prompt and a language tag as inputs. Answer the prompt in the given language, making sure to also think in that language."",
    }] + examples + [{
        ""role"": ""user"",
        ""content"": text + ""\n<"" + lang_map[lang] + "">"",
    }] for lang, text in zip(cat_ds[""language""], cat_ds[""translated_prompt""])
]

sampling_params = SamplingParams(temperature=0.5, max_tokens=2048)

outputs = llm.chat(conversations,
                   sampling_params=sampling_params,
                   continue_final_message=False,
                   add_generation_prompt=True)

finish_reasons = [x.outputs[0].finish_reason for x in outputs]
responses = [x.outputs[0].text for x in outputs]

cat_ds = cat_ds.add_column(""res_finish_reason"", finish_reasons)
cat_ds = cat_ds.add_column(""response"", responses)

cat_ds.push_to_hub(""lightblue/reasoning-multilingual-R1-Llama-70B-multirow"", private=True)
```

### 3. Filter out \<think\> sections with incorrect language and format training data

```python
from datasets import load_dataset
import pycld2 as cld2
import re
from datasets import Dataset
from openai import OpenAI
from google.colab import userdata
import httpx

def remove_markdown_code_blocks(text):
    # Remove code blocks from text to avoid incorrect language detection
    code_block_pattern = r""```.*?```""
    cleaned_text = re.sub(code_block_pattern, '', text, flags=re.DOTALL)
    return cleaned_text

def detect_language(text):
    if text is None:
        return None

    isReliable, textBytesFound, details = cld2.detect(
        remove_markdown_code_blocks(text)
    )

    if not isReliable:
        return None

    return details[0][0].replace(""LAOTHIAN"", ""LAO"")

def extract_think_block(text):
    pattern = r'^\s*<think>(.*?)</think>'
    match = re.search(pattern, text, re.DOTALL)  # re.DOTALL allows '.' to match newlines

    if match:
        return match.group(1).strip()
    else:
        return False

def extract_answer(text):
    if text.count(""</think>"") == 1:
        return text.split(""</think>"")[1].strip()
    else:
        return False

client = OpenAI(timeout=httpx.Timeout(15.0, read=5.0, write=5.0, connect=3.0), api_key=userdata.get(""OPENAI_API_KEY""))

def parse_float(sys_msg, user_input, max_completion_tokens=4):
    response = client.chat.completions.create(
      model=""gpt-4o-mini-2024-07-18"",
      messages=[
        {
          ""role"": ""system"",
          ""content"": [
            {
              ""type"": ""text"",
              ""text"": sys_msg
            }
          ]
        }]+[
        {
          ""role"": ""user"",
          ""content"": [
            {
              ""type"": ""text"",
              ""text"": user_input
            }
          ]
        }
      ],
      response_format={
        ""type"": ""text""
      },
      temperature=0.0,
      max_completion_tokens=max_completion_tokens,
      top_p=1,
      frequency_penalty=0,
      presence_penalty=0
    )

    if response.choices[0].finish_reason == ""stop"":
        try:
            return float(response.choices[0].message.content)
        except:
            return None
    else:
        return None

def evaluate_think_fluency(row):
    language = row[""language""]

    think_block = extract_think_block(row[""response""])

    if not isinstance(think_block, str):
        return None

    fluency_system_message = f""""""You are a {language} fluency evaluation AI. Given a piece of text, give the fluency and naturalness of the {language} in the text a score from 1-5. Only include your final number in your output.""""""

    return parse_float(fluency_system_message, think_block)

def evaluate_correctness(row):
    language = row[""language""]

    prompt = row[""translated_prompt""]
    model_response = row[""response""]

    if not isinstance(extract_think_block(model_response), str):
        return None

    correctness_system_message = f""""""You are a {language} answer evaluation AI. Given a prompt and a chain-of-thought reasoning answer, rate the correctness of the answer with a score from 1-5. Only include your final number in your output.""""""

    return parse_float(correctness_system_message, 
                       ""# Prompt\n"" + prompt + ""\n\n# Response\n"" + model_response)

ds = load_dataset(""lightblue/reasoning-multilingual-R1-Llama-70B-multirow"", split=""train"")

# Validate answers
ds = ds.map(lambda x: {""is_valid_think"": isinstance(extract_think_block(x[""response""]), str)})
ds = ds.map(lambda x: {""is_valid_res"": isinstance(extract_answer(x[""response""]), str)})
ds = ds.map(lambda x: {""think_lang"": detect_language(extract_think_block(x[""response""])) if x[""is_valid_think""] else None})
ds = ds.map(lambda x: {""res_lang"": detect_language(extract_answer(x[""response""])) if x[""is_valid_res""] else None})
ds = ds.map(lambda x: {""think_fluency"": evaluate_think_fluency(x)}, num_proc=32)
ds = ds.map(lambda x: {""answer_correctness"": evaluate_correctness(x)}, num_proc=32)

ds = ds.map(lambda x: {
    ""prompt_think_lang_correct"": bool(
            x[""is_valid_prompt""]
        ) and bool(
            x[""is_valid_think""]
        ) and bool(
            x[""res_finish_reason""] == ""stop""
        ) and bool(
            x[""think_lang""] is not None
        ) and bool(
            x[""language""].lower() == x[""think_lang""].lower()
        ) and bool(
            x[""res_lang""] is not None
        ) and bool(
            x[""language""].lower() == x[""res_lang""].lower()
        ) and bool(
            x[""think_fluency""] == 5
        ) and bool(
            x[""answer_correctness""] == 5
        )
})

df = ds.to_pandas()
selected_df = df.groupby(""row_id"").apply(
    lambda x: x[x.prompt_think_lang_correct].sample(min(1, x.prompt_think_lang_correct.sum()))
)

selected_ds = Dataset.from_pandas(selected_df.reset_index(drop=True))

selected_ds = selected_ds.map(lambda x: {
    ""conversations"": {
        {""from"": ""human"", ""value"": x[""translated_prompt""]},
        {""from"": ""gpt"", ""value"": x[""response""]},
    }
})

selected_ds.push_to_hub(""lightblue/reasoning-multilingual-R1-Llama-70B-train"")
ds.push_to_hub(""lightblue/reasoning-multilingual-R1-Llama-70B-train-nonunique"")
```

# License

We have endeavoured to base our dataset only on source datasets which allow for fully free use. Therefore, we share this dataset with the Apache 2.0 license.

# Developed by

<a href=""https://www.lightblue-tech.com"">
<img src=""https://www.lightblue-tech.com/wp-content/uploads/2023/08/color_%E6%A8%AA%E5%9E%8B-1536x469.png"" alt=""Lightblue technology logo"" width=""400""/>
</a>

This dataset was created by Peter Devine ([ptrdvn](https://huggingface.co/ptrdvn)) for Lightblue",Medium,3.0
Reasoning & Multi-step Thinking ,Jr23xd23/Arabic-Optimized-Reasoning-Dataset,2.0,32.0,2025-02-25 13:29:24+00:00,apache-2.0,0.0,7.45 MB,7811891.2,2.99 MB,3135242.24,1600,none,none,"---
license: apache-2.0
task_categories:
- question-answering
- table-question-answering
language:
- ar
tags:
- reasoning
- dataset
- llm
- fine-tuning
pretty_name: arabic reasoning dataset
size_categories:
- 1K<n<10K
---
# Arabic Optimized Reasoning Dataset

**Dataset Name**: Arabic Optimized Reasoning  
**License**: Apache-2.0  
**Formats**: CSV  
**Size**: 1600 rows  
**Base Dataset**: cognitivecomputations/dolphin-r1  
**Libraries Used**: Datasets, Dask, Croissant

## Overview

The **Arabic Optimized Reasoning Dataset** helps AI models get better at reasoning in Arabic. While AI models are good at many tasks, they often struggle with reasoning in languages other than English. This dataset helps fix this problem by:

1. Using fewer tokens, which makes training more efficient
2. Teaching models to tell simple questions from complex ones that need step-by-step reasoning

The dataset is based on Dolphin-R1 and has 1,600 rows translated to Arabic.

## Dataset Structure

Each entry in the dataset has:
```csv
instruction, input, output
```

* **instruction**: What the model needs to do
* **input**: Information to help answer the question
* **output**: The expected answer

### Tags for Reasoning

The dataset uses tags to mark different types of questions:

* **Simple questions**:
```xml
<think> This query is simple; no detailed reasoning is needed. </think>
```

* **Complex questions**:
```xml
<think> This query is complex and requires multi-step reasoning. </think>
```

### Example:

Here's an example from the dataset:
```csv
""instruction"": ""You are an AI assistant that always begins by assessing whether detailed reasoning is needed before answering..."",
""input"": ""أنت متسلق صخور ذو خبرة قضيت سنوات عديدة في استكشاف جنوب غرب أمريكا واكتشاف بعض من أفضل مسارات التسلق في المنطقة. كيف أصبحت مهتمًا بتسلق الصخور لأول مرة؟"",
""output"": ""<think> This query is simple; no detailed reasoning is needed. </think>
أنت تعلم، لطالما كنت منجذبًا إلى الهواء الطلق والمغامرة. نشأت في أريزونا، وقضيت الكثير من الوقت في التنزه وحمل الحقائب مع عائلتي في الجبال والصحراء. لكن أعتقد أن ما أثار اهتمامي حقًا في التسلق كان رحلة قمت بها إلى سيدونا عندما كنت مراهقًا. كنت أتسلق كاثيدرال روك مع بعض الأصدقاء، ووجدنا مجموعة من المتسلقين يشقون طريقهم إلى أعلى وجه الصخرة. كنت مفتونًا بالطريقة التي يتحركون بها، والتركيز والشدة في أعينهم... لقد أُسرت في تلك اللحظة.""
```

## Key Features

* **Arabic Translation**: Data from Dolphin-R1 translated to Arabic
* **Separates Question Types**: Tells simple questions from complex ones
* **Uses Fewer Tokens**: Makes training more efficient
* **Better Arabic Reasoning**: Specifically made to improve reasoning in Arabic

## Acknowledgments

This dataset is inspired by the Deepseek-R1 reasoning data from `cognitivecomputations/dolphin-r1`. No data from Gemini was used.

## Citation

If you use this dataset, please cite:

```
@software{jaber2025arabic,
  author = {Jaber Jaber, Zaid Rjoub},
  title = {Arabic Optimized Reasoning Dataset},
  url = {https://github.com/jaberjaber23/Arabic-Optimized-Reasoning-Dataset},
  year = {2025}
}
```

## Support

If you find this dataset useful, please star the GitHub repository: [jaberjaber23/Arabic-Optimized-Reasoning-Dataset](https://github.com/jaberjaber23/Arabic-Optimized-Reasoning-Dataset)",High,5.0
Reasoning & Multi-step Thinking ,Pinkstack/OpenHumanreasoning-multilingual-2.2k,2.0,42.0,2025-03-03 17:16:04+00:00,apache-2.0,0.0,2.83 MB,2967470.08,1.46 MB,1530920.96,2296,none,none,"---
license: apache-2.0
language:
- ar
- zh
- cs
- da
- nl
- en
- fi
- fr
- de
- he
- hu
- it
- ja
- ko
- 'no'
- pl
- pt
- ru
- es
- sv
- th
- tr
- uk
task_categories:
- text-generation
tags:
- reasoning
- superthoughts
- cot
size_categories:
- 1K<n<10K
---
Based off of Pinkstackorg/humanreasoning-testing-en, it is a human-generated dataset where a real person had to create most of the reasoning and the final output. If there are any mistakes please let us know.

We offer this dataset at an apache-2.0 license to make it useful for everybody.
note: translations are **not** human generated.",Low,1.0
Reasoning & Multi-step Thinking ,miscovery/Math_CoT_Arabic_English_Reasoning,16.0,137.0,2025-05-12 00:14:13+00:00,mit,1.0,3.34 MB,3502243.84,1.59 MB,1667235.84,2834,none,none,"---
license: mit
task_categories:
- question-answering
- text-generation
- fill-mask
language:
- en
- ar
pretty_name: Math CoT Arabic English Reasoning
size_categories:
- 1K<n<10K
tags:
- code
---


# Math CoT Arabic English Dataset

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A high-quality, bilingual (English & Arabic) dataset for Chain-of-Thought (COT) reasoning in mathematics and related disciplines, developed by Miscovery AI.

## Overview

Math-COT is a unique dataset designed to facilitate and benchmark the development of chain-of-thought reasoning capabilities in language models across mathematical domains. With meticulously crafted examples, explicit reasoning steps, and bilingual support, this dataset offers a robust foundation for training and evaluating mathematical reasoning abilities.

## Key Features

- **99% Clean & High-Quality Data**: Human-reviewed, accurately annotated examples with verified solutions
- **Bilingual Support**: Complete English and Arabic parallel content for cross-lingual research and applications
- **Structured Reasoning Steps**: Each problem solution is broken down into explicit step-by-step reasoning
- **Diverse Subject Coverage**: Spans 21 different categories within mathematics and adjacent fields
- **Comprehensive Format**: Includes questions, answers, reasoning chains, and relevant metadata


## Dataset Structure

Each entry in the dataset contains the following fields:

```
{
  ""en_question"": ""Question text in English"",
  ""ar_question"": ""Question text in Arabic"",
  ""en_answer"": ""Detailed step-by-step solution in English"",
  ""ar_answer"": ""Detailed step-by-step solution in Arabic"",
  ""category"": ""Mathematical category"",
  ""en_q_word"": ""Word count of English question"",
  ""ar_q_word"": ""Word count of Arabic question"",
  ""en_a_word"": ""Word count of English answer"",
  ""ar_a_word"": ""Word count of Arabic answer""
}
```

## Categories

The dataset covers 21 distinct categories:

1. Mathematics - Arithmetic
2. Mathematics - Algebra
3. Mathematics - Geometry
4. Mathematics - Trigonometry
5. Mathematics - Calculus
6. Mathematics - Linear Algebra
7. Mathematics - Probability
8. Mathematics - Statistics
9. Mathematics - Set Theory
10. Mathematics - Number Theory
11. Mathematics - Discrete Math
12. Mathematics - Topology
13. Mathematics - Differential Equations
14. Mathematics - Real Analysis
15. Math Puzzles
16. Linguistics
17. Logic and Reasoning
18. Philosophy
19. Sports and Games
20. Psychology
21. Cultural Traditions

## Example

Here's a sample entry from the dataset:

```json
{
  ""en_question"": ""A bag contains only red and blue balls. If one ball is drawn at random, the probability that it is red is 2/5. If 8 more red balls are added, the probability of drawing a red ball becomes 4/5. How many blue balls are there in the bag?"",
  ""ar_question"": ""تحتوي الحقيبة على كرات حمراء وزرقاء فقط. إذا تم سحب كرة واحدة عشوائيًا ، فإن احتمال أن تكون حمراء هو 2/5. إذا تمت إضافة 8 كرات حمراء أخرى ، يصبح احتمال سحب كرة حمراء 4/5. كم عدد الكرات الزرقاء الموجودة في الحقيبة؟"",
  ""en_answer"": ""#step 1: Let r be the number of red balls and b be the number of blue balls. We have r / (r + b) = 2/5 and (r + 8) / (r + b + 8) = 4/5.\n#step 2: From the first equation, 5r = 2r + 2b => 3r = 2b.\n#step 3: From the second equation, 5(r + 8) = 4(r + b + 8) => 5r + 40 = 4r + 4b + 32 => r + 8 = 4b.\n#step 4: Substituting r = (2/3)b into the second equation, (2/3)b + 8 = 4b => 8 = (10/3)b => b = 24/10 = 12/5. This is wrong, check the math.\n#step 5: r = 2b/3 , and r + 8 = 4b/5. substitute r. 2b/3 + 8 = 4b  => 2b + 24 = 12b => 10b = 24 => b = 12/5. There must be an error.\n#step 6: 5r = 2(r+b), 5(r+8)=4(r+b+8), 5r=2r+2b, 5r+40 = 4r+4b+32 => r+8 = 4b-r, => r = 4b-8, 5(2b/3) + 40 = 4(2b/3) + 32, 10/3 b+40 = 8/3b +32, 2/3b = -8, b =12.\n#step 7: If b=12, r = (2/5)(r+12), 5r = 2r + 24, r = 8. (8+8)/(8+12+8)= 16/28 = 4/7 != 4/5.  The step 6 equations are wrong. r/(r+b) = 2/5, (r+8)/(r+b+8) = 4/5. then 5r = 2r+2b so 3r=2b. Then 5r+40 = 4r+4b+32 so r+8 = 4b-32, 2b/3+8 = 4b/5. so r+8 = 4b+32 so r + 8 = 4/5 (r+b+8), and r+8 = 4/5 r + 4/5b + 32/5, =>1/5r - 4b/5 = -8/5, r-4b = -8, and r = 4b-8. 3r=2b, 3(4b-8) = 2b so 12b - 24 = 2b => 10b =24 so b=2.4\n#step 8: r/(r+b) = 2/5, (r+8)/(r+b+8) = 4/5. So we get 5r = 2r + 2b, meaning 3r = 2b and we get 5(r+8) = 4(r+b+8), meaning 5r + 40 = 4r + 4b + 32, so r + 8 = 4b. substitute b = 3r/2 , r + 8 = 6r so 5r = 8. then r=8/5.\n#step 9: 5r=2r+2b so r =2b/3 and we have (r+8)/(r+b+8) = 4/5. Substituting yields (2b/3+8)/(2b/3+b+8) = 4/5 => (2b+24)/(5b+24) = 4/5 => 10b + 120 = 20b + 96, so 10b = 24, thus b=2.4. This makes no sense since number of balls can't be a fraction. Recalculating.\n#step 10: Let's check the equation (r+8)/(r+b+8)=4/5. If the red ball is 2, and blue ball is 3 (so total is 5 and r/total = 2/5), (r+8)/(total+8) = (10)/(13), !=4/5. Thus the original numbers of red and blue balls does not have to conform to the ratio. Red = 2k, Blue = 3k. (2k+8)/(5k+8) = 4/5 => 10k + 40 = 20k+32 => 10k = 8 => k=4/5. Thus there are 3k=12/5, so this cannot be the solution. If the problem assumes the original solution MUST be an integer number, then something might be off. Let total = t. So red = 2/5 t. The blue is 3/5 t. And (2/5 t + 8)/(t+8) = 4/5, 10/5 t + 40/5 = 4/5 t + 32/5 so 6/5 t = -8/5 . t < 0 which does not make sense.\n#step 11: 5r = 2r+2b and 5(r+8)=4(r+b+8) , so 3r = 2b, and r+8=4(b+8)/5 so 5r+40 = 4b+32, or 5r -4b = -8. substituting 3r/2 for b, we get 5r - 4(3r/2) = -8, so 5r-6r =-8 so -r=-8. Thus r=8. 3r = 2b => 24 = 2b so b=12.\n#answer: 12"",
  ""ar_answer"": ""#الخطوة 1: ليكن r هو عدد الكرات الحمراء و b هو عدد الكرات الزرقاء. لدينا r / (r + b) = 2/5 و (r + 8) / (r + b + 8) = 4/5.\n#الخطوة 2: من المعادلة الأولى، 5r = 2r + 2b => 3r = 2b.\n#الخطوة 3: من المعادلة الثانية، 5(r + 8) = 4(r + b + 8) => 5r + 40 = 4r + 4b + 32 => r + 8 = 4b.\n#الخطوة 4: بالتعويض بـ r = (2/3)b في المعادلة الثانية، (2/3)b + 8 = 4b => 8 = (10/3)b => b = 24/10 = 12/5. هذا خطأ.\n#الخطوة 5: إذا كان r = 2b/3 و r + 8 = 4b/5. نعوض r، 2b/3 + 8 = 4b => 2b + 24 = 12b => 10b = 24 => b = 12/5. يجب أن يكون هناك خطأ.\n#الخطوة 6: 5r = 2(r+b), 5(r+8)=4(r+b+8), 5r=2r+2b, 5r+40 = 4r+4b+32 => r+8 = 4b-r, => r = 4b-8, 5(2b/3) + 40 = 4(2b/3) + 32, 10/3 b+40 = 8/3b +32, 2/3b = -8, b =12.\n#الخطوة 7: إذا كان b=12، r = (2/5)(r+12), 5r = 2r + 24, r = 8. (8+8)/(8+12+8)= 16/28 = 4/7 != 4/5. معادلات الخطوة 6 خاطئة. r/(r+b) = 2/5, (r+8)/(r+b+8) = 4/5. ثم 5r = 2r+2b إذن 3r=2b. ثم 5r+40 = 4r+4b+32 إذن r+8 = 4b-32, 2b/3+8 = 4b/5. إذن r+8 = 4b+32 إذن r + 8 = 4/5 (r+b+8), and r+8 = 4/5 r + 4/5b + 32/5, =>1/5r - 4b/5 = -8/5, r-4b = -8, and r = 4b-8. 3r=2b, 3(4b-8) = 2b إذن 12b - 24 = 2b => 10b =24 إذن b=2.4\n#الخطوة 8: r/(r+b) = 2/5, (r+8)/(r+b+8) = 4/5. لذلك نحصل على 5r = 2r + 2b، مما يعني 3r = 2b ونحصل على 5(r+8) = 4(r+b+8)، مما يعني 5r + 40 = 4r + 4b + 32، إذن r + 8 = 4b. بالتعويض بـ b = 3r/2 , r + 8 = 6r إذن 5r = 8. إذن r=8/5.\n#الخطوة 9: 5r=2r+2b إذن r =2b/3 ولدينا (r+8)/(r+b+8) = 4/5. بالتعويض نحصل على (2b/3+8)/(2b/3+b+8) = 4/5 => (2b+24)/(5b+24) = 4/5 => 10b + 120 = 20b + 96، إذن 10b = 24، بالتالي b=2.4. هذا غير منطقي لأن عدد الكرات لا يمكن أن يكون كسرًا. إعادة الحساب.\n#الخطوة 10: دعنا نتحقق من المعادلة (r+8)/(r+b+8)=4/5. إذا كانت الكرة الحمراء هي 2، والكرة الزرقاء هي 3 (إذن الإجمالي هو 5 وr/الإجمالي = 2/5)، (r+8)/(الإجمالي+8) = (10)/(13)، !=4/5. وبالتالي فإن الأعداد الأصلية للكرات الحمراء والزرقاء لا يجب أن تتوافق مع النسبة. أحمر = 2k، أزرق = 3k. (2k+8)/(5k+8) = 4/5 => 10k + 40 = 20k+32 => 10k = 8 => k=4/5. وبالتالي يوجد 3k=12/5، لذلك لا يمكن أن يكون هذا هو الحل. إذا افترضت المشكلة أن الحل الأصلي يجب أن يكون عددًا صحيحًا، فقد يكون هناك خطأ ما. ليكن الإجمالي = t. إذن الأحمر = 2/5 t. الأزرق هو 3/5 t. و (2/5 t + 8)/(t+8) = 4/5, 10/5 t + 40/5 = 4/5 t + 32/5 إذن 6/5 t = -8/5 . t < 0 وهو أمر غير منطقي.\n#الخطوة 11: 5r = 2r+2b and 5(r+8)=4(r+b+8) , إذن 3r = 2b، و r+8=4(b+8)/5 إذن 5r+40 = 4b+32، أو 5r -4b = -8. بالتعويض بـ 3r/2 عن b، نحصل على 5r - 4(3r/2) = -8، إذن 5r-6r =-8 إذن -r=-8. إذن r=8. 3r = 2b => 24 = 2b إذن b=12.\n#الإجابة: 12"",
  ""category"": ""Math Puzzles"",
  ""en_q_word"": 48,
  ""ar_q_word"": 42,
  ""en_a_word"": 583,
  ""ar_a_word"": 566
}
```

## Usage

This dataset is especially valuable for:

- Training and evaluating mathematical reasoning in language models
- Research on step-by-step problem solving approaches
- Developing educational AI assistants for mathematics
- Cross-lingual research on mathematical reasoning
- Benchmarking Chain-of-Thought (COT) capabilities

## Citation

If you use this dataset in your research, please cite:

```bibtex
@dataset{miscoveryai2025mathcot,
  title={Math_CoT_Arabic_English_Reasoning: A Bilingual Dataset for Chain-of-Thought Mathematical Reasoning},
  author={Miscovery AI},
  year={2025},
  publisher={Hugging Face},
  url={https://huggingface.co/datasets/miscovery/Math_CoT_Arabic_English_Reasoning}
}
```

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Contact

For questions, feedback, or issues related to this dataset, please contact Miscovery AI at [info@miscovery.com](mailto:info@miscovery.com).",High,6.0
Reasoning & Multi-step Thinking ,beetleware/arabic-reasoning-dataset-logic,8.0,118.0,2025-05-21 11:02:11+00:00,mit,0.0,792 KB,811008.0,223 KB,228352.0,1186,none,none,"---
language: ar
license: mit # أو الترخيص الذي اخترته (مثال: cc-by-4.0)
tags:
  - logical-reasoning
  - arabic
  - question-answering
  - text-generation
pretty_name: ""arabic-reasoning-dataset-logic""
dataset_summary: |
  A dataset of Arabic logical reasoning tasks. Each task includes an ID, task type, 
  the task text (question and proposed answer), and a detailed solution 
  with thinking steps and the final answer.
---



# Arabic Logical Reasoning Tasks Dataset (Maximum 1000 Tasks)

## Overview

This dataset comprises a series of logical reasoning tasks designed to evaluate and train artificial intelligence models on understanding and generating logical inferences in the Arabic language. Each task includes a unique identifier, the task type, the task text (a question and a proposed answer), and a detailed solution that outlines the thinking steps and the final answer.

## Data Format

The data is provided in JSON format, where each object in the list represents a single logical task. Each object contains the following fields:

* **`id` (string):** A unique identifier for the task (e.g., `""task_1""`, `""task_2""`). It is noted that the provided sample contains duplicate `id` values, which might require processing or re-assigning identifiers to ensure uniqueness if required.
* **`task_type` (string):** Specifies the type of logical reasoning required for the task. Observed values in the sample include:
    * `""abduction""`: (Abductive reasoning) - Inferring the most plausible explanation for a set of observations.
    * `""induction""`: (Inductive reasoning) - Deducing a general rule from specific examples.
    * `""deduction""`: (Deductive reasoning) - Deriving a specific conclusion from general premises.
* **`task_text` (string):** The actual text of the task, typically consisting of two parts:
    * `- Question:`: The logical question being posed.
    * `- Answer:`: The suggested or expected answer to the question.
* **`solution_text` (string):** A detailed explanation of how to arrive at the solution. This field usually includes:
    * `<think>` and `</think>` tags: Enclosing the chain of logical thought processes that lead to the solution.
    * `<answer>` and `</answer>` tags: Enclosing the final answer derived after the thinking process. These tags might sometimes be preceded by a description of the context or task.

## Example Data Records

Below is a sample of the data records to illustrate the structure:

```json
[
    {
        ""id"": ""task_1"",
        ""task_type"": ""abduction"",
        ""task_text"": ""- Question: إذا كان هناك طائر يطير في السماء ويصدر أصواتًا، ما الذي يمكن أن نستنتجه عن الطائر؟\n- Answer: الطائر يمكن أن يكون مغردًا أو يبحث عن الطعام."",
        ""solution_text"": ""<think> إذا كان هناك طائر يطير في السماء ويصدر أصواتًا، فإن هذا يشير إلى أن الطائر قد يكون مغردًا، حيث أن العديد من الطيور تصدر أصواتًا كجزء من سلوكها الطبيعي. بالإضافة إلى ذلك، يمكن أن يكون الطائر يبحث عن الطعام، حيث أن بعض الطيور تصدر أصواتًا لجذب انتباه الطيور الأخرى أو للإشارة إلى وجود الطعام. لذا، يمكن أن نستنتج أن الطائر يمكن أن يكون مغردًا أو يبحث عن الطعام.</think>\n<answer> الطائر يمكن أن يكون مغردًا أو يبحث عن الطعام.</answer>""
    },
    {
        ""id"": ""task_2"",
        ""task_type"": ""abduction"",
        ""task_text"": ""- Question: إذا كان هناك طفل يحمل مظلة، وكانت السماء ملبدة بالغيوم، ما الذي يمكن أن نستنتجه عن حالة الطقس؟\n- Answer: الطقس قد يكون ممطرًا."",
        ""solution_text"": ""<think> إذا كان هناك طفل يحمل مظلة، فهذا يدل على أنه مستعد للاحتماء من المطر. وعندما تكون السماء ملبدة بالغيوم، فهذا يشير عادة إلى أن هناك احتمالًا كبيرًا لتساقط الأمطار. لذلك، يمكننا أن نستنتج أن حالة الطقس قد تكون ممطرة. </think>  \n<answer> الطقس قد يكون ممطرًا </answer>""
    },
    {
        ""id"": ""task_3"",
        ""task_type"": ""abduction"",
        ""task_text"": ""- Question: إذا كان الشخص يحمل مظلة، فما هو الاستنتاج الأكثر احتمالاً بشأن الطقس؟  \n- Answer: الطقس ممطر أو غائم."",
        ""solution_text"": ""<think> إذا كان الشخص يحمل مظلة، فمن المحتمل أن يكون هناك أمطار أو أن الطقس غائم. فالمظلة عادة ما تستخدم لحماية الشخص من الأمطار، وبالتالي فإن وجودها يشير إلى احتمال وجود طقس ممطر أو على الأقل ظروف جوية غير مستقرة. </think>\n<answer> الطقس ممطر أو غائم </answer>""
    },
    {
        ""id"": ""task_4"",
        ""task_type"": ""induction"",
        ""task_text"": ""- Question: إذا كان لديك 5 كتب وتضيف 3 كتب أخرى، كم عدد الكتب لديك الآن؟\n- Answer: 8 كتب"",
        ""solution_text"": ""<think> لدي 5 كتب، وعندما أضيف 3 كتب أخرى، فإن العدد الإجمالي سيكون 5 + 3 = 8.</think>  \n<answer> 8 كتب</answer>""
    },
    {
        ""id"": ""task_5"",
        ""task_type"": ""abduction"",
        ""task_text"": ""- Question: إذا كانت السماء ملبدة بالغيوم والأرض مبتلة، ما هو السبب المحتمل لذلك؟\n- Answer: المطر."",
        ""solution_text"": ""<think> إذا كانت السماء ملبدة بالغيوم، فهذا يشير إلى احتمال وجود أمطار قادمة، حيث أن الغيوم غالبًا ما تتكون نتيجة تكثف بخار الماء في الجو. والأرض مبتلة تعني أنه قد نزل عليها ماء، وهذا يمكن أن يكون نتيجة لهطول المطر. لذا، السبب المحتمل لوجود الغيوم الملبدة والأرض المبتلة هو المطر. </think>\n<answer> المطر </answer>""
    },
    {
        ""id"": ""task_1"",
        ""task_type"": ""abduction"",
        ""task_text"": ""- Question: إذا كان الجو مشمساً والناس يرتدون نظارات شمسية، ما هو السبب المحتمل لذلك؟\n- Answer: الجو مشمس."",
        ""solution_text"": ""<think> إذا كان الجو مشمساً، فهذا هو السبب المحتمل لارتداء الناس نظارات شمسية. الناس عادةً يرتدون النظارات الشمسية لحماية عيونهم من أشعة الشمس القوية. </think>\n<answer> الجو مشمس </answer>""
    },
    {
        ""id"": ""task_2"",
        ""task_type"": ""deduction"",
        ""task_text"": ""- Question: إذا كان كل طائر لديه جناحان، وكنت تعرف أن هناك 5 طيور في الحديقة، كم عدد الأجنحة في الحديقة بالكامل؟\n- Answer: 10 أجنحة"",
        ""solution_text"": ""<think> إذا كان كل طائر لديه جناحان، ولدينا 5 طيور في الحديقة، يمكننا حساب إجمالي عدد الأجنحة بضرب عدد الطيور في عدد الأجنحة لكل طائر. وبالتالي، 5 طيور × 2 جناحان لكل طائر = 10 أجنحة. </think>  \n<answer> 10 أجنحة </answer>""
    },
    {
        ""id"": ""task_3"",
        ""task_type"": ""deduction"",
        ""task_text"": ""- Question: إذا كان لديك 5 كتب وأعطيت صديقك 2 كتاب، كم عدد الكتب المتبقية لديك؟  \n- Answer: 3 كتب"",
        ""solution_text"": ""<think> لدي 5 كتب، وعندما أعطي صديقي 2 كتاب، يجب أن أطرح 2 من 5. 5 - 2 = 3. لذا، سيكون لدي 3 كتب متبقية. </think>  \n<answer> 3 كتب </answer>""
    },
    {
        ""id"": ""task_4"",
        ""task_type"": ""deduction"",
        ""task_text"": ""- Question: إذا كان لديك 5 كتب وأعطيت صديقك 2 كتاب، كم عدد الكتب المتبقية لديك؟  \n- Answer: 3 كتب"",
        ""solution_text"": ""- Task: إذا كان لديك 5 كتب وأعطيت صديقك 2 كتاب، كم عدد الكتب المتبقية لديك؟  \n- <think> لدي 5 كتب، وأعطيت صديقي 2 كتاب. لذلك يجب أن أطرح عدد الكتب التي أعطيتها من العدد الإجمالي الذي لدي. 5 - 2 = 3</think>  \n- <answer> 3 كتب</answer>""
    },
    {
        ""id"": ""task_5"",
        ""task_type"": ""deduction"",
        ""task_text"": ""- Question: إذا كان لديك 5 كتب وأعطيت صديقك 2 كتابين، كم كتاباً يبقى لديك؟  \n- Answer: 3 كتب"",
        ""solution_text"": ""<think> لدي 5 كتب، وإذا أعطيت صديقي 2 كتابين، سأطرح العدد الذي أعطيته من العدد الأصلي. 5 - 2 = 3 </think>  \n<answer> 3 كتب </answer>""
    },
    {
        ""id"": ""task_6"",
        ""task_type"": ""deduction"",
        ""task_text"": ""- Question: إذا كان لديك 5 كرات، ثم أخذت منها 2، كم كرة تبقى لديك؟  \n- Answer: 3 كرات"",
        ""solution_text"": ""- السؤال: إذا كان لديك 5 كرات، ثم أخذت منها 2، كم كرة تبقى لديك؟  \n<think> لدي 5 كرات، وعندما أخذت 2 منها، يجب أن أطرح العدد الذي أخذته من العدد الأصلي. لذلك، 5 - 2 = 3. </think>  \n<answer> 3 كرات </answer>""
    }
]
```
## Potential Uses
This dataset can be utilized in several applications, including:

### Training Large Language Models (LLMs): To enhance their logical reasoning capabilities in Arabic.
### Evaluating Model Performance: As a benchmark to measure the accuracy of models in solving logical problems.
### Developing Explainable AI (XAI) Systems: The solution_text field provides insights into the reasoning process.
### Natural Language Processing (NLP) Research: To study how logic is represented and understood in Arabic texts.


## Notes
### Data Quality: The tasks and solutions have been generated to simulate logical reasoning. The quality and consistency of solutions may vary.
### Duplicate Identifiers (id): As mentioned earlier, the provided sample contains duplicate id values. It is recommended to address this if uniqueness is required for each record.
### Task Variety: The dataset covers different types of logical reasoning, offering a comprehensive challenge for models.
<!-- end list -->

",Medium,3.0
Robustness & Safety ,textdetox/multilingual_toxicity_dataset,25.0,636.0,2025-03-21 18:52:31+00:00,openrail++,8.0,6.8 MB,7130316.8,6.8 MB,7130316.8,71374,none,"['https://aclanthology.org/2021.germeval-1.1/)', 'https://aclanthology.org/2024.woah-1.19/"",']","---
language:
- en
- ru
- uk
- de
- es
- am
- zh
- ar
- hi
- it
- fr
- he
- ja
- tt
license: openrail++
size_categories:
- 10K<n<100K
task_categories:
- text-classification
dataset_info:
  features:
  - name: text
    dtype: string
  - name: toxic
    dtype: int64
  splits:
  - name: en
    num_bytes: 411178
    num_examples: 5000
  - name: ru
    num_bytes: 710001
    num_examples: 5000
  - name: uk
    num_bytes: 630595
    num_examples: 5000
  - name: de
    num_bytes: 941017
    num_examples: 5000
  - name: es
    num_bytes: 978750
    num_examples: 5000
  - name: am
    num_bytes: 1102628
    num_examples: 5000
  - name: zh
    num_bytes: 359235
    num_examples: 5000
  - name: ar
    num_bytes: 889661
    num_examples: 5000
  - name: hi
    num_bytes: 1842662
    num_examples: 5000
  - name: it
    num_bytes: 791069
    num_examples: 5000
  - name: fr
    num_bytes: 621103
    num_examples: 5000
  - name: he
    num_bytes: 243823
    num_examples: 2011
  - name: hin
    num_bytes: 836167
    num_examples: 4363
  - name: tt
    num_bytes: 764917
    num_examples: 5000
  - name: ja
    num_bytes: 714729
    num_examples: 5000
  download_size: 6802095
  dataset_size: 11837535
configs:
- config_name: default
  data_files:
  - split: en
    path: data/en-*
  - split: ru
    path: data/ru-*
  - split: uk
    path: data/uk-*
  - split: de
    path: data/de-*
  - split: es
    path: data/es-*
  - split: am
    path: data/am-*
  - split: zh
    path: data/zh-*
  - split: ar
    path: data/ar-*
  - split: hi
    path: data/hi-*
  - split: it
    path: data/it-*
  - split: fr
    path: data/fr-*
  - split: he
    path: data/he-*
  - split: hin
    path: data/hin-*
  - split: tt
    path: data/tt-*
  - split: ja
    path: data/ja-*
---

# Multilingual Toxicity Detection Dataset

**[2025]** We extend our binary toxicity classification dataset to **more languages**! Now also covered: Italian, French, Hebrew, Hindglish, Japanese, Tatar. The data is prepared for [TextDetox 2025](https://pan.webis.de/clef25/pan25-web/text-detoxification.html) shared task.

**[2024]** For the shared task [TextDetox 2024](https://pan.webis.de/clef24/pan24-web/text-detoxification.html), we provide a compilation of binary toxicity classification datasets for each language.
Namely, for each language, we provide 5k subparts of the datasets -- 2.5k toxic and 2.5k non-toxic samples.

The list of original sources:
* English: [Jigsaw](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge), [Unitary AI Toxicity Dataset](https://github.com/unitaryai/detoxify)
* Russian: [Russian Language Toxic Comments](https://www.kaggle.com/datasets/blackmoon/russian-language-toxic-comments), [Toxic Russian Comments](https://www.kaggle.com/datasets/alexandersemiletov/toxic-russian-comments)
* Ukrainian: [ours](https://huggingface.co/datasets/ukr-detect/ukr-toxicity-dataset)
* Spanish: [CLANDESTINO, the Spanish toxic language dataset](https://github.com/microsoft/Clandestino/tree/main)
* German: [DeTox-Dataset](https://github.com/hdaSprachtechnologie/detox), [GemEval 2018, 2021](https://aclanthology.org/2021.germeval-1.1/)
* Amhairc: [Amharic Hate Speech](https://github.com/uhh-lt/AmharicHateSpeech)
* Arabic: [OSACT4](https://edinburghnlp.inf.ed.ac.uk/workshops/OSACT4/)
* Hindi: [Hostility Detection Dataset in Hindi](https://competitions.codalab.org/competitions/26654#learn_the_details-dataset), [Overview of the HASOC track at FIRE 2019: Hate Speech and Offensive Content Identification in Indo-European Languages](https://dl.acm.org/doi/pdf/10.1145/3368567.3368584?download=true)
* Italian: [AMI](https://github.com/dnozza/ami2020), [HODI](https://github.com/HODI-EVALITA/HODI_2023), [Jigsaw Multilingual Toxic Comment](https://www.kaggle.com/competitions/jigsaw-multilingual-toxic-comment-classification/overview)
* French: []
* Hebrew: [Hebrew Offensive Language Dataset](https://github.com/NataliaVanetik/HebrewOffensiveLanguageDatasetForTheDetoxificationProject/blob/main/OLaH-dataset-filtered.xlsx)
* Hinglish: []
* Japanese: [filtered](https://huggingface.co/datasets/sobamchan/ja-toxic-text-classification-open2ch) [2chan posts](https://huggingface.co/datasets/p1atdev/open2ch) by Perspective API;
* Tatar: ours. 

All credits go to the authors of the original corpora.

## Citation
If you would like to acknowledge our work, please, cite the following manuscripts:

**[2024]**

```
@inproceedings{dementieva2024overview,
  title={Overview of the Multilingual Text Detoxification Task at PAN 2024},
  author={Dementieva, Daryna and Moskovskiy, Daniil and Babakov, Nikolay and Ayele, Abinew Ali and Rizwan, Naquee and Schneider, Frolian and Wang, Xintog and Yimam, Seid Muhie and Ustalov, Dmitry and Stakovskii, Elisei and Smirnova, Alisa and Elnagar, Ashraf and Mukherjee, Animesh and Panchenko, Alexander},
  booktitle={Working Notes of CLEF 2024 - Conference and Labs of the Evaluation Forum},
  editor={Guglielmo Faggioli and Nicola Ferro and Petra Galu{\v{s}}{\v{c}}{\'a}kov{\'a} and Alba Garc{\'i}a Seco de Herrera},
  year={2024},
  organization={CEUR-WS.org}
}
```

```
@inproceedings{dementieva-etal-2024-toxicity,
    title = ""Toxicity Classification in {U}krainian"",
    author = ""Dementieva, Daryna  and
      Khylenko, Valeriia  and
      Babakov, Nikolay  and
      Groh, Georg"",
    editor = {Chung, Yi-Ling  and
      Talat, Zeerak  and
      Nozza, Debora  and
      Plaza-del-Arco, Flor Miriam  and
      R{\""o}ttger, Paul  and
      Mostafazadeh Davani, Aida  and
      Calabrese, Agostina},
    booktitle = ""Proceedings of the 8th Workshop on Online Abuse and Harms (WOAH 2024)"",
    month = jun,
    year = ""2024"",
    address = ""Mexico City, Mexico"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.woah-1.19/"",
    doi = ""10.18653/v1/2024.woah-1.19"",
    pages = ""244--255"",
    abstract = ""The task of toxicity detection is still a relevant task, especially in the context of safe and fair LMs development. Nevertheless, labeled binary toxicity classification corpora are not available for all languages, which is understandable given the resource-intensive nature of the annotation process. Ukrainian, in particular, is among the languages lacking such resources. To our knowledge, there has been no existing toxicity classification corpus in Ukrainian. In this study, we aim to fill this gap by investigating cross-lingual knowledge transfer techniques and creating labeled corpora by: (i){\textasciitilde}translating from an English corpus, (ii){\textasciitilde}filtering toxic samples using keywords, and (iii){\textasciitilde}annotating with crowdsourcing. We compare LLMs prompting and other cross-lingual transfer approaches with and without fine-tuning offering insights into the most robust and efficient baselines.""
}
```

```
@inproceedings{DBLP:conf/ecir/BevendorffCCDEFFKMMPPRRSSSTUWZ24,
  author       = {Janek Bevendorff and
                  Xavier Bonet Casals and
                  Berta Chulvi and
                  Daryna Dementieva and
                  Ashaf Elnagar and
                  Dayne Freitag and
                  Maik Fr{\""{o}}be and
                  Damir Korencic and
                  Maximilian Mayerl and
                  Animesh Mukherjee and
                  Alexander Panchenko and
                  Martin Potthast and
                  Francisco Rangel and
                  Paolo Rosso and
                  Alisa Smirnova and
                  Efstathios Stamatatos and
                  Benno Stein and
                  Mariona Taul{\'{e}} and
                  Dmitry Ustalov and
                  Matti Wiegmann and
                  Eva Zangerle},
  editor       = {Nazli Goharian and
                  Nicola Tonellotto and
                  Yulan He and
                  Aldo Lipani and
                  Graham McDonald and
                  Craig Macdonald and
                  Iadh Ounis},
  title        = {Overview of {PAN} 2024: Multi-author Writing Style Analysis, Multilingual
                  Text Detoxification, Oppositional Thinking Analysis, and Generative
                  {AI} Authorship Verification - Extended Abstract},
  booktitle    = {Advances in Information Retrieval - 46th European Conference on Information
                  Retrieval, {ECIR} 2024, Glasgow, UK, March 24-28, 2024, Proceedings,
                  Part {VI}},
  series       = {Lecture Notes in Computer Science},
  volume       = {14613},
  pages        = {3--10},
  publisher    = {Springer},
  year         = {2024},
  url          = {https://doi.org/10.1007/978-3-031-56072-9\_1},
  doi          = {10.1007/978-3-031-56072-9\_1},
  timestamp    = {Fri, 29 Mar 2024 23:01:36 +0100},
  biburl       = {https://dblp.org/rec/conf/ecir/BevendorffCCDEFFKMMPPRRSSSTUWZ24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
```",High,5.0
Robustness & Safety ,ToxicityPrompts/PolygloToxicityPrompts,11.0,256.0,2024-05-16 07:02:28+00:00,none,0.0,2.27 GB,unknown,2.27 GB,unknown,527000,https://arxiv.org/abs/2405.09373,none,"---
dataset_info:
- config_name: ptp-ar
  features:
  - name: text
    dtype: string
  - name: meta_data
    struct:
    - name: data_category
      dtype: string
    - name: lang
      dtype: string
    - name: pile_set_name
      dtype: string
    - name: timestamp
      dtype: string
    - name: url
      dtype: string
  - name: perspective
    struct:
    - name: attributeScores
      struct:
      - name: IDENTITY_ATTACK
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: INSULT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: PROFANITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: SEVERE_TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: THREAT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
    - name: detectedLanguages
      sequence: string
    - name: languages
      sequence: string
  - name: toxicity
    dtype: float64
  - name: toxicity_bucket
    dtype: float64
  - name: prompt
    dtype: string
  - name: continuation
    dtype: string
  - name: prompt_perspective
    struct:
    - name: attributeScores
      struct:
      - name: IDENTITY_ATTACK
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: INSULT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: PROFANITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: SEVERE_TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: THREAT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
    - name: detectedLanguages
      sequence: string
    - name: languages
      sequence: string
  - name: continuation_perspective
    struct:
    - name: attributeScores
      struct:
      - name: IDENTITY_ATTACK
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: INSULT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: PROFANITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: SEVERE_TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: THREAT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
    - name: detectedLanguages
      sequence: string
    - name: languages
      sequence: string
  splits:
  - name: full
    num_bytes: 183639335
    num_examples: 25000
  - name: small
    num_bytes: 37360253
    num_examples: 5000
  download_size: 99259141
  dataset_size: 220999588
- config_name: ptp-cs
  features:
  - name: text
    dtype: string
  - name: meta_data
    struct:
    - name: data_category
      dtype: string
    - name: lang
      dtype: string
    - name: pile_set_name
      dtype: string
    - name: timestamp
      dtype: string
    - name: url
      dtype: string
  - name: perspective
    struct:
    - name: attributeScores
      struct:
      - name: IDENTITY_ATTACK
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: INSULT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: PROFANITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: SEVERE_TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: THREAT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
    - name: detectedLanguages
      sequence: string
    - name: languages
      sequence: string
  - name: toxicity
    dtype: float64
  - name: toxicity_bucket
    dtype: float64
  - name: prompt
    dtype: string
  - name: continuation
    dtype: string
  - name: prompt_perspective
    struct:
    - name: attributeScores
      struct:
      - name: IDENTITY_ATTACK
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: INSULT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: PROFANITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: SEVERE_TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: THREAT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
    - name: detectedLanguages
      sequence: string
    - name: languages
      sequence: string
  - name: continuation_perspective
    struct:
    - name: attributeScores
      struct:
      - name: IDENTITY_ATTACK
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: INSULT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: PROFANITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: SEVERE_TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: THREAT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
    - name: detectedLanguages
      sequence: string
    - name: languages
      sequence: string
  splits:
  - name: full
    num_bytes: 172463628
    num_examples: 25000
  - name: small
    num_bytes: 36025157
    num_examples: 5000
  download_size: 119993908
  dataset_size: 208488785
- config_name: ptp-de
  features:
  - name: text
    dtype: string
  - name: meta_data
    struct:
    - name: data_category
      dtype: string
    - name: lang
      dtype: string
    - name: timestamp
      dtype: string
    - name: url
      dtype: string
  - name: perspective
    struct:
    - name: attributeScores
      struct:
      - name: IDENTITY_ATTACK
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: INSULT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: PROFANITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: SEVERE_TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: THREAT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
    - name: detectedLanguages
      sequence: string
    - name: languages
      sequence: string
  - name: toxicity
    dtype: float64
  - name: toxicity_bucket
    dtype: float64
  - name: prompt
    dtype: string
  - name: continuation
    dtype: string
  - name: prompt_perspective
    struct:
    - name: attributeScores
      struct:
      - name: IDENTITY_ATTACK
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: INSULT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: PROFANITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: SEVERE_TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: THREAT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
    - name: detectedLanguages
      sequence: string
    - name: languages
      sequence: string
  - name: continuation_perspective
    struct:
    - name: attributeScores
      struct:
      - name: IDENTITY_ATTACK
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: INSULT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: PROFANITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: SEVERE_TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: THREAT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
    - name: detectedLanguages
      sequence: string
    - name: languages
      sequence: string
  splits:
  - name: full
    num_bytes: 202660324
    num_examples: 25000
  - name: small
    num_bytes: 40535712
    num_examples: 5000
  download_size: 139277097
  dataset_size: 243196036
- config_name: ptp-en
  features:
  - name: text
    dtype: string
  - name: meta_data
    struct:
    - name: data_category
      dtype: string
    - name: lang
      dtype: string
    - name: pile_set_name
      dtype: string
    - name: timestamp
      dtype: string
    - name: url
      dtype: string
  - name: perspective
    struct:
    - name: attributeScores
      struct:
      - name: IDENTITY_ATTACK
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: INSULT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: PROFANITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: SEVERE_TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: THREAT
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
      - name: TOXICITY
        struct:
        - name: spanScores
          list:
          - name: begin
            dtype: int64
          - name: end
            dtype: int64
          - name: score
            struct:
            - name: type
              dtype: string
            - name: value
              dtype: float64
        - name: summaryScore
          struct:
          - name: type
            dtype: string
          - name: value
            dtype: float64
    - name: detectedLanguages
      sequence: string
    - name: languages
      sequence: st",High,5.0
Robustness & Safety ,luizapzbn/from-one-to-many-toxicity-mitigation,0.0,242.0,2024-05-24 17:09:53+00:00,apache-2.0,0.0,UNKNOWN,unknown,UNKNOWN,unknown,unknown,https://arxiv.org/abs/2403.03893,none,"---
license: apache-2.0
task_categories:
- text-generation
- text-classification
language:
- en
- pt
- hi
- it
- fr
- ru
- ar
- ko
- es
tags:
- harmful
- toxic
---

# From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models

[[arxiv]](https://arxiv.org/pdf/2403.03893)[[code]](https://github.com/for-ai/goodtriever)[[data]](https://huggingface.co/datasets/luizapzbn/from-one-to-many-toxicity-mitigation)

Data accompanying the paper ""From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models"" accepted to ACL Findings 2024.

_Abstract_: To date, toxicity mitigation in language models has almost entirely been focused on single-language settings. As language models embrace multilingual capabilities, it’s crucial our safety measures keep pace. Recognizing this research gap, our approach expands the scope of conventional toxicity mitigation to address the complexities presented by multiple languages. In the absence of sufficient annotated datasets across languages, we employ translated data to evaluate and enhance our mitigation techniques. We also compare finetuning mitigation approaches against retrieval-augmented techniques under both static and continual toxicity mitigation scenarios. This allows us to examine the effects of translation quality and the cross-lingual transfer on toxicity mitigation. We also explore how model size and data quantity affect the success of these mitigation efforts. Covering nine languages, our study represents a broad array of linguistic families and levels of resource availability, ranging from high to mid-resource languages. Through comprehensive experiments, we provide insights into the complexities of multilingual toxicity mitigation, offering valuable insights and paving the way for future research in this increasingly important field. 

## Dataset Description

- **Language(s) (NLP):** English, Portuguese, Spanish, Italian, French, Russian, Arabic, Hindi, Korean
- **License:** This dataset is a translation of existing datasets. Each dataset's original license applies. For more details see the ""Source Data"" section.

## Dataset Structure

- train:
  - jigsaw_english: original Jigsaw Unintended Bias dataset in the English language.
  - multilingual:
    - jigsaw_multilingual: in-language examples from the Jigsaw Multilingual Toxicity classification challenge.
    - translated_jigsaw_english: translated samples from the Jigsaw Unintended Bias Challenge. Original samples are in the ""jigsaw_english"" folder one level up.
      - full_sized: translations of the jigsaw dataset in its entirety
      - minimal: for our main experiments, we selected ~3K (or 3.5K) and ~10K toxic and non-toxic samples, respectively. Here are those subsets, translated by NLLB 600M model.
        - nllb1.3b: the same subset of data for all languages, but translated with the NLLB 1.3B model (higher translation quality)
        - m2m: the same subset of data for all languages, but translated with the M2M 418M model (lower translation quality)
        - different_subsets: we selected different subsets for each of the languages (unparalleled content) and translated them with NLLB 600M
        - bleu_subset: samples used to compute BLEU scores for the paper
- eval: a random subset of 200 samples of holistic bias (English) translated with Google Translate to each of the target languages. The contents are the same across all languages.
  - _hi: the eval set of the high-resource language experiments
  - _mid: the eval set of the mid-resource language experiments
  - individual: folder with the individual samples for each language
- results: all of the models generations and experiments from the paper. to be used with the results notebook to generate plots (15GB of data though)


## Source Data

The datasets from this repository are subsets or translations of three others:

- [jigsaw multilingual toxicity classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification)
- [jigsaw unintended bias (english)](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)
- [holistic bias](https://arxiv.org/abs/2205.09209)

## Bias, Risks, and Limitations

To generate these datasets, we leveraged machine translation. There are inherent risks of either increasing or reducing existing toxicity from the original sentences due to this processing. 
The datasets contain toxic sentences that might be used to make models more toxic. This usage is highly discouraged by the authors and the original purpose of this dataset is to make models less harmful.

## Citation [optional]
```
@article{pozzobon2024one,
  title={From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models},
  author={Pozzobon, Luiza and Lewis, Patrick and Hooker, Sara and Ermis, Beyza},
  journal={arXiv preprint arXiv:2403.03893},
  year={2024}
}
```",High,6.0
Robustness & Safety ,textdetox/multilingual_toxicity_explained,1.0,144.0,2025-02-04 21:03:23+00:00,openrail++,1.0,779 KB,797696.0,779 KB,797696.0,8237,https://arxiv.org/abs/2412.11691,"['https://aclanthology.org/2025.coling-main.535/)', 'https://aclanthology.org/2025.coling-main.535/"",']","---
dataset_info:
  features:
  - name: Sentence
    dtype: string
  - name: Toxicity Level
    dtype: string
  - name: Tone
    dtype: string
  - name: Language
    dtype: string
  - name: Implied Sentiment
    dtype: string
  - name: Negative Connotations
    dtype: string
  - name: __index_level_0__
    dtype: int64
  splits:
  - name: en
    num_bytes: 131026
    num_examples: 917
  - name: es
    num_bytes: 140525
    num_examples: 931
  - name: de
    num_bytes: 182808
    num_examples: 862
  - name: zh
    num_bytes: 160560
    num_examples: 891
  - name: ar
    num_bytes: 194304
    num_examples: 954
  - name: am
    num_bytes: 277872
    num_examples: 945
  - name: ru
    num_bytes: 209548
    num_examples: 956
  - name: uk
    num_bytes: 167154
    num_examples: 879
  - name: hi
    num_bytes: 216922
    num_examples: 902
  download_size: 827545
  dataset_size: 1680719
configs:
- config_name: default
  data_files:
  - split: en
    path: data/en-*
  - split: es
    path: data/es-*
  - split: de
    path: data/de-*
  - split: zh
    path: data/zh-*
  - split: ar
    path: data/ar-*
  - split: am
    path: data/am-*
  - split: ru
    path: data/ru-*
  - split: uk
    path: data/uk-*
  - split: hi
    path: data/hi-*
license: openrail++
language:
- en
- de
- es
- ru
- uk
- ar
- am
- zh
- hi
size_categories:
- 1K<n<10K
---

## Multilingual and Explainable Toxicity 
[![COLING2025](https://img.shields.io/badge/COLING%202025-b31b1b)](https://aclanthology.org/2025.coling-main.535/) 
[![Github](https://img.shields.io/badge/🌎-Website-blue.svg)](https://github.com/textdetox/multilingual_explainable_paradetox)



We explained the toxic part of our [multilingual ParaDetox](https://huggingface.co/datasets/textdetox/multilingual_paradetox) dataset utilizing GPT-4 (May, 2024) with the following prompt:

```
Please analyze the provided sentence using the structure below to identify elements of
toxicity and suggest improvements, when I tell you, use words from the keywords list (can be
more than one word!):
keywords = [Neutral, Informative, Casual, Assertive, Dismissive, Condescending,
Friendly, Commanding, Instructive Derogatory, Confrontational, Insulting,
Vulgar, Formal, Informal, Offensive, Technical, Playful, Positive,
Frustration, Analytical, Professional, Hostile, Hatred, Helpful,
Angry, Friendly, Arrogant]
Analysis Structure (do not use ” and [] and """" in your answer and do not suggest improvement!):
{
  Sentence: {sentence},
  Toxicity Level: Specify here (Low/Medium/High),
  Tone: the overall tone of the sentence- choose from keywords,
  Language: Language style—choose from keywords,
  Implied Sentiment: the overall sentiment- choose from keywords,
  Context: Brief description of how context contributes to toxicity,
  Negative Connotations: List specific negative words/phrases here,
  Intent: Describe the perceived intent behind the sentence.
}
```

diving into insights of various descriptive attributes and the toxicity variety across 9 languages.

For example, top toxic keywords extrated per language:

<img  src=""Multilingual_Toxic_Keywords.jpg"" style=""max-width: 50%;"" alt=""Top Toxic Keywords per Language"">

We hope, the data can serve as the great source for explainable toxicity mitigation!

Presented in [Multilingual and Explainable Text Detoxification with Parallel Corpora](https://huggingface.co/papers/2412.11691) @ COLING2025


## Citation

```
@inproceedings{dementieva-etal-2025-multilingual,
    title = ""Multilingual and Explainable Text Detoxification with Parallel Corpora"",
    author = ""Dementieva, Daryna  and
      Babakov, Nikolay  and
      Ronen, Amit  and
      Ayele, Abinew Ali  and
      Rizwan, Naquee  and
      Schneider, Florian  and
      Wang, Xintong  and
      Yimam, Seid Muhie  and
      Moskovskiy, Daniil Alekhseevich  and
      Stakovskii, Elisei  and
      Kaufman, Eran  and
      Elnagar, Ashraf  and
      Mukherjee, Animesh  and
      Panchenko, Alexander"",
    editor = ""Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven"",
    booktitle = ""Proceedings of the 31st International Conference on Computational Linguistics"",
    month = jan,
    year = ""2025"",
    address = ""Abu Dhabi, UAE"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2025.coling-main.535/"",
    pages = ""7998--8025"",
    abstract = ""Even with various regulations in place across countries and social media platforms (Government of India, 2021; European Parliament and Council of the European Union, 2022), digital abusive speech remains a significant issue. One potential approach to address this challenge is automatic text detoxification, a text style transfer (TST) approach that transforms toxic language into a more neutral or non-toxic form. To date, the availability of parallel corpora for the text detoxification task (Logacheva et al., 2022; Atwell et al., 2022; Dementieva et al., 2024a) has proven to be crucial for state-of-the-art approaches. With this work, we extend parallel text detoxification corpus to new languages{---}German, Chinese, Arabic, Hindi, and Amharic{---}testing in the extensive multilingual setup TST baselines. Next, we conduct the first of its kind an automated, explainable analysis of the descriptive features of both toxic and non-toxic sentences, diving deeply into the nuances, similarities, and differences of toxicity and detoxification across 9 languages. Finally, based on the obtained insights, we experiment with a novel text detoxification method inspired by the Chain-of-Thoughts reasoning approach, enhancing the prompting process through clustering on relevant descriptive attributes.""
}

```

## Dataset Card Corresponding Contact

[Daryna Dementieva](https://huggingface.co/dardem)",High,5.0
Robustness & Safety ,ToxicityPrompts/PolyGuardMix,1.0,229.0,2025-06-23 21:39:12+00:00,cc-by-4.0,1.0,2.31 GB,2480343613.44,2.31 GB,2480343613.44,1910372,https://arxiv.org/abs/2504.04377,none,"---
dataset_info:
  features:
  - name: prompt
    dtype: string
  - name: response
    dtype: string
  - name: prompt_harm_label
    dtype: string
  - name: response_refusal_label
    dtype: string
  - name: response_harm_label
    dtype: string
  - name: prompt_safety_categories
    dtype: string
  - name: response_safety_categories
    dtype: string
  - name: metadata
    struct:
    - name: language
      dtype: string
    - name: source
      dtype: string
  splits:
  - name: train
    num_bytes: 3783037965
    num_examples: 1910372
  download_size: 2306303141
  dataset_size: 3783037965
configs:
- config_name: default
  data_files:
  - split: train
    path: data/train-*
task_categories:
- text2text-generation
language:
- ar
- zh
- cs
- nl
- en
- fr
- de
- hi
- th
- it
- ja
- ko
- pl
- pt
- ru
- es
- sv
tags:
- safety
- multilingual
size_categories:
- 1M<n<10M
license: cc-by-4.0
---


# PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages

Abstract: Truly multilingual safety moderation efforts for Large Language Models (LLMs) have been hindered by a narrow focus on a small set of languages (e.g., English, Chinese) as well as a limited scope of safety definition, resulting in significant gaps in moderation capabilities. To bridge these gaps, we release PolyGuard, a new state-of-the-art multilingual safety model for safeguarding LLM generations, and the corresponding training and evaluation datasets. PolyGuard is trained on PolyGuardMix, the largest multilingual safety training corpus to date containing 1.91M samples across 17 languages (e.g., Chinese, Czech, English, Hindi). We also introduce PolyGuardPrompts, a high quality multilingual benchmark with 29K samples for the evaluation of safety guardrails. Created by combining naturally occurring multilingual human-LLM interactions and human-verified machine translations of an English-only safety dataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output pairs with labels of prompt harmfulness, response harmfulness, and response refusal. Through extensive evaluations across multiple safety and toxicity benchmarks, we demonstrate that PolyGuard outperforms existing state-of-the-art open-weight and commercial safety classifiers by 5.5%. Our contributions advance efforts toward safer multilingual LLMs for all global users. 

### Languages


The data supports 17 languages and are reported in the table below.


| language code   | language name        |
|:----------------|:---------------------|
| ar              | Arabic               |
| cs              | Czech                |
| de              | German               |
| en              | English              |
| es              | Spanish              |
| hi              | Hindi                |
| it              | Italian              |
| ja              | Japanese             |
| ko              | Korean               |
| nl              | Dutch                |
| pl              | Polish               |
| pt              | Portuguese           |
| ru              | Russian              |
| sv              | Swedish              |
| zh              | Chinese              |
| th              | Thai                 |


### Data Fields

- `prompt`: user prompt input by user
- `response`: model's response to the user prompt
- `prompt_harm_label`: if the prompt is harmful
- `response_refusal_label`: if the model refuses the user's request
- `response_harm_label`: if the response is harmful
- `prompt_safety_categories`: list of violated safety categories by harmful prompt
- `response_safety_categories`: list of violated safety categories by harmful response
- `metadata`: language and source of data sample


### Citation 

```
@misc{kumar2025polyguardmultilingualsafetymoderation,
      title={PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages}, 
      author={Priyanshu Kumar and Devansh Jain and Akhila Yerukola and Liwei Jiang and Himanshu Beniwal and Thomas Hartvigsen and Maarten Sap},
      year={2025},
      eprint={2504.04377},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.04377}, 
}
```",High,4.0
Robustness & Safety ,ToxicityPrompts/PolyGuardPrompts,0.0,156.0,2025-06-23 21:37:19+00:00,cc-by-4.0,0.0,52.9 MB,55469670.4,52.9 MB,55469670.4,29325,https://arxiv.org/abs/2504.04377,none,"---
dataset_info:
  features:
  - name: prompt
    dtype: string
  - name: response
    dtype: string
  - name: adversarial
    dtype: bool
  - name: prompt_harm_label
    dtype: string
  - name: response_refusal_agreement
    dtype: float64
  - name: response_refusal_label
    dtype: string
  - name: response_harm_label
    dtype: string
  - name: subcategory
    dtype: string
  - name: prompt_harm_agreement
    dtype: float64
  - name: response_harm_agreement
    dtype: float64
  - name: id
    dtype: int64
  - name: language
    dtype: string
  - name: prompt_label
    dtype: string
  - name: prompt_categories
    dtype: string
  - name: response_label
    dtype: string
  - name: response_categories
    dtype: string
  splits:
  - name: test
    num_bytes: 93846872
    num_examples: 29325
  download_size: 52852473
  dataset_size: 93846872
configs:
- config_name: default
  data_files:
  - split: test
    path: data/test-*
task_categories:
- text2text-generation
language:
- ar
- zh
- cs
- nl
- en
- fr
- de
- hi
- th
- it
- ja
- ko
- pl
- pt
- ru
- es
- sv
tags:
- safety
- multilingual
size_categories:
- 10K<n<100K
license: cc-by-4.0
---


# PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages

Abstract: Truly multilingual safety moderation efforts for Large Language Models (LLMs) have been hindered by a narrow focus on a small set of languages (e.g., English, Chinese) as well as a limited scope of safety definition, resulting in significant gaps in moderation capabilities. To bridge these gaps, we release PolyGuard, a new state-of-the-art multilingual safety model for safeguarding LLM generations, and the corresponding training and evaluation datasets. PolyGuard is trained on PolyGuardMix, the largest multilingual safety training corpus to date containing 1.91M samples across 17 languages (e.g., Chinese, Czech, English, Hindi). We also introduce PolyGuardPrompts, a high quality multilingual benchmark with 29K samples for the evaluation of safety guardrails. Created by combining naturally occurring multilingual human-LLM interactions and human-verified machine translations of an English-only safety dataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output pairs with labels of prompt harmfulness, response harmfulness, and response refusal. Through extensive evaluations across multiple safety and toxicity benchmarks, we demonstrate that PolyGuard outperforms existing state-of-the-art open-weight and commercial safety classifiers by 5.5%. Our contributions advance efforts toward safer multilingual LLMs for all global users. 

### Languages


The data supports 17 languages and are reported in the table below.


| language code   | language name        |
|:----------------|:---------------------|
| ar              | Arabic               |
| cs              | Czech                |
| de              | German               |
| en              | English              |
| es              | Spanish              |
| hi              | Hindi                |
| it              | Italian              |
| ja              | Japanese             |
| ko              | Korean               |
| nl              | Dutch                |
| pl              | Polish               |
| pt              | Portuguese           |
| ru              | Russian              |
| sv              | Swedish              |
| zh              | Chinese              |
| th              | Thai                 |


### Data Fields

- `prompt`: user prompt input by user
- `response`: model's response to the user prompt
- `prompt_harm_label`: if the prompt is harmful
- `response_refusal_label`: if the model refuses the user's request
- `response_harm_label`: if the response is harmful
- `prompt_safety_categories`: list of violated safety categories by harmful prompt
- `response_safety_categories`: list of violated safety categories by harmful response
- `metadata`: language and source of data sample


### Citation 

```
@misc{kumar2025polyguardmultilingualsafetymoderation,
      title={PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages}, 
      author={Priyanshu Kumar and Devansh Jain and Akhila Yerukola and Liwei Jiang and Himanshu Beniwal and Thomas Hartvigsen and Maarten Sap},
      year={2025},
      eprint={2504.04377},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.04377}, 
}
```",High,4.0
Robustness & Safety ,Malikeh1375/code-switching-tokenizer-robustness,1.0,267.0,2025-05-24 04:28:43+00:00,cc-by-4.0,0.0,1.06 MB,1111490.56,1.06 MB,1111490.56,960,none,none,"---
license: cc-by-4.0
language:
- en
- ru
- zh
- ja
- de
- es
- fr
- it
- th
- ar
- tr
- ko
- hi
- bn
- te
- sw
multilinguality:
- multilingual
size_categories:
- n<1K
task_categories:
- text-generation
- text-classification
pretty_name: Code-Switching Dataset for Tokenizer Robustness Analysis
tags:
- code-switching
- tokenizer-robustness
- multilingual
- cross-lingual
- evaluation
configs:
- config_name: en
  data_files:
    - split: train
      path: en/train-*
    - split: test
      path: en/test-*
    - split: validation
      path: en/validation-*
- config_name: en-rus
  data_files:
    - split: train
      path: en-rus/train-*
    - split: test
      path: en-rus/test-*
    - split: validation
      path: en-rus/validation-*
- config_name: en-zho
  data_files:
    - split: train
      path: en-zho/train-*
    - split: test
      path: en-zho/test-*
    - split: validation
      path: en-zho/validation-*
- config_name: en-jpn
  data_files:
    - split: train
      path: en-jpn/train-*
    - split: test
      path: en-jpn/test-*
    - split: validation
      path: en-jpn/validation-*
- config_name: en-deu
  data_files:
    - split: train
      path: en-deu/train-*
    - split: test
      path: en-deu/test-*
    - split: validation
      path: en-deu/validation-*
- config_name: en-spa
  data_files:
    - split: train
      path: en-spa/train-*
    - split: test
      path: en-spa/test-*
    - split: validation
      path: en-spa/validation-*
- config_name: en-fra
  data_files:
    - split: train
      path: en-fra/train-*
    - split: test
      path: en-fra/test-*
    - split: validation
      path: en-fra/validation-*
- config_name: en-ita
  data_files:
    - split: train
      path: en-ita/train-*
    - split: test
      path: en-ita/test-*
    - split: validation
      path: en-ita/validation-*
- config_name: en-tha
  data_files:
    - split: train
      path: en-tha/train-*
    - split: test
      path: en-tha/test-*
    - split: validation
      path: en-tha/validation-*
- config_name: en-arb
  data_files:
    - split: train
      path: en-arb/train-*
    - split: test
      path: en-arb/test-*
    - split: validation
      path: en-arb/validation-*
- config_name: en-tur
  data_files:
    - split: train
      path: en-tur/train-*
    - split: test
      path: en-tur/test-*
    - split: validation
      path: en-tur/validation-*
- config_name: en-kor
  data_files:
    - split: train
      path: en-kor/train-*
    - split: test
      path: en-kor/test-*
    - split: validation
      path: en-kor/validation-*
- config_name: en-hin
  data_files:
    - split: train
      path: en-hin/train-*
    - split: test
      path: en-hin/test-*
    - split: validation
      path: en-hin/validation-*
- config_name: en-ben
  data_files:
    - split: train
      path: en-ben/train-*
    - split: test
      path: en-ben/test-*
    - split: validation
      path: en-ben/validation-*
- config_name: en-tel
  data_files:
    - split: train
      path: en-tel/train-*
    - split: test
      path: en-tel/test-*
    - split: validation
      path: en-tel/validation-*
- config_name: en-swa
  data_files:
    - split: train
      path: en-swa/train-*
    - split: test
      path: en-swa/test-*
    - split: validation
      path: en-swa/validation-*
- config_name: combined
  data_files:
    - split: train
      path: combined/train-*
    - split: test
      path: combined/test-*
    - split: validation
      path: combined/validation-*
---

# Code-Switching Dataset for Tokenizer Robustness Analysis

## Dataset Description

This dataset is designed for **tokenizer robustness testing** in multilingual and code-switching contexts. It contains identical content expressed across 16 different language variants, including pure English and 15 English-X code-switching pairs, allowing researchers to isolate tokenization effects from semantic differences when evaluating language models.

### Purpose

- **Tokenizer Comparison**: Compare how different tokenizers (BPE, SentencePiece, byte-level) handle code-switching text
- **Perplexity Testing**: Measure perplexity differences on semantically identical content with varying language mixing
- **Robustness Evaluation**: Assess model performance across different code-switching patterns
- **Cross-lingual Research**: Support studies on multilingual text processing and code-switching understanding

## Dataset Structure

### Language Pairs

The dataset includes 16 language variants:

1. **English** (`en`): Pure English baseline
2. **English-Russian** (`en-rus`): Code-switching with Cyrillic script
3. **English-Chinese** (`en-zho`): Code-switching with Simplified Chinese (Hans)
4. **English-Japanese** (`en-jpn`): Code-switching with Japanese scripts (Hiragana/Katakana/Kanji)
5. **English-German** (`en-deu`): Code-switching with German
6. **English-Spanish** (`en-spa`): Code-switching with Spanish
7. **English-French** (`en-fra`): Code-switching with French
8. **English-Italian** (`en-ita`): Code-switching with Italian
9. **English-Thai** (`en-tha`): Code-switching with Thai script
10. **English-Arabic** (`en-arb`): Code-switching with Arabic script (RTL)
11. **English-Turkish** (`en-tur`): Code-switching with Turkish
12. **English-Korean** (`en-kor`): Code-switching with Hangul script
13. **English-Hindi** (`en-hin`): Code-switching with Devanagari script
14. **English-Bengali** (`en-ben`): Code-switching with Bengali script
15. **English-Telugu** (`en-tel`): Code-switching with Telugu script
16. **English-Swahili** (`en-swa`): Code-switching with Swahili

### Content Coverage

The dataset covers diverse topics and domains:

- **Technology**: Digital innovation, AI, software development
- **Education**: Learning systems, academic development
- **Health**: Medical care, wellness, mental health
- **Culture**: Food traditions, art, music, heritage
- **Environment**: Conservation, climate change, sustainability
- **Society**: Community, relationships, social responsibility
- **Science**: Innovation, research, discovery
- **Economics**: Finance, markets, personal economics

### Code-Switching Patterns

Each code-switched text demonstrates natural mixing patterns:

```
""Technology has become неотъемлемой частью нашей повседневной жизни. We use smartphones чтобы общаться, computers для работы, and various apps to manage наши расписания.""
```

Features:
- **Intra-sentential switching**: Language changes within sentences
- **Natural flow**: Realistic code-switching patterns
- **Functional distribution**: Content words vs function words
- **Cultural adaptation**: Context-appropriate language mixing

## Dataset Splits

- **Train**: 20 samples per language variant (320 total entries)
- **Validation**: 5 samples per language variant (80 total entries)
- **Test**: 5 samples per language variant (80 total entries)
- **Total**: 30 unique concepts, 480 total text entries across all variants

## Usage Examples

### Loading Specific Language Pairs

```python
from datasets import load_dataset

# Load specific language pair
english_data = load_dataset(""Malikeh1375/code-switching-tokenizer-robustness"", ""en"")
russian_cs_data = load_dataset(""Malikeh1375/code-switching-tokenizer-robustness"", ""en-rus"")
chinese_cs_data = load_dataset(""Malikeh1375/code-switching-tokenizer-robustness"", ""en-zho"")

# Load all data combined
all_data = load_dataset(""Malikeh1375/code-switching-tokenizer-robustness"", ""combined"")
```

### Tokenizer Robustness Testing

```python
from transformers import AutoTokenizer

# Compare tokenizers on code-switching text
tokenizer_gpt2 = AutoTokenizer.from_pretrained(""gpt2"")
tokenizer_bert = AutoTokenizer.from_pretrained(""bert-base-multilingual-cased"")

# Same content, different languages
english_text = english_data['train'][0]['text']
russian_cs_text = russian_cs_data['train'][0]['text']

# Tokenize and compare
en_tokens_gpt2 = tokenizer_gpt2.encode(english_text)
ru_cs_tokens_gpt2 = tokenizer_gpt2.encode(russian_cs_text)

print(f""English tokens (GPT-2): {len(en_tokens_gpt2)}"")
print(f""Russian CS tokens (GPT-2): {len(ru_cs_tokens_gpt2)}"")
```

### Perplexity Analysis Across Languages

```python
import torch
from transformers import GPT2LMHeadModel, GPT2TokenizerFast

model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')

def calculate_perplexity(text):
    encodings = tokenizer(text, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**encodings, labels=encodings.input_ids)
    return torch.exp(outputs.loss).item()

# Compare perplexity across language variants
results = {}
for lang_pair in ['en', 'en-rus', 'en-zho', 'en-jpn']:
    dataset = load_dataset(""Malikeh1375/code-switching-tokenizer-robustness"", lang_pair)
    perplexities = [calculate_perplexity(sample['text']) for sample in dataset['test']]
    results[lang_pair] = sum(perplexities) / len(perplexities)

for lang, ppl in results.items():
    print(f""{lang}: {ppl:.2f}"")
```

### Filtering and Analysis

```python
# Filter by code-switching status
english_only = all_data['train'].filter(lambda x: not x['is_code_switched'])
code_switched = all_data['train'].filter(lambda x: x['is_code_switched'])

# Filter by specific topics
tech_samples = all_data['train'].filter(lambda x: 'technology' in x['concept'].lower())

# Group by language family or script
cyrillic_data = all_data['train'].filter(lambda x: x['language_pair'] == 'en-rus')
cjk_data = all_data['train'].filter(lambda x: x['language_pair'] in ['en-zho', 'en-jpn', 'en-kor'])
```

## Dataset Subsets

Each language pair is available as a separate subset:

### Monolingual
- `en`: Pure English baseline

### Code-Switching Pairs
- `en-rus`: English-Russian (Cyrillic script)
- `en-zho`: English-Chinese Simplified (Han script)
- `en-jpn`: English-Japanese (Mixed scripts)
- `en-deu`: English-German (Latin script)
- `en-spa`: English-Spanish (Latin script)
- `en-fra`: English-French (Latin script)
- `en-ita`: English-Italian (Latin script)
- `en-tha`: English-Thai (Thai script)
- `en-arb`: English-Arabic (Arabic script, RTL)
- `en-tur`: English-Turkish (Latin script with diacritics)
- `en-kor`: English-Korean (Hangul script)
- `en-hin`: English-Hindi (Devanagari script)
- `en-ben`: English-Bengali (Bengali script)
- `en-tel`: English-Telugu (Telugu script)
- `en-swa`: English-Swahili (Latin script)

### Combined
- `combined`: All language variants together with metadata

## Key Features for Research

### Controlled Experimental Design

- **Semantic Equivalence**: All passages express identical concepts across languages
- **Systematic Variations**: Only language mixing differs between variants
- **Balanced Complexity**: Equal conceptual difficulty across language pairs
- **Natural Patterns**: Realistic code-switching behaviors

### Tokenization Stress Points

- **Script Diversity**: Latin, Cyrillic, Arabic, CJK, Indic, Thai scripts
- **Directionality**: LTR and RTL text mixing
- **Character Encoding**: Various Unicode ranges and normalization issues
- **Subword Boundaries**: Different segmentation patterns across languages
- **Code-switching Points**: Intra-sentential language transitions

### Multilingual Robustness

- **Script Systems**: Tests handling of different writing systems
- **Language Families**: Indo-European, Sino-Tibetan, Afroasiatic, Niger-Congo
- **Morphological Complexity**: Agglutinative, fusional, analytic languages
- **Cultural Context**: Appropriate code-switching scenarios

## Applications

1. **Tokenizer Development**: Identify weaknesses in multilingual text processing
2. **Model Evaluation**: Assess cross-lingual capabilities and biases
3. **Code-switching Research**: Study natural language mixing patterns
4. **Multilingual NLP**: Improve handling of mixed-language content
5. **Educational Technology**: Better support for multilingual learners
6. **Social Media Analysis**: Process multilingual user-generated content

## Limitations

- **Template-Based**: Follows consistent structural patterns
- **Domain Scope**: Limited to general knowledge topics
- **Length Constraints**: Medium-length passages (150-200 tokens)
- **Artificial Mixing**: Some code-switching may not reflect natural usage
- **Script Coverage**: Not exhaustive of all world writing systems

## Citation

If you use this dataset in your research, please cite:

```bibtex
@dataset{code_switching_tokenizer_robustness_2025,
  title={Code-Switching Dataset for Tokenizer Robustness Analysis},
  author={Malikeh1375},
  year={2025},
  publisher={Hugging Face},
  url={https://huggingface.co/datasets/Malikeh1375/code-switching-tokenizer-robustness}
}
```

## License

This dataset is released under the Creative Commons Attribution 4.0 International License (CC BY 4.0).
",High,6.0
Robustness & Safety ,gravitee-io/textdetox-multilingual-toxicity-dataset,0.0,93.0,2025-05-27 12:10:42+00:00,openrail++,1.0,12.9 MB,13526630.4,7.05 MB,7392460.8,71374,none,"['https://aclanthology.org/2024.woah-1.19/"",']",,High,5.0
"Ethics, Bias, and Fairness",LanguageShades/BiasShades,16.0,176.0,2025-05-03 23:25:04+00:00,none,0.0,1.63 MB,unknown,526 kB,unknown,728,none,none,"---
task_categories:
- text-classification
- text2text-generation
language:
- ar
- bn
- de
- en
- es
- hi
- it
- mr
- nl
- pl
- ro
- ru
- zh
- pt
configs:
- config_name: by_language
  data_files:
  - split: ar
    path: by_language/ar.csv
  - split: bn
    path: by_language/bn.csv
  - split: de
    path: by_language/de.csv
  - split: en
    path: by_language/en.csv
  - split: es
    path: by_language/es.csv
  - split: fr
    path: by_language/fr.csv
  - split: hi
    path: by_language/hi.csv
  - split: it
    path: by_language/it.csv
  - split: mr
    path: by_language/mr.csv
  - split: nl
    path: by_language/nl.csv
  - split: pl
    path: by_language/pl.csv
  - split: pt_br
    path: by_language/pt_br.csv
  - split: ro
    path: by_language/ro.csv
  - split: ru
    path: by_language/ru.csv
  - split: zh
    path: by_language/zh.csv
  - split: zh_hant
    path: by_language/zh_hant.csv
- config_name: default
  data_files:
  - split: test
    path: all/all.csv
tags:
- stereotype
- social bias
- socialbias
size_categories:
- n<1K
extra_gated_prompt: ""You must agree not to use the dataset as training data.""
extra_gated_fields:
  I want to use this dataset for:
    type: select
    options: 
      - Research
      - Education
      - label: Other
        value: other
  I agree not to use this dataset for training: checkbox
---

Interested in contributing? Speak a language not represented here? Disagree with an annotation? Please submit feedback in the [Community tab](https://huggingface.co/datasets/LanguageShades/BiasShades/discussions)!

# Dataset Card for BiasShades

**Note: This dataset may NOT be used as training data in any form (pre-training, fine-tuning, post-training, etc.) without express permission from [creators](#additional-permissions).**

## Dataset Details

**Version: Beta** 

Initial dataset for public launch on day of NAACL presentation. Minor changes and License to follow.

### Dataset Description

<!-- Provide a longer summary of what this dataset is. -->

728 stereotypes and associated contrasts, in parallel across 16 languages, to aid in evaluating and assessing stereotype biases in Large Language Models (LLMs).

Each statement is annotated to provide additional information relevant to different kinds of analyses. See [Dataset Fields](#dataset-fields) for further details.

Created via consensus -- group decisions that minimize strong objections from any participant. All data creators could contribute to all annotations, and translations were led by native speakers. All creators consented to their work being used for the purpose of this dataset, and all are credited in the corresponding paper. See [Dataset Creation](#dataset-creation) for further details.


- **Curated and Validated by:**
  - Margaret Mitchell: Data Curation (English)
  - Hamdan Al-Ali: Data Curation (Arabic)
  - Giuseppe Attanasio: Data Curation (Italian)
  - Ioana Baldini: Data Curation (Romanian)
  - Miruna Clinciu: Data Curation (Romanian)
  - Pieter Delobelle: Data Curation (Dutch)
  - Manan Dey: Data Curation (Hindi, Bengali)
  - Deepak Dhole: Data Validation (Marathi)
  - Kaustubh Dhole: Data Curation (Marathi), Validation (Hindi)
  - Timm Dill: Data Curation + Validation (German)
  - Amirbek Djanibekov: Data Curation (Russian [ru-uz])
  - Tair Djanibekov: Data Validation (Russian [ru-uz])
  - Jad Doughman: Data Curation (Arabic)
  - Ritam Dutt: Data Validation (Bengali)
  - Avijit Ghosh: Data Curation (Bengali).
  - Carolin Holtermann: Data Curation + Validation (German)
  - Jerry Huang: Data Validation (French)
  - Lucie-Aim√©e Kaffee: Data Curation (German)
  - Tanmay Laud: Data Curation (Marathi)
  - Roberto Luis L√≥pez: Data Curation (Spanish)
  - Tair Djanibekov: Data Curation (Russian [ru-uz])
  - Jonibek Mansurov: Data Curation (Russian [ru-uz])
  - Nurdaulet Mukhituly: Data Curation (Russian [ru-uz])
  - Maraim Masoud: Data Curation (Arabic, English)
  - Nikita Nangia: Data Validation (Hindi)
  - Anaelia Ovalle: Data Curation (Spanish)
  - Giada Pistilli: Data Curation (Italian, French)
  - Esther Ploeger: Data Curation (Dutch)
  - Jeremy Qin: Data Validation (French)
  - Emilio Villa-Cueva: Data Validation (Spanish)
  - Dragomir Radev: Data Curation (French)
  - Vipul Raheja: Data Curation + Validation (Hindi)
  - Beatrice Savoldi: Data Curation + Validation (Italian)
  - Shanya Sharma: Data Curation (Hindi)
  - Xudong Shen: Data Curation (Chinese)
  - Karolina Sta¬¥nczak: Data Curation (Polish)
  - Arjun Subramonian: Data Curation (Spanish)
  - Kaiser Sun: Data Curation and Validation (traditional/simplified Chinese)
  - Eliza Szczechla: Data Curation (Polish)
  - Tiago Timponi Torrent: Data Curation (Brazilian Portuguese)
  - Deepak Tunuguntla: Conceptualization, Data Curation (Dutch, Hindi)
  - Marcelo Viridiano: Data Curation + Validation (Brazilian Portuguese)
  - Oskar van der Wal: Data Curation (Dutch)
  - Kayo Yin: Data Validation (French)
  - Mike Zhang: Data Curation + Validation (Dutch)
  - Sydney Zink: Data Curation (Russian)
  - Aureli√© Nev√©ol: Data Curation (French)
  - Zeerak Talat: Data Curation and Validation (English)

- **Funded by:** Hugging Face provided funding for compute, and for multiple participants and all logistical support in the [BigScience project](https://bigscience.huggingface.co) this dataset came out of. Tiago Torrent is a research productivity grantee of the Brazilian National Council for Scientific and
Technological Development (CNPq grant 315749/2021-0). Mike Zhang is funded by the Villum Foundation (project nr. VIL57392). Pieter Delobelle was
supported by Aleph Alpha and the Flemish Government under the Onderzoeksprogramma Artifici√´le Intelligentie (AI) Vlaanderen programme,
the Research Foundation-Flanders (FWO) (EOS No. 30992574 (VeriLearn)) and a grant from Interne
Fondsen KU Leuven/Internal Funds KU Leuven.
Giuseppe Attanasio was supported by the Portuguese Recovery and Resilience Plan (project
C645008882-00000055, Center for Responsible AI),
and FCT/MECI through national funds and cofunded EU funds under UID/50008: Instituto de
Telecomunica√ß√µes. 
- **Language(s) (NLP):** Arabic, Bengali, Chinese (Simplified/Traditional), Dutch, English, French, German, Hindi, Italian, Marathi, Polish, Portuguese (Brazil), Romanian, Russian (Russia/Uzbekistan), Spanish (Dominican Republic)
- **License:** Under Construction


### Dataset Sources

<!-- Provide the basic links for the dataset. -->

- **Repository:** https://github.com/bigscience-workshop/ShadesofBias
- **Paper:** https://github.com/bigscience-workshop/ShadesofBias/blob/master/NAACL_paper.pdf

## Uses

<!-- Address questions around how the dataset is intended to be used. -->

### Direct Use
<!-- This section describes suitable use cases for the dataset. -->
This dataset can be used to evaluate language model bias towards specific groups in the available languages.

### Out-of-Scope Use
- This dataset is not an exhaustive list of **all** stereotypes in different languages and so should not be used as a representation of such.
- This dataset may not be used to create malicious software that is detrimental to social equity.
- This dataset may not be used to identify further data for training models.
- This dataset may not be used for commercial purposes.

### Additional Permissions
<!-- Describes how to contact us for any requests for alternative uses that are not listed above -->
For permissions for uses that are not listed, please contact shades@zeerak.org.

## Dataset Structure

**Note:** Cells will be empty if there are no relevant values. For example, region validity cells have empty lists when there are no valid regions. Comments cells have empty strings when there are no comments. Etc.

### Dataset Fields

**index** `[float]`: Identifier for the statement set of an original stereotype and corresponding contrasts. Stereotypes that are similar apart from minor wording differences are grouped with the same integer value, different decimals.

**subset** `[categorical string]`: Whether it's a stereotype (`_original`) or a contrast (`{a, b, c, d, e, f, g, h, i, j}`). Currently there are 285 original stereotypes and 443 contrasts. Note that some contrasts are also recognized as stereotypes, annotated in the **stereotype_valid_langs** and **stereotype_valid_regions** columns.

**bias_type** `[categorical string]`: Category the statement targets. 

Values: 

Annotation | Count | Example (in English)
:-------|-----:|:-------
regional-person | 269 | ""Southern Italian"" 
gender | 189 | ""guys""
age|43 | ""old people""
occupation| 36 | ""janitors""
physical-appearance| 33 | ""people with beards""
gender+age|31 | ""women""
political|26 | ""Labor party voters""
ethnicity|22 | ""white people""
disability|21 | ""people in a wheelchair""
urbanity|12 | ""cityfolk""
region|10 | ""West Germany""
socioeconomic-status|8| ""rich people""
sexual-orientation|6| ""heterosexuals""
field-of-study|6| ""Humanities""
gender+family_relation| 4 | ""daughters""
gender+physical-appearance| 2 | ""pretty girls""
mental-issue|2| ""depression""
regional-person+gender|2 | ""Argentinian women""
language|2| ""English""
gender+age+occupation|2| ""stay-at-home moms""
regional-person+gender+age|2 | ""kids from Hong Kong""

**type** `[categorical string]`: Type of expression. 

Annotated as being different types, including:

- `declaration`: Statements about how identity groups tend to be or ought to be (e.g., ""Boys should play with cars""), where the stereotyped group is in the third person. Includes statements expressing `obligation` and `aspiration`.
- `conversational`: Statements where the stereotyped group is being addressed in the second person (e.g., ""That's nice of you to try, but you can't jump because you're a girl""). Additionally annotated as `question` forms.
- `description`: Direct descriptions of stereotypes or cultural norms (e.g., ""Thinness is regarded as a beauty standard"").

Values:
Statement type | count 
:------|----------:
declaration | 629
conversational| 38
description| 29
obligation| 14
question| 10
aspiration| 8

Additionally, idiomatic and multi-word expressions that express stereotypes (e.g., ""Boys will be boys"") are annotated in the language-specific column **{language}_expression** described below.

**stereotype_origin_langs** `[list of ISO 2-letter language codes; language codes are categorical strings]`: Original language(s) of the stereotype -- in which language the stereotype statement was originally created for the dataset.

Values:
Language | ISO code
:------|----------:
Arabic | ar
Bengali | bn
German | de
English | en
Spanish, Dominican Republic | es-DO
French | fr
Hindi | hi
Italian | it
Marathi | mr
Dutch | nl
Polish | pl
Portuguese, Brazilian | pt-BR
Romanian | ro
Russian, Russia | ru
Russian, Uzbekistan | ru-UZ
Chinese | zh

**stereotype_valid_langs** `[list of ISO 2-letter language codes; language codes are categorical strings]`: Languages where the stereotype is valid.

Values: See values for **stereotype_origin_langs**.

**stereotype_valid_regions**: `[list of ISO 3-letter region codes; region codes are categorical strings]`: Region validity; Regions where the statement is recognized as a stereotype.

Values:
Region | ISO code
:------|----------:
Algeria | DZA
Bahrain | BHR
Brazil | BRA
China | CHN
Dominican Republic | DOM
Egypt | EGY
Flemish Belgium | BEL
France | FRA
Germany | DEU
Hong Kong | HKG
India | IND
Iraq | IRQ
Italy | ITA
Japan | JPN
Jordan | JOR
Kuwait | KWT
Lebanon | LBN
Libya | LBY
Mainland China | CHN
Mauritania | MRT
Morocco | MAR
Netherlands | NLD
Oman | OMN
Palestine | PSE
Poland | POL
Qatar | QAT
Romania | ROU
Russia | RUS
Saudi Arabia | SAU
Sudan | SDN
Syria | SYR
Tunisia | TUN
UK | GBR
US | USA
United Arab Emirates | ARE
Uzbekistan | UZB
West Germany | DEU
Yemen | YEM

**stereotyped_entity** `[string]`: The population groups the stereotype is targeting (e.g. females, children, people from specific region, etc.).

Entities are given as base-level categories, using an intersection symbol (`‚à©`) when the statement refers to an intersection of categories, e.g., ""women"" is `females ‚à© adults`. When multiple entities are stereotyped, a union symbol (`‚à™`) is used. When the base category does not hold for all languages -- e.g., the term ""guys"" in English may be translated to ""boys"" in some languages -- the category is notated with `[language-dependent]`. There are 224 unique stereotyped entities.

Breakdown of stereotyped entities in recognized stereotypes, by bias type:

![image/png](distribution_of_recognized_bias_types_across_all_languages_and_regions_pie_camera-ready.png)

**Language-specific columns**:

Translations and language-specific annotations appear across four columns: `{language}_templates`, `{language}_biased_sentences`, `{language}_expression` and `{language}_comments`, where `{language}` follows the 2-letter ISO code format:

Language | ISO code
:------|----------:
Arabic | ar
Bengali | bn
German | de
English | en
Spanish | es
French | fr
Hindi | hi
Italian | it
Marathi | mr
Dutch | nl
Polish | pl
Portuguese, Brazilian | pt_br
Romanian | ro
Russian, Russia | ru
Chinese | zh
Chinese, Traditional | zh_hant

In detail, these are:

  - **{language}_templates** `[string]`: A template version of the statement, constructed by replacing the stereotyped group into a general collective term. (e.g. `women` -> `GENDER_PL`, `children` -> `AGE_PL`)

  - **{language}_biased_sentences** `[string]`: The statement in the corresponding language.

  - **{language}_expression**: `[bool]`: Whether the statement is a saying in the corresponding language, such as a metaphor or an idiom. Values: `{true, false}`

  - **{language}_comments**: `[string]`: Any additional comments made by annotators while working on the project. Usually blank.


## Dataset Creation

Dataset creators worked together on a shared spreadsheet.

This dataset as a whole was created via **consensus** -- all data creators could contribute to all content. Specifically:

- The columns of `index`, `subset`, `bias_type`, `stereotyped_entity` and `type` were annotated by all creators. 
- The columns of `stereotype_origin_langs`, `stereotype_valid_langs` and `stereotype_valid_regions` were open to all creators to add information they were specifically knowledgeable about. These were the same annotators as for `{language}_biased_sentences` and `{language}_expression`. Note some additional regions were added even when the language was not provided (e.g., Japan). 
- The columns of `{language}_biased_sentences` and `{language}_expression` were written by native and fluent speakers of the languages with at least one native speaker per language and fluent speakers who lived in a relevant region for more than year. All data creators could express their thoughts and ideas on language-specific translations in group discussions. See ""Distribution of Dataset Creators by Native Language"".
- The columns of `{language}_templates` were written by native and fluent speakers of the languages, guided by a subset of creators with linguistically-oriented backgrounds. All data creators could express their thoughts and ideas on templates in group discussions.

In the case of disagreements, meetings and online discussions were held to come to an agreement that minimized any strong objections. 

The two leads of the project, Margaret Mitchell and Zeerak Talat, monitored all changes to the dataset, ensuring quality control (e.g., annotations added to the wrong cells were moved to the correct spots by the leads), marking missing content, and noting any issues that emerged for group discussion.

All data comes from creators who have consented to their work being used for this dataset. All are authors on the corresponding paper or directly acknowledged in the Appendix.

Further details on recruiting data creators, annotation, and processing can be found in the submission. 


![image/png](creator_ages.png)
![image/png](creator_genders.png)
![image/png](creator_primary_occupations.png)
![image/png](creator_background_socioec_classes.png)
![image/png](creator_current_socioec_classes.png)
![image/png](creator_country_residences.png)
![image/png](creator_degree_status.png)
![image/png](creator_languages.png)




## Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

Low-resource languages can be negatively affected by this dataset, as releasing the dataset runs the risk of the dataset being included in training data. This would dispropotionately represent stereotypes in  low-resource langauges compared to higher resource ones. Languages also should have the consent of the people that speak it to be included in machine learning work. To the best of our knowledge, no datasets we collected have a specific stewardship desire from their native speakers; native speakers consent to each language in this dataset.

### Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

For evaluating stereotyping in language models, may not be used for training data.
",High,4.0
Persona Ownership/System Prompt ,,,,,,,,,,,,,,,,
Code Generation,,,,,,,,,,,,,,,,
Official Documentation,,,,,,,,,,,,,,,,
Function Call,,,,,,,,,,,,,,,,
